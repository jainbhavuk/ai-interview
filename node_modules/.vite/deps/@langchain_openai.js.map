{
  "version": 3,
  "sources": ["../../.pnpm/base64-js@1.5.1/node_modules/base64-js/index.js", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/utils/errors.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/internal/tslib.mjs", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/uuid.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/errors.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/core/error.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/values.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/sleep.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/version.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/detect-platform.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/shims.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/request-options.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/qs/formats.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/qs/utils.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/qs/stringify.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/bytes.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/decoders/line.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/log.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/core/streaming.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/parse.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/core/api-promise.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/core/pagination.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/uploads.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/to-file.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/core/resource.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/path.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/chat/completions/messages.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/parser.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/chatCompletionUtils.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/EventStream.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/RunnableFunction.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/AbstractChatCompletionRunner.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/ChatCompletionRunner.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/partial-json-parser/parser.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/ChatCompletionStream.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/ChatCompletionStreamingRunner.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/chat/completions/completions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/chat/chat.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/headers.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/audio/speech.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/audio/transcriptions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/audio/translations.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/audio/audio.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/batches.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/assistants.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/realtime/sessions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/realtime/transcription-sessions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/realtime/realtime.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/chatkit/sessions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/chatkit/threads.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/chatkit/chatkit.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/threads/messages.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/threads/runs/steps.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/base64.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/internal/utils/env.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/AssistantStream.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/threads/runs/runs.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/threads/threads.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/beta/beta.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/completions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/containers/files/content.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/containers/files/files.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/containers/containers.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/conversations/items.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/conversations/conversations.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/embeddings.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/evals/runs/output-items.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/evals/runs/runs.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/evals/evals.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/files.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/methods.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/alpha/graders.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/alpha/alpha.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/checkpoints/permissions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/checkpoints/checkpoints.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/jobs/checkpoints.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/jobs/jobs.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/fine-tuning/fine-tuning.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/graders/grader-models.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/graders/graders.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/images.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/models.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/moderations.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/realtime/calls.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/realtime/client-secrets.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/realtime/realtime.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/ResponsesParser.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/responses/ResponseStream.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/responses/input-items.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/responses/input-tokens.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/responses/responses.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/skills/content.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/skills/versions/content.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/skills/versions/versions.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/skills/skills.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/uploads/parts.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/uploads/uploads.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/Util.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/vector-stores/file-batches.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/vector-stores/files.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/vector-stores/vector-stores.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/videos.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/resources/webhooks/webhooks.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/client.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/azure.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/utils/client.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/utils/misc.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/utils/azure.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/tools/types.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/utils/function_calling.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/dist/utils/types/index.js", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/utils/tools.ts", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/external.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/schemas.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/checks.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/iso.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/errors.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/parse.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/compat.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/from-json-schema.js", "../../.pnpm/zod@4.3.6/node_modules/zod/v4/classic/coerce.js", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/Options.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/util.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/Refs.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/errorMessages.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/any.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/array.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/bigint.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/boolean.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/branded.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/catch.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/date.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/default.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/effects.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/enum.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/intersection.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/literal.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/string.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/record.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/map.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/nativeEnum.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/never.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/null.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/union.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/nullable.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/number.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/object.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/optional.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/pipeline.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/promise.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/set.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/tuple.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/undefined.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/unknown.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parsers/readonly.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/parseDef.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/_vendor/zod-to-json-schema/zodToJsonSchema.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/lib/transform.ts", "../../.pnpm/openai@6.25.0_ws@8.19.0_zod@4.3.6/node_modules/openai/src/helpers/zod.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/utils/output.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/chat_models/profiles.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/utils/js-sha256/hash.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/dist/utils/hash.js", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/caches/index.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/prompt_values.ts", "../../.pnpm/js-tiktoken@1.0.21/node_modules/js-tiktoken/dist/chunk-VL2OQCWN.js", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/utils/tiktoken.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/language_models/base.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/runnables/passthrough.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/language_models/utils.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/language_models/chat_models.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/runnables/router.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/runnables/branch.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/runnables/history.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/dist/runnables/index.js", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/base.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/transform.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/bytes.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/list.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/string.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/structured.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/dist/utils/json_patch.js", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/json.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/utils/sax-js/sax.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/xml.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/dist/output_parsers/index.js", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/output_parsers/openai_tools/json_output_tools_parsers.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/dist/output_parsers/openai_tools/index.js", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/chat_models/base.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/converters/completions.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/chat_models/completions.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/azure/chat_models/common.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/azure/chat_models/completions.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/converters/responses.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/chat_models/responses.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/azure/chat_models/responses.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/chat_models/index.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/azure/chat_models/index.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/language_models/llms.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/utils/chunk_array.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/llms.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/azure/llms.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/embeddings.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/embeddings.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/azure/embeddings.ts", "../../.pnpm/@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6_/node_modules/@langchain/core/src/tools/index.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/dalle.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/webSearch.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/mcp.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/codeInterpreter.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/fileSearch.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/imageGeneration.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/computerUse.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/localShell.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/shell.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/applyPatch.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/index.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/tools/custom.ts", "../../.pnpm/@langchain+openai@1.2.10_@langchain+core@1.1.28_openai@6.25.0_ws@8.19.0_zod@4.3.6___ws@8.19.0/node_modules/@langchain/openai/src/utils/prompts.ts"],
  "sourcesContent": ["'use strict'\n\nexports.byteLength = byteLength\nexports.toByteArray = toByteArray\nexports.fromByteArray = fromByteArray\n\nvar lookup = []\nvar revLookup = []\nvar Arr = typeof Uint8Array !== 'undefined' ? Uint8Array : Array\n\nvar code = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/'\nfor (var i = 0, len = code.length; i < len; ++i) {\n  lookup[i] = code[i]\n  revLookup[code.charCodeAt(i)] = i\n}\n\n// Support decoding URL-safe base64 strings, as Node.js does.\n// See: https://en.wikipedia.org/wiki/Base64#URL_applications\nrevLookup['-'.charCodeAt(0)] = 62\nrevLookup['_'.charCodeAt(0)] = 63\n\nfunction getLens (b64) {\n  var len = b64.length\n\n  if (len % 4 > 0) {\n    throw new Error('Invalid string. Length must be a multiple of 4')\n  }\n\n  // Trim off extra bytes after placeholder bytes are found\n  // See: https://github.com/beatgammit/base64-js/issues/42\n  var validLen = b64.indexOf('=')\n  if (validLen === -1) validLen = len\n\n  var placeHoldersLen = validLen === len\n    ? 0\n    : 4 - (validLen % 4)\n\n  return [validLen, placeHoldersLen]\n}\n\n// base64 is 4/3 + up to two characters of the original data\nfunction byteLength (b64) {\n  var lens = getLens(b64)\n  var validLen = lens[0]\n  var placeHoldersLen = lens[1]\n  return ((validLen + placeHoldersLen) * 3 / 4) - placeHoldersLen\n}\n\nfunction _byteLength (b64, validLen, placeHoldersLen) {\n  return ((validLen + placeHoldersLen) * 3 / 4) - placeHoldersLen\n}\n\nfunction toByteArray (b64) {\n  var tmp\n  var lens = getLens(b64)\n  var validLen = lens[0]\n  var placeHoldersLen = lens[1]\n\n  var arr = new Arr(_byteLength(b64, validLen, placeHoldersLen))\n\n  var curByte = 0\n\n  // if there are placeholders, only get up to the last complete 4 chars\n  var len = placeHoldersLen > 0\n    ? validLen - 4\n    : validLen\n\n  var i\n  for (i = 0; i < len; i += 4) {\n    tmp =\n      (revLookup[b64.charCodeAt(i)] << 18) |\n      (revLookup[b64.charCodeAt(i + 1)] << 12) |\n      (revLookup[b64.charCodeAt(i + 2)] << 6) |\n      revLookup[b64.charCodeAt(i + 3)]\n    arr[curByte++] = (tmp >> 16) & 0xFF\n    arr[curByte++] = (tmp >> 8) & 0xFF\n    arr[curByte++] = tmp & 0xFF\n  }\n\n  if (placeHoldersLen === 2) {\n    tmp =\n      (revLookup[b64.charCodeAt(i)] << 2) |\n      (revLookup[b64.charCodeAt(i + 1)] >> 4)\n    arr[curByte++] = tmp & 0xFF\n  }\n\n  if (placeHoldersLen === 1) {\n    tmp =\n      (revLookup[b64.charCodeAt(i)] << 10) |\n      (revLookup[b64.charCodeAt(i + 1)] << 4) |\n      (revLookup[b64.charCodeAt(i + 2)] >> 2)\n    arr[curByte++] = (tmp >> 8) & 0xFF\n    arr[curByte++] = tmp & 0xFF\n  }\n\n  return arr\n}\n\nfunction tripletToBase64 (num) {\n  return lookup[num >> 18 & 0x3F] +\n    lookup[num >> 12 & 0x3F] +\n    lookup[num >> 6 & 0x3F] +\n    lookup[num & 0x3F]\n}\n\nfunction encodeChunk (uint8, start, end) {\n  var tmp\n  var output = []\n  for (var i = start; i < end; i += 3) {\n    tmp =\n      ((uint8[i] << 16) & 0xFF0000) +\n      ((uint8[i + 1] << 8) & 0xFF00) +\n      (uint8[i + 2] & 0xFF)\n    output.push(tripletToBase64(tmp))\n  }\n  return output.join('')\n}\n\nfunction fromByteArray (uint8) {\n  var tmp\n  var len = uint8.length\n  var extraBytes = len % 3 // if we have 1 byte left, pad 2 bytes\n  var parts = []\n  var maxChunkLength = 16383 // must be multiple of 3\n\n  // go through the array every three bytes, we'll deal with trailing stuff later\n  for (var i = 0, len2 = len - extraBytes; i < len2; i += maxChunkLength) {\n    parts.push(encodeChunk(uint8, i, (i + maxChunkLength) > len2 ? len2 : (i + maxChunkLength)))\n  }\n\n  // pad the end with zeros, but make sure to not forget the extra bytes\n  if (extraBytes === 1) {\n    tmp = uint8[len - 1]\n    parts.push(\n      lookup[tmp >> 2] +\n      lookup[(tmp << 4) & 0x3F] +\n      '=='\n    )\n  } else if (extraBytes === 2) {\n    tmp = (uint8[len - 2] << 8) + uint8[len - 1]\n    parts.push(\n      lookup[tmp >> 10] +\n      lookup[(tmp >> 4) & 0x3F] +\n      lookup[(tmp << 2) & 0x3F] +\n      '='\n    )\n  }\n\n  return parts.join('')\n}\n", "/* eslint-disable @typescript-eslint/no-explicit-any */\n\n// Duplicate of core\n// TODO: Remove once we stop supporting 0.2.x core versions\nexport type LangChainErrorCodes =\n  | \"CONTEXT_OVERFLOW\"\n  | \"INVALID_PROMPT_INPUT\"\n  | \"INVALID_TOOL_RESULTS\"\n  | \"MESSAGE_COERCION_FAILURE\"\n  | \"MODEL_AUTHENTICATION\"\n  | \"MODEL_NOT_FOUND\"\n  | \"MODEL_RATE_LIMIT\"\n  | \"OUTPUT_PARSING_FAILURE\";\n\nexport function addLangChainErrorFields(\n  error: any,\n  lc_error_code: LangChainErrorCodes\n) {\n  (error as any).lc_error_code = lc_error_code;\n  error.message = `${error.message}\\n\\nTroubleshooting URL: https://docs.langchain.com/oss/javascript/langchain/errors/${lc_error_code}/\\n`;\n  return error;\n}\n", "function __classPrivateFieldSet(receiver, state, value, kind, f) {\n    if (kind === \"m\")\n        throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f)\n        throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver))\n        throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return kind === \"a\" ? f.call(receiver, value) : f ? (f.value = value) : state.set(receiver, value), value;\n}\nfunction __classPrivateFieldGet(receiver, state, kind, f) {\n    if (kind === \"a\" && !f)\n        throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver))\n        throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n}\nexport { __classPrivateFieldSet, __classPrivateFieldGet };\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n/**\n * https://stackoverflow.com/a/2117523\n */\nexport let uuid4 = function () {\n  const { crypto } = globalThis as any;\n  if (crypto?.randomUUID) {\n    uuid4 = crypto.randomUUID.bind(crypto);\n    return crypto.randomUUID();\n  }\n  const u8 = new Uint8Array(1);\n  const randomByte = crypto ? () => crypto.getRandomValues(u8)[0]! : () => (Math.random() * 0xff) & 0xff;\n  return '10000000-1000-4000-8000-100000000000'.replace(/[018]/g, (c) =>\n    (+c ^ (randomByte() & (15 >> (+c / 4)))).toString(16),\n  );\n};\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nexport function isAbortError(err: unknown) {\n  return (\n    typeof err === 'object' &&\n    err !== null &&\n    // Spec-compliant fetch implementations\n    (('name' in err && (err as any).name === 'AbortError') ||\n      // Expo fetch\n      ('message' in err && String((err as any).message).includes('FetchRequestCanceledException')))\n  );\n}\n\nexport const castToError = (err: any): Error => {\n  if (err instanceof Error) return err;\n  if (typeof err === 'object' && err !== null) {\n    try {\n      if (Object.prototype.toString.call(err) === '[object Error]') {\n        // @ts-ignore - not all envs have native support for cause yet\n        const error = new Error(err.message, err.cause ? { cause: err.cause } : {});\n        if (err.stack) error.stack = err.stack;\n        // @ts-ignore - not all envs have native support for cause yet\n        if (err.cause && !error.cause) error.cause = err.cause;\n        if (err.name) error.name = err.name;\n        return error;\n      }\n    } catch {}\n    try {\n      return new Error(JSON.stringify(err));\n    } catch {}\n  }\n  return new Error(err);\n};\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { castToError } from '../internal/errors';\n\nexport class OpenAIError extends Error {}\n\nexport class APIError<\n  TStatus extends number | undefined = number | undefined,\n  THeaders extends Headers | undefined = Headers | undefined,\n  TError extends Object | undefined = Object | undefined,\n> extends OpenAIError {\n  /** HTTP status for the response that caused the error */\n  readonly status: TStatus;\n  /** HTTP headers for the response that caused the error */\n  readonly headers: THeaders;\n  /** JSON body of the response that caused the error */\n  readonly error: TError;\n\n  readonly code: string | null | undefined;\n  readonly param: string | null | undefined;\n  readonly type: string | undefined;\n\n  readonly requestID: string | null | undefined;\n\n  constructor(status: TStatus, error: TError, message: string | undefined, headers: THeaders) {\n    super(`${APIError.makeMessage(status, error, message)}`);\n    this.status = status;\n    this.headers = headers;\n    this.requestID = headers?.get('x-request-id');\n    this.error = error;\n\n    const data = error as Record<string, any>;\n    this.code = data?.['code'];\n    this.param = data?.['param'];\n    this.type = data?.['type'];\n  }\n\n  private static makeMessage(status: number | undefined, error: any, message: string | undefined) {\n    const msg =\n      error?.message ?\n        typeof error.message === 'string' ?\n          error.message\n        : JSON.stringify(error.message)\n      : error ? JSON.stringify(error)\n      : message;\n\n    if (status && msg) {\n      return `${status} ${msg}`;\n    }\n    if (status) {\n      return `${status} status code (no body)`;\n    }\n    if (msg) {\n      return msg;\n    }\n    return '(no status code or body)';\n  }\n\n  static generate(\n    status: number | undefined,\n    errorResponse: Object | undefined,\n    message: string | undefined,\n    headers: Headers | undefined,\n  ): APIError {\n    if (!status || !headers) {\n      return new APIConnectionError({ message, cause: castToError(errorResponse) });\n    }\n\n    const error = (errorResponse as Record<string, any>)?.['error'];\n\n    if (status === 400) {\n      return new BadRequestError(status, error, message, headers);\n    }\n\n    if (status === 401) {\n      return new AuthenticationError(status, error, message, headers);\n    }\n\n    if (status === 403) {\n      return new PermissionDeniedError(status, error, message, headers);\n    }\n\n    if (status === 404) {\n      return new NotFoundError(status, error, message, headers);\n    }\n\n    if (status === 409) {\n      return new ConflictError(status, error, message, headers);\n    }\n\n    if (status === 422) {\n      return new UnprocessableEntityError(status, error, message, headers);\n    }\n\n    if (status === 429) {\n      return new RateLimitError(status, error, message, headers);\n    }\n\n    if (status >= 500) {\n      return new InternalServerError(status, error, message, headers);\n    }\n\n    return new APIError(status, error, message, headers);\n  }\n}\n\nexport class APIUserAbortError extends APIError<undefined, undefined, undefined> {\n  constructor({ message }: { message?: string } = {}) {\n    super(undefined, undefined, message || 'Request was aborted.', undefined);\n  }\n}\n\nexport class APIConnectionError extends APIError<undefined, undefined, undefined> {\n  constructor({ message, cause }: { message?: string | undefined; cause?: Error | undefined }) {\n    super(undefined, undefined, message || 'Connection error.', undefined);\n    // in some environments the 'cause' property is already declared\n    // @ts-ignore\n    if (cause) this.cause = cause;\n  }\n}\n\nexport class APIConnectionTimeoutError extends APIConnectionError {\n  constructor({ message }: { message?: string } = {}) {\n    super({ message: message ?? 'Request timed out.' });\n  }\n}\n\nexport class BadRequestError extends APIError<400, Headers> {}\n\nexport class AuthenticationError extends APIError<401, Headers> {}\n\nexport class PermissionDeniedError extends APIError<403, Headers> {}\n\nexport class NotFoundError extends APIError<404, Headers> {}\n\nexport class ConflictError extends APIError<409, Headers> {}\n\nexport class UnprocessableEntityError extends APIError<422, Headers> {}\n\nexport class RateLimitError extends APIError<429, Headers> {}\n\nexport class InternalServerError extends APIError<number, Headers> {}\n\nexport class LengthFinishReasonError extends OpenAIError {\n  constructor() {\n    super(`Could not parse response content as the length limit was reached`);\n  }\n}\n\nexport class ContentFilterFinishReasonError extends OpenAIError {\n  constructor() {\n    super(`Could not parse response content as the request was rejected by the content filter`);\n  }\n}\n\nexport class InvalidWebhookSignatureError extends Error {\n  constructor(message: string) {\n    super(message);\n  }\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { OpenAIError } from '../../core/error';\n\n// https://url.spec.whatwg.org/#url-scheme-string\nconst startsWithSchemeRegexp = /^[a-z][a-z0-9+.-]*:/i;\n\nexport const isAbsoluteURL = (url: string): boolean => {\n  return startsWithSchemeRegexp.test(url);\n};\n\nexport let isArray = (val: unknown): val is unknown[] => ((isArray = Array.isArray), isArray(val));\nexport let isReadonlyArray = isArray as (val: unknown) => val is readonly unknown[];\n\n/** Returns an object if the given value isn't an object, otherwise returns as-is */\nexport function maybeObj(x: unknown): object {\n  if (typeof x !== 'object') {\n    return {};\n  }\n\n  return x ?? {};\n}\n\n// https://stackoverflow.com/a/34491287\nexport function isEmptyObj(obj: Object | null | undefined): boolean {\n  if (!obj) return true;\n  for (const _k in obj) return false;\n  return true;\n}\n\n// https://eslint.org/docs/latest/rules/no-prototype-builtins\nexport function hasOwn<T extends object = object>(obj: T, key: PropertyKey): key is keyof T {\n  return Object.prototype.hasOwnProperty.call(obj, key);\n}\n\nexport function isObj(obj: unknown): obj is Record<string, unknown> {\n  return obj != null && typeof obj === 'object' && !Array.isArray(obj);\n}\n\nexport const ensurePresent = <T>(value: T | null | undefined): T => {\n  if (value == null) {\n    throw new OpenAIError(`Expected a value to be given but received ${value} instead.`);\n  }\n\n  return value;\n};\n\nexport const validatePositiveInteger = (name: string, n: unknown): number => {\n  if (typeof n !== 'number' || !Number.isInteger(n)) {\n    throw new OpenAIError(`${name} must be an integer`);\n  }\n  if (n < 0) {\n    throw new OpenAIError(`${name} must be a positive integer`);\n  }\n  return n;\n};\n\nexport const coerceInteger = (value: unknown): number => {\n  if (typeof value === 'number') return Math.round(value);\n  if (typeof value === 'string') return parseInt(value, 10);\n\n  throw new OpenAIError(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\n\nexport const coerceFloat = (value: unknown): number => {\n  if (typeof value === 'number') return value;\n  if (typeof value === 'string') return parseFloat(value);\n\n  throw new OpenAIError(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\n\nexport const coerceBoolean = (value: unknown): boolean => {\n  if (typeof value === 'boolean') return value;\n  if (typeof value === 'string') return value === 'true';\n  return Boolean(value);\n};\n\nexport const maybeCoerceInteger = (value: unknown): number | undefined => {\n  if (value == null) {\n    return undefined;\n  }\n  return coerceInteger(value);\n};\n\nexport const maybeCoerceFloat = (value: unknown): number | undefined => {\n  if (value == null) {\n    return undefined;\n  }\n  return coerceFloat(value);\n};\n\nexport const maybeCoerceBoolean = (value: unknown): boolean | undefined => {\n  if (value == null) {\n    return undefined;\n  }\n  return coerceBoolean(value);\n};\n\nexport const safeJSON = (text: string) => {\n  try {\n    return JSON.parse(text);\n  } catch (err) {\n    return undefined;\n  }\n};\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nexport const sleep = (ms: number) => new Promise<void>((resolve) => setTimeout(resolve, ms));\n", "export const VERSION = '6.25.0'; // x-release-please-version\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { VERSION } from '../version';\n\nexport const isRunningInBrowser = () => {\n  return (\n    // @ts-ignore\n    typeof window !== 'undefined' &&\n    // @ts-ignore\n    typeof window.document !== 'undefined' &&\n    // @ts-ignore\n    typeof navigator !== 'undefined'\n  );\n};\n\ntype DetectedPlatform = 'deno' | 'node' | 'edge' | 'unknown';\n\n/**\n * Note this does not detect 'browser'; for that, use getBrowserInfo().\n */\nfunction getDetectedPlatform(): DetectedPlatform {\n  if (typeof Deno !== 'undefined' && Deno.build != null) {\n    return 'deno';\n  }\n  if (typeof EdgeRuntime !== 'undefined') {\n    return 'edge';\n  }\n  if (\n    Object.prototype.toString.call(\n      typeof (globalThis as any).process !== 'undefined' ? (globalThis as any).process : 0,\n    ) === '[object process]'\n  ) {\n    return 'node';\n  }\n  return 'unknown';\n}\n\ndeclare const Deno: any;\ndeclare const EdgeRuntime: any;\ntype Arch = 'x32' | 'x64' | 'arm' | 'arm64' | `other:${string}` | 'unknown';\ntype PlatformName =\n  | 'MacOS'\n  | 'Linux'\n  | 'Windows'\n  | 'FreeBSD'\n  | 'OpenBSD'\n  | 'iOS'\n  | 'Android'\n  | `Other:${string}`\n  | 'Unknown';\ntype Browser = 'ie' | 'edge' | 'chrome' | 'firefox' | 'safari';\ntype PlatformProperties = {\n  'X-Stainless-Lang': 'js';\n  'X-Stainless-Package-Version': string;\n  'X-Stainless-OS': PlatformName;\n  'X-Stainless-Arch': Arch;\n  'X-Stainless-Runtime': 'node' | 'deno' | 'edge' | `browser:${Browser}` | 'unknown';\n  'X-Stainless-Runtime-Version': string;\n};\nconst getPlatformProperties = (): PlatformProperties => {\n  const detectedPlatform = getDetectedPlatform();\n  if (detectedPlatform === 'deno') {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': normalizePlatform(Deno.build.os),\n      'X-Stainless-Arch': normalizeArch(Deno.build.arch),\n      'X-Stainless-Runtime': 'deno',\n      'X-Stainless-Runtime-Version':\n        typeof Deno.version === 'string' ? Deno.version : Deno.version?.deno ?? 'unknown',\n    };\n  }\n  if (typeof EdgeRuntime !== 'undefined') {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': 'Unknown',\n      'X-Stainless-Arch': `other:${EdgeRuntime}`,\n      'X-Stainless-Runtime': 'edge',\n      'X-Stainless-Runtime-Version': (globalThis as any).process.version,\n    };\n  }\n  // Check if Node.js\n  if (detectedPlatform === 'node') {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': normalizePlatform((globalThis as any).process.platform ?? 'unknown'),\n      'X-Stainless-Arch': normalizeArch((globalThis as any).process.arch ?? 'unknown'),\n      'X-Stainless-Runtime': 'node',\n      'X-Stainless-Runtime-Version': (globalThis as any).process.version ?? 'unknown',\n    };\n  }\n\n  const browserInfo = getBrowserInfo();\n  if (browserInfo) {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': 'Unknown',\n      'X-Stainless-Arch': 'unknown',\n      'X-Stainless-Runtime': `browser:${browserInfo.browser}`,\n      'X-Stainless-Runtime-Version': browserInfo.version,\n    };\n  }\n\n  // TODO add support for Cloudflare workers, etc.\n  return {\n    'X-Stainless-Lang': 'js',\n    'X-Stainless-Package-Version': VERSION,\n    'X-Stainless-OS': 'Unknown',\n    'X-Stainless-Arch': 'unknown',\n    'X-Stainless-Runtime': 'unknown',\n    'X-Stainless-Runtime-Version': 'unknown',\n  };\n};\n\ntype BrowserInfo = {\n  browser: Browser;\n  version: string;\n};\n\ndeclare const navigator: { userAgent: string } | undefined;\n\n// Note: modified from https://github.com/JS-DevTools/host-environment/blob/b1ab79ecde37db5d6e163c050e54fe7d287d7c92/src/isomorphic.browser.ts\nfunction getBrowserInfo(): BrowserInfo | null {\n  if (typeof navigator === 'undefined' || !navigator) {\n    return null;\n  }\n\n  // NOTE: The order matters here!\n  const browserPatterns = [\n    { key: 'edge' as const, pattern: /Edge(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'ie' as const, pattern: /MSIE(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'ie' as const, pattern: /Trident(?:.*rv\\:(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'chrome' as const, pattern: /Chrome(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'firefox' as const, pattern: /Firefox(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'safari' as const, pattern: /(?:Version\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?(?:\\W+Mobile\\S*)?\\W+Safari/ },\n  ];\n\n  // Find the FIRST matching browser\n  for (const { key, pattern } of browserPatterns) {\n    const match = pattern.exec(navigator.userAgent);\n    if (match) {\n      const major = match[1] || 0;\n      const minor = match[2] || 0;\n      const patch = match[3] || 0;\n\n      return { browser: key, version: `${major}.${minor}.${patch}` };\n    }\n  }\n\n  return null;\n}\n\nconst normalizeArch = (arch: string): Arch => {\n  // Node docs:\n  // - https://nodejs.org/api/process.html#processarch\n  // Deno docs:\n  // - https://doc.deno.land/deno/stable/~/Deno.build\n  if (arch === 'x32') return 'x32';\n  if (arch === 'x86_64' || arch === 'x64') return 'x64';\n  if (arch === 'arm') return 'arm';\n  if (arch === 'aarch64' || arch === 'arm64') return 'arm64';\n  if (arch) return `other:${arch}`;\n  return 'unknown';\n};\n\nconst normalizePlatform = (platform: string): PlatformName => {\n  // Node platforms:\n  // - https://nodejs.org/api/process.html#processplatform\n  // Deno platforms:\n  // - https://doc.deno.land/deno/stable/~/Deno.build\n  // - https://github.com/denoland/deno/issues/14799\n\n  platform = platform.toLowerCase();\n\n  // NOTE: this iOS check is untested and may not work\n  // Node does not work natively on IOS, there is a fork at\n  // https://github.com/nodejs-mobile/nodejs-mobile\n  // however it is unknown at the time of writing how to detect if it is running\n  if (platform.includes('ios')) return 'iOS';\n  if (platform === 'android') return 'Android';\n  if (platform === 'darwin') return 'MacOS';\n  if (platform === 'win32') return 'Windows';\n  if (platform === 'freebsd') return 'FreeBSD';\n  if (platform === 'openbsd') return 'OpenBSD';\n  if (platform === 'linux') return 'Linux';\n  if (platform) return `Other:${platform}`;\n  return 'Unknown';\n};\n\nlet _platformHeaders: PlatformProperties;\nexport const getPlatformHeaders = () => {\n  return (_platformHeaders ??= getPlatformProperties());\n};\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n/**\n * This module provides internal shims and utility functions for environments where certain Node.js or global types may not be available.\n *\n * These are used to ensure we can provide a consistent behaviour between different JavaScript environments and good error\n * messages in cases where an environment isn't fully supported.\n */\n\nimport type { Fetch } from './builtin-types';\nimport type { ReadableStream } from './shim-types';\n\nexport function getDefaultFetch(): Fetch {\n  if (typeof fetch !== 'undefined') {\n    return fetch as any;\n  }\n\n  throw new Error(\n    '`fetch` is not defined as a global; Either pass `fetch` to the client, `new OpenAI({ fetch })` or polyfill the global, `globalThis.fetch = fetch`',\n  );\n}\n\ntype ReadableStreamArgs = ConstructorParameters<typeof ReadableStream>;\n\nexport function makeReadableStream(...args: ReadableStreamArgs): ReadableStream {\n  const ReadableStream = (globalThis as any).ReadableStream;\n  if (typeof ReadableStream === 'undefined') {\n    // Note: All of the platforms / runtimes we officially support already define\n    // `ReadableStream` as a global, so this should only ever be hit on unsupported runtimes.\n    throw new Error(\n      '`ReadableStream` is not defined as a global; You will need to polyfill it, `globalThis.ReadableStream = ReadableStream`',\n    );\n  }\n\n  return new ReadableStream(...args);\n}\n\nexport function ReadableStreamFrom<T>(iterable: Iterable<T> | AsyncIterable<T>): ReadableStream<T> {\n  let iter: AsyncIterator<T> | Iterator<T> =\n    Symbol.asyncIterator in iterable ? iterable[Symbol.asyncIterator]() : iterable[Symbol.iterator]();\n\n  return makeReadableStream({\n    start() {},\n    async pull(controller: any) {\n      const { done, value } = await iter.next();\n      if (done) {\n        controller.close();\n      } else {\n        controller.enqueue(value);\n      }\n    },\n    async cancel() {\n      await iter.return?.();\n    },\n  });\n}\n\n/**\n * Most browsers don't yet have async iterable support for ReadableStream,\n * and Node has a very different way of reading bytes from its \"ReadableStream\".\n *\n * This polyfill was pulled from https://github.com/MattiasBuelens/web-streams-polyfill/pull/122#issuecomment-1627354490\n */\nexport function ReadableStreamToAsyncIterable<T>(stream: any): AsyncIterableIterator<T> {\n  if (stream[Symbol.asyncIterator]) return stream;\n\n  const reader = stream.getReader();\n  return {\n    async next() {\n      try {\n        const result = await reader.read();\n        if (result?.done) reader.releaseLock(); // release lock when stream becomes closed\n        return result;\n      } catch (e) {\n        reader.releaseLock(); // release lock when stream becomes errored\n        throw e;\n      }\n    },\n    async return() {\n      const cancelPromise = reader.cancel();\n      reader.releaseLock();\n      await cancelPromise;\n      return { done: true, value: undefined };\n    },\n    [Symbol.asyncIterator]() {\n      return this;\n    },\n  };\n}\n\n/**\n * Cancels a ReadableStream we don't need to consume.\n * See https://undici.nodejs.org/#/?id=garbage-collection\n */\nexport async function CancelReadableStream(stream: any): Promise<void> {\n  if (stream === null || typeof stream !== 'object') return;\n\n  if (stream[Symbol.asyncIterator]) {\n    await stream[Symbol.asyncIterator]().return?.();\n    return;\n  }\n\n  const reader = stream.getReader();\n  const cancelPromise = reader.cancel();\n  reader.releaseLock();\n  await cancelPromise;\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { NullableHeaders } from './headers';\n\nimport type { BodyInit } from './builtin-types';\nimport { Stream } from '../core/streaming';\nimport type { HTTPMethod, MergedRequestInit } from './types';\nimport { type HeadersLike } from './headers';\n\nexport type FinalRequestOptions = RequestOptions & { method: HTTPMethod; path: string };\n\nexport type RequestOptions = {\n  /**\n   * The HTTP method for the request (e.g., 'get', 'post', 'put', 'delete').\n   */\n  method?: HTTPMethod;\n\n  /**\n   * The URL path for the request.\n   *\n   * @example \"/v1/foo\"\n   */\n  path?: string;\n\n  /**\n   * Query parameters to include in the request URL.\n   */\n  query?: object | undefined | null;\n\n  /**\n   * The request body. Can be a string, JSON object, FormData, or other supported types.\n   */\n  body?: unknown;\n\n  /**\n   * HTTP headers to include with the request. Can be a Headers object, plain object, or array of tuples.\n   */\n  headers?: HeadersLike;\n\n  /**\n   * The maximum number of times that the client will retry a request in case of a\n   * temporary failure, like a network error or a 5XX error from the server.\n   *\n   * @default 2\n   */\n  maxRetries?: number;\n\n  stream?: boolean | undefined;\n\n  /**\n   * The maximum amount of time (in milliseconds) that the client should wait for a response\n   * from the server before timing out a single request.\n   *\n   * @unit milliseconds\n   */\n  timeout?: number;\n\n  /**\n   * Additional `RequestInit` options to be passed to the underlying `fetch` call.\n   * These options will be merged with the client's default fetch options.\n   */\n  fetchOptions?: MergedRequestInit;\n\n  /**\n   * An AbortSignal that can be used to cancel the request.\n   */\n  signal?: AbortSignal | undefined | null;\n\n  /**\n   * A unique key for this request to enable idempotency.\n   */\n  idempotencyKey?: string;\n\n  /**\n   * Override the default base URL for this specific request.\n   */\n  defaultBaseURL?: string | undefined;\n\n  __metadata?: Record<string, unknown>;\n  __binaryResponse?: boolean | undefined;\n  __streamClass?: typeof Stream;\n  __synthesizeEventData?: boolean;\n};\n\nexport type EncodedContent = { bodyHeaders: HeadersLike; body: BodyInit };\nexport type RequestEncoder = (request: { headers: NullableHeaders; body: unknown }) => EncodedContent;\n\nexport const FallbackEncoder: RequestEncoder = ({ headers, body }) => {\n  return {\n    bodyHeaders: {\n      'content-type': 'application/json',\n    },\n    body: JSON.stringify(body),\n  };\n};\n", "import type { Format } from './types';\n\nexport const default_format: Format = 'RFC3986';\nexport const default_formatter = (v: PropertyKey) => String(v);\nexport const formatters: Record<Format, (str: PropertyKey) => string> = {\n  RFC1738: (v: PropertyKey) => String(v).replace(/%20/g, '+'),\n  RFC3986: default_formatter,\n};\nexport const RFC1738 = 'RFC1738';\nexport const RFC3986 = 'RFC3986';\n", "import { RFC1738 } from './formats';\nimport type { DefaultEncoder, Format } from './types';\nimport { isArray } from '../utils/values';\n\nexport let has = (obj: object, key: PropertyKey): boolean => (\n  (has = (Object as any).hasOwn ?? Function.prototype.call.bind(Object.prototype.hasOwnProperty)),\n  has(obj, key)\n);\n\nconst hex_table = /* @__PURE__ */ (() => {\n  const array = [];\n  for (let i = 0; i < 256; ++i) {\n    array.push('%' + ((i < 16 ? '0' : '') + i.toString(16)).toUpperCase());\n  }\n\n  return array;\n})();\n\nfunction compact_queue<T extends Record<string, any>>(queue: Array<{ obj: T; prop: string }>) {\n  while (queue.length > 1) {\n    const item = queue.pop();\n    if (!item) continue;\n\n    const obj = item.obj[item.prop];\n\n    if (isArray(obj)) {\n      const compacted: unknown[] = [];\n\n      for (let j = 0; j < obj.length; ++j) {\n        if (typeof obj[j] !== 'undefined') {\n          compacted.push(obj[j]);\n        }\n      }\n\n      // @ts-ignore\n      item.obj[item.prop] = compacted;\n    }\n  }\n}\n\nfunction array_to_object(source: any[], options: { plainObjects: boolean }) {\n  const obj = options && options.plainObjects ? Object.create(null) : {};\n  for (let i = 0; i < source.length; ++i) {\n    if (typeof source[i] !== 'undefined') {\n      obj[i] = source[i];\n    }\n  }\n\n  return obj;\n}\n\nexport function merge(\n  target: any,\n  source: any,\n  options: { plainObjects?: boolean; allowPrototypes?: boolean } = {},\n) {\n  if (!source) {\n    return target;\n  }\n\n  if (typeof source !== 'object') {\n    if (isArray(target)) {\n      target.push(source);\n    } else if (target && typeof target === 'object') {\n      if ((options && (options.plainObjects || options.allowPrototypes)) || !has(Object.prototype, source)) {\n        target[source] = true;\n      }\n    } else {\n      return [target, source];\n    }\n\n    return target;\n  }\n\n  if (!target || typeof target !== 'object') {\n    return [target].concat(source);\n  }\n\n  let mergeTarget = target;\n  if (isArray(target) && !isArray(source)) {\n    // @ts-ignore\n    mergeTarget = array_to_object(target, options);\n  }\n\n  if (isArray(target) && isArray(source)) {\n    source.forEach(function (item, i) {\n      if (has(target, i)) {\n        const targetItem = target[i];\n        if (targetItem && typeof targetItem === 'object' && item && typeof item === 'object') {\n          target[i] = merge(targetItem, item, options);\n        } else {\n          target.push(item);\n        }\n      } else {\n        target[i] = item;\n      }\n    });\n    return target;\n  }\n\n  return Object.keys(source).reduce(function (acc, key) {\n    const value = source[key];\n\n    if (has(acc, key)) {\n      acc[key] = merge(acc[key], value, options);\n    } else {\n      acc[key] = value;\n    }\n    return acc;\n  }, mergeTarget);\n}\n\nexport function assign_single_source(target: any, source: any) {\n  return Object.keys(source).reduce(function (acc, key) {\n    acc[key] = source[key];\n    return acc;\n  }, target);\n}\n\nexport function decode(str: string, _: any, charset: string) {\n  const strWithoutPlus = str.replace(/\\+/g, ' ');\n  if (charset === 'iso-8859-1') {\n    // unescape never throws, no try...catch needed:\n    return strWithoutPlus.replace(/%[0-9a-f]{2}/gi, unescape);\n  }\n  // utf-8\n  try {\n    return decodeURIComponent(strWithoutPlus);\n  } catch (e) {\n    return strWithoutPlus;\n  }\n}\n\nconst limit = 1024;\n\nexport const encode: (\n  str: any,\n  defaultEncoder: DefaultEncoder,\n  charset: string,\n  type: 'key' | 'value',\n  format: Format,\n) => string = (str, _defaultEncoder, charset, _kind, format: Format) => {\n  // This code was originally written by Brian White for the io.js core querystring library.\n  // It has been adapted here for stricter adherence to RFC 3986\n  if (str.length === 0) {\n    return str;\n  }\n\n  let string = str;\n  if (typeof str === 'symbol') {\n    string = Symbol.prototype.toString.call(str);\n  } else if (typeof str !== 'string') {\n    string = String(str);\n  }\n\n  if (charset === 'iso-8859-1') {\n    return escape(string).replace(/%u[0-9a-f]{4}/gi, function ($0) {\n      return '%26%23' + parseInt($0.slice(2), 16) + '%3B';\n    });\n  }\n\n  let out = '';\n  for (let j = 0; j < string.length; j += limit) {\n    const segment = string.length >= limit ? string.slice(j, j + limit) : string;\n    const arr = [];\n\n    for (let i = 0; i < segment.length; ++i) {\n      let c = segment.charCodeAt(i);\n      if (\n        c === 0x2d || // -\n        c === 0x2e || // .\n        c === 0x5f || // _\n        c === 0x7e || // ~\n        (c >= 0x30 && c <= 0x39) || // 0-9\n        (c >= 0x41 && c <= 0x5a) || // a-z\n        (c >= 0x61 && c <= 0x7a) || // A-Z\n        (format === RFC1738 && (c === 0x28 || c === 0x29)) // ( )\n      ) {\n        arr[arr.length] = segment.charAt(i);\n        continue;\n      }\n\n      if (c < 0x80) {\n        arr[arr.length] = hex_table[c];\n        continue;\n      }\n\n      if (c < 0x800) {\n        arr[arr.length] = hex_table[0xc0 | (c >> 6)]! + hex_table[0x80 | (c & 0x3f)];\n        continue;\n      }\n\n      if (c < 0xd800 || c >= 0xe000) {\n        arr[arr.length] =\n          hex_table[0xe0 | (c >> 12)]! + hex_table[0x80 | ((c >> 6) & 0x3f)] + hex_table[0x80 | (c & 0x3f)];\n        continue;\n      }\n\n      i += 1;\n      c = 0x10000 + (((c & 0x3ff) << 10) | (segment.charCodeAt(i) & 0x3ff));\n\n      arr[arr.length] =\n        hex_table[0xf0 | (c >> 18)]! +\n        hex_table[0x80 | ((c >> 12) & 0x3f)] +\n        hex_table[0x80 | ((c >> 6) & 0x3f)] +\n        hex_table[0x80 | (c & 0x3f)];\n    }\n\n    out += arr.join('');\n  }\n\n  return out;\n};\n\nexport function compact(value: any) {\n  const queue = [{ obj: { o: value }, prop: 'o' }];\n  const refs = [];\n\n  for (let i = 0; i < queue.length; ++i) {\n    const item = queue[i];\n    // @ts-ignore\n    const obj = item.obj[item.prop];\n\n    const keys = Object.keys(obj);\n    for (let j = 0; j < keys.length; ++j) {\n      const key = keys[j]!;\n      const val = obj[key];\n      if (typeof val === 'object' && val !== null && refs.indexOf(val) === -1) {\n        queue.push({ obj: obj, prop: key });\n        refs.push(val);\n      }\n    }\n  }\n\n  compact_queue(queue);\n\n  return value;\n}\n\nexport function is_regexp(obj: any) {\n  return Object.prototype.toString.call(obj) === '[object RegExp]';\n}\n\nexport function is_buffer(obj: any) {\n  if (!obj || typeof obj !== 'object') {\n    return false;\n  }\n\n  return !!(obj.constructor && obj.constructor.isBuffer && obj.constructor.isBuffer(obj));\n}\n\nexport function combine(a: any, b: any) {\n  return [].concat(a, b);\n}\n\nexport function maybe_map<T>(val: T[], fn: (v: T) => T) {\n  if (isArray(val)) {\n    const mapped = [];\n    for (let i = 0; i < val.length; i += 1) {\n      mapped.push(fn(val[i]!));\n    }\n    return mapped;\n  }\n  return fn(val);\n}\n", "import { encode, is_buffer, maybe_map, has } from './utils';\nimport { default_format, default_formatter, formatters } from './formats';\nimport type { NonNullableProperties, StringifyOptions } from './types';\nimport { isArray } from '../utils/values';\n\nconst array_prefix_generators = {\n  brackets(prefix: PropertyKey) {\n    return String(prefix) + '[]';\n  },\n  comma: 'comma',\n  indices(prefix: PropertyKey, key: string) {\n    return String(prefix) + '[' + key + ']';\n  },\n  repeat(prefix: PropertyKey) {\n    return String(prefix);\n  },\n};\n\nconst push_to_array = function (arr: any[], value_or_array: any) {\n  Array.prototype.push.apply(arr, isArray(value_or_array) ? value_or_array : [value_or_array]);\n};\n\nlet toISOString;\n\nconst defaults = {\n  addQueryPrefix: false,\n  allowDots: false,\n  allowEmptyArrays: false,\n  arrayFormat: 'indices',\n  charset: 'utf-8',\n  charsetSentinel: false,\n  delimiter: '&',\n  encode: true,\n  encodeDotInKeys: false,\n  encoder: encode,\n  encodeValuesOnly: false,\n  format: default_format,\n  formatter: default_formatter,\n  /** @deprecated */\n  indices: false,\n  serializeDate(date) {\n    return (toISOString ??= Function.prototype.call.bind(Date.prototype.toISOString))(date);\n  },\n  skipNulls: false,\n  strictNullHandling: false,\n} as NonNullableProperties<StringifyOptions & { formatter: (typeof formatters)['RFC1738'] }>;\n\nfunction is_non_nullish_primitive(v: unknown): v is string | number | boolean | symbol | bigint {\n  return (\n    typeof v === 'string' ||\n    typeof v === 'number' ||\n    typeof v === 'boolean' ||\n    typeof v === 'symbol' ||\n    typeof v === 'bigint'\n  );\n}\n\nconst sentinel = {};\n\nfunction inner_stringify(\n  object: any,\n  prefix: PropertyKey,\n  generateArrayPrefix: StringifyOptions['arrayFormat'] | ((prefix: string, key: string) => string),\n  commaRoundTrip: boolean,\n  allowEmptyArrays: boolean,\n  strictNullHandling: boolean,\n  skipNulls: boolean,\n  encodeDotInKeys: boolean,\n  encoder: StringifyOptions['encoder'],\n  filter: StringifyOptions['filter'],\n  sort: StringifyOptions['sort'],\n  allowDots: StringifyOptions['allowDots'],\n  serializeDate: StringifyOptions['serializeDate'],\n  format: StringifyOptions['format'],\n  formatter: StringifyOptions['formatter'],\n  encodeValuesOnly: boolean,\n  charset: StringifyOptions['charset'],\n  sideChannel: WeakMap<any, any>,\n) {\n  let obj = object;\n\n  let tmp_sc = sideChannel;\n  let step = 0;\n  let find_flag = false;\n  while ((tmp_sc = tmp_sc.get(sentinel)) !== void undefined && !find_flag) {\n    // Where object last appeared in the ref tree\n    const pos = tmp_sc.get(object);\n    step += 1;\n    if (typeof pos !== 'undefined') {\n      if (pos === step) {\n        throw new RangeError('Cyclic object value');\n      } else {\n        find_flag = true; // Break while\n      }\n    }\n    if (typeof tmp_sc.get(sentinel) === 'undefined') {\n      step = 0;\n    }\n  }\n\n  if (typeof filter === 'function') {\n    obj = filter(prefix, obj);\n  } else if (obj instanceof Date) {\n    obj = serializeDate?.(obj);\n  } else if (generateArrayPrefix === 'comma' && isArray(obj)) {\n    obj = maybe_map(obj, function (value) {\n      if (value instanceof Date) {\n        return serializeDate?.(value);\n      }\n      return value;\n    });\n  }\n\n  if (obj === null) {\n    if (strictNullHandling) {\n      return encoder && !encodeValuesOnly ?\n          // @ts-expect-error\n          encoder(prefix, defaults.encoder, charset, 'key', format)\n        : prefix;\n    }\n\n    obj = '';\n  }\n\n  if (is_non_nullish_primitive(obj) || is_buffer(obj)) {\n    if (encoder) {\n      const key_value =\n        encodeValuesOnly ? prefix\n          // @ts-expect-error\n        : encoder(prefix, defaults.encoder, charset, 'key', format);\n      return [\n        formatter?.(key_value) +\n          '=' +\n          // @ts-expect-error\n          formatter?.(encoder(obj, defaults.encoder, charset, 'value', format)),\n      ];\n    }\n    return [formatter?.(prefix) + '=' + formatter?.(String(obj))];\n  }\n\n  const values: string[] = [];\n\n  if (typeof obj === 'undefined') {\n    return values;\n  }\n\n  let obj_keys;\n  if (generateArrayPrefix === 'comma' && isArray(obj)) {\n    // we need to join elements in\n    if (encodeValuesOnly && encoder) {\n      // @ts-expect-error values only\n      obj = maybe_map(obj, encoder);\n    }\n    obj_keys = [{ value: obj.length > 0 ? obj.join(',') || null : void undefined }];\n  } else if (isArray(filter)) {\n    obj_keys = filter;\n  } else {\n    const keys = Object.keys(obj);\n    obj_keys = sort ? keys.sort(sort) : keys;\n  }\n\n  const encoded_prefix = encodeDotInKeys ? String(prefix).replace(/\\./g, '%2E') : String(prefix);\n\n  const adjusted_prefix =\n    commaRoundTrip && isArray(obj) && obj.length === 1 ? encoded_prefix + '[]' : encoded_prefix;\n\n  if (allowEmptyArrays && isArray(obj) && obj.length === 0) {\n    return adjusted_prefix + '[]';\n  }\n\n  for (let j = 0; j < obj_keys.length; ++j) {\n    const key = obj_keys[j];\n    const value =\n      // @ts-ignore\n      typeof key === 'object' && typeof key.value !== 'undefined' ? key.value : obj[key as any];\n\n    if (skipNulls && value === null) {\n      continue;\n    }\n\n    // @ts-ignore\n    const encoded_key = allowDots && encodeDotInKeys ? (key as any).replace(/\\./g, '%2E') : key;\n    const key_prefix =\n      isArray(obj) ?\n        typeof generateArrayPrefix === 'function' ?\n          generateArrayPrefix(adjusted_prefix, encoded_key)\n        : adjusted_prefix\n      : adjusted_prefix + (allowDots ? '.' + encoded_key : '[' + encoded_key + ']');\n\n    sideChannel.set(object, step);\n    const valueSideChannel = new WeakMap();\n    valueSideChannel.set(sentinel, sideChannel);\n    push_to_array(\n      values,\n      inner_stringify(\n        value,\n        key_prefix,\n        generateArrayPrefix,\n        commaRoundTrip,\n        allowEmptyArrays,\n        strictNullHandling,\n        skipNulls,\n        encodeDotInKeys,\n        // @ts-ignore\n        generateArrayPrefix === 'comma' && encodeValuesOnly && isArray(obj) ? null : encoder,\n        filter,\n        sort,\n        allowDots,\n        serializeDate,\n        format,\n        formatter,\n        encodeValuesOnly,\n        charset,\n        valueSideChannel,\n      ),\n    );\n  }\n\n  return values;\n}\n\nfunction normalize_stringify_options(\n  opts: StringifyOptions = defaults,\n): NonNullableProperties<Omit<StringifyOptions, 'indices'>> & { indices?: boolean } {\n  if (typeof opts.allowEmptyArrays !== 'undefined' && typeof opts.allowEmptyArrays !== 'boolean') {\n    throw new TypeError('`allowEmptyArrays` option can only be `true` or `false`, when provided');\n  }\n\n  if (typeof opts.encodeDotInKeys !== 'undefined' && typeof opts.encodeDotInKeys !== 'boolean') {\n    throw new TypeError('`encodeDotInKeys` option can only be `true` or `false`, when provided');\n  }\n\n  if (opts.encoder !== null && typeof opts.encoder !== 'undefined' && typeof opts.encoder !== 'function') {\n    throw new TypeError('Encoder has to be a function.');\n  }\n\n  const charset = opts.charset || defaults.charset;\n  if (typeof opts.charset !== 'undefined' && opts.charset !== 'utf-8' && opts.charset !== 'iso-8859-1') {\n    throw new TypeError('The charset option must be either utf-8, iso-8859-1, or undefined');\n  }\n\n  let format = default_format;\n  if (typeof opts.format !== 'undefined') {\n    if (!has(formatters, opts.format)) {\n      throw new TypeError('Unknown format option provided.');\n    }\n    format = opts.format;\n  }\n  const formatter = formatters[format];\n\n  let filter = defaults.filter;\n  if (typeof opts.filter === 'function' || isArray(opts.filter)) {\n    filter = opts.filter;\n  }\n\n  let arrayFormat: StringifyOptions['arrayFormat'];\n  if (opts.arrayFormat && opts.arrayFormat in array_prefix_generators) {\n    arrayFormat = opts.arrayFormat;\n  } else if ('indices' in opts) {\n    arrayFormat = opts.indices ? 'indices' : 'repeat';\n  } else {\n    arrayFormat = defaults.arrayFormat;\n  }\n\n  if ('commaRoundTrip' in opts && typeof opts.commaRoundTrip !== 'boolean') {\n    throw new TypeError('`commaRoundTrip` must be a boolean, or absent');\n  }\n\n  const allowDots =\n    typeof opts.allowDots === 'undefined' ?\n      !!opts.encodeDotInKeys === true ?\n        true\n      : defaults.allowDots\n    : !!opts.allowDots;\n\n  return {\n    addQueryPrefix: typeof opts.addQueryPrefix === 'boolean' ? opts.addQueryPrefix : defaults.addQueryPrefix,\n    // @ts-ignore\n    allowDots: allowDots,\n    allowEmptyArrays:\n      typeof opts.allowEmptyArrays === 'boolean' ? !!opts.allowEmptyArrays : defaults.allowEmptyArrays,\n    arrayFormat: arrayFormat,\n    charset: charset,\n    charsetSentinel:\n      typeof opts.charsetSentinel === 'boolean' ? opts.charsetSentinel : defaults.charsetSentinel,\n    commaRoundTrip: !!opts.commaRoundTrip,\n    delimiter: typeof opts.delimiter === 'undefined' ? defaults.delimiter : opts.delimiter,\n    encode: typeof opts.encode === 'boolean' ? opts.encode : defaults.encode,\n    encodeDotInKeys:\n      typeof opts.encodeDotInKeys === 'boolean' ? opts.encodeDotInKeys : defaults.encodeDotInKeys,\n    encoder: typeof opts.encoder === 'function' ? opts.encoder : defaults.encoder,\n    encodeValuesOnly:\n      typeof opts.encodeValuesOnly === 'boolean' ? opts.encodeValuesOnly : defaults.encodeValuesOnly,\n    filter: filter,\n    format: format,\n    formatter: formatter,\n    serializeDate: typeof opts.serializeDate === 'function' ? opts.serializeDate : defaults.serializeDate,\n    skipNulls: typeof opts.skipNulls === 'boolean' ? opts.skipNulls : defaults.skipNulls,\n    // @ts-ignore\n    sort: typeof opts.sort === 'function' ? opts.sort : null,\n    strictNullHandling:\n      typeof opts.strictNullHandling === 'boolean' ? opts.strictNullHandling : defaults.strictNullHandling,\n  };\n}\n\nexport function stringify(object: any, opts: StringifyOptions = {}) {\n  let obj = object;\n  const options = normalize_stringify_options(opts);\n\n  let obj_keys: PropertyKey[] | undefined;\n  let filter;\n\n  if (typeof options.filter === 'function') {\n    filter = options.filter;\n    obj = filter('', obj);\n  } else if (isArray(options.filter)) {\n    filter = options.filter;\n    obj_keys = filter;\n  }\n\n  const keys: string[] = [];\n\n  if (typeof obj !== 'object' || obj === null) {\n    return '';\n  }\n\n  const generateArrayPrefix = array_prefix_generators[options.arrayFormat];\n  const commaRoundTrip = generateArrayPrefix === 'comma' && options.commaRoundTrip;\n\n  if (!obj_keys) {\n    obj_keys = Object.keys(obj);\n  }\n\n  if (options.sort) {\n    obj_keys.sort(options.sort);\n  }\n\n  const sideChannel = new WeakMap();\n  for (let i = 0; i < obj_keys.length; ++i) {\n    const key = obj_keys[i]!;\n\n    if (options.skipNulls && obj[key] === null) {\n      continue;\n    }\n    push_to_array(\n      keys,\n      inner_stringify(\n        obj[key],\n        key,\n        // @ts-expect-error\n        generateArrayPrefix,\n        commaRoundTrip,\n        options.allowEmptyArrays,\n        options.strictNullHandling,\n        options.skipNulls,\n        options.encodeDotInKeys,\n        options.encode ? options.encoder : null,\n        options.filter,\n        options.sort,\n        options.allowDots,\n        options.serializeDate,\n        options.format,\n        options.formatter,\n        options.encodeValuesOnly,\n        options.charset,\n        sideChannel,\n      ),\n    );\n  }\n\n  const joined = keys.join(options.delimiter);\n  let prefix = options.addQueryPrefix === true ? '?' : '';\n\n  if (options.charsetSentinel) {\n    if (options.charset === 'iso-8859-1') {\n      // encodeURIComponent('&#10003;'), the \"numeric entity\" representation of a checkmark\n      prefix += 'utf8=%26%2310003%3B&';\n    } else {\n      // encodeURIComponent('')\n      prefix += 'utf8=%E2%9C%93&';\n    }\n  }\n\n  return joined.length > 0 ? prefix + joined : '';\n}\n", "export function concatBytes(buffers: Uint8Array[]): Uint8Array {\n  let length = 0;\n  for (const buffer of buffers) {\n    length += buffer.length;\n  }\n  const output = new Uint8Array(length);\n  let index = 0;\n  for (const buffer of buffers) {\n    output.set(buffer, index);\n    index += buffer.length;\n  }\n\n  return output;\n}\n\nlet encodeUTF8_: (str: string) => Uint8Array;\nexport function encodeUTF8(str: string) {\n  let encoder;\n  return (\n    encodeUTF8_ ??\n    ((encoder = new (globalThis as any).TextEncoder()), (encodeUTF8_ = encoder.encode.bind(encoder)))\n  )(str);\n}\n\nlet decodeUTF8_: (bytes: Uint8Array) => string;\nexport function decodeUTF8(bytes: Uint8Array) {\n  let decoder;\n  return (\n    decodeUTF8_ ??\n    ((decoder = new (globalThis as any).TextDecoder()), (decodeUTF8_ = decoder.decode.bind(decoder)))\n  )(bytes);\n}\n", "import { concatBytes, decodeUTF8, encodeUTF8 } from '../utils/bytes';\n\nexport type Bytes = string | ArrayBuffer | Uint8Array | null | undefined;\n\n/**\n * A re-implementation of httpx's `LineDecoder` in Python that handles incrementally\n * reading lines from text.\n *\n * https://github.com/encode/httpx/blob/920333ea98118e9cf617f246905d7b202510941c/httpx/_decoders.py#L258\n */\nexport class LineDecoder {\n  // prettier-ignore\n  static NEWLINE_CHARS = new Set(['\\n', '\\r']);\n  static NEWLINE_REGEXP = /\\r\\n|[\\n\\r]/g;\n\n  #buffer: Uint8Array;\n  #carriageReturnIndex: number | null;\n\n  constructor() {\n    this.#buffer = new Uint8Array();\n    this.#carriageReturnIndex = null;\n  }\n\n  decode(chunk: Bytes): string[] {\n    if (chunk == null) {\n      return [];\n    }\n\n    const binaryChunk =\n      chunk instanceof ArrayBuffer ? new Uint8Array(chunk)\n      : typeof chunk === 'string' ? encodeUTF8(chunk)\n      : chunk;\n\n    this.#buffer = concatBytes([this.#buffer, binaryChunk]);\n\n    const lines: string[] = [];\n    let patternIndex;\n    while ((patternIndex = findNewlineIndex(this.#buffer, this.#carriageReturnIndex)) != null) {\n      if (patternIndex.carriage && this.#carriageReturnIndex == null) {\n        // skip until we either get a corresponding `\\n`, a new `\\r` or nothing\n        this.#carriageReturnIndex = patternIndex.index;\n        continue;\n      }\n\n      // we got double \\r or \\rtext\\n\n      if (\n        this.#carriageReturnIndex != null &&\n        (patternIndex.index !== this.#carriageReturnIndex + 1 || patternIndex.carriage)\n      ) {\n        lines.push(decodeUTF8(this.#buffer.subarray(0, this.#carriageReturnIndex - 1)));\n        this.#buffer = this.#buffer.subarray(this.#carriageReturnIndex);\n        this.#carriageReturnIndex = null;\n        continue;\n      }\n\n      const endIndex =\n        this.#carriageReturnIndex !== null ? patternIndex.preceding - 1 : patternIndex.preceding;\n\n      const line = decodeUTF8(this.#buffer.subarray(0, endIndex));\n      lines.push(line);\n\n      this.#buffer = this.#buffer.subarray(patternIndex.index);\n      this.#carriageReturnIndex = null;\n    }\n\n    return lines;\n  }\n\n  flush(): string[] {\n    if (!this.#buffer.length) {\n      return [];\n    }\n    return this.decode('\\n');\n  }\n}\n\n/**\n * This function searches the buffer for the end patterns, (\\r or \\n)\n * and returns an object with the index preceding the matched newline and the\n * index after the newline char. `null` is returned if no new line is found.\n *\n * ```ts\n * findNewLineIndex('abc\\ndef') -> { preceding: 2, index: 3 }\n * ```\n */\nfunction findNewlineIndex(\n  buffer: Uint8Array,\n  startIndex: number | null,\n): { preceding: number; index: number; carriage: boolean } | null {\n  const newline = 0x0a; // \\n\n  const carriage = 0x0d; // \\r\n\n  for (let i = startIndex ?? 0; i < buffer.length; i++) {\n    if (buffer[i] === newline) {\n      return { preceding: i, index: i + 1, carriage: false };\n    }\n\n    if (buffer[i] === carriage) {\n      return { preceding: i, index: i + 1, carriage: true };\n    }\n  }\n\n  return null;\n}\n\nexport function findDoubleNewlineIndex(buffer: Uint8Array): number {\n  // This function searches the buffer for the end patterns (\\r\\r, \\n\\n, \\r\\n\\r\\n)\n  // and returns the index right after the first occurrence of any pattern,\n  // or -1 if none of the patterns are found.\n  const newline = 0x0a; // \\n\n  const carriage = 0x0d; // \\r\n\n  for (let i = 0; i < buffer.length - 1; i++) {\n    if (buffer[i] === newline && buffer[i + 1] === newline) {\n      // \\n\\n\n      return i + 2;\n    }\n    if (buffer[i] === carriage && buffer[i + 1] === carriage) {\n      // \\r\\r\n      return i + 2;\n    }\n    if (\n      buffer[i] === carriage &&\n      buffer[i + 1] === newline &&\n      i + 3 < buffer.length &&\n      buffer[i + 2] === carriage &&\n      buffer[i + 3] === newline\n    ) {\n      // \\r\\n\\r\\n\n      return i + 4;\n    }\n  }\n\n  return -1;\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { hasOwn } from './values';\nimport { type OpenAI } from '../../client';\nimport { RequestOptions } from '../request-options';\n\ntype LogFn = (message: string, ...rest: unknown[]) => void;\nexport type Logger = {\n  error: LogFn;\n  warn: LogFn;\n  info: LogFn;\n  debug: LogFn;\n};\nexport type LogLevel = 'off' | 'error' | 'warn' | 'info' | 'debug';\n\nconst levelNumbers = {\n  off: 0,\n  error: 200,\n  warn: 300,\n  info: 400,\n  debug: 500,\n};\n\nexport const parseLogLevel = (\n  maybeLevel: string | undefined,\n  sourceName: string,\n  client: OpenAI,\n): LogLevel | undefined => {\n  if (!maybeLevel) {\n    return undefined;\n  }\n  if (hasOwn(levelNumbers, maybeLevel)) {\n    return maybeLevel;\n  }\n  loggerFor(client).warn(\n    `${sourceName} was set to ${JSON.stringify(maybeLevel)}, expected one of ${JSON.stringify(\n      Object.keys(levelNumbers),\n    )}`,\n  );\n  return undefined;\n};\n\nfunction noop() {}\n\nfunction makeLogFn(fnLevel: keyof Logger, logger: Logger | undefined, logLevel: LogLevel) {\n  if (!logger || levelNumbers[fnLevel] > levelNumbers[logLevel]) {\n    return noop;\n  } else {\n    // Don't wrap logger functions, we want the stacktrace intact!\n    return logger[fnLevel].bind(logger);\n  }\n}\n\nconst noopLogger = {\n  error: noop,\n  warn: noop,\n  info: noop,\n  debug: noop,\n};\n\nlet cachedLoggers = /* @__PURE__ */ new WeakMap<Logger, [LogLevel, Logger]>();\n\nexport function loggerFor(client: OpenAI): Logger {\n  const logger = client.logger;\n  const logLevel = client.logLevel ?? 'off';\n  if (!logger) {\n    return noopLogger;\n  }\n\n  const cachedLogger = cachedLoggers.get(logger);\n  if (cachedLogger && cachedLogger[0] === logLevel) {\n    return cachedLogger[1];\n  }\n\n  const levelLogger = {\n    error: makeLogFn('error', logger, logLevel),\n    warn: makeLogFn('warn', logger, logLevel),\n    info: makeLogFn('info', logger, logLevel),\n    debug: makeLogFn('debug', logger, logLevel),\n  };\n\n  cachedLoggers.set(logger, [logLevel, levelLogger]);\n\n  return levelLogger;\n}\n\nexport const formatRequestDetails = (details: {\n  options?: RequestOptions | undefined;\n  headers?: Headers | Record<string, string> | undefined;\n  retryOfRequestLogID?: string | undefined;\n  retryOf?: string | undefined;\n  url?: string | undefined;\n  status?: number | undefined;\n  method?: string | undefined;\n  durationMs?: number | undefined;\n  message?: unknown;\n  body?: unknown;\n}) => {\n  if (details.options) {\n    details.options = { ...details.options };\n    delete details.options['headers']; // redundant + leaks internals\n  }\n  if (details.headers) {\n    details.headers = Object.fromEntries(\n      (details.headers instanceof Headers ? [...details.headers] : Object.entries(details.headers)).map(\n        ([name, value]) => [\n          name,\n          (\n            name.toLowerCase() === 'authorization' ||\n            name.toLowerCase() === 'cookie' ||\n            name.toLowerCase() === 'set-cookie'\n          ) ?\n            '***'\n          : value,\n        ],\n      ),\n    );\n  }\n  if ('retryOfRequestLogID' in details) {\n    if (details.retryOfRequestLogID) {\n      details.retryOf = details.retryOfRequestLogID;\n    }\n    delete details.retryOfRequestLogID;\n  }\n  return details;\n};\n", "import { OpenAIError } from './error';\nimport { type ReadableStream } from '../internal/shim-types';\nimport { makeReadableStream } from '../internal/shims';\nimport { findDoubleNewlineIndex, LineDecoder } from '../internal/decoders/line';\nimport { ReadableStreamToAsyncIterable } from '../internal/shims';\nimport { isAbortError } from '../internal/errors';\nimport { encodeUTF8 } from '../internal/utils/bytes';\nimport { loggerFor } from '../internal/utils/log';\nimport type { OpenAI } from '../client';\n\nimport { APIError } from './error';\n\ntype Bytes = string | ArrayBuffer | Uint8Array | null | undefined;\n\nexport type ServerSentEvent = {\n  event: string | null;\n  data: string;\n  raw: string[];\n};\n\nexport class Stream<Item> implements AsyncIterable<Item> {\n  controller: AbortController;\n  #client: OpenAI | undefined;\n\n  constructor(\n    private iterator: () => AsyncIterator<Item>,\n    controller: AbortController,\n    client?: OpenAI,\n  ) {\n    this.controller = controller;\n    this.#client = client;\n  }\n\n  static fromSSEResponse<Item>(\n    response: Response,\n    controller: AbortController,\n    client?: OpenAI,\n    synthesizeEventData?: boolean,\n  ): Stream<Item> {\n    let consumed = false;\n    const logger = client ? loggerFor(client) : console;\n\n    async function* iterator(): AsyncIterator<Item, any, undefined> {\n      if (consumed) {\n        throw new OpenAIError('Cannot iterate over a consumed stream, use `.tee()` to split the stream.');\n      }\n      consumed = true;\n      let done = false;\n      try {\n        for await (const sse of _iterSSEMessages(response, controller)) {\n          if (done) continue;\n\n          if (sse.data.startsWith('[DONE]')) {\n            done = true;\n            continue;\n          }\n\n          if (sse.event === null || !sse.event.startsWith('thread.')) {\n            let data;\n\n            try {\n              data = JSON.parse(sse.data) as any;\n            } catch (e) {\n              logger.error(`Could not parse message into JSON:`, sse.data);\n              logger.error(`From chunk:`, sse.raw);\n              throw e;\n            }\n\n            if (data && data.error) {\n              throw new APIError(undefined, data.error, undefined, response.headers);\n            }\n\n            yield synthesizeEventData ? { event: sse.event, data } : data;\n          } else {\n            let data;\n            try {\n              data = JSON.parse(sse.data);\n            } catch (e) {\n              console.error(`Could not parse message into JSON:`, sse.data);\n              console.error(`From chunk:`, sse.raw);\n              throw e;\n            }\n            // TODO: Is this where the error should be thrown?\n            if (sse.event == 'error') {\n              throw new APIError(undefined, data.error, data.message, undefined);\n            }\n            yield { event: sse.event, data: data } as any;\n          }\n        }\n        done = true;\n      } catch (e) {\n        // If the user calls `stream.controller.abort()`, we should exit without throwing.\n        if (isAbortError(e)) return;\n        throw e;\n      } finally {\n        // If the user `break`s, abort the ongoing request.\n        if (!done) controller.abort();\n      }\n    }\n\n    return new Stream(iterator, controller, client);\n  }\n\n  /**\n   * Generates a Stream from a newline-separated ReadableStream\n   * where each item is a JSON value.\n   */\n  static fromReadableStream<Item>(\n    readableStream: ReadableStream,\n    controller: AbortController,\n    client?: OpenAI,\n  ): Stream<Item> {\n    let consumed = false;\n\n    async function* iterLines(): AsyncGenerator<string, void, unknown> {\n      const lineDecoder = new LineDecoder();\n\n      const iter = ReadableStreamToAsyncIterable<Bytes>(readableStream);\n      for await (const chunk of iter) {\n        for (const line of lineDecoder.decode(chunk)) {\n          yield line;\n        }\n      }\n\n      for (const line of lineDecoder.flush()) {\n        yield line;\n      }\n    }\n\n    async function* iterator(): AsyncIterator<Item, any, undefined> {\n      if (consumed) {\n        throw new OpenAIError('Cannot iterate over a consumed stream, use `.tee()` to split the stream.');\n      }\n      consumed = true;\n      let done = false;\n      try {\n        for await (const line of iterLines()) {\n          if (done) continue;\n          if (line) yield JSON.parse(line) as Item;\n        }\n        done = true;\n      } catch (e) {\n        // If the user calls `stream.controller.abort()`, we should exit without throwing.\n        if (isAbortError(e)) return;\n        throw e;\n      } finally {\n        // If the user `break`s, abort the ongoing request.\n        if (!done) controller.abort();\n      }\n    }\n\n    return new Stream(iterator, controller, client);\n  }\n\n  [Symbol.asyncIterator](): AsyncIterator<Item> {\n    return this.iterator();\n  }\n\n  /**\n   * Splits the stream into two streams which can be\n   * independently read from at different speeds.\n   */\n  tee(): [Stream<Item>, Stream<Item>] {\n    const left: Array<Promise<IteratorResult<Item>>> = [];\n    const right: Array<Promise<IteratorResult<Item>>> = [];\n    const iterator = this.iterator();\n\n    const teeIterator = (queue: Array<Promise<IteratorResult<Item>>>): AsyncIterator<Item> => {\n      return {\n        next: () => {\n          if (queue.length === 0) {\n            const result = iterator.next();\n            left.push(result);\n            right.push(result);\n          }\n          return queue.shift()!;\n        },\n      };\n    };\n\n    return [\n      new Stream(() => teeIterator(left), this.controller, this.#client),\n      new Stream(() => teeIterator(right), this.controller, this.#client),\n    ];\n  }\n\n  /**\n   * Converts this stream to a newline-separated ReadableStream of\n   * JSON stringified values in the stream\n   * which can be turned back into a Stream with `Stream.fromReadableStream()`.\n   */\n  toReadableStream(): ReadableStream {\n    const self = this;\n    let iter: AsyncIterator<Item>;\n\n    return makeReadableStream({\n      async start() {\n        iter = self[Symbol.asyncIterator]();\n      },\n      async pull(ctrl: any) {\n        try {\n          const { value, done } = await iter.next();\n          if (done) return ctrl.close();\n\n          const bytes = encodeUTF8(JSON.stringify(value) + '\\n');\n\n          ctrl.enqueue(bytes);\n        } catch (err) {\n          ctrl.error(err);\n        }\n      },\n      async cancel() {\n        await iter.return?.();\n      },\n    });\n  }\n}\n\nexport async function* _iterSSEMessages(\n  response: Response,\n  controller: AbortController,\n): AsyncGenerator<ServerSentEvent, void, unknown> {\n  if (!response.body) {\n    controller.abort();\n    if (\n      typeof (globalThis as any).navigator !== 'undefined' &&\n      (globalThis as any).navigator.product === 'ReactNative'\n    ) {\n      throw new OpenAIError(\n        `The default react-native fetch implementation does not support streaming. Please use expo/fetch: https://docs.expo.dev/versions/latest/sdk/expo/#expofetch-api`,\n      );\n    }\n    throw new OpenAIError(`Attempted to iterate over a response with no body`);\n  }\n\n  const sseDecoder = new SSEDecoder();\n  const lineDecoder = new LineDecoder();\n\n  const iter = ReadableStreamToAsyncIterable<Bytes>(response.body);\n  for await (const sseChunk of iterSSEChunks(iter)) {\n    for (const line of lineDecoder.decode(sseChunk)) {\n      const sse = sseDecoder.decode(line);\n      if (sse) yield sse;\n    }\n  }\n\n  for (const line of lineDecoder.flush()) {\n    const sse = sseDecoder.decode(line);\n    if (sse) yield sse;\n  }\n}\n\n/**\n * Given an async iterable iterator, iterates over it and yields full\n * SSE chunks, i.e. yields when a double new-line is encountered.\n */\nasync function* iterSSEChunks(iterator: AsyncIterableIterator<Bytes>): AsyncGenerator<Uint8Array> {\n  let data = new Uint8Array();\n\n  for await (const chunk of iterator) {\n    if (chunk == null) {\n      continue;\n    }\n\n    const binaryChunk =\n      chunk instanceof ArrayBuffer ? new Uint8Array(chunk)\n      : typeof chunk === 'string' ? encodeUTF8(chunk)\n      : chunk;\n\n    let newData = new Uint8Array(data.length + binaryChunk.length);\n    newData.set(data);\n    newData.set(binaryChunk, data.length);\n    data = newData;\n\n    let patternIndex;\n    while ((patternIndex = findDoubleNewlineIndex(data)) !== -1) {\n      yield data.slice(0, patternIndex);\n      data = data.slice(patternIndex);\n    }\n  }\n\n  if (data.length > 0) {\n    yield data;\n  }\n}\n\nclass SSEDecoder {\n  private data: string[];\n  private event: string | null;\n  private chunks: string[];\n\n  constructor() {\n    this.event = null;\n    this.data = [];\n    this.chunks = [];\n  }\n\n  decode(line: string) {\n    if (line.endsWith('\\r')) {\n      line = line.substring(0, line.length - 1);\n    }\n\n    if (!line) {\n      // empty line and we didn't previously encounter any messages\n      if (!this.event && !this.data.length) return null;\n\n      const sse: ServerSentEvent = {\n        event: this.event,\n        data: this.data.join('\\n'),\n        raw: this.chunks,\n      };\n\n      this.event = null;\n      this.data = [];\n      this.chunks = [];\n\n      return sse;\n    }\n\n    this.chunks.push(line);\n\n    if (line.startsWith(':')) {\n      return null;\n    }\n\n    let [fieldname, _, value] = partition(line, ':');\n\n    if (value.startsWith(' ')) {\n      value = value.substring(1);\n    }\n\n    if (fieldname === 'event') {\n      this.event = value;\n    } else if (fieldname === 'data') {\n      this.data.push(value);\n    }\n\n    return null;\n  }\n}\n\nfunction partition(str: string, delimiter: string): [string, string, string] {\n  const index = str.indexOf(delimiter);\n  if (index !== -1) {\n    return [str.substring(0, index), delimiter, str.substring(index + delimiter.length)];\n  }\n\n  return [str, '', ''];\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport type { FinalRequestOptions } from './request-options';\nimport { Stream } from '../core/streaming';\nimport { type OpenAI } from '../client';\nimport { formatRequestDetails, loggerFor } from './utils/log';\nimport type { AbstractPage } from '../pagination';\n\nexport type APIResponseProps = {\n  response: Response;\n  options: FinalRequestOptions;\n  controller: AbortController;\n  requestLogID: string;\n  retryOfRequestLogID: string | undefined;\n  startTime: number;\n};\n\nexport async function defaultParseResponse<T>(\n  client: OpenAI,\n  props: APIResponseProps,\n): Promise<WithRequestID<T>> {\n  const { response, requestLogID, retryOfRequestLogID, startTime } = props;\n  const body = await (async () => {\n    if (props.options.stream) {\n      loggerFor(client).debug('response', response.status, response.url, response.headers, response.body);\n\n      // Note: there is an invariant here that isn't represented in the type system\n      // that if you set `stream: true` the response type must also be `Stream<T>`\n\n      if (props.options.__streamClass) {\n        return props.options.__streamClass.fromSSEResponse(\n          response,\n          props.controller,\n          client,\n          props.options.__synthesizeEventData,\n        ) as any;\n      }\n\n      return Stream.fromSSEResponse(\n        response,\n        props.controller,\n        client,\n        props.options.__synthesizeEventData,\n      ) as any;\n    }\n\n    // fetch refuses to read the body when the status code is 204.\n    if (response.status === 204) {\n      return null as T;\n    }\n\n    if (props.options.__binaryResponse) {\n      return response as unknown as T;\n    }\n\n    const contentType = response.headers.get('content-type');\n    const mediaType = contentType?.split(';')[0]?.trim();\n    const isJSON = mediaType?.includes('application/json') || mediaType?.endsWith('+json');\n    if (isJSON) {\n      const contentLength = response.headers.get('content-length');\n      if (contentLength === '0') {\n        // if there is no content we can't do anything\n        return undefined as T;\n      }\n\n      const json = await response.json();\n      return addRequestID(json as T, response);\n    }\n\n    const text = await response.text();\n    return text as unknown as T;\n  })();\n  loggerFor(client).debug(\n    `[${requestLogID}] response parsed`,\n    formatRequestDetails({\n      retryOfRequestLogID,\n      url: response.url,\n      status: response.status,\n      body,\n      durationMs: Date.now() - startTime,\n    }),\n  );\n  return body;\n}\n\nexport type WithRequestID<T> =\n  T extends Array<any> | Response | AbstractPage<any> ? T\n  : T extends Record<string, any> ? T & { _request_id?: string | null }\n  : T;\n\nexport function addRequestID<T>(value: T, response: Response): WithRequestID<T> {\n  if (!value || typeof value !== 'object' || Array.isArray(value)) {\n    return value as WithRequestID<T>;\n  }\n\n  return Object.defineProperty(value, '_request_id', {\n    value: response.headers.get('x-request-id'),\n    enumerable: false,\n  }) as WithRequestID<T>;\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { type OpenAI } from '../client';\n\nimport { type PromiseOrValue } from '../internal/types';\nimport {\n  type APIResponseProps,\n  defaultParseResponse,\n  type WithRequestID,\n  addRequestID,\n} from '../internal/parse';\n\n/**\n * A subclass of `Promise` providing additional helper methods\n * for interacting with the SDK.\n */\nexport class APIPromise<T> extends Promise<WithRequestID<T>> {\n  private parsedPromise: Promise<WithRequestID<T>> | undefined;\n  #client: OpenAI;\n\n  constructor(\n    client: OpenAI,\n    private responsePromise: Promise<APIResponseProps>,\n    private parseResponse: (\n      client: OpenAI,\n      props: APIResponseProps,\n    ) => PromiseOrValue<WithRequestID<T>> = defaultParseResponse,\n  ) {\n    super((resolve) => {\n      // this is maybe a bit weird but this has to be a no-op to not implicitly\n      // parse the response body; instead .then, .catch, .finally are overridden\n      // to parse the response\n      resolve(null as any);\n    });\n    this.#client = client;\n  }\n\n  _thenUnwrap<U>(transform: (data: T, props: APIResponseProps) => U): APIPromise<U> {\n    return new APIPromise(this.#client, this.responsePromise, async (client, props) =>\n      addRequestID(transform(await this.parseResponse(client, props), props), props.response),\n    );\n  }\n\n  /**\n   * Gets the raw `Response` instance instead of parsing the response\n   * data.\n   *\n   * If you want to parse the response body but still get the `Response`\n   * instance, you can use {@link withResponse()}.\n   *\n   *  Getting the wrong TypeScript type for `Response`?\n   * Try setting `\"moduleResolution\": \"NodeNext\"` or add `\"lib\": [\"DOM\"]`\n   * to your `tsconfig.json`.\n   */\n  asResponse(): Promise<Response> {\n    return this.responsePromise.then((p) => p.response);\n  }\n\n  /**\n   * Gets the parsed response data, the raw `Response` instance and the ID of the request,\n   * returned via the X-Request-ID header which is useful for debugging requests and reporting\n   * issues to OpenAI.\n   *\n   * If you just want to get the raw `Response` instance without parsing it,\n   * you can use {@link asResponse()}.\n   *\n   *  Getting the wrong TypeScript type for `Response`?\n   * Try setting `\"moduleResolution\": \"NodeNext\"` or add `\"lib\": [\"DOM\"]`\n   * to your `tsconfig.json`.\n   */\n  async withResponse(): Promise<{ data: T; response: Response; request_id: string | null }> {\n    const [data, response] = await Promise.all([this.parse(), this.asResponse()]);\n    return { data, response, request_id: response.headers.get('x-request-id') };\n  }\n\n  private parse(): Promise<WithRequestID<T>> {\n    if (!this.parsedPromise) {\n      this.parsedPromise = this.responsePromise.then((data) =>\n        this.parseResponse(this.#client, data),\n      ) as any as Promise<WithRequestID<T>>;\n    }\n    return this.parsedPromise;\n  }\n\n  override then<TResult1 = WithRequestID<T>, TResult2 = never>(\n    onfulfilled?: ((value: WithRequestID<T>) => TResult1 | PromiseLike<TResult1>) | undefined | null,\n    onrejected?: ((reason: any) => TResult2 | PromiseLike<TResult2>) | undefined | null,\n  ): Promise<TResult1 | TResult2> {\n    return this.parse().then(onfulfilled, onrejected);\n  }\n\n  override catch<TResult = never>(\n    onrejected?: ((reason: any) => TResult | PromiseLike<TResult>) | undefined | null,\n  ): Promise<WithRequestID<T> | TResult> {\n    return this.parse().catch(onrejected);\n  }\n\n  override finally(onfinally?: (() => void) | undefined | null): Promise<WithRequestID<T>> {\n    return this.parse().finally(onfinally);\n  }\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { OpenAIError } from './error';\nimport { FinalRequestOptions } from '../internal/request-options';\nimport { defaultParseResponse, WithRequestID } from '../internal/parse';\nimport { APIPromise } from './api-promise';\nimport { type OpenAI } from '../client';\nimport { type APIResponseProps } from '../internal/parse';\nimport { maybeObj } from '../internal/utils/values';\n\nexport type PageRequestOptions = Pick<FinalRequestOptions, 'query' | 'headers' | 'body' | 'path' | 'method'>;\n\nexport abstract class AbstractPage<Item> implements AsyncIterable<Item> {\n  #client: OpenAI;\n  protected options: FinalRequestOptions;\n\n  protected response: Response;\n  protected body: unknown;\n\n  constructor(client: OpenAI, response: Response, body: unknown, options: FinalRequestOptions) {\n    this.#client = client;\n    this.options = options;\n    this.response = response;\n    this.body = body;\n  }\n\n  abstract nextPageRequestOptions(): PageRequestOptions | null;\n\n  abstract getPaginatedItems(): Item[];\n\n  hasNextPage(): boolean {\n    const items = this.getPaginatedItems();\n    if (!items.length) return false;\n    return this.nextPageRequestOptions() != null;\n  }\n\n  async getNextPage(): Promise<this> {\n    const nextOptions = this.nextPageRequestOptions();\n    if (!nextOptions) {\n      throw new OpenAIError(\n        'No next page expected; please check `.hasNextPage()` before calling `.getNextPage()`.',\n      );\n    }\n\n    return await this.#client.requestAPIList(this.constructor as any, nextOptions);\n  }\n\n  async *iterPages(): AsyncGenerator<this> {\n    let page: this = this;\n    yield page;\n    while (page.hasNextPage()) {\n      page = await page.getNextPage();\n      yield page;\n    }\n  }\n\n  async *[Symbol.asyncIterator](): AsyncGenerator<Item> {\n    for await (const page of this.iterPages()) {\n      for (const item of page.getPaginatedItems()) {\n        yield item;\n      }\n    }\n  }\n}\n\n/**\n * This subclass of Promise will resolve to an instantiated Page once the request completes.\n *\n * It also implements AsyncIterable to allow auto-paginating iteration on an unawaited list call, eg:\n *\n *    for await (const item of client.items.list()) {\n *      console.log(item)\n *    }\n */\nexport class PagePromise<\n    PageClass extends AbstractPage<Item>,\n    Item = ReturnType<PageClass['getPaginatedItems']>[number],\n  >\n  extends APIPromise<PageClass>\n  implements AsyncIterable<Item>\n{\n  constructor(\n    client: OpenAI,\n    request: Promise<APIResponseProps>,\n    Page: new (...args: ConstructorParameters<typeof AbstractPage>) => PageClass,\n  ) {\n    super(\n      client,\n      request,\n      async (client, props) =>\n        new Page(\n          client,\n          props.response,\n          await defaultParseResponse(client, props),\n          props.options,\n        ) as WithRequestID<PageClass>,\n    );\n  }\n\n  /**\n   * Allow auto-paginating iteration on an unawaited list call, eg:\n   *\n   *    for await (const item of client.items.list()) {\n   *      console.log(item)\n   *    }\n   */\n  async *[Symbol.asyncIterator](): AsyncGenerator<Item> {\n    const page = await this;\n    for await (const item of page) {\n      yield item;\n    }\n  }\n}\n\nexport interface PageResponse<Item> {\n  data: Array<Item>;\n\n  object: string;\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class Page<Item> extends AbstractPage<Item> implements PageResponse<Item> {\n  data: Array<Item>;\n\n  object: string;\n\n  constructor(client: OpenAI, response: Response, body: PageResponse<Item>, options: FinalRequestOptions) {\n    super(client, response, body, options);\n\n    this.data = body.data || [];\n    this.object = body.object;\n  }\n\n  getPaginatedItems(): Item[] {\n    return this.data ?? [];\n  }\n\n  nextPageRequestOptions(): PageRequestOptions | null {\n    return null;\n  }\n}\n\nexport interface CursorPageResponse<Item> {\n  data: Array<Item>;\n\n  has_more: boolean;\n}\n\nexport interface CursorPageParams {\n  after?: string;\n\n  limit?: number;\n}\n\nexport class CursorPage<Item extends { id: string }>\n  extends AbstractPage<Item>\n  implements CursorPageResponse<Item>\n{\n  data: Array<Item>;\n\n  has_more: boolean;\n\n  constructor(\n    client: OpenAI,\n    response: Response,\n    body: CursorPageResponse<Item>,\n    options: FinalRequestOptions,\n  ) {\n    super(client, response, body, options);\n\n    this.data = body.data || [];\n    this.has_more = body.has_more || false;\n  }\n\n  getPaginatedItems(): Item[] {\n    return this.data ?? [];\n  }\n\n  override hasNextPage(): boolean {\n    if (this.has_more === false) {\n      return false;\n    }\n\n    return super.hasNextPage();\n  }\n\n  nextPageRequestOptions(): PageRequestOptions | null {\n    const data = this.getPaginatedItems();\n    const id = data[data.length - 1]?.id;\n    if (!id) {\n      return null;\n    }\n\n    return {\n      ...this.options,\n      query: {\n        ...maybeObj(this.options.query),\n        after: id,\n      },\n    };\n  }\n}\n\nexport interface ConversationCursorPageResponse<Item> {\n  data: Array<Item>;\n\n  has_more: boolean;\n\n  last_id: string;\n}\n\nexport interface ConversationCursorPageParams {\n  after?: string;\n\n  limit?: number;\n}\n\nexport class ConversationCursorPage<Item>\n  extends AbstractPage<Item>\n  implements ConversationCursorPageResponse<Item>\n{\n  data: Array<Item>;\n\n  has_more: boolean;\n\n  last_id: string;\n\n  constructor(\n    client: OpenAI,\n    response: Response,\n    body: ConversationCursorPageResponse<Item>,\n    options: FinalRequestOptions,\n  ) {\n    super(client, response, body, options);\n\n    this.data = body.data || [];\n    this.has_more = body.has_more || false;\n    this.last_id = body.last_id || '';\n  }\n\n  getPaginatedItems(): Item[] {\n    return this.data ?? [];\n  }\n\n  override hasNextPage(): boolean {\n    if (this.has_more === false) {\n      return false;\n    }\n\n    return super.hasNextPage();\n  }\n\n  nextPageRequestOptions(): PageRequestOptions | null {\n    const cursor = this.last_id;\n    if (!cursor) {\n      return null;\n    }\n\n    return {\n      ...this.options,\n      query: {\n        ...maybeObj(this.options.query),\n        after: cursor,\n      },\n    };\n  }\n}\n", "import { type RequestOptions } from './request-options';\nimport type { FilePropertyBag, Fetch } from './builtin-types';\nimport type { OpenAI } from '../client';\nimport { ReadableStreamFrom } from './shims';\n\nexport type BlobPart = string | ArrayBuffer | ArrayBufferView | Blob | DataView;\ntype FsReadStream = AsyncIterable<Uint8Array> & { path: string | { toString(): string } };\n\n// https://github.com/oven-sh/bun/issues/5980\ninterface BunFile extends Blob {\n  readonly name?: string | undefined;\n}\n\nexport const checkFileSupport = () => {\n  if (typeof File === 'undefined') {\n    const { process } = globalThis as any;\n    const isOldNode =\n      typeof process?.versions?.node === 'string' && parseInt(process.versions.node.split('.')) < 20;\n    throw new Error(\n      '`File` is not defined as a global, which is required for file uploads.' +\n        (isOldNode ?\n          \" Update to Node 20 LTS or newer, or set `globalThis.File` to `import('node:buffer').File`.\"\n        : ''),\n    );\n  }\n};\n\n/**\n * Typically, this is a native \"File\" class.\n *\n * We provide the {@link toFile} utility to convert a variety of objects\n * into the File class.\n *\n * For convenience, you can also pass a fetch Response, or in Node,\n * the result of fs.createReadStream().\n */\nexport type Uploadable = File | Response | FsReadStream | BunFile;\n\n/**\n * Construct a `File` instance. This is used to ensure a helpful error is thrown\n * for environments that don't define a global `File` yet.\n */\nexport function makeFile(\n  fileBits: BlobPart[],\n  fileName: string | undefined,\n  options?: FilePropertyBag,\n): File {\n  checkFileSupport();\n  return new File(fileBits as any, fileName ?? 'unknown_file', options);\n}\n\nexport function getName(value: any): string | undefined {\n  return (\n    (\n      (typeof value === 'object' &&\n        value !== null &&\n        (('name' in value && value.name && String(value.name)) ||\n          ('url' in value && value.url && String(value.url)) ||\n          ('filename' in value && value.filename && String(value.filename)) ||\n          ('path' in value && value.path && String(value.path)))) ||\n      ''\n    )\n      .split(/[\\\\/]/)\n      .pop() || undefined\n  );\n}\n\nexport const isAsyncIterable = (value: any): value is AsyncIterable<any> =>\n  value != null && typeof value === 'object' && typeof value[Symbol.asyncIterator] === 'function';\n\n/**\n * Returns a multipart/form-data request if any part of the given request body contains a File / Blob value.\n * Otherwise returns the request as is.\n */\nexport const maybeMultipartFormRequestOptions = async (\n  opts: RequestOptions,\n  fetch: OpenAI | Fetch,\n): Promise<RequestOptions> => {\n  if (!hasUploadableValue(opts.body)) return opts;\n\n  return { ...opts, body: await createForm(opts.body, fetch) };\n};\n\ntype MultipartFormRequestOptions = Omit<RequestOptions, 'body'> & { body: unknown };\n\nexport const multipartFormRequestOptions = async (\n  opts: MultipartFormRequestOptions,\n  fetch: OpenAI | Fetch,\n): Promise<RequestOptions> => {\n  return { ...opts, body: await createForm(opts.body, fetch) };\n};\n\nconst supportsFormDataMap = /* @__PURE__ */ new WeakMap<Fetch, Promise<boolean>>();\n\n/**\n * node-fetch doesn't support the global FormData object in recent node versions. Instead of sending\n * properly-encoded form data, it just stringifies the object, resulting in a request body of \"[object FormData]\".\n * This function detects if the fetch function provided supports the global FormData object to avoid\n * confusing error messages later on.\n */\nfunction supportsFormData(fetchObject: OpenAI | Fetch): Promise<boolean> {\n  const fetch: Fetch = typeof fetchObject === 'function' ? fetchObject : (fetchObject as any).fetch;\n  const cached = supportsFormDataMap.get(fetch);\n  if (cached) return cached;\n  const promise = (async () => {\n    try {\n      const FetchResponse = (\n        'Response' in fetch ?\n          fetch.Response\n        : (await fetch('data:,')).constructor) as typeof Response;\n      const data = new FormData();\n      if (data.toString() === (await new FetchResponse(data).text())) {\n        return false;\n      }\n      return true;\n    } catch {\n      // avoid false negatives\n      return true;\n    }\n  })();\n  supportsFormDataMap.set(fetch, promise);\n  return promise;\n}\n\nexport const createForm = async <T = Record<string, unknown>>(\n  body: T | undefined,\n  fetch: OpenAI | Fetch,\n): Promise<FormData> => {\n  if (!(await supportsFormData(fetch))) {\n    throw new TypeError(\n      'The provided fetch function does not support file uploads with the current global FormData class.',\n    );\n  }\n  const form = new FormData();\n  await Promise.all(Object.entries(body || {}).map(([key, value]) => addFormValue(form, key, value)));\n  return form;\n};\n\n// We check for Blob not File because Bun.File doesn't inherit from File,\n// but they both inherit from Blob and have a `name` property at runtime.\nconst isNamedBlob = (value: unknown) => value instanceof Blob && 'name' in value;\n\nconst isUploadable = (value: unknown) =>\n  typeof value === 'object' &&\n  value !== null &&\n  (value instanceof Response || isAsyncIterable(value) || isNamedBlob(value));\n\nconst hasUploadableValue = (value: unknown): boolean => {\n  if (isUploadable(value)) return true;\n  if (Array.isArray(value)) return value.some(hasUploadableValue);\n  if (value && typeof value === 'object') {\n    for (const k in value) {\n      if (hasUploadableValue((value as any)[k])) return true;\n    }\n  }\n  return false;\n};\n\nconst addFormValue = async (form: FormData, key: string, value: unknown): Promise<void> => {\n  if (value === undefined) return;\n  if (value == null) {\n    throw new TypeError(\n      `Received null for \"${key}\"; to pass null in FormData, you must use the string 'null'`,\n    );\n  }\n\n  // TODO: make nested formats configurable\n  if (typeof value === 'string' || typeof value === 'number' || typeof value === 'boolean') {\n    form.append(key, String(value));\n  } else if (value instanceof Response) {\n    form.append(key, makeFile([await value.blob()], getName(value)));\n  } else if (isAsyncIterable(value)) {\n    form.append(key, makeFile([await new Response(ReadableStreamFrom(value)).blob()], getName(value)));\n  } else if (isNamedBlob(value)) {\n    form.append(key, value, getName(value));\n  } else if (Array.isArray(value)) {\n    await Promise.all(value.map((entry) => addFormValue(form, key + '[]', entry)));\n  } else if (typeof value === 'object') {\n    await Promise.all(\n      Object.entries(value).map(([name, prop]) => addFormValue(form, `${key}[${name}]`, prop)),\n    );\n  } else {\n    throw new TypeError(\n      `Invalid value given to form, expected a string, number, boolean, object, Array, File or Blob but got ${value} instead`,\n    );\n  }\n};\n", "import { BlobPart, getName, makeFile, isAsyncIterable } from './uploads';\nimport type { FilePropertyBag } from './builtin-types';\nimport { checkFileSupport } from './uploads';\n\ntype BlobLikePart = string | ArrayBuffer | ArrayBufferView | BlobLike | DataView;\n\n/**\n * Intended to match DOM Blob, node-fetch Blob, node:buffer Blob, etc.\n * Don't add arrayBuffer here, node-fetch doesn't have it\n */\ninterface BlobLike {\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/size) */\n  readonly size: number;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/type) */\n  readonly type: string;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/text) */\n  text(): Promise<string>;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/slice) */\n  slice(start?: number, end?: number): BlobLike;\n}\n\n/**\n * This check adds the arrayBuffer() method type because it is available and used at runtime\n */\nconst isBlobLike = (value: any): value is BlobLike & { arrayBuffer(): Promise<ArrayBuffer> } =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.size === 'number' &&\n  typeof value.type === 'string' &&\n  typeof value.text === 'function' &&\n  typeof value.slice === 'function' &&\n  typeof value.arrayBuffer === 'function';\n\n/**\n * Intended to match DOM File, node:buffer File, undici File, etc.\n */\ninterface FileLike extends BlobLike {\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/File/lastModified) */\n  readonly lastModified: number;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/File/name) */\n  readonly name?: string | undefined;\n}\n\n/**\n * This check adds the arrayBuffer() method type because it is available and used at runtime\n */\nconst isFileLike = (value: any): value is FileLike & { arrayBuffer(): Promise<ArrayBuffer> } =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.name === 'string' &&\n  typeof value.lastModified === 'number' &&\n  isBlobLike(value);\n\n/**\n * Intended to match DOM Response, node-fetch Response, undici Response, etc.\n */\nexport interface ResponseLike {\n  url: string;\n  blob(): Promise<BlobLike>;\n}\n\nconst isResponseLike = (value: any): value is ResponseLike =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.url === 'string' &&\n  typeof value.blob === 'function';\n\nexport type ToFileInput =\n  | FileLike\n  | ResponseLike\n  | Exclude<BlobLikePart, string>\n  | AsyncIterable<BlobLikePart>;\n\n/**\n * Helper for creating a {@link File} to pass to an SDK upload method from a variety of different data formats\n * @param value the raw content of the file. Can be an {@link Uploadable}, BlobLikePart, or AsyncIterable of BlobLikeParts\n * @param {string=} name the name of the file. If omitted, toFile will try to determine a file name from bits if possible\n * @param {Object=} options additional properties\n * @param {string=} options.type the MIME type of the content\n * @param {number=} options.lastModified the last modified timestamp\n * @returns a {@link File} with the given properties\n */\nexport async function toFile(\n  value: ToFileInput | PromiseLike<ToFileInput>,\n  name?: string | null | undefined,\n  options?: FilePropertyBag | undefined,\n): Promise<File> {\n  checkFileSupport();\n\n  // If it's a promise, resolve it.\n  value = await value;\n\n  // If we've been given a `File` we don't need to do anything\n  if (isFileLike(value)) {\n    if (value instanceof File) {\n      return value;\n    }\n    return makeFile([await value.arrayBuffer()], value.name);\n  }\n\n  if (isResponseLike(value)) {\n    const blob = await value.blob();\n    name ||= new URL(value.url).pathname.split(/[\\\\/]/).pop();\n\n    return makeFile(await getBytes(blob), name, options);\n  }\n\n  const parts = await getBytes(value);\n\n  name ||= getName(value);\n\n  if (!options?.type) {\n    const type = parts.find((part) => typeof part === 'object' && 'type' in part && part.type);\n    if (typeof type === 'string') {\n      options = { ...options, type };\n    }\n  }\n\n  return makeFile(parts, name, options);\n}\n\nasync function getBytes(value: BlobLikePart | AsyncIterable<BlobLikePart>): Promise<Array<BlobPart>> {\n  let parts: Array<BlobPart> = [];\n  if (\n    typeof value === 'string' ||\n    ArrayBuffer.isView(value) || // includes Uint8Array, Buffer, etc.\n    value instanceof ArrayBuffer\n  ) {\n    parts.push(value);\n  } else if (isBlobLike(value)) {\n    parts.push(value instanceof Blob ? value : await value.arrayBuffer());\n  } else if (\n    isAsyncIterable(value) // includes Readable, ReadableStream, etc.\n  ) {\n    for await (const chunk of value) {\n      parts.push(...(await getBytes(chunk as BlobLikePart))); // TODO, consider validating?\n    }\n  } else {\n    const constructor = value?.constructor?.name;\n    throw new Error(\n      `Unexpected data type: ${typeof value}${\n        constructor ? `; constructor: ${constructor}` : ''\n      }${propsForError(value)}`,\n    );\n  }\n\n  return parts;\n}\n\nfunction propsForError(value: unknown): string {\n  if (typeof value !== 'object' || value === null) return '';\n  const props = Object.getOwnPropertyNames(value);\n  return `; props: [${props.map((p) => `\"${p}\"`).join(', ')}]`;\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport type { OpenAI } from '../client';\n\nexport abstract class APIResource {\n  protected _client: OpenAI;\n\n  constructor(client: OpenAI) {\n    this._client = client;\n  }\n}\n", "import { OpenAIError } from '../../core/error';\n\n/**\n * Percent-encode everything that isn't safe to have in a path without encoding safe chars.\n *\n * Taken from https://datatracker.ietf.org/doc/html/rfc3986#section-3.3:\n * > unreserved  = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n * > sub-delims  = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n * > pchar       = unreserved / pct-encoded / sub-delims / \":\" / \"@\"\n */\nexport function encodeURIPath(str: string) {\n  return str.replace(/[^A-Za-z0-9\\-._~!$&'()*+,;=:@]+/g, encodeURIComponent);\n}\n\nconst EMPTY = /* @__PURE__ */ Object.freeze(/* @__PURE__ */ Object.create(null));\n\nexport const createPathTagFunction = (pathEncoder = encodeURIPath) =>\n  function path(statics: readonly string[], ...params: readonly unknown[]): string {\n    // If there are no params, no processing is needed.\n    if (statics.length === 1) return statics[0]!;\n\n    let postPath = false;\n    const invalidSegments = [];\n    const path = statics.reduce((previousValue, currentValue, index) => {\n      if (/[?#]/.test(currentValue)) {\n        postPath = true;\n      }\n      const value = params[index];\n      let encoded = (postPath ? encodeURIComponent : pathEncoder)('' + value);\n      if (\n        index !== params.length &&\n        (value == null ||\n          (typeof value === 'object' &&\n            // handle values from other realms\n            value.toString ===\n              Object.getPrototypeOf(Object.getPrototypeOf((value as any).hasOwnProperty ?? EMPTY) ?? EMPTY)\n                ?.toString))\n      ) {\n        encoded = value + '';\n        invalidSegments.push({\n          start: previousValue.length + currentValue.length,\n          length: encoded.length,\n          error: `Value of type ${Object.prototype.toString\n            .call(value)\n            .slice(8, -1)} is not a valid path parameter`,\n        });\n      }\n      return previousValue + currentValue + (index === params.length ? '' : encoded);\n    }, '');\n\n    const pathOnly = path.split(/[?#]/, 1)[0]!;\n    const invalidSegmentPattern = /(?<=^|\\/)(?:\\.|%2e){1,2}(?=\\/|$)/gi;\n    let match;\n\n    // Find all invalid segments\n    while ((match = invalidSegmentPattern.exec(pathOnly)) !== null) {\n      invalidSegments.push({\n        start: match.index,\n        length: match[0].length,\n        error: `Value \"${match[0]}\" can\\'t be safely passed as a path parameter`,\n      });\n    }\n\n    invalidSegments.sort((a, b) => a.start - b.start);\n\n    if (invalidSegments.length > 0) {\n      let lastEnd = 0;\n      const underline = invalidSegments.reduce((acc, segment) => {\n        const spaces = ' '.repeat(segment.start - lastEnd);\n        const arrows = '^'.repeat(segment.length);\n        lastEnd = segment.start + segment.length;\n        return acc + spaces + arrows;\n      }, '');\n\n      throw new OpenAIError(\n        `Path parameters result in path with invalid segments:\\n${invalidSegments\n          .map((e) => e.error)\n          .join('\\n')}\\n${path}\\n${underline}`,\n      );\n    }\n\n    return path;\n  };\n\n/**\n * URI-encodes path params and ensures no unsafe /./ or /../ path segments are introduced.\n */\nexport const path = /* @__PURE__ */ createPathTagFunction(encodeURIPath);\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as CompletionsAPI from './completions';\nimport { ChatCompletionStoreMessagesPage } from './completions';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Messages extends APIResource {\n  /**\n   * Get the messages in a stored chat completion. Only Chat Completions that have\n   * been created with the `store` parameter set to `true` will be returned.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const chatCompletionStoreMessage of client.chat.completions.messages.list(\n   *   'completion_id',\n   * )) {\n   *   // ...\n   * }\n   * ```\n   */\n  list(\n    completionID: string,\n    query: MessageListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<ChatCompletionStoreMessagesPage, CompletionsAPI.ChatCompletionStoreMessage> {\n    return this._client.getAPIList(\n      path`/chat/completions/${completionID}/messages`,\n      CursorPage<CompletionsAPI.ChatCompletionStoreMessage>,\n      { query, ...options },\n    );\n  }\n}\n\nexport interface MessageListParams extends CursorPageParams {\n  /**\n   * Sort order for messages by timestamp. Use `asc` for ascending order or `desc`\n   * for descending order. Defaults to `asc`.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport declare namespace Messages {\n  export { type MessageListParams as MessageListParams };\n}\n\nexport { type ChatCompletionStoreMessagesPage };\n", "import { ContentFilterFinishReasonError, LengthFinishReasonError, OpenAIError } from '../error';\nimport {\n  ChatCompletion,\n  ChatCompletionCreateParams,\n  ChatCompletionCreateParamsBase,\n  ChatCompletionFunctionTool,\n  ChatCompletionMessage,\n  ChatCompletionMessageFunctionToolCall,\n  ChatCompletionStreamingToolRunnerParams,\n  ChatCompletionStreamParams,\n  ChatCompletionToolRunnerParams,\n  ParsedChatCompletion,\n  ParsedChoice,\n  ParsedFunctionToolCall,\n} from '../resources/chat/completions';\nimport { type ResponseFormatTextJSONSchemaConfig } from '../resources/responses/responses';\nimport { ResponseFormatJSONSchema } from '../resources/shared';\n\ntype AnyChatCompletionCreateParams =\n  | ChatCompletionCreateParams\n  | ChatCompletionToolRunnerParams<any>\n  | ChatCompletionStreamingToolRunnerParams<any>\n  | ChatCompletionStreamParams;\n\ntype Unpacked<T> = T extends (infer U)[] ? U : T;\n\ntype ToolCall = Unpacked<ChatCompletionCreateParamsBase['tools']>;\n\nexport function isChatCompletionFunctionTool(tool: ToolCall): tool is ChatCompletionFunctionTool {\n  return tool !== undefined && 'function' in tool && tool.function !== undefined;\n}\n\nexport type ExtractParsedContentFromParams<Params extends AnyChatCompletionCreateParams> =\n  Params['response_format'] extends AutoParseableResponseFormat<infer P> ? P : null;\n\nexport type AutoParseableResponseFormat<ParsedT> = ResponseFormatJSONSchema & {\n  __output: ParsedT; // type-level only\n\n  $brand: 'auto-parseable-response-format';\n  $parseRaw(content: string): ParsedT;\n};\n\nexport function makeParseableResponseFormat<ParsedT>(\n  response_format: ResponseFormatJSONSchema,\n  parser: (content: string) => ParsedT,\n): AutoParseableResponseFormat<ParsedT> {\n  const obj = { ...response_format };\n\n  Object.defineProperties(obj, {\n    $brand: {\n      value: 'auto-parseable-response-format',\n      enumerable: false,\n    },\n    $parseRaw: {\n      value: parser,\n      enumerable: false,\n    },\n  });\n\n  return obj as AutoParseableResponseFormat<ParsedT>;\n}\n\nexport type AutoParseableTextFormat<ParsedT> = ResponseFormatTextJSONSchemaConfig & {\n  __output: ParsedT; // type-level only\n\n  $brand: 'auto-parseable-response-format';\n  $parseRaw(content: string): ParsedT;\n};\n\nexport function makeParseableTextFormat<ParsedT>(\n  response_format: ResponseFormatTextJSONSchemaConfig,\n  parser: (content: string) => ParsedT,\n): AutoParseableTextFormat<ParsedT> {\n  const obj = { ...response_format };\n\n  Object.defineProperties(obj, {\n    $brand: {\n      value: 'auto-parseable-response-format',\n      enumerable: false,\n    },\n    $parseRaw: {\n      value: parser,\n      enumerable: false,\n    },\n  });\n\n  return obj as AutoParseableTextFormat<ParsedT>;\n}\n\nexport function isAutoParsableResponseFormat<ParsedT>(\n  response_format: any,\n): response_format is AutoParseableResponseFormat<ParsedT> {\n  return response_format?.['$brand'] === 'auto-parseable-response-format';\n}\n\ntype ToolOptions = {\n  name: string;\n  arguments: any;\n  function?: ((args: any) => any) | undefined;\n};\n\nexport type AutoParseableTool<\n  OptionsT extends ToolOptions,\n  HasFunction = OptionsT['function'] extends Function ? true : false,\n> = ChatCompletionFunctionTool & {\n  __arguments: OptionsT['arguments']; // type-level only\n  __name: OptionsT['name']; // type-level only\n  __hasFunction: HasFunction; // type-level only\n\n  $brand: 'auto-parseable-tool';\n  $callback: ((args: OptionsT['arguments']) => any) | undefined;\n  $parseRaw(args: string): OptionsT['arguments'];\n};\n\nexport function makeParseableTool<OptionsT extends ToolOptions>(\n  tool: ChatCompletionFunctionTool,\n  {\n    parser,\n    callback,\n  }: {\n    parser: (content: string) => OptionsT['arguments'];\n    callback: ((args: any) => any) | undefined;\n  },\n): AutoParseableTool<OptionsT['arguments']> {\n  const obj = { ...tool };\n\n  Object.defineProperties(obj, {\n    $brand: {\n      value: 'auto-parseable-tool',\n      enumerable: false,\n    },\n    $parseRaw: {\n      value: parser,\n      enumerable: false,\n    },\n    $callback: {\n      value: callback,\n      enumerable: false,\n    },\n  });\n\n  return obj as AutoParseableTool<OptionsT['arguments']>;\n}\n\nexport function isAutoParsableTool(tool: any): tool is AutoParseableTool<any> {\n  return tool?.['$brand'] === 'auto-parseable-tool';\n}\n\nexport function maybeParseChatCompletion<\n  Params extends ChatCompletionCreateParams | null,\n  ParsedT = Params extends null ? null : ExtractParsedContentFromParams<NonNullable<Params>>,\n>(completion: ChatCompletion, params: Params): ParsedChatCompletion<ParsedT> {\n  if (!params || !hasAutoParseableInput(params)) {\n    return {\n      ...completion,\n      choices: completion.choices.map((choice) => {\n        assertToolCallsAreChatCompletionFunctionToolCalls(choice.message.tool_calls);\n\n        return {\n          ...choice,\n          message: {\n            ...choice.message,\n            parsed: null,\n            ...(choice.message.tool_calls ?\n              {\n                tool_calls: choice.message.tool_calls,\n              }\n            : undefined),\n          },\n        };\n      }),\n    } as ParsedChatCompletion<ParsedT>;\n  }\n\n  return parseChatCompletion(completion, params);\n}\n\nexport function parseChatCompletion<\n  Params extends ChatCompletionCreateParams,\n  ParsedT = ExtractParsedContentFromParams<Params>,\n>(completion: ChatCompletion, params: Params): ParsedChatCompletion<ParsedT> {\n  const choices: Array<ParsedChoice<ParsedT>> = completion.choices.map((choice): ParsedChoice<ParsedT> => {\n    if (choice.finish_reason === 'length') {\n      throw new LengthFinishReasonError();\n    }\n\n    if (choice.finish_reason === 'content_filter') {\n      throw new ContentFilterFinishReasonError();\n    }\n\n    assertToolCallsAreChatCompletionFunctionToolCalls(choice.message.tool_calls);\n\n    return {\n      ...choice,\n      message: {\n        ...choice.message,\n        ...(choice.message.tool_calls ?\n          {\n            tool_calls:\n              choice.message.tool_calls?.map((toolCall) => parseToolCall(params, toolCall)) ?? undefined,\n          }\n        : undefined),\n        parsed:\n          choice.message.content && !choice.message.refusal ?\n            parseResponseFormat(params, choice.message.content)\n          : null,\n      },\n    } as ParsedChoice<ParsedT>;\n  });\n\n  return { ...completion, choices };\n}\n\nfunction parseResponseFormat<\n  Params extends ChatCompletionCreateParams,\n  ParsedT = ExtractParsedContentFromParams<Params>,\n>(params: Params, content: string): ParsedT | null {\n  if (params.response_format?.type !== 'json_schema') {\n    return null;\n  }\n\n  if (params.response_format?.type === 'json_schema') {\n    if ('$parseRaw' in params.response_format) {\n      const response_format = params.response_format as AutoParseableResponseFormat<ParsedT>;\n\n      return response_format.$parseRaw(content);\n    }\n\n    return JSON.parse(content);\n  }\n\n  return null;\n}\n\nfunction parseToolCall<Params extends ChatCompletionCreateParams>(\n  params: Params,\n  toolCall: ChatCompletionMessageFunctionToolCall,\n): ParsedFunctionToolCall {\n  const inputTool = params.tools?.find(\n    (inputTool) =>\n      isChatCompletionFunctionTool(inputTool) && inputTool.function?.name === toolCall.function.name,\n  ) as ChatCompletionFunctionTool | undefined; // TS doesn't narrow based on isChatCompletionTool\n  return {\n    ...toolCall,\n    function: {\n      ...toolCall.function,\n      parsed_arguments:\n        isAutoParsableTool(inputTool) ? inputTool.$parseRaw(toolCall.function.arguments)\n        : inputTool?.function.strict ? JSON.parse(toolCall.function.arguments)\n        : null,\n    },\n  };\n}\n\nexport function shouldParseToolCall(\n  params: ChatCompletionCreateParams | null | undefined,\n  toolCall: ChatCompletionMessageFunctionToolCall,\n): boolean {\n  if (!params || !('tools' in params) || !params.tools) {\n    return false;\n  }\n\n  const inputTool = params.tools?.find(\n    (inputTool) =>\n      isChatCompletionFunctionTool(inputTool) && inputTool.function?.name === toolCall.function.name,\n  );\n  return (\n    isChatCompletionFunctionTool(inputTool) &&\n    (isAutoParsableTool(inputTool) || inputTool?.function.strict || false)\n  );\n}\n\nexport function hasAutoParseableInput(params: AnyChatCompletionCreateParams): boolean {\n  if (isAutoParsableResponseFormat(params.response_format)) {\n    return true;\n  }\n\n  return (\n    params.tools?.some(\n      (t) => isAutoParsableTool(t) || (t.type === 'function' && t.function.strict === true),\n    ) ?? false\n  );\n}\n\nexport function assertToolCallsAreChatCompletionFunctionToolCalls(\n  toolCalls: ChatCompletionMessage['tool_calls'],\n): asserts toolCalls is ChatCompletionMessageFunctionToolCall[] {\n  for (const toolCall of toolCalls || []) {\n    if (toolCall.type !== 'function') {\n      throw new OpenAIError(\n        `Currently only \\`function\\` tool calls are supported; Received \\`${toolCall.type}\\``,\n      );\n    }\n  }\n}\n\nexport function validateInputTools(tools: ChatCompletionCreateParamsBase['tools']) {\n  for (const tool of tools ?? []) {\n    if (tool.type !== 'function') {\n      throw new OpenAIError(\n        `Currently only \\`function\\` tool types support auto-parsing; Received \\`${tool.type}\\``,\n      );\n    }\n\n    if (tool.function.strict !== true) {\n      throw new OpenAIError(\n        `The \\`${tool.function.name}\\` tool is not marked with \\`strict: true\\`. Only strict function tools can be auto-parsed`,\n      );\n    }\n  }\n}\n", "import {\n  type ChatCompletionAssistantMessageParam,\n  type ChatCompletionMessageParam,\n  type ChatCompletionToolMessageParam,\n} from '../resources';\n\nexport const isAssistantMessage = (\n  message: ChatCompletionMessageParam | null | undefined,\n): message is ChatCompletionAssistantMessageParam => {\n  return message?.role === 'assistant';\n};\n\nexport const isToolMessage = (\n  message: ChatCompletionMessageParam | null | undefined,\n): message is ChatCompletionToolMessageParam => {\n  return message?.role === 'tool';\n};\n\nexport function isPresent<T>(obj: T | null | undefined): obj is T {\n  return obj != null;\n}\n", "import { APIUserAbortError, OpenAIError } from '../error';\n\nexport class EventStream<EventTypes extends BaseEvents> {\n  controller: AbortController = new AbortController();\n\n  #connectedPromise: Promise<void>;\n  #resolveConnectedPromise: () => void = () => {};\n  #rejectConnectedPromise: (error: OpenAIError) => void = () => {};\n\n  #endPromise: Promise<void>;\n  #resolveEndPromise: () => void = () => {};\n  #rejectEndPromise: (error: OpenAIError) => void = () => {};\n\n  #listeners: {\n    [Event in keyof EventTypes]?: EventListeners<EventTypes, Event>;\n  } = {};\n\n  #ended = false;\n  #errored = false;\n  #aborted = false;\n  #catchingPromiseCreated = false;\n\n  constructor() {\n    this.#connectedPromise = new Promise<void>((resolve, reject) => {\n      this.#resolveConnectedPromise = resolve;\n      this.#rejectConnectedPromise = reject;\n    });\n\n    this.#endPromise = new Promise<void>((resolve, reject) => {\n      this.#resolveEndPromise = resolve;\n      this.#rejectEndPromise = reject;\n    });\n\n    // Don't let these promises cause unhandled rejection errors.\n    // we will manually cause an unhandled rejection error later\n    // if the user hasn't registered any error listener or called\n    // any promise-returning method.\n    this.#connectedPromise.catch(() => {});\n    this.#endPromise.catch(() => {});\n  }\n\n  protected _run(this: EventStream<EventTypes>, executor: () => Promise<any>) {\n    // Unfortunately if we call `executor()` immediately we get runtime errors about\n    // references to `this` before the `super()` constructor call returns.\n    setTimeout(() => {\n      executor().then(() => {\n        this._emitFinal();\n        this._emit('end');\n      }, this.#handleError.bind(this));\n    }, 0);\n  }\n\n  protected _connected(this: EventStream<EventTypes>) {\n    if (this.ended) return;\n    this.#resolveConnectedPromise();\n    this._emit('connect');\n  }\n\n  get ended(): boolean {\n    return this.#ended;\n  }\n\n  get errored(): boolean {\n    return this.#errored;\n  }\n\n  get aborted(): boolean {\n    return this.#aborted;\n  }\n\n  abort() {\n    this.controller.abort();\n  }\n\n  /**\n   * Adds the listener function to the end of the listeners array for the event.\n   * No checks are made to see if the listener has already been added. Multiple calls passing\n   * the same combination of event and listener will result in the listener being added, and\n   * called, multiple times.\n   * @returns this ChatCompletionStream, so that calls can be chained\n   */\n  on<Event extends keyof EventTypes>(event: Event, listener: EventListener<EventTypes, Event>): this {\n    const listeners: EventListeners<EventTypes, Event> =\n      this.#listeners[event] || (this.#listeners[event] = []);\n    listeners.push({ listener });\n    return this;\n  }\n\n  /**\n   * Removes the specified listener from the listener array for the event.\n   * off() will remove, at most, one instance of a listener from the listener array. If any single\n   * listener has been added multiple times to the listener array for the specified event, then\n   * off() must be called multiple times to remove each instance.\n   * @returns this ChatCompletionStream, so that calls can be chained\n   */\n  off<Event extends keyof EventTypes>(event: Event, listener: EventListener<EventTypes, Event>): this {\n    const listeners = this.#listeners[event];\n    if (!listeners) return this;\n    const index = listeners.findIndex((l) => l.listener === listener);\n    if (index >= 0) listeners.splice(index, 1);\n    return this;\n  }\n\n  /**\n   * Adds a one-time listener function for the event. The next time the event is triggered,\n   * this listener is removed and then invoked.\n   * @returns this ChatCompletionStream, so that calls can be chained\n   */\n  once<Event extends keyof EventTypes>(event: Event, listener: EventListener<EventTypes, Event>): this {\n    const listeners: EventListeners<EventTypes, Event> =\n      this.#listeners[event] || (this.#listeners[event] = []);\n    listeners.push({ listener, once: true });\n    return this;\n  }\n\n  /**\n   * This is similar to `.once()`, but returns a Promise that resolves the next time\n   * the event is triggered, instead of calling a listener callback.\n   * @returns a Promise that resolves the next time given event is triggered,\n   * or rejects if an error is emitted.  (If you request the 'error' event,\n   * returns a promise that resolves with the error).\n   *\n   * Example:\n   *\n   *   const message = await stream.emitted('message') // rejects if the stream errors\n   */\n  emitted<Event extends keyof EventTypes>(\n    event: Event,\n  ): Promise<\n    EventParameters<EventTypes, Event> extends [infer Param] ? Param\n    : EventParameters<EventTypes, Event> extends [] ? void\n    : EventParameters<EventTypes, Event>\n  > {\n    return new Promise((resolve, reject) => {\n      this.#catchingPromiseCreated = true;\n      if (event !== 'error') this.once('error', reject);\n      this.once(event, resolve as any);\n    });\n  }\n\n  async done(): Promise<void> {\n    this.#catchingPromiseCreated = true;\n    await this.#endPromise;\n  }\n\n  #handleError(this: EventStream<EventTypes>, error: unknown) {\n    this.#errored = true;\n    if (error instanceof Error && error.name === 'AbortError') {\n      error = new APIUserAbortError();\n    }\n    if (error instanceof APIUserAbortError) {\n      this.#aborted = true;\n      return this._emit('abort', error);\n    }\n    if (error instanceof OpenAIError) {\n      return this._emit('error', error);\n    }\n    if (error instanceof Error) {\n      const openAIError: OpenAIError = new OpenAIError(error.message);\n      // @ts-ignore\n      openAIError.cause = error;\n      return this._emit('error', openAIError);\n    }\n    return this._emit('error', new OpenAIError(String(error)));\n  }\n\n  _emit<Event extends keyof BaseEvents>(event: Event, ...args: EventParameters<BaseEvents, Event>): void;\n  _emit<Event extends keyof EventTypes>(event: Event, ...args: EventParameters<EventTypes, Event>): void;\n  _emit<Event extends keyof EventTypes>(\n    this: EventStream<EventTypes>,\n    event: Event,\n    ...args: EventParameters<EventTypes, Event>\n  ) {\n    // make sure we don't emit any events after end\n    if (this.#ended) {\n      return;\n    }\n\n    if (event === 'end') {\n      this.#ended = true;\n      this.#resolveEndPromise();\n    }\n\n    const listeners: EventListeners<EventTypes, Event> | undefined = this.#listeners[event];\n    if (listeners) {\n      this.#listeners[event] = listeners.filter((l) => !l.once) as any;\n      listeners.forEach(({ listener }: any) => listener(...(args as any)));\n    }\n\n    if (event === 'abort') {\n      const error = args[0] as APIUserAbortError;\n      if (!this.#catchingPromiseCreated && !listeners?.length) {\n        Promise.reject(error);\n      }\n      this.#rejectConnectedPromise(error);\n      this.#rejectEndPromise(error);\n      this._emit('end');\n      return;\n    }\n\n    if (event === 'error') {\n      // NOTE: _emit('error', error) should only be called from #handleError().\n\n      const error = args[0] as OpenAIError;\n      if (!this.#catchingPromiseCreated && !listeners?.length) {\n        // Trigger an unhandled rejection if the user hasn't registered any error handlers.\n        // If you are seeing stack traces here, make sure to handle errors via either:\n        // - runner.on('error', () => ...)\n        // - await runner.done()\n        // - await runner.finalChatCompletion()\n        // - etc.\n        Promise.reject(error);\n      }\n      this.#rejectConnectedPromise(error);\n      this.#rejectEndPromise(error);\n      this._emit('end');\n    }\n  }\n\n  protected _emitFinal(): void {}\n}\n\ntype EventListener<Events, EventType extends keyof Events> = Events[EventType];\n\ntype EventListeners<Events, EventType extends keyof Events> = Array<{\n  listener: EventListener<Events, EventType>;\n  once?: boolean;\n}>;\n\nexport type EventParameters<Events, EventType extends keyof Events> = {\n  [Event in EventType]: EventListener<Events, EventType> extends (...args: infer P) => any ? P : never;\n}[EventType];\n\nexport interface BaseEvents {\n  connect: () => void;\n  error: (error: OpenAIError) => void;\n  abort: (error: APIUserAbortError) => void;\n  end: () => void;\n}\n", "import { type ChatCompletionRunner } from './ChatCompletionRunner';\nimport { type ChatCompletionStreamingRunner } from './ChatCompletionStreamingRunner';\nimport { JSONSchema } from './jsonschema';\n\ntype PromiseOrValue<T> = T | Promise<T>;\n\nexport type RunnableFunctionWithParse<Args extends object> = {\n  /**\n   * @param args the return value from `parse`.\n   * @param runner the runner evaluating this callback.\n   * @returns a string to send back to OpenAI.\n   */\n  function: (\n    args: Args,\n    runner: ChatCompletionRunner<unknown> | ChatCompletionStreamingRunner<unknown>,\n  ) => PromiseOrValue<unknown>;\n  /**\n   * @param input the raw args from the OpenAI function call.\n   * @returns the parsed arguments to pass to `function`\n   */\n  parse: (input: string) => PromiseOrValue<Args>;\n  /**\n   * The parameters the function accepts, describes as a JSON Schema object.\n   */\n  parameters: JSONSchema;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the function.\n   */\n  description: string;\n  /**\n   * The name of the function to be called. Will default to function.name if omitted.\n   */\n  name?: string | undefined;\n  strict?: boolean | undefined;\n};\n\nexport type RunnableFunctionWithoutParse = {\n  /**\n   * @param args the raw args from the OpenAI function call.\n   * @returns a string to send back to OpenAI\n   */\n  function: (\n    args: string,\n    runner: ChatCompletionRunner<unknown> | ChatCompletionStreamingRunner<unknown>,\n  ) => PromiseOrValue<unknown>;\n  /**\n   * The parameters the function accepts, describes as a JSON Schema object.\n   */\n  parameters: JSONSchema;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the function.\n   */\n  description: string;\n  /**\n   * The name of the function to be called. Will default to function.name if omitted.\n   */\n  name?: string | undefined;\n  strict?: boolean | undefined;\n};\n\nexport type RunnableFunction<Args extends object | string> =\n  Args extends string ? RunnableFunctionWithoutParse\n  : Args extends object ? RunnableFunctionWithParse<Args>\n  : never;\n\nexport type RunnableToolFunction<Args extends object | string> =\n  Args extends string ? RunnableToolFunctionWithoutParse\n  : Args extends object ? RunnableToolFunctionWithParse<Args>\n  : never;\n\nexport type RunnableToolFunctionWithoutParse = {\n  type: 'function';\n  function: RunnableFunctionWithoutParse;\n};\nexport type RunnableToolFunctionWithParse<Args extends object> = {\n  type: 'function';\n  function: RunnableFunctionWithParse<Args>;\n};\n\nexport function isRunnableFunctionWithParse<Args extends object>(\n  fn: any,\n): fn is RunnableFunctionWithParse<Args> {\n  return typeof (fn as any).parse === 'function';\n}\n\nexport type BaseFunctionsArgs = readonly (object | string)[];\n\nexport type RunnableFunctions<FunctionsArgs extends BaseFunctionsArgs> =\n  [any[]] extends [FunctionsArgs] ? readonly RunnableFunction<any>[]\n  : {\n      [Index in keyof FunctionsArgs]: Index extends number ? RunnableFunction<FunctionsArgs[Index]>\n      : FunctionsArgs[Index];\n    };\n\nexport type RunnableTools<FunctionsArgs extends BaseFunctionsArgs> =\n  [any[]] extends [FunctionsArgs] ? readonly RunnableToolFunction<any>[]\n  : {\n      [Index in keyof FunctionsArgs]: Index extends number ? RunnableToolFunction<FunctionsArgs[Index]>\n      : FunctionsArgs[Index];\n    };\n\n/**\n * This is helper class for passing a `function` and `parse` where the `function`\n * argument type matches the `parse` return type.\n */\nexport class ParsingToolFunction<Args extends object> {\n  type: 'function';\n  function: RunnableFunctionWithParse<Args>;\n\n  constructor(input: RunnableFunctionWithParse<Args>) {\n    this.type = 'function';\n    this.function = input;\n  }\n}\n", "import { OpenAIError } from '../error';\nimport type OpenAI from '../index';\nimport type { RequestOptions } from '../internal/request-options';\nimport { isAutoParsableTool, parseChatCompletion } from '../lib/parser';\nimport type {\n  ChatCompletion,\n  ChatCompletionCreateParams,\n  ChatCompletionMessage,\n  ChatCompletionMessageFunctionToolCall,\n  ChatCompletionMessageParam,\n  ChatCompletionTool,\n  ParsedChatCompletion,\n} from '../resources/chat/completions';\nimport type { CompletionUsage } from '../resources/completions';\nimport type { ChatCompletionToolRunnerParams } from './ChatCompletionRunner';\nimport type { ChatCompletionStreamingToolRunnerParams } from './ChatCompletionStreamingRunner';\nimport { isAssistantMessage, isToolMessage } from './chatCompletionUtils';\nimport { BaseEvents, EventStream } from './EventStream';\nimport {\n  isRunnableFunctionWithParse,\n  type BaseFunctionsArgs,\n  type RunnableFunction,\n  type RunnableToolFunction,\n} from './RunnableFunction';\n\nconst DEFAULT_MAX_CHAT_COMPLETIONS = 10;\nexport interface RunnerOptions extends RequestOptions {\n  /** How many requests to make before canceling. Default 10. */\n  maxChatCompletions?: number;\n}\n\nexport class AbstractChatCompletionRunner<\n  EventTypes extends AbstractChatCompletionRunnerEvents,\n  ParsedT,\n> extends EventStream<EventTypes> {\n  protected _chatCompletions: ParsedChatCompletion<ParsedT>[] = [];\n  messages: ChatCompletionMessageParam[] = [];\n\n  protected _addChatCompletion(\n    this: AbstractChatCompletionRunner<AbstractChatCompletionRunnerEvents, ParsedT>,\n    chatCompletion: ParsedChatCompletion<ParsedT>,\n  ): ParsedChatCompletion<ParsedT> {\n    this._chatCompletions.push(chatCompletion);\n    this._emit('chatCompletion', chatCompletion);\n    const message = chatCompletion.choices[0]?.message;\n    if (message) this._addMessage(message as ChatCompletionMessageParam);\n    return chatCompletion;\n  }\n\n  protected _addMessage(\n    this: AbstractChatCompletionRunner<AbstractChatCompletionRunnerEvents, ParsedT>,\n    message: ChatCompletionMessageParam,\n    emit = true,\n  ) {\n    if (!('content' in message)) message.content = null;\n\n    this.messages.push(message);\n\n    if (emit) {\n      this._emit('message', message);\n      if (isToolMessage(message) && message.content) {\n        // Note, this assumes that {role: 'tool', content: } is always the result of a call of tool of type=function.\n        this._emit('functionToolCallResult', message.content as string);\n      } else if (isAssistantMessage(message) && message.tool_calls) {\n        for (const tool_call of message.tool_calls) {\n          if (tool_call.type === 'function') {\n            this._emit('functionToolCall', tool_call.function);\n          }\n        }\n      }\n    }\n  }\n\n  /**\n   * @returns a promise that resolves with the final ChatCompletion, or rejects\n   * if an error occurred or the stream ended prematurely without producing a ChatCompletion.\n   */\n  async finalChatCompletion(): Promise<ParsedChatCompletion<ParsedT>> {\n    await this.done();\n    const completion = this._chatCompletions[this._chatCompletions.length - 1];\n    if (!completion) throw new OpenAIError('stream ended without producing a ChatCompletion');\n    return completion;\n  }\n\n  #getFinalContent(): string | null {\n    return this.#getFinalMessage().content ?? null;\n  }\n\n  /**\n   * @returns a promise that resolves with the content of the final ChatCompletionMessage, or rejects\n   * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n   */\n  async finalContent(): Promise<string | null> {\n    await this.done();\n    return this.#getFinalContent();\n  }\n\n  #getFinalMessage(): ChatCompletionMessage {\n    let i = this.messages.length;\n    while (i-- > 0) {\n      const message = this.messages[i];\n      if (isAssistantMessage(message)) {\n        // TODO: support audio here\n        const ret: Omit<ChatCompletionMessage, 'audio'> = {\n          ...message,\n          content: (message as ChatCompletionMessage).content ?? null,\n          refusal: (message as ChatCompletionMessage).refusal ?? null,\n        };\n        return ret;\n      }\n    }\n    throw new OpenAIError('stream ended without producing a ChatCompletionMessage with role=assistant');\n  }\n\n  /**\n   * @returns a promise that resolves with the the final assistant ChatCompletionMessage response,\n   * or rejects if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n   */\n  async finalMessage(): Promise<ChatCompletionMessage> {\n    await this.done();\n    return this.#getFinalMessage();\n  }\n\n  #getFinalFunctionToolCall(): ChatCompletionMessageFunctionToolCall.Function | undefined {\n    for (let i = this.messages.length - 1; i >= 0; i--) {\n      const message = this.messages[i];\n      if (isAssistantMessage(message) && message?.tool_calls?.length) {\n        return message.tool_calls.filter((x) => x.type === 'function').at(-1)?.function;\n      }\n    }\n\n    return;\n  }\n\n  /**\n   * @returns a promise that resolves with the content of the final FunctionCall, or rejects\n   * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n   */\n  async finalFunctionToolCall(): Promise<ChatCompletionMessageFunctionToolCall.Function | undefined> {\n    await this.done();\n    return this.#getFinalFunctionToolCall();\n  }\n\n  #getFinalFunctionToolCallResult(): string | undefined {\n    for (let i = this.messages.length - 1; i >= 0; i--) {\n      const message = this.messages[i];\n      if (\n        isToolMessage(message) &&\n        message.content != null &&\n        typeof message.content === 'string' &&\n        this.messages.some(\n          (x) =>\n            x.role === 'assistant' &&\n            x.tool_calls?.some((y) => y.type === 'function' && y.id === message.tool_call_id),\n        )\n      ) {\n        return message.content;\n      }\n    }\n\n    return;\n  }\n\n  async finalFunctionToolCallResult(): Promise<string | undefined> {\n    await this.done();\n    return this.#getFinalFunctionToolCallResult();\n  }\n\n  #calculateTotalUsage(): CompletionUsage {\n    const total: CompletionUsage = {\n      completion_tokens: 0,\n      prompt_tokens: 0,\n      total_tokens: 0,\n    };\n    for (const { usage } of this._chatCompletions) {\n      if (usage) {\n        total.completion_tokens += usage.completion_tokens;\n        total.prompt_tokens += usage.prompt_tokens;\n        total.total_tokens += usage.total_tokens;\n      }\n    }\n    return total;\n  }\n\n  async totalUsage(): Promise<CompletionUsage> {\n    await this.done();\n    return this.#calculateTotalUsage();\n  }\n\n  allChatCompletions(): ChatCompletion[] {\n    return [...this._chatCompletions];\n  }\n\n  protected override _emitFinal(\n    this: AbstractChatCompletionRunner<AbstractChatCompletionRunnerEvents, ParsedT>,\n  ) {\n    const completion = this._chatCompletions[this._chatCompletions.length - 1];\n    if (completion) this._emit('finalChatCompletion', completion);\n    const finalMessage = this.#getFinalMessage();\n    if (finalMessage) this._emit('finalMessage', finalMessage);\n    const finalContent = this.#getFinalContent();\n    if (finalContent) this._emit('finalContent', finalContent);\n\n    const finalFunctionCall = this.#getFinalFunctionToolCall();\n    if (finalFunctionCall) this._emit('finalFunctionToolCall', finalFunctionCall);\n\n    const finalFunctionCallResult = this.#getFinalFunctionToolCallResult();\n    if (finalFunctionCallResult != null) this._emit('finalFunctionToolCallResult', finalFunctionCallResult);\n\n    if (this._chatCompletions.some((c) => c.usage)) {\n      this._emit('totalUsage', this.#calculateTotalUsage());\n    }\n  }\n\n  #validateParams(params: ChatCompletionCreateParams): void {\n    if (params.n != null && params.n > 1) {\n      throw new OpenAIError(\n        'ChatCompletion convenience helpers only support n=1 at this time. To use n>1, please use chat.completions.create() directly.',\n      );\n    }\n  }\n\n  protected async _createChatCompletion(\n    client: OpenAI,\n    params: ChatCompletionCreateParams,\n    options?: RequestOptions,\n  ): Promise<ParsedChatCompletion<ParsedT>> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this.#validateParams(params);\n\n    const chatCompletion = await client.chat.completions.create(\n      { ...params, stream: false },\n      { ...options, signal: this.controller.signal },\n    );\n    this._connected();\n    return this._addChatCompletion(parseChatCompletion(chatCompletion, params));\n  }\n\n  protected async _runChatCompletion(\n    client: OpenAI,\n    params: ChatCompletionCreateParams,\n    options?: RequestOptions,\n  ): Promise<ChatCompletion> {\n    for (const message of params.messages) {\n      this._addMessage(message, false);\n    }\n    return await this._createChatCompletion(client, params, options);\n  }\n\n  protected async _runTools<FunctionsArgs extends BaseFunctionsArgs>(\n    client: OpenAI,\n    params:\n      | ChatCompletionToolRunnerParams<FunctionsArgs>\n      | ChatCompletionStreamingToolRunnerParams<FunctionsArgs>,\n    options?: RunnerOptions,\n  ) {\n    const role = 'tool' as const;\n    const { tool_choice = 'auto', stream, ...restParams } = params;\n    const singleFunctionToCall =\n      typeof tool_choice !== 'string' && tool_choice.type === 'function' && tool_choice?.function?.name;\n    const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};\n\n    // TODO(someday): clean this logic up\n    const inputTools = params.tools.map((tool): RunnableToolFunction<any> => {\n      if (isAutoParsableTool(tool)) {\n        if (!tool.$callback) {\n          throw new OpenAIError('Tool given to `.runTools()` that does not have an associated function');\n        }\n\n        return {\n          type: 'function',\n          function: {\n            function: tool.$callback,\n            name: tool.function.name,\n            description: tool.function.description || '',\n            parameters: tool.function.parameters as any,\n            parse: tool.$parseRaw,\n            strict: true,\n          },\n        };\n      }\n\n      return tool as any as RunnableToolFunction<any>;\n    });\n\n    const functionsByName: Record<string, RunnableFunction<any>> = {};\n    for (const f of inputTools) {\n      if (f.type === 'function') {\n        functionsByName[f.function.name || f.function.function.name] = f.function;\n      }\n    }\n\n    const tools: ChatCompletionTool[] =\n      'tools' in params ?\n        inputTools.map((t) =>\n          t.type === 'function' ?\n            {\n              type: 'function',\n              function: {\n                name: t.function.name || t.function.function.name,\n                parameters: t.function.parameters as Record<string, unknown>,\n                description: t.function.description,\n                strict: t.function.strict,\n              },\n            }\n          : (t as unknown as ChatCompletionTool),\n        )\n      : (undefined as any);\n\n    for (const message of params.messages) {\n      this._addMessage(message, false);\n    }\n\n    for (let i = 0; i < maxChatCompletions; ++i) {\n      const chatCompletion: ChatCompletion = await this._createChatCompletion(\n        client,\n        {\n          ...restParams,\n          tool_choice,\n          tools,\n          messages: [...this.messages],\n        },\n        options,\n      );\n      const message = chatCompletion.choices[0]?.message;\n      if (!message) {\n        throw new OpenAIError(`missing message in ChatCompletion response`);\n      }\n      if (!message.tool_calls?.length) {\n        return;\n      }\n\n      for (const tool_call of message.tool_calls) {\n        if (tool_call.type !== 'function') continue;\n        const tool_call_id = tool_call.id;\n        const { name, arguments: args } = tool_call.function;\n        const fn = functionsByName[name];\n\n        if (!fn) {\n          const content = `Invalid tool_call: ${JSON.stringify(name)}. Available options are: ${Object.keys(\n            functionsByName,\n          )\n            .map((name) => JSON.stringify(name))\n            .join(', ')}. Please try again`;\n\n          this._addMessage({ role, tool_call_id, content });\n          continue;\n        } else if (singleFunctionToCall && singleFunctionToCall !== name) {\n          const content = `Invalid tool_call: ${JSON.stringify(name)}. ${JSON.stringify(\n            singleFunctionToCall,\n          )} requested. Please try again`;\n\n          this._addMessage({ role, tool_call_id, content });\n          continue;\n        }\n\n        let parsed;\n        try {\n          parsed = isRunnableFunctionWithParse(fn) ? await fn.parse(args) : args;\n        } catch (error) {\n          const content = error instanceof Error ? error.message : String(error);\n          this._addMessage({ role, tool_call_id, content });\n          continue;\n        }\n\n        // @ts-expect-error it can't rule out `never` type.\n        const rawContent = await fn.function(parsed, this);\n        const content = this.#stringifyFunctionCallResult(rawContent);\n        this._addMessage({ role, tool_call_id, content });\n\n        if (singleFunctionToCall) {\n          return;\n        }\n      }\n    }\n\n    return;\n  }\n\n  #stringifyFunctionCallResult(rawContent: unknown): string {\n    return (\n      typeof rawContent === 'string' ? rawContent\n      : rawContent === undefined ? 'undefined'\n      : JSON.stringify(rawContent)\n    );\n  }\n}\n\nexport interface AbstractChatCompletionRunnerEvents extends BaseEvents {\n  functionToolCall: (functionCall: ChatCompletionMessageFunctionToolCall.Function) => void;\n  message: (message: ChatCompletionMessageParam) => void;\n  chatCompletion: (completion: ChatCompletion) => void;\n  finalContent: (contentSnapshot: string) => void;\n  finalMessage: (message: ChatCompletionMessageParam) => void;\n  finalChatCompletion: (completion: ChatCompletion) => void;\n  finalFunctionToolCall: (functionCall: ChatCompletionMessageFunctionToolCall.Function) => void;\n  functionToolCallResult: (content: string) => void;\n  finalFunctionToolCallResult: (content: string) => void;\n  totalUsage: (usage: CompletionUsage) => void;\n}\n", "import {\n  type ChatCompletionMessageParam,\n  type ChatCompletionCreateParamsNonStreaming,\n} from '../resources/chat/completions';\nimport { type BaseFunctionsArgs, RunnableTools } from './RunnableFunction';\nimport {\n  AbstractChatCompletionRunner,\n  AbstractChatCompletionRunnerEvents,\n  RunnerOptions,\n} from './AbstractChatCompletionRunner';\nimport { isAssistantMessage } from './chatCompletionUtils';\nimport OpenAI from '../index';\nimport { AutoParseableTool } from '../lib/parser';\n\nexport interface ChatCompletionRunnerEvents extends AbstractChatCompletionRunnerEvents {\n  content: (content: string) => void;\n}\n\nexport type ChatCompletionToolRunnerParams<FunctionsArgs extends BaseFunctionsArgs> = Omit<\n  ChatCompletionCreateParamsNonStreaming,\n  'tools'\n> & {\n  tools: RunnableTools<FunctionsArgs> | AutoParseableTool<any, true>[];\n};\n\nexport class ChatCompletionRunner<ParsedT = null> extends AbstractChatCompletionRunner<\n  ChatCompletionRunnerEvents,\n  ParsedT\n> {\n  static runTools<ParsedT>(\n    client: OpenAI,\n    params: ChatCompletionToolRunnerParams<any[]>,\n    options?: RunnerOptions,\n  ): ChatCompletionRunner<ParsedT> {\n    const runner = new ChatCompletionRunner<ParsedT>();\n    const opts = {\n      ...options,\n      headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runTools' },\n    };\n    runner._run(() => runner._runTools(client, params, opts));\n    return runner;\n  }\n\n  override _addMessage(\n    this: ChatCompletionRunner<ParsedT>,\n    message: ChatCompletionMessageParam,\n    emit: boolean = true,\n  ) {\n    super._addMessage(message, emit);\n    if (isAssistantMessage(message) && message.content) {\n      this._emit('content', message.content as string);\n    }\n  }\n}\n", "const STR = 0b000000001;\nconst NUM = 0b000000010;\nconst ARR = 0b000000100;\nconst OBJ = 0b000001000;\nconst NULL = 0b000010000;\nconst BOOL = 0b000100000;\nconst NAN = 0b001000000;\nconst INFINITY = 0b010000000;\nconst MINUS_INFINITY = 0b100000000;\n\nconst INF = INFINITY | MINUS_INFINITY;\nconst SPECIAL = NULL | BOOL | INF | NAN;\nconst ATOM = STR | NUM | SPECIAL;\nconst COLLECTION = ARR | OBJ;\nconst ALL = ATOM | COLLECTION;\n\nconst Allow = {\n  STR,\n  NUM,\n  ARR,\n  OBJ,\n  NULL,\n  BOOL,\n  NAN,\n  INFINITY,\n  MINUS_INFINITY,\n  INF,\n  SPECIAL,\n  ATOM,\n  COLLECTION,\n  ALL,\n};\n\n// The JSON string segment was unable to be parsed completely\nclass PartialJSON extends Error {}\n\nclass MalformedJSON extends Error {}\n\n/**\n * Parse incomplete JSON\n * @param {string} jsonString Partial JSON to be parsed\n * @param {number} allowPartial Specify what types are allowed to be partial, see {@link Allow} for details\n * @returns The parsed JSON\n * @throws {PartialJSON} If the JSON is incomplete (related to the `allow` parameter)\n * @throws {MalformedJSON} If the JSON is malformed\n */\nfunction parseJSON(jsonString: string, allowPartial: number = Allow.ALL): any {\n  if (typeof jsonString !== 'string') {\n    throw new TypeError(`expecting str, got ${typeof jsonString}`);\n  }\n  if (!jsonString.trim()) {\n    throw new Error(`${jsonString} is empty`);\n  }\n  return _parseJSON(jsonString.trim(), allowPartial);\n}\n\nconst _parseJSON = (jsonString: string, allow: number) => {\n  const length = jsonString.length;\n  let index = 0;\n\n  const markPartialJSON = (msg: string) => {\n    throw new PartialJSON(`${msg} at position ${index}`);\n  };\n\n  const throwMalformedError = (msg: string) => {\n    throw new MalformedJSON(`${msg} at position ${index}`);\n  };\n\n  const parseAny: () => any = () => {\n    skipBlank();\n    if (index >= length) markPartialJSON('Unexpected end of input');\n    if (jsonString[index] === '\"') return parseStr();\n    if (jsonString[index] === '{') return parseObj();\n    if (jsonString[index] === '[') return parseArr();\n    if (\n      jsonString.substring(index, index + 4) === 'null' ||\n      (Allow.NULL & allow && length - index < 4 && 'null'.startsWith(jsonString.substring(index)))\n    ) {\n      index += 4;\n      return null;\n    }\n    if (\n      jsonString.substring(index, index + 4) === 'true' ||\n      (Allow.BOOL & allow && length - index < 4 && 'true'.startsWith(jsonString.substring(index)))\n    ) {\n      index += 4;\n      return true;\n    }\n    if (\n      jsonString.substring(index, index + 5) === 'false' ||\n      (Allow.BOOL & allow && length - index < 5 && 'false'.startsWith(jsonString.substring(index)))\n    ) {\n      index += 5;\n      return false;\n    }\n    if (\n      jsonString.substring(index, index + 8) === 'Infinity' ||\n      (Allow.INFINITY & allow && length - index < 8 && 'Infinity'.startsWith(jsonString.substring(index)))\n    ) {\n      index += 8;\n      return Infinity;\n    }\n    if (\n      jsonString.substring(index, index + 9) === '-Infinity' ||\n      (Allow.MINUS_INFINITY & allow &&\n        1 < length - index &&\n        length - index < 9 &&\n        '-Infinity'.startsWith(jsonString.substring(index)))\n    ) {\n      index += 9;\n      return -Infinity;\n    }\n    if (\n      jsonString.substring(index, index + 3) === 'NaN' ||\n      (Allow.NAN & allow && length - index < 3 && 'NaN'.startsWith(jsonString.substring(index)))\n    ) {\n      index += 3;\n      return NaN;\n    }\n    return parseNum();\n  };\n\n  const parseStr: () => string = () => {\n    const start = index;\n    let escape = false;\n    index++; // skip initial quote\n    while (index < length && (jsonString[index] !== '\"' || (escape && jsonString[index - 1] === '\\\\'))) {\n      escape = jsonString[index] === '\\\\' ? !escape : false;\n      index++;\n    }\n    if (jsonString.charAt(index) == '\"') {\n      try {\n        return JSON.parse(jsonString.substring(start, ++index - Number(escape)));\n      } catch (e) {\n        throwMalformedError(String(e));\n      }\n    } else if (Allow.STR & allow) {\n      try {\n        return JSON.parse(jsonString.substring(start, index - Number(escape)) + '\"');\n      } catch (e) {\n        // SyntaxError: Invalid escape sequence\n        return JSON.parse(jsonString.substring(start, jsonString.lastIndexOf('\\\\')) + '\"');\n      }\n    }\n    markPartialJSON('Unterminated string literal');\n  };\n\n  const parseObj = () => {\n    index++; // skip initial brace\n    skipBlank();\n    const obj: Record<string, any> = {};\n    try {\n      while (jsonString[index] !== '}') {\n        skipBlank();\n        if (index >= length && Allow.OBJ & allow) return obj;\n        const key = parseStr();\n        skipBlank();\n        index++; // skip colon\n        try {\n          const value = parseAny();\n          Object.defineProperty(obj, key, { value, writable: true, enumerable: true, configurable: true });\n        } catch (e) {\n          if (Allow.OBJ & allow) return obj;\n          else throw e;\n        }\n        skipBlank();\n        if (jsonString[index] === ',') index++; // skip comma\n      }\n    } catch (e) {\n      if (Allow.OBJ & allow) return obj;\n      else markPartialJSON(\"Expected '}' at end of object\");\n    }\n    index++; // skip final brace\n    return obj;\n  };\n\n  const parseArr = () => {\n    index++; // skip initial bracket\n    const arr = [];\n    try {\n      while (jsonString[index] !== ']') {\n        arr.push(parseAny());\n        skipBlank();\n        if (jsonString[index] === ',') {\n          index++; // skip comma\n        }\n      }\n    } catch (e) {\n      if (Allow.ARR & allow) {\n        return arr;\n      }\n      markPartialJSON(\"Expected ']' at end of array\");\n    }\n    index++; // skip final bracket\n    return arr;\n  };\n\n  const parseNum = () => {\n    if (index === 0) {\n      if (jsonString === '-' && Allow.NUM & allow) markPartialJSON(\"Not sure what '-' is\");\n      try {\n        return JSON.parse(jsonString);\n      } catch (e) {\n        if (Allow.NUM & allow) {\n          try {\n            if ('.' === jsonString[jsonString.length - 1])\n              return JSON.parse(jsonString.substring(0, jsonString.lastIndexOf('.')));\n            return JSON.parse(jsonString.substring(0, jsonString.lastIndexOf('e')));\n          } catch (e) {}\n        }\n        throwMalformedError(String(e));\n      }\n    }\n\n    const start = index;\n\n    if (jsonString[index] === '-') index++;\n    while (jsonString[index] && !',]}'.includes(jsonString[index]!)) index++;\n\n    if (index == length && !(Allow.NUM & allow)) markPartialJSON('Unterminated number literal');\n\n    try {\n      return JSON.parse(jsonString.substring(start, index));\n    } catch (e) {\n      if (jsonString.substring(start, index) === '-' && Allow.NUM & allow)\n        markPartialJSON(\"Not sure what '-' is\");\n      try {\n        return JSON.parse(jsonString.substring(start, jsonString.lastIndexOf('e')));\n      } catch (e) {\n        throwMalformedError(String(e));\n      }\n    }\n  };\n\n  const skipBlank = () => {\n    while (index < length && ' \\n\\r\\t'.includes(jsonString[index]!)) {\n      index++;\n    }\n  };\n\n  return parseAny();\n};\n\n// using this function with malformed JSON is undefined behavior\nconst partialParse = (input: string) => parseJSON(input, Allow.ALL ^ Allow.NUM);\n\nexport { partialParse, PartialJSON, MalformedJSON };\n", "import { partialParse } from '../_vendor/partial-json-parser/parser';\nimport {\n  APIUserAbortError,\n  ContentFilterFinishReasonError,\n  LengthFinishReasonError,\n  OpenAIError,\n} from '../error';\nimport OpenAI from '../index';\nimport { RequestOptions } from '../internal/request-options';\nimport { type ReadableStream } from '../internal/shim-types';\nimport {\n  AutoParseableResponseFormat,\n  hasAutoParseableInput,\n  isAutoParsableResponseFormat,\n  isAutoParsableTool,\n  isChatCompletionFunctionTool,\n  maybeParseChatCompletion,\n  shouldParseToolCall,\n} from '../lib/parser';\nimport { ChatCompletionFunctionTool, ParsedChatCompletion } from '../resources/chat/completions';\nimport {\n  ChatCompletionTokenLogprob,\n  type ChatCompletion,\n  type ChatCompletionChunk,\n  type ChatCompletionCreateParams,\n  type ChatCompletionCreateParamsBase,\n  type ChatCompletionCreateParamsStreaming,\n  type ChatCompletionRole,\n} from '../resources/chat/completions/completions';\nimport { Stream } from '../streaming';\nimport {\n  AbstractChatCompletionRunner,\n  type AbstractChatCompletionRunnerEvents,\n} from './AbstractChatCompletionRunner';\n\nexport interface ContentDeltaEvent {\n  delta: string;\n  snapshot: string;\n  parsed: unknown | null;\n}\n\nexport interface ContentDoneEvent<ParsedT = null> {\n  content: string;\n  parsed: ParsedT | null;\n}\n\nexport interface RefusalDeltaEvent {\n  delta: string;\n  snapshot: string;\n}\n\nexport interface RefusalDoneEvent {\n  refusal: string;\n}\n\nexport interface FunctionToolCallArgumentsDeltaEvent {\n  name: string;\n\n  index: number;\n\n  arguments: string;\n\n  parsed_arguments: unknown;\n\n  arguments_delta: string;\n}\n\nexport interface FunctionToolCallArgumentsDoneEvent {\n  name: string;\n\n  index: number;\n\n  arguments: string;\n\n  parsed_arguments: unknown;\n}\n\nexport interface LogProbsContentDeltaEvent {\n  content: Array<ChatCompletionTokenLogprob>;\n  snapshot: Array<ChatCompletionTokenLogprob>;\n}\n\nexport interface LogProbsContentDoneEvent {\n  content: Array<ChatCompletionTokenLogprob>;\n}\n\nexport interface LogProbsRefusalDeltaEvent {\n  refusal: Array<ChatCompletionTokenLogprob>;\n  snapshot: Array<ChatCompletionTokenLogprob>;\n}\n\nexport interface LogProbsRefusalDoneEvent {\n  refusal: Array<ChatCompletionTokenLogprob>;\n}\n\nexport interface ChatCompletionStreamEvents<ParsedT = null> extends AbstractChatCompletionRunnerEvents {\n  content: (contentDelta: string, contentSnapshot: string) => void;\n  chunk: (chunk: ChatCompletionChunk, snapshot: ChatCompletionSnapshot) => void;\n\n  'content.delta': (props: ContentDeltaEvent) => void;\n  'content.done': (props: ContentDoneEvent<ParsedT>) => void;\n\n  'refusal.delta': (props: RefusalDeltaEvent) => void;\n  'refusal.done': (props: RefusalDoneEvent) => void;\n\n  'tool_calls.function.arguments.delta': (props: FunctionToolCallArgumentsDeltaEvent) => void;\n  'tool_calls.function.arguments.done': (props: FunctionToolCallArgumentsDoneEvent) => void;\n\n  'logprobs.content.delta': (props: LogProbsContentDeltaEvent) => void;\n  'logprobs.content.done': (props: LogProbsContentDoneEvent) => void;\n\n  'logprobs.refusal.delta': (props: LogProbsRefusalDeltaEvent) => void;\n  'logprobs.refusal.done': (props: LogProbsRefusalDoneEvent) => void;\n}\n\nexport type ChatCompletionStreamParams = Omit<ChatCompletionCreateParamsBase, 'stream'> & {\n  stream?: true;\n};\n\ninterface ChoiceEventState {\n  content_done: boolean;\n  refusal_done: boolean;\n  logprobs_content_done: boolean;\n  logprobs_refusal_done: boolean;\n  current_tool_call_index: number | null;\n  done_tool_calls: Set<number>;\n}\n\nexport class ChatCompletionStream<ParsedT = null>\n  extends AbstractChatCompletionRunner<ChatCompletionStreamEvents<ParsedT>, ParsedT>\n  implements AsyncIterable<ChatCompletionChunk>\n{\n  #params: ChatCompletionCreateParams | null;\n  #choiceEventStates: ChoiceEventState[];\n  #currentChatCompletionSnapshot: ChatCompletionSnapshot | undefined;\n\n  constructor(params: ChatCompletionCreateParams | null) {\n    super();\n    this.#params = params;\n    this.#choiceEventStates = [];\n  }\n\n  get currentChatCompletionSnapshot(): ChatCompletionSnapshot | undefined {\n    return this.#currentChatCompletionSnapshot;\n  }\n\n  /**\n   * Intended for use on the frontend, consuming a stream produced with\n   * `.toReadableStream()` on the backend.\n   *\n   * Note that messages sent to the model do not appear in `.on('message')`\n   * in this context.\n   */\n  static fromReadableStream(stream: ReadableStream): ChatCompletionStream<null> {\n    const runner = new ChatCompletionStream(null);\n    runner._run(() => runner._fromReadableStream(stream));\n    return runner;\n  }\n\n  static createChatCompletion<ParsedT>(\n    client: OpenAI,\n    params: ChatCompletionStreamParams,\n    options?: RequestOptions,\n  ): ChatCompletionStream<ParsedT> {\n    const runner = new ChatCompletionStream<ParsedT>(params as ChatCompletionCreateParamsStreaming);\n    runner._run(() =>\n      runner._runChatCompletion(\n        client,\n        { ...params, stream: true },\n        { ...options, headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' } },\n      ),\n    );\n    return runner;\n  }\n\n  #beginRequest() {\n    if (this.ended) return;\n    this.#currentChatCompletionSnapshot = undefined;\n  }\n\n  #getChoiceEventState(choice: ChatCompletionSnapshot.Choice): ChoiceEventState {\n    let state = this.#choiceEventStates[choice.index];\n    if (state) {\n      return state;\n    }\n\n    state = {\n      content_done: false,\n      refusal_done: false,\n      logprobs_content_done: false,\n      logprobs_refusal_done: false,\n      done_tool_calls: new Set(),\n      current_tool_call_index: null,\n    };\n    this.#choiceEventStates[choice.index] = state;\n    return state;\n  }\n\n  #addChunk(this: ChatCompletionStream<ParsedT>, chunk: ChatCompletionChunk) {\n    if (this.ended) return;\n\n    const completion = this.#accumulateChatCompletion(chunk);\n    this._emit('chunk', chunk, completion);\n\n    for (const choice of chunk.choices) {\n      const choiceSnapshot = completion.choices[choice.index]!;\n\n      if (\n        choice.delta.content != null &&\n        choiceSnapshot.message?.role === 'assistant' &&\n        choiceSnapshot.message?.content\n      ) {\n        this._emit('content', choice.delta.content, choiceSnapshot.message.content);\n        this._emit('content.delta', {\n          delta: choice.delta.content,\n          snapshot: choiceSnapshot.message.content,\n          parsed: choiceSnapshot.message.parsed,\n        });\n      }\n\n      if (\n        choice.delta.refusal != null &&\n        choiceSnapshot.message?.role === 'assistant' &&\n        choiceSnapshot.message?.refusal\n      ) {\n        this._emit('refusal.delta', {\n          delta: choice.delta.refusal,\n          snapshot: choiceSnapshot.message.refusal,\n        });\n      }\n\n      if (choice.logprobs?.content != null && choiceSnapshot.message?.role === 'assistant') {\n        this._emit('logprobs.content.delta', {\n          content: choice.logprobs?.content,\n          snapshot: choiceSnapshot.logprobs?.content ?? [],\n        });\n      }\n\n      if (choice.logprobs?.refusal != null && choiceSnapshot.message?.role === 'assistant') {\n        this._emit('logprobs.refusal.delta', {\n          refusal: choice.logprobs?.refusal,\n          snapshot: choiceSnapshot.logprobs?.refusal ?? [],\n        });\n      }\n\n      const state = this.#getChoiceEventState(choiceSnapshot);\n\n      if (choiceSnapshot.finish_reason) {\n        this.#emitContentDoneEvents(choiceSnapshot);\n\n        if (state.current_tool_call_index != null) {\n          this.#emitToolCallDoneEvent(choiceSnapshot, state.current_tool_call_index);\n        }\n      }\n\n      for (const toolCall of choice.delta.tool_calls ?? []) {\n        if (state.current_tool_call_index !== toolCall.index) {\n          this.#emitContentDoneEvents(choiceSnapshot);\n\n          // new tool call started, the previous one is done\n          if (state.current_tool_call_index != null) {\n            this.#emitToolCallDoneEvent(choiceSnapshot, state.current_tool_call_index);\n          }\n        }\n\n        state.current_tool_call_index = toolCall.index;\n      }\n\n      for (const toolCallDelta of choice.delta.tool_calls ?? []) {\n        const toolCallSnapshot = choiceSnapshot.message.tool_calls?.[toolCallDelta.index];\n        if (!toolCallSnapshot?.type) {\n          continue;\n        }\n\n        if (toolCallSnapshot?.type === 'function') {\n          this._emit('tool_calls.function.arguments.delta', {\n            name: toolCallSnapshot.function?.name,\n            index: toolCallDelta.index,\n            arguments: toolCallSnapshot.function.arguments,\n            parsed_arguments: toolCallSnapshot.function.parsed_arguments,\n            arguments_delta: toolCallDelta.function?.arguments ?? '',\n          });\n        } else {\n          assertNever(toolCallSnapshot?.type);\n        }\n      }\n    }\n  }\n\n  #emitToolCallDoneEvent(choiceSnapshot: ChatCompletionSnapshot.Choice, toolCallIndex: number) {\n    const state = this.#getChoiceEventState(choiceSnapshot);\n    if (state.done_tool_calls.has(toolCallIndex)) {\n      // we've already fired the done event\n      return;\n    }\n\n    const toolCallSnapshot = choiceSnapshot.message.tool_calls?.[toolCallIndex];\n    if (!toolCallSnapshot) {\n      throw new Error('no tool call snapshot');\n    }\n    if (!toolCallSnapshot.type) {\n      throw new Error('tool call snapshot missing `type`');\n    }\n\n    if (toolCallSnapshot.type === 'function') {\n      const inputTool = this.#params?.tools?.find(\n        (tool) => isChatCompletionFunctionTool(tool) && tool.function.name === toolCallSnapshot.function.name,\n      ) as ChatCompletionFunctionTool | undefined; // TS doesn't narrow based on isChatCompletionTool\n\n      this._emit('tool_calls.function.arguments.done', {\n        name: toolCallSnapshot.function.name,\n        index: toolCallIndex,\n        arguments: toolCallSnapshot.function.arguments,\n        parsed_arguments:\n          isAutoParsableTool(inputTool) ? inputTool.$parseRaw(toolCallSnapshot.function.arguments)\n          : inputTool?.function.strict ? JSON.parse(toolCallSnapshot.function.arguments)\n          : null,\n      });\n    } else {\n      assertNever(toolCallSnapshot.type);\n    }\n  }\n\n  #emitContentDoneEvents(choiceSnapshot: ChatCompletionSnapshot.Choice) {\n    const state = this.#getChoiceEventState(choiceSnapshot);\n\n    if (choiceSnapshot.message.content && !state.content_done) {\n      state.content_done = true;\n\n      const responseFormat = this.#getAutoParseableResponseFormat();\n\n      this._emit('content.done', {\n        content: choiceSnapshot.message.content,\n        parsed: responseFormat ? responseFormat.$parseRaw(choiceSnapshot.message.content) : (null as any),\n      });\n    }\n\n    if (choiceSnapshot.message.refusal && !state.refusal_done) {\n      state.refusal_done = true;\n\n      this._emit('refusal.done', { refusal: choiceSnapshot.message.refusal });\n    }\n\n    if (choiceSnapshot.logprobs?.content && !state.logprobs_content_done) {\n      state.logprobs_content_done = true;\n\n      this._emit('logprobs.content.done', { content: choiceSnapshot.logprobs.content });\n    }\n\n    if (choiceSnapshot.logprobs?.refusal && !state.logprobs_refusal_done) {\n      state.logprobs_refusal_done = true;\n\n      this._emit('logprobs.refusal.done', { refusal: choiceSnapshot.logprobs.refusal });\n    }\n  }\n\n  #endRequest(): ParsedChatCompletion<ParsedT> {\n    if (this.ended) {\n      throw new OpenAIError(`stream has ended, this shouldn't happen`);\n    }\n    const snapshot = this.#currentChatCompletionSnapshot;\n    if (!snapshot) {\n      throw new OpenAIError(`request ended without sending any chunks`);\n    }\n    this.#currentChatCompletionSnapshot = undefined;\n    this.#choiceEventStates = [];\n    return finalizeChatCompletion(snapshot, this.#params);\n  }\n\n  protected override async _createChatCompletion(\n    client: OpenAI,\n    params: ChatCompletionCreateParams,\n    options?: RequestOptions,\n  ): Promise<ParsedChatCompletion<ParsedT>> {\n    super._createChatCompletion;\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this.#beginRequest();\n\n    const stream = await client.chat.completions.create(\n      { ...params, stream: true },\n      { ...options, signal: this.controller.signal },\n    );\n    this._connected();\n    for await (const chunk of stream) {\n      this.#addChunk(chunk);\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n    return this._addChatCompletion(this.#endRequest());\n  }\n\n  protected async _fromReadableStream(\n    readableStream: ReadableStream,\n    options?: RequestOptions,\n  ): Promise<ChatCompletion> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this.#beginRequest();\n    this._connected();\n    const stream = Stream.fromReadableStream<ChatCompletionChunk>(readableStream, this.controller);\n    let chatId;\n    for await (const chunk of stream) {\n      if (chatId && chatId !== chunk.id) {\n        // A new request has been made.\n        this._addChatCompletion(this.#endRequest());\n      }\n\n      this.#addChunk(chunk);\n      chatId = chunk.id;\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n    return this._addChatCompletion(this.#endRequest());\n  }\n\n  #getAutoParseableResponseFormat(): AutoParseableResponseFormat<ParsedT> | null {\n    const responseFormat = this.#params?.response_format;\n    if (isAutoParsableResponseFormat<ParsedT>(responseFormat)) {\n      return responseFormat;\n    }\n\n    return null;\n  }\n\n  #accumulateChatCompletion(chunk: ChatCompletionChunk): ChatCompletionSnapshot {\n    let snapshot = this.#currentChatCompletionSnapshot;\n    const { choices, ...rest } = chunk;\n    if (!snapshot) {\n      snapshot = this.#currentChatCompletionSnapshot = {\n        ...rest,\n        choices: [],\n      };\n    } else {\n      Object.assign(snapshot, rest);\n    }\n\n    for (const { delta, finish_reason, index, logprobs = null, ...other } of chunk.choices) {\n      let choice = snapshot.choices[index];\n      if (!choice) {\n        choice = snapshot.choices[index] = { finish_reason, index, message: {}, logprobs, ...other };\n      }\n\n      if (logprobs) {\n        if (!choice.logprobs) {\n          choice.logprobs = Object.assign({}, logprobs);\n        } else {\n          const { content, refusal, ...rest } = logprobs;\n          assertIsEmpty(rest);\n          Object.assign(choice.logprobs, rest);\n\n          if (content) {\n            choice.logprobs.content ??= [];\n            choice.logprobs.content.push(...content);\n          }\n\n          if (refusal) {\n            choice.logprobs.refusal ??= [];\n            choice.logprobs.refusal.push(...refusal);\n          }\n        }\n      }\n\n      if (finish_reason) {\n        choice.finish_reason = finish_reason;\n\n        if (this.#params && hasAutoParseableInput(this.#params)) {\n          if (finish_reason === 'length') {\n            throw new LengthFinishReasonError();\n          }\n\n          if (finish_reason === 'content_filter') {\n            throw new ContentFilterFinishReasonError();\n          }\n        }\n      }\n\n      Object.assign(choice, other);\n\n      if (!delta) continue; // Shouldn't happen; just in case.\n\n      const { content, refusal, function_call, role, tool_calls, ...rest } = delta;\n      assertIsEmpty(rest);\n      Object.assign(choice.message, rest);\n\n      if (refusal) {\n        choice.message.refusal = (choice.message.refusal || '') + refusal;\n      }\n\n      if (role) choice.message.role = role;\n      if (function_call) {\n        if (!choice.message.function_call) {\n          choice.message.function_call = function_call;\n        } else {\n          if (function_call.name) choice.message.function_call.name = function_call.name;\n          if (function_call.arguments) {\n            choice.message.function_call.arguments ??= '';\n            choice.message.function_call.arguments += function_call.arguments;\n          }\n        }\n      }\n      if (content) {\n        choice.message.content = (choice.message.content || '') + content;\n\n        if (!choice.message.refusal && this.#getAutoParseableResponseFormat()) {\n          choice.message.parsed = partialParse(choice.message.content);\n        }\n      }\n\n      if (tool_calls) {\n        if (!choice.message.tool_calls) choice.message.tool_calls = [];\n\n        for (const { index, id, type, function: fn, ...rest } of tool_calls) {\n          const tool_call = (choice.message.tool_calls[index] ??=\n            {} as ChatCompletionSnapshot.Choice.Message.ToolCall);\n          Object.assign(tool_call, rest);\n          if (id) tool_call.id = id;\n          if (type) tool_call.type = type;\n          if (fn) tool_call.function ??= { name: fn.name ?? '', arguments: '' };\n          if (fn?.name) tool_call.function!.name = fn.name;\n          if (fn?.arguments) {\n            tool_call.function!.arguments += fn.arguments;\n\n            if (shouldParseToolCall(this.#params, tool_call)) {\n              tool_call.function!.parsed_arguments = partialParse(tool_call.function!.arguments);\n            }\n          }\n        }\n      }\n    }\n    return snapshot;\n  }\n\n  [Symbol.asyncIterator](this: ChatCompletionStream<ParsedT>): AsyncIterator<ChatCompletionChunk> {\n    const pushQueue: ChatCompletionChunk[] = [];\n    const readQueue: {\n      resolve: (chunk: ChatCompletionChunk | undefined) => void;\n      reject: (err: unknown) => void;\n    }[] = [];\n    let done = false;\n\n    this.on('chunk', (chunk) => {\n      const reader = readQueue.shift();\n      if (reader) {\n        reader.resolve(chunk);\n      } else {\n        pushQueue.push(chunk);\n      }\n    });\n\n    this.on('end', () => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.resolve(undefined);\n      }\n      readQueue.length = 0;\n    });\n\n    this.on('abort', (err) => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.reject(err);\n      }\n      readQueue.length = 0;\n    });\n\n    this.on('error', (err) => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.reject(err);\n      }\n      readQueue.length = 0;\n    });\n\n    return {\n      next: async (): Promise<IteratorResult<ChatCompletionChunk>> => {\n        if (!pushQueue.length) {\n          if (done) {\n            return { value: undefined, done: true };\n          }\n          return new Promise<ChatCompletionChunk | undefined>((resolve, reject) =>\n            readQueue.push({ resolve, reject }),\n          ).then((chunk) => (chunk ? { value: chunk, done: false } : { value: undefined, done: true }));\n        }\n        const chunk = pushQueue.shift()!;\n        return { value: chunk, done: false };\n      },\n      return: async () => {\n        this.abort();\n        return { value: undefined, done: true };\n      },\n    };\n  }\n\n  toReadableStream(): ReadableStream {\n    const stream = new Stream(this[Symbol.asyncIterator].bind(this), this.controller);\n    return stream.toReadableStream();\n  }\n}\n\nfunction finalizeChatCompletion<ParsedT>(\n  snapshot: ChatCompletionSnapshot,\n  params: ChatCompletionCreateParams | null,\n): ParsedChatCompletion<ParsedT> {\n  const { id, choices, created, model, system_fingerprint, ...rest } = snapshot;\n  const completion: ChatCompletion = {\n    ...rest,\n    id,\n    choices: choices.map(\n      ({ message, finish_reason, index, logprobs, ...choiceRest }): ChatCompletion.Choice => {\n        if (!finish_reason) {\n          throw new OpenAIError(`missing finish_reason for choice ${index}`);\n        }\n\n        const { content = null, function_call, tool_calls, ...messageRest } = message;\n        const role = message.role as 'assistant'; // this is what we expect; in theory it could be different which would make our types a slight lie but would be fine.\n        if (!role) {\n          throw new OpenAIError(`missing role for choice ${index}`);\n        }\n\n        if (function_call) {\n          const { arguments: args, name } = function_call;\n          if (args == null) {\n            throw new OpenAIError(`missing function_call.arguments for choice ${index}`);\n          }\n\n          if (!name) {\n            throw new OpenAIError(`missing function_call.name for choice ${index}`);\n          }\n\n          return {\n            ...choiceRest,\n            message: {\n              content,\n              function_call: { arguments: args, name },\n              role,\n              refusal: message.refusal ?? null,\n            },\n            finish_reason,\n            index,\n            logprobs,\n          };\n        }\n\n        if (tool_calls) {\n          return {\n            ...choiceRest,\n            index,\n            finish_reason,\n            logprobs,\n            message: {\n              ...messageRest,\n              role,\n              content,\n              refusal: message.refusal ?? null,\n              tool_calls: tool_calls.map((tool_call, i) => {\n                const { function: fn, type, id, ...toolRest } = tool_call;\n                const { arguments: args, name, ...fnRest } = fn || {};\n                if (id == null) {\n                  throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].id\\n${str(snapshot)}`);\n                }\n                if (type == null) {\n                  throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].type\\n${str(snapshot)}`);\n                }\n                if (name == null) {\n                  throw new OpenAIError(\n                    `missing choices[${index}].tool_calls[${i}].function.name\\n${str(snapshot)}`,\n                  );\n                }\n                if (args == null) {\n                  throw new OpenAIError(\n                    `missing choices[${index}].tool_calls[${i}].function.arguments\\n${str(snapshot)}`,\n                  );\n                }\n\n                return { ...toolRest, id, type, function: { ...fnRest, name, arguments: args } };\n              }),\n            },\n          };\n        }\n        return {\n          ...choiceRest,\n          message: { ...messageRest, content, role, refusal: message.refusal ?? null },\n          finish_reason,\n          index,\n          logprobs,\n        };\n      },\n    ),\n    created,\n    model,\n    object: 'chat.completion',\n    ...(system_fingerprint ? { system_fingerprint } : {}),\n  };\n\n  return maybeParseChatCompletion(completion, params);\n}\n\nfunction str(x: unknown) {\n  return JSON.stringify(x);\n}\n\n/**\n * Represents a streamed chunk of a chat completion response returned by model,\n * based on the provided input.\n */\nexport interface ChatCompletionSnapshot {\n  /**\n   * A unique identifier for the chat completion.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletionSnapshot.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created.\n   */\n  created: number;\n\n  /**\n   * The model to generate the completion.\n   */\n  model: string;\n\n  // Note we do not include an \"object\" type on the snapshot,\n  // because the object is not a valid \"chat.completion\" until finalized.\n  // object: 'chat.completion';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   *\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n}\n\nexport namespace ChatCompletionSnapshot {\n  export interface Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    message: Choice.Message;\n\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, or `function_call`\n     * if the model called a function.\n     */\n    finish_reason: ChatCompletion.Choice['finish_reason'] | null;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs: ChatCompletion.Choice.Logprobs | null;\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n  }\n\n  export namespace Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    export interface Message {\n      /**\n       * The contents of the chunk message.\n       */\n      content?: string | null;\n\n      refusal?: string | null;\n\n      parsed?: unknown | null;\n\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      function_call?: Message.FunctionCall;\n\n      tool_calls?: Array<Message.ToolCall>;\n\n      /**\n       * The role of the author of this message.\n       */\n      role?: ChatCompletionRole;\n    }\n\n    export namespace Message {\n      export interface ToolCall {\n        /**\n         * The ID of the tool call.\n         */\n        id: string;\n\n        function: ToolCall.Function;\n\n        /**\n         * The type of the tool.\n         */\n        type: 'function';\n      }\n\n      export namespace ToolCall {\n        export interface Function {\n          /**\n           * The arguments to call the function with, as generated by the model in JSON\n           * format. Note that the model does not always generate valid JSON, and may\n           * hallucinate parameters not defined by your function schema. Validate the\n           * arguments in your code before calling your function.\n           */\n          arguments: string;\n\n          parsed_arguments?: unknown;\n\n          /**\n           * The name of the function to call.\n           */\n          name: string;\n        }\n      }\n\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      export interface FunctionCall {\n        /**\n         * The arguments to call the function with, as generated by the model in JSON\n         * format. Note that the model does not always generate valid JSON, and may\n         * hallucinate parameters not defined by your function schema. Validate the\n         * arguments in your code before calling your function.\n         */\n        arguments?: string;\n\n        /**\n         * The name of the function to call.\n         */\n        name?: string;\n      }\n    }\n  }\n}\n\ntype AssertIsEmpty<T extends {}> = keyof T extends never ? T : never;\n\n/**\n * Ensures the given argument is an empty object, useful for\n * asserting that all known properties on an object have been\n * destructured.\n */\nfunction assertIsEmpty<T extends {}>(obj: AssertIsEmpty<T>): asserts obj is AssertIsEmpty<T> {\n  return;\n}\n\nfunction assertNever(_x: never) {}\n", "import {\n  type ChatCompletionChunk,\n  type ChatCompletionCreateParamsStreaming,\n} from '../resources/chat/completions';\nimport { RunnerOptions, type AbstractChatCompletionRunnerEvents } from './AbstractChatCompletionRunner';\nimport { type ReadableStream } from '../internal/shim-types';\nimport { RunnableTools, type BaseFunctionsArgs } from './RunnableFunction';\nimport { ChatCompletionSnapshot, ChatCompletionStream } from './ChatCompletionStream';\nimport OpenAI from '../index';\nimport { AutoParseableTool } from '../lib/parser';\n\nexport interface ChatCompletionStreamEvents extends AbstractChatCompletionRunnerEvents {\n  content: (contentDelta: string, contentSnapshot: string) => void;\n  chunk: (chunk: ChatCompletionChunk, snapshot: ChatCompletionSnapshot) => void;\n}\n\nexport type ChatCompletionStreamingToolRunnerParams<FunctionsArgs extends BaseFunctionsArgs> = Omit<\n  ChatCompletionCreateParamsStreaming,\n  'tools'\n> & {\n  tools: RunnableTools<FunctionsArgs> | AutoParseableTool<any, true>[];\n};\n\nexport class ChatCompletionStreamingRunner<ParsedT = null>\n  extends ChatCompletionStream<ParsedT>\n  implements AsyncIterable<ChatCompletionChunk>\n{\n  static override fromReadableStream(stream: ReadableStream): ChatCompletionStreamingRunner<null> {\n    const runner = new ChatCompletionStreamingRunner(null);\n    runner._run(() => runner._fromReadableStream(stream));\n    return runner;\n  }\n\n  static runTools<T extends (string | object)[], ParsedT = null>(\n    client: OpenAI,\n    params: ChatCompletionStreamingToolRunnerParams<T>,\n    options?: RunnerOptions,\n  ): ChatCompletionStreamingRunner<ParsedT> {\n    const runner = new ChatCompletionStreamingRunner<ParsedT>(\n      // @ts-expect-error TODO these types are incompatible\n      params,\n    );\n    const opts = {\n      ...options,\n      headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runTools' },\n    };\n    runner._run(() => runner._runTools(client, params, opts));\n    return runner;\n  }\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as CompletionsCompletionsAPI from './completions';\nimport * as CompletionsAPI from '../../completions';\nimport * as Shared from '../../shared';\nimport * as MessagesAPI from './messages';\nimport { MessageListParams, Messages } from './messages';\nimport { APIPromise } from '../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { Stream } from '../../../core/streaming';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nimport { ChatCompletionRunner } from '../../../lib/ChatCompletionRunner';\nimport { ChatCompletionStreamingRunner } from '../../../lib/ChatCompletionStreamingRunner';\nimport { RunnerOptions } from '../../../lib/AbstractChatCompletionRunner';\nimport { ChatCompletionToolRunnerParams } from '../../../lib/ChatCompletionRunner';\nimport { ChatCompletionStreamingToolRunnerParams } from '../../../lib/ChatCompletionStreamingRunner';\nimport { ChatCompletionStream, type ChatCompletionStreamParams } from '../../../lib/ChatCompletionStream';\nimport { ExtractParsedContentFromParams, parseChatCompletion, validateInputTools } from '../../../lib/parser';\n\nexport class Completions extends APIResource {\n  messages: MessagesAPI.Messages = new MessagesAPI.Messages(this._client);\n\n  /**\n   * **Starting a new project?** We recommend trying\n   * [Responses](https://platform.openai.com/docs/api-reference/responses) to take\n   * advantage of the latest OpenAI platform features. Compare\n   * [Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).\n   *\n   * ---\n   *\n   * Creates a model response for the given chat conversation. Learn more in the\n   * [text generation](https://platform.openai.com/docs/guides/text-generation),\n   * [vision](https://platform.openai.com/docs/guides/vision), and\n   * [audio](https://platform.openai.com/docs/guides/audio) guides.\n   *\n   * Parameter support can differ depending on the model used to generate the\n   * response, particularly for newer reasoning models. Parameters that are only\n   * supported for reasoning models are noted below. For the current state of\n   * unsupported parameters in reasoning models,\n   * [refer to the reasoning guide](https://platform.openai.com/docs/guides/reasoning).\n   *\n   * Returns a chat completion object, or a streamed sequence of chat completion\n   * chunk objects if the request is streamed.\n   *\n   * @example\n   * ```ts\n   * const chatCompletion = await client.chat.completions.create(\n   *   {\n   *     messages: [{ content: 'string', role: 'developer' }],\n   *     model: 'gpt-4o',\n   *   },\n   * );\n   * ```\n   */\n  create(body: ChatCompletionCreateParamsNonStreaming, options?: RequestOptions): APIPromise<ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk>>;\n  create(\n    body: ChatCompletionCreateParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk> | ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<ChatCompletion> | APIPromise<Stream<ChatCompletionChunk>> {\n    return this._client.post('/chat/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<ChatCompletion>\n      | APIPromise<Stream<ChatCompletionChunk>>;\n  }\n\n  /**\n   * Get a stored chat completion. Only Chat Completions that have been created with\n   * the `store` parameter set to `true` will be returned.\n   *\n   * @example\n   * ```ts\n   * const chatCompletion =\n   *   await client.chat.completions.retrieve('completion_id');\n   * ```\n   */\n  retrieve(completionID: string, options?: RequestOptions): APIPromise<ChatCompletion> {\n    return this._client.get(path`/chat/completions/${completionID}`, options);\n  }\n\n  /**\n   * Modify a stored chat completion. Only Chat Completions that have been created\n   * with the `store` parameter set to `true` can be modified. Currently, the only\n   * supported modification is to update the `metadata` field.\n   *\n   * @example\n   * ```ts\n   * const chatCompletion = await client.chat.completions.update(\n   *   'completion_id',\n   *   { metadata: { foo: 'string' } },\n   * );\n   * ```\n   */\n  update(\n    completionID: string,\n    body: ChatCompletionUpdateParams,\n    options?: RequestOptions,\n  ): APIPromise<ChatCompletion> {\n    return this._client.post(path`/chat/completions/${completionID}`, { body, ...options });\n  }\n\n  /**\n   * List stored Chat Completions. Only Chat Completions that have been stored with\n   * the `store` parameter set to `true` will be returned.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const chatCompletion of client.chat.completions.list()) {\n   *   // ...\n   * }\n   * ```\n   */\n  list(\n    query: ChatCompletionListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<ChatCompletionsPage, ChatCompletion> {\n    return this._client.getAPIList('/chat/completions', CursorPage<ChatCompletion>, { query, ...options });\n  }\n\n  /**\n   * Delete a stored chat completion. Only Chat Completions that have been created\n   * with the `store` parameter set to `true` can be deleted.\n   *\n   * @example\n   * ```ts\n   * const chatCompletionDeleted =\n   *   await client.chat.completions.delete('completion_id');\n   * ```\n   */\n  delete(completionID: string, options?: RequestOptions): APIPromise<ChatCompletionDeleted> {\n    return this._client.delete(path`/chat/completions/${completionID}`, options);\n  }\n\n  parse<Params extends ChatCompletionParseParams, ParsedT = ExtractParsedContentFromParams<Params>>(\n    body: Params,\n    options?: RequestOptions,\n  ): APIPromise<ParsedChatCompletion<ParsedT>> {\n    validateInputTools(body.tools);\n\n    return this._client.chat.completions\n      .create(body, {\n        ...options,\n        headers: {\n          ...options?.headers,\n          'X-Stainless-Helper-Method': 'chat.completions.parse',\n        },\n      })\n      ._thenUnwrap((completion) => parseChatCompletion(completion, body));\n  }\n\n  /**\n   * A convenience helper for using tool calls with the /chat/completions endpoint\n   * which automatically calls the JavaScript functions you provide and sends their\n   * results back to the /chat/completions endpoint, looping as long as the model\n   * requests function calls.\n   *\n   * For more details and examples, see\n   * [the docs](https://github.com/openai/openai-node#automated-function-calls)\n   */\n  runTools<\n    Params extends ChatCompletionToolRunnerParams<any>,\n    ParsedT = ExtractParsedContentFromParams<Params>,\n  >(body: Params, options?: RunnerOptions): ChatCompletionRunner<ParsedT>;\n\n  runTools<\n    Params extends ChatCompletionStreamingToolRunnerParams<any>,\n    ParsedT = ExtractParsedContentFromParams<Params>,\n  >(body: Params, options?: RunnerOptions): ChatCompletionStreamingRunner<ParsedT>;\n\n  runTools<\n    Params extends ChatCompletionToolRunnerParams<any> | ChatCompletionStreamingToolRunnerParams<any>,\n    ParsedT = ExtractParsedContentFromParams<Params>,\n  >(\n    body: Params,\n    options?: RunnerOptions,\n  ): ChatCompletionRunner<ParsedT> | ChatCompletionStreamingRunner<ParsedT> {\n    if (body.stream) {\n      return ChatCompletionStreamingRunner.runTools(\n        this._client,\n        body as ChatCompletionStreamingToolRunnerParams<any>,\n        options,\n      );\n    }\n\n    return ChatCompletionRunner.runTools(this._client, body as ChatCompletionToolRunnerParams<any>, options);\n  }\n\n  /**\n   * Creates a chat completion stream\n   */\n  stream<Params extends ChatCompletionStreamParams, ParsedT = ExtractParsedContentFromParams<Params>>(\n    body: Params,\n    options?: RequestOptions,\n  ): ChatCompletionStream<ParsedT> {\n    return ChatCompletionStream.createChatCompletion(this._client, body, options);\n  }\n}\n\nexport interface ParsedFunction extends ChatCompletionMessageFunctionToolCall.Function {\n  parsed_arguments?: unknown;\n}\n\nexport interface ParsedFunctionToolCall extends ChatCompletionMessageFunctionToolCall {\n  function: ParsedFunction;\n}\n\nexport interface ParsedChatCompletionMessage<ParsedT> extends ChatCompletionMessage {\n  parsed: ParsedT | null;\n  tool_calls?: Array<ParsedFunctionToolCall>;\n}\n\nexport interface ParsedChoice<ParsedT> extends ChatCompletion.Choice {\n  message: ParsedChatCompletionMessage<ParsedT>;\n}\n\nexport interface ParsedChatCompletion<ParsedT> extends ChatCompletion {\n  choices: Array<ParsedChoice<ParsedT>>;\n}\n\nexport type ChatCompletionParseParams = ChatCompletionCreateParamsNonStreaming;\n\nexport { ChatCompletionStreamingRunner } from '../../../lib/ChatCompletionStreamingRunner';\nexport {\n  type RunnableFunctionWithParse,\n  type RunnableFunctionWithoutParse,\n  ParsingToolFunction,\n} from '../../../lib/RunnableFunction';\nexport { type ChatCompletionToolRunnerParams } from '../../../lib/ChatCompletionRunner';\nexport { type ChatCompletionStreamingToolRunnerParams } from '../../../lib/ChatCompletionStreamingRunner';\nexport { ChatCompletionStream, type ChatCompletionStreamParams } from '../../../lib/ChatCompletionStream';\nexport { ChatCompletionRunner } from '../../../lib/ChatCompletionRunner';\n\nexport type ChatCompletionsPage = CursorPage<ChatCompletion>;\n\nexport type ChatCompletionStoreMessagesPage = CursorPage<ChatCompletionStoreMessage>;\n\n/**\n * Represents a chat completion response returned by model, based on the provided\n * input.\n */\nexport interface ChatCompletion {\n  /**\n   * A unique identifier for the chat completion.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletion.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for the chat completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion`.\n   */\n  object: 'chat.completion';\n\n  /**\n   * Specifies the processing type used for serving the request.\n   *\n   * - If set to 'auto', then the request will be processed with the service tier\n   *   configured in the Project settings. Unless otherwise configured, the Project\n   *   will use 'default'.\n   * - If set to 'default', then the request will be processed with the standard\n   *   pricing and performance for the selected model.\n   * - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\n   *   '[priority](https://openai.com/api-priority-processing/)', then the request\n   *   will be processed with the corresponding service tier.\n   * - When not set, the default behavior is 'auto'.\n   *\n   * When the `service_tier` parameter is set, the response body will include the\n   * `service_tier` value based on the processing mode actually used to serve the\n   * request. This response value may be different from the value set in the\n   * parameter.\n   */\n  service_tier?: 'auto' | 'default' | 'flex' | 'scale' | 'priority' | null;\n\n  /**\n   * @deprecated This fingerprint represents the backend configuration that the model\n   * runs with.\n   *\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionsAPI.CompletionUsage;\n}\n\nexport namespace ChatCompletion {\n  export interface Choice {\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, `tool_calls` if the\n     * model called a tool, or `function_call` (deprecated) if the model called a\n     * function.\n     */\n    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs: Choice.Logprobs | null;\n\n    /**\n     * A chat completion message generated by the model.\n     */\n    message: CompletionsCompletionsAPI.ChatCompletionMessage;\n  }\n\n  export namespace Choice {\n    /**\n     * Log probability information for the choice.\n     */\n    export interface Logprobs {\n      /**\n       * A list of message content tokens with log probability information.\n       */\n      content: Array<CompletionsCompletionsAPI.ChatCompletionTokenLogprob> | null;\n\n      /**\n       * A list of message refusal tokens with log probability information.\n       */\n      refusal: Array<CompletionsCompletionsAPI.ChatCompletionTokenLogprob> | null;\n    }\n  }\n}\n\n/**\n * Constrains the tools available to the model to a pre-defined set.\n */\nexport interface ChatCompletionAllowedToolChoice {\n  /**\n   * Constrains the tools available to the model to a pre-defined set.\n   */\n  allowed_tools: ChatCompletionAllowedTools;\n\n  /**\n   * Allowed tool configuration type. Always `allowed_tools`.\n   */\n  type: 'allowed_tools';\n}\n\n/**\n * Messages sent by the model in response to user messages.\n */\nexport interface ChatCompletionAssistantMessageParam {\n  /**\n   * The role of the messages author, in this case `assistant`.\n   */\n  role: 'assistant';\n\n  /**\n   * Data about a previous audio response from the model.\n   * [Learn more](https://platform.openai.com/docs/guides/audio).\n   */\n  audio?: ChatCompletionAssistantMessageParam.Audio | null;\n\n  /**\n   * The contents of the assistant message. Required unless `tool_calls` or\n   * `function_call` is specified.\n   */\n  content?: string | Array<ChatCompletionContentPartText | ChatCompletionContentPartRefusal> | null;\n\n  /**\n   * @deprecated Deprecated and replaced by `tool_calls`. The name and arguments of a\n   * function that should be called, as generated by the model.\n   */\n  function_call?: ChatCompletionAssistantMessageParam.FunctionCall | null;\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n\n  /**\n   * The refusal message by the assistant.\n   */\n  refusal?: string | null;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: Array<ChatCompletionMessageToolCall>;\n}\n\nexport namespace ChatCompletionAssistantMessageParam {\n  /**\n   * Data about a previous audio response from the model.\n   * [Learn more](https://platform.openai.com/docs/guides/audio).\n   */\n  export interface Audio {\n    /**\n     * Unique identifier for a previous audio response from the model.\n     */\n    id: string;\n  }\n\n  /**\n   * @deprecated Deprecated and replaced by `tool_calls`. The name and arguments of a\n   * function that should be called, as generated by the model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * If the audio output modality is requested, this object contains data about the\n * audio response from the model.\n * [Learn more](https://platform.openai.com/docs/guides/audio).\n */\nexport interface ChatCompletionAudio {\n  /**\n   * Unique identifier for this audio response.\n   */\n  id: string;\n\n  /**\n   * Base64 encoded audio bytes generated by the model, in the format specified in\n   * the request.\n   */\n  data: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when this audio response will no longer be\n   * accessible on the server for use in multi-turn conversations.\n   */\n  expires_at: number;\n\n  /**\n   * Transcript of the audio generated by the model.\n   */\n  transcript: string;\n}\n\n/**\n * Parameters for audio output. Required when audio output is requested with\n * `modalities: [\"audio\"]`.\n * [Learn more](https://platform.openai.com/docs/guides/audio).\n */\nexport interface ChatCompletionAudioParam {\n  /**\n   * Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`, `opus`,\n   * or `pcm16`.\n   */\n  format: 'wav' | 'aac' | 'mp3' | 'flac' | 'opus' | 'pcm16';\n\n  /**\n   * The voice the model uses to respond. Supported built-in voices are `alloy`,\n   * `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`,\n   * `marin`, and `cedar`.\n   */\n  voice:\n    | (string & {})\n    | 'alloy'\n    | 'ash'\n    | 'ballad'\n    | 'coral'\n    | 'echo'\n    | 'sage'\n    | 'shimmer'\n    | 'verse'\n    | 'marin'\n    | 'cedar';\n}\n\n/**\n * Represents a streamed chunk of a chat completion response returned by the model,\n * based on the provided input.\n * [Learn more](https://platform.openai.com/docs/guides/streaming-responses).\n */\nexport interface ChatCompletionChunk {\n  /**\n   * A unique identifier for the chat completion. Each chunk has the same ID.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can contain more than one elements if `n` is\n   * greater than 1. Can also be empty for the last chunk if you set\n   * `stream_options: {\"include_usage\": true}`.\n   */\n  choices: Array<ChatCompletionChunk.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created. Each\n   * chunk has the same timestamp.\n   */\n  created: number;\n\n  /**\n   * The model to generate the completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion.chunk`.\n   */\n  object: 'chat.completion.chunk';\n\n  /**\n   * Specifies the processing type used for serving the request.\n   *\n   * - If set to 'auto', then the request will be processed with the service tier\n   *   configured in the Project settings. Unless otherwise configured, the Project\n   *   will use 'default'.\n   * - If set to 'default', then the request will be processed with the standard\n   *   pricing and performance for the selected model.\n   * - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\n   *   '[priority](https://openai.com/api-priority-processing/)', then the request\n   *   will be processed with the corresponding service tier.\n   * - When not set, the default behavior is 'auto'.\n   *\n   * When the `service_tier` parameter is set, the response body will include the\n   * `service_tier` value based on the processing mode actually used to serve the\n   * request. This response value may be different from the value set in the\n   * parameter.\n   */\n  service_tier?: 'auto' | 'default' | 'flex' | 'scale' | 'priority' | null;\n\n  /**\n   * @deprecated This fingerprint represents the backend configuration that the model\n   * runs with. Can be used in conjunction with the `seed` request parameter to\n   * understand when backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n\n  /**\n   * An optional field that will only be present when you set\n   * `stream_options: {\"include_usage\": true}` in your request. When present, it\n   * contains a null value **except for the last chunk** which contains the token\n   * usage statistics for the entire request.\n   *\n   * **NOTE:** If the stream is interrupted or cancelled, you may not receive the\n   * final usage chunk which contains the total token usage for the request.\n   */\n  usage?: CompletionsAPI.CompletionUsage | null;\n}\n\nexport namespace ChatCompletionChunk {\n  export interface Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    delta: Choice.Delta;\n\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, `tool_calls` if the\n     * model called a tool, or `function_call` (deprecated) if the model called a\n     * function.\n     */\n    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call' | null;\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs?: Choice.Logprobs | null;\n  }\n\n  export namespace Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    export interface Delta {\n      /**\n       * The contents of the chunk message.\n       */\n      content?: string | null;\n\n      /**\n       * @deprecated Deprecated and replaced by `tool_calls`. The name and arguments of a\n       * function that should be called, as generated by the model.\n       */\n      function_call?: Delta.FunctionCall;\n\n      /**\n       * The refusal message generated by the model.\n       */\n      refusal?: string | null;\n\n      /**\n       * The role of the author of this message.\n       */\n      role?: 'developer' | 'system' | 'user' | 'assistant' | 'tool';\n\n      tool_calls?: Array<Delta.ToolCall>;\n    }\n\n    export namespace Delta {\n      /**\n       * @deprecated Deprecated and replaced by `tool_calls`. The name and arguments of a\n       * function that should be called, as generated by the model.\n       */\n      export interface FunctionCall {\n        /**\n         * The arguments to call the function with, as generated by the model in JSON\n         * format. Note that the model does not always generate valid JSON, and may\n         * hallucinate parameters not defined by your function schema. Validate the\n         * arguments in your code before calling your function.\n         */\n        arguments?: string;\n\n        /**\n         * The name of the function to call.\n         */\n        name?: string;\n      }\n\n      export interface ToolCall {\n        index: number;\n\n        /**\n         * The ID of the tool call.\n         */\n        id?: string;\n\n        function?: ToolCall.Function;\n\n        /**\n         * The type of the tool. Currently, only `function` is supported.\n         */\n        type?: 'function';\n      }\n\n      export namespace ToolCall {\n        export interface Function {\n          /**\n           * The arguments to call the function with, as generated by the model in JSON\n           * format. Note that the model does not always generate valid JSON, and may\n           * hallucinate parameters not defined by your function schema. Validate the\n           * arguments in your code before calling your function.\n           */\n          arguments?: string;\n\n          /**\n           * The name of the function to call.\n           */\n          name?: string;\n        }\n      }\n    }\n\n    /**\n     * Log probability information for the choice.\n     */\n    export interface Logprobs {\n      /**\n       * A list of message content tokens with log probability information.\n       */\n      content: Array<CompletionsCompletionsAPI.ChatCompletionTokenLogprob> | null;\n\n      /**\n       * A list of message refusal tokens with log probability information.\n       */\n      refusal: Array<CompletionsCompletionsAPI.ChatCompletionTokenLogprob> | null;\n    }\n  }\n}\n\n/**\n * Learn about\n * [text inputs](https://platform.openai.com/docs/guides/text-generation).\n */\nexport type ChatCompletionContentPart =\n  | ChatCompletionContentPartText\n  | ChatCompletionContentPartImage\n  | ChatCompletionContentPartInputAudio\n  | ChatCompletionContentPart.File;\n\nexport namespace ChatCompletionContentPart {\n  /**\n   * Learn about [file inputs](https://platform.openai.com/docs/guides/text) for text\n   * generation.\n   */\n  export interface File {\n    file: File.File;\n\n    /**\n     * The type of the content part. Always `file`.\n     */\n    type: 'file';\n  }\n\n  export namespace File {\n    export interface File {\n      /**\n       * The base64 encoded file data, used when passing the file to the model as a\n       * string.\n       */\n      file_data?: string;\n\n      /**\n       * The ID of an uploaded file to use as input.\n       */\n      file_id?: string;\n\n      /**\n       * The name of the file, used when passing the file to the model as a string.\n       */\n      filename?: string;\n    }\n  }\n}\n\n/**\n * Learn about [image inputs](https://platform.openai.com/docs/guides/vision).\n */\nexport interface ChatCompletionContentPartImage {\n  image_url: ChatCompletionContentPartImage.ImageURL;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'image_url';\n}\n\nexport namespace ChatCompletionContentPartImage {\n  export interface ImageURL {\n    /**\n     * Either a URL of the image or the base64 encoded image data.\n     */\n    url: string;\n\n    /**\n     * Specifies the detail level of the image. Learn more in the\n     * [Vision guide](https://platform.openai.com/docs/guides/vision#low-or-high-fidelity-image-understanding).\n     */\n    detail?: 'auto' | 'low' | 'high';\n  }\n}\n\n/**\n * Learn about [audio inputs](https://platform.openai.com/docs/guides/audio).\n */\nexport interface ChatCompletionContentPartInputAudio {\n  input_audio: ChatCompletionContentPartInputAudio.InputAudio;\n\n  /**\n   * The type of the content part. Always `input_audio`.\n   */\n  type: 'input_audio';\n}\n\nexport namespace ChatCompletionContentPartInputAudio {\n  export interface InputAudio {\n    /**\n     * Base64 encoded audio data.\n     */\n    data: string;\n\n    /**\n     * The format of the encoded audio data. Currently supports \"wav\" and \"mp3\".\n     */\n    format: 'wav' | 'mp3';\n  }\n}\n\nexport interface ChatCompletionContentPartRefusal {\n  /**\n   * The refusal message generated by the model.\n   */\n  refusal: string;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'refusal';\n}\n\n/**\n * Learn about\n * [text inputs](https://platform.openai.com/docs/guides/text-generation).\n */\nexport interface ChatCompletionContentPartText {\n  /**\n   * The text content.\n   */\n  text: string;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'text';\n}\n\n/**\n * A custom tool that processes input using a specified format.\n */\nexport interface ChatCompletionCustomTool {\n  /**\n   * Properties of the custom tool.\n   */\n  custom: ChatCompletionCustomTool.Custom;\n\n  /**\n   * The type of the custom tool. Always `custom`.\n   */\n  type: 'custom';\n}\n\nexport namespace ChatCompletionCustomTool {\n  /**\n   * Properties of the custom tool.\n   */\n  export interface Custom {\n    /**\n     * The name of the custom tool, used to identify it in tool calls.\n     */\n    name: string;\n\n    /**\n     * Optional description of the custom tool, used to provide more context.\n     */\n    description?: string;\n\n    /**\n     * The input format for the custom tool. Default is unconstrained text.\n     */\n    format?: Custom.Text | Custom.Grammar;\n  }\n\n  export namespace Custom {\n    /**\n     * Unconstrained free-form text.\n     */\n    export interface Text {\n      /**\n       * Unconstrained text format. Always `text`.\n       */\n      type: 'text';\n    }\n\n    /**\n     * A grammar defined by the user.\n     */\n    export interface Grammar {\n      /**\n       * Your chosen grammar.\n       */\n      grammar: Grammar.Grammar;\n\n      /**\n       * Grammar format. Always `grammar`.\n       */\n      type: 'grammar';\n    }\n\n    export namespace Grammar {\n      /**\n       * Your chosen grammar.\n       */\n      export interface Grammar {\n        /**\n         * The grammar definition.\n         */\n        definition: string;\n\n        /**\n         * The syntax of the grammar definition. One of `lark` or `regex`.\n         */\n        syntax: 'lark' | 'regex';\n      }\n    }\n  }\n}\n\nexport interface ChatCompletionDeleted {\n  /**\n   * The ID of the chat completion that was deleted.\n   */\n  id: string;\n\n  /**\n   * Whether the chat completion was deleted.\n   */\n  deleted: boolean;\n\n  /**\n   * The type of object being deleted.\n   */\n  object: 'chat.completion.deleted';\n}\n\n/**\n * Developer-provided instructions that the model should follow, regardless of\n * messages sent by the user. With o1 models and newer, `developer` messages\n * replace the previous `system` messages.\n */\nexport interface ChatCompletionDeveloperMessageParam {\n  /**\n   * The contents of the developer message.\n   */\n  content: string | Array<ChatCompletionContentPartText>;\n\n  /**\n   * The role of the messages author, in this case `developer`.\n   */\n  role: 'developer';\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n}\n\n/**\n * Specifying a particular function via `{\"name\": \"my_function\"}` forces the model\n * to call that function.\n */\nexport interface ChatCompletionFunctionCallOption {\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n}\n\n/**\n * @deprecated\n */\nexport interface ChatCompletionFunctionMessageParam {\n  /**\n   * The contents of the function message.\n   */\n  content: string | null;\n\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n\n  /**\n   * The role of the messages author, in this case `function`.\n   */\n  role: 'function';\n}\n\n/**\n * A function tool that can be used to generate a response.\n */\nexport interface ChatCompletionFunctionTool {\n  function: Shared.FunctionDefinition;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\n/**\n * A chat completion message generated by the model.\n */\nexport interface ChatCompletionMessage {\n  /**\n   * The contents of the message.\n   */\n  content: string | null;\n\n  /**\n   * The refusal message generated by the model.\n   */\n  refusal: string | null;\n\n  /**\n   * The role of the author of this message.\n   */\n  role: 'assistant';\n\n  /**\n   * Annotations for the message, when applicable, as when using the\n   * [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n   */\n  annotations?: Array<ChatCompletionMessage.Annotation>;\n\n  /**\n   * If the audio output modality is requested, this object contains data about the\n   * audio response from the model.\n   * [Learn more](https://platform.openai.com/docs/guides/audio).\n   */\n  audio?: ChatCompletionAudio | null;\n\n  /**\n   * @deprecated Deprecated and replaced by `tool_calls`. The name and arguments of a\n   * function that should be called, as generated by the model.\n   */\n  function_call?: ChatCompletionMessage.FunctionCall | null;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: Array<ChatCompletionMessageToolCall>;\n}\n\nexport namespace ChatCompletionMessage {\n  /**\n   * A URL citation when using web search.\n   */\n  export interface Annotation {\n    /**\n     * The type of the URL citation. Always `url_citation`.\n     */\n    type: 'url_citation';\n\n    /**\n     * A URL citation when using web search.\n     */\n    url_citation: Annotation.URLCitation;\n  }\n\n  export namespace Annotation {\n    /**\n     * A URL citation when using web search.\n     */\n    export interface URLCitation {\n      /**\n       * The index of the last character of the URL citation in the message.\n       */\n      end_index: number;\n\n      /**\n       * The index of the first character of the URL citation in the message.\n       */\n      start_index: number;\n\n      /**\n       * The title of the web resource.\n       */\n      title: string;\n\n      /**\n       * The URL of the web resource.\n       */\n      url: string;\n    }\n  }\n\n  /**\n   * @deprecated Deprecated and replaced by `tool_calls`. The name and arguments of a\n   * function that should be called, as generated by the model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * A call to a custom tool created by the model.\n */\nexport interface ChatCompletionMessageCustomToolCall {\n  /**\n   * The ID of the tool call.\n   */\n  id: string;\n\n  /**\n   * The custom tool that the model called.\n   */\n  custom: ChatCompletionMessageCustomToolCall.Custom;\n\n  /**\n   * The type of the tool. Always `custom`.\n   */\n  type: 'custom';\n}\n\nexport namespace ChatCompletionMessageCustomToolCall {\n  /**\n   * The custom tool that the model called.\n   */\n  export interface Custom {\n    /**\n     * The input for the custom tool call generated by the model.\n     */\n    input: string;\n\n    /**\n     * The name of the custom tool to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * A call to a function tool created by the model.\n */\nexport interface ChatCompletionMessageFunctionToolCall {\n  /**\n   * The ID of the tool call.\n   */\n  id: string;\n\n  /**\n   * The function that the model called.\n   */\n  function: ChatCompletionMessageFunctionToolCall.Function;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\nexport namespace ChatCompletionMessageFunctionToolCall {\n  /**\n   * The function that the model called.\n   */\n  export interface Function {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * Developer-provided instructions that the model should follow, regardless of\n * messages sent by the user. With o1 models and newer, `developer` messages\n * replace the previous `system` messages.\n */\nexport type ChatCompletionMessageParam =\n  | ChatCompletionDeveloperMessageParam\n  | ChatCompletionSystemMessageParam\n  | ChatCompletionUserMessageParam\n  | ChatCompletionAssistantMessageParam\n  | ChatCompletionToolMessageParam\n  | ChatCompletionFunctionMessageParam;\n\n/**\n * A call to a function tool created by the model.\n */\nexport type ChatCompletionMessageToolCall =\n  | ChatCompletionMessageFunctionToolCall\n  | ChatCompletionMessageCustomToolCall;\n\nexport type ChatCompletionModality = 'text' | 'audio';\n\n/**\n * Specifies a tool the model should use. Use to force the model to call a specific\n * function.\n */\nexport interface ChatCompletionNamedToolChoice {\n  function: ChatCompletionNamedToolChoice.Function;\n\n  /**\n   * For function calling, the type is always `function`.\n   */\n  type: 'function';\n}\n\nexport namespace ChatCompletionNamedToolChoice {\n  export interface Function {\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * Specifies a tool the model should use. Use to force the model to call a specific\n * custom tool.\n */\nexport interface ChatCompletionNamedToolChoiceCustom {\n  custom: ChatCompletionNamedToolChoiceCustom.Custom;\n\n  /**\n   * For custom tool calling, the type is always `custom`.\n   */\n  type: 'custom';\n}\n\nexport namespace ChatCompletionNamedToolChoiceCustom {\n  export interface Custom {\n    /**\n     * The name of the custom tool to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * Static predicted output content, such as the content of a text file that is\n * being regenerated.\n */\nexport interface ChatCompletionPredictionContent {\n  /**\n   * The content that should be matched when generating a model response. If\n   * generated tokens would match this content, the entire model response can be\n   * returned much more quickly.\n   */\n  content: string | Array<ChatCompletionContentPartText>;\n\n  /**\n   * The type of the predicted content you want to provide. This type is currently\n   * always `content`.\n   */\n  type: 'content';\n}\n\n/**\n * The role of the author of a message\n */\nexport type ChatCompletionRole = 'developer' | 'system' | 'user' | 'assistant' | 'tool' | 'function';\n\n/**\n * A chat completion message generated by the model.\n */\nexport interface ChatCompletionStoreMessage extends ChatCompletionMessage {\n  /**\n   * The identifier of the chat message.\n   */\n  id: string;\n\n  /**\n   * If a content parts array was provided, this is an array of `text` and\n   * `image_url` parts. Otherwise, null.\n   */\n  content_parts?: Array<ChatCompletionContentPartText | ChatCompletionContentPartImage> | null;\n}\n\n/**\n * Options for streaming response. Only set this when you set `stream: true`.\n */\nexport interface ChatCompletionStreamOptions {\n  /**\n   * When true, stream obfuscation will be enabled. Stream obfuscation adds random\n   * characters to an `obfuscation` field on streaming delta events to normalize\n   * payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n   * fields are included by default, but add a small amount of overhead to the data\n   * stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n   * you trust the network links between your application and the OpenAI API.\n   */\n  include_obfuscation?: boolean;\n\n  /**\n   * If set, an additional chunk will be streamed before the `data: [DONE]` message.\n   * The `usage` field on this chunk shows the token usage statistics for the entire\n   * request, and the `choices` field will always be an empty array.\n   *\n   * All other chunks will also include a `usage` field, but with a null value.\n   * **NOTE:** If the stream is interrupted, you may not receive the final usage\n   * chunk which contains the total token usage for the request.\n   */\n  include_usage?: boolean;\n}\n\n/**\n * Developer-provided instructions that the model should follow, regardless of\n * messages sent by the user. With o1 models and newer, use `developer` messages\n * for this purpose instead.\n */\nexport interface ChatCompletionSystemMessageParam {\n  /**\n   * The contents of the system message.\n   */\n  content: string | Array<ChatCompletionContentPartText>;\n\n  /**\n   * The role of the messages author, in this case `system`.\n   */\n  role: 'system';\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n}\n\nexport interface ChatCompletionTokenLogprob {\n  /**\n   * The token.\n   */\n  token: string;\n\n  /**\n   * A list of integers representing the UTF-8 bytes representation of the token.\n   * Useful in instances where characters are represented by multiple tokens and\n   * their byte representations must be combined to generate the correct text\n   * representation. Can be `null` if there is no bytes representation for the token.\n   */\n  bytes: Array<number> | null;\n\n  /**\n   * The log probability of this token, if it is within the top 20 most likely\n   * tokens. Otherwise, the value `-9999.0` is used to signify that the token is very\n   * unlikely.\n   */\n  logprob: number;\n\n  /**\n   * List of the most likely tokens and their log probability, at this token\n   * position. In rare cases, there may be fewer than the number of requested\n   * `top_logprobs` returned.\n   */\n  top_logprobs: Array<ChatCompletionTokenLogprob.TopLogprob>;\n}\n\nexport namespace ChatCompletionTokenLogprob {\n  export interface TopLogprob {\n    /**\n     * The token.\n     */\n    token: string;\n\n    /**\n     * A list of integers representing the UTF-8 bytes representation of the token.\n     * Useful in instances where characters are represented by multiple tokens and\n     * their byte representations must be combined to generate the correct text\n     * representation. Can be `null` if there is no bytes representation for the token.\n     */\n    bytes: Array<number> | null;\n\n    /**\n     * The log probability of this token, if it is within the top 20 most likely\n     * tokens. Otherwise, the value `-9999.0` is used to signify that the token is very\n     * unlikely.\n     */\n    logprob: number;\n  }\n}\n\n/**\n * A function tool that can be used to generate a response.\n */\nexport type ChatCompletionTool = ChatCompletionFunctionTool | ChatCompletionCustomTool;\n\n/**\n * Controls which (if any) tool is called by the model. `none` means the model will\n * not call any tool and instead generates a message. `auto` means the model can\n * pick between generating a message or calling one or more tools. `required` means\n * the model must call one or more tools. Specifying a particular tool via\n * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n * call that tool.\n *\n * `none` is the default when no tools are present. `auto` is the default if tools\n * are present.\n */\nexport type ChatCompletionToolChoiceOption =\n  | 'none'\n  | 'auto'\n  | 'required'\n  | ChatCompletionAllowedToolChoice\n  | ChatCompletionNamedToolChoice\n  | ChatCompletionNamedToolChoiceCustom;\n\nexport interface ChatCompletionToolMessageParam {\n  /**\n   * The contents of the tool message.\n   */\n  content: string | Array<ChatCompletionContentPartText>;\n\n  /**\n   * The role of the messages author, in this case `tool`.\n   */\n  role: 'tool';\n\n  /**\n   * Tool call that this message is responding to.\n   */\n  tool_call_id: string;\n}\n\n/**\n * Messages sent by an end user, containing prompts or additional context\n * information.\n */\nexport interface ChatCompletionUserMessageParam {\n  /**\n   * The contents of the user message.\n   */\n  content: string | Array<ChatCompletionContentPart>;\n\n  /**\n   * The role of the messages author, in this case `user`.\n   */\n  role: 'user';\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n}\n\n/**\n * Constrains the tools available to the model to a pre-defined set.\n */\nexport interface ChatCompletionAllowedTools {\n  /**\n   * Constrains the tools available to the model to a pre-defined set.\n   *\n   * `auto` allows the model to pick from among the allowed tools and generate a\n   * message.\n   *\n   * `required` requires the model to call one or more of the allowed tools.\n   */\n  mode: 'auto' | 'required';\n\n  /**\n   * A list of tool definitions that the model should be allowed to call.\n   *\n   * For the Chat Completions API, the list of tool definitions might look like:\n   *\n   * ```json\n   * [\n   *   { \"type\": \"function\", \"function\": { \"name\": \"get_weather\" } },\n   *   { \"type\": \"function\", \"function\": { \"name\": \"get_time\" } }\n   * ]\n   * ```\n   */\n  tools: Array<{ [key: string]: unknown }>;\n}\n\nexport type ChatCompletionReasoningEffort = Shared.ReasoningEffort | null;\n\nexport type ChatCompletionCreateParams =\n  | ChatCompletionCreateParamsNonStreaming\n  | ChatCompletionCreateParamsStreaming;\n\nexport interface ChatCompletionCreateParamsBase {\n  /**\n   * A list of messages comprising the conversation so far. Depending on the\n   * [model](https://platform.openai.com/docs/models) you use, different message\n   * types (modalities) are supported, like\n   * [text](https://platform.openai.com/docs/guides/text-generation),\n   * [images](https://platform.openai.com/docs/guides/vision), and\n   * [audio](https://platform.openai.com/docs/guides/audio).\n   */\n  messages: Array<ChatCompletionMessageParam>;\n\n  /**\n   * Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a\n   * wide range of models with different capabilities, performance characteristics,\n   * and price points. Refer to the\n   * [model guide](https://platform.openai.com/docs/models) to browse and compare\n   * available models.\n   */\n  model: (string & {}) | Shared.ChatModel;\n\n  /**\n   * Parameters for audio output. Required when audio output is requested with\n   * `modalities: [\"audio\"]`.\n   * [Learn more](https://platform.openai.com/docs/guides/audio).\n   */\n  audio?: ChatCompletionAudioParam | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * @deprecated Deprecated in favor of `tool_choice`.\n   *\n   * Controls which (if any) function is called by the model.\n   *\n   * `none` means the model will not call a function and instead generates a message.\n   *\n   * `auto` means the model can pick between generating a message or calling a\n   * function.\n   *\n   * Specifying a particular function via `{\"name\": \"my_function\"}` forces the model\n   * to call that function.\n   *\n   * `none` is the default when no functions are present. `auto` is the default if\n   * functions are present.\n   */\n  function_call?: 'none' | 'auto' | ChatCompletionFunctionCallOption;\n\n  /**\n   * @deprecated Deprecated in favor of `tools`.\n   *\n   * A list of functions the model may generate JSON inputs for.\n   */\n  functions?: Array<ChatCompletionCreateParams.Function>;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in the\n   * tokenizer) to an associated bias value from -100 to 100. Mathematically, the\n   * bias is added to the logits generated by the model prior to sampling. The exact\n   * effect will vary per model, but values between -1 and 1 should decrease or\n   * increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: { [key: string]: number } | null;\n\n  /**\n   * Whether to return log probabilities of the output tokens or not. If true,\n   * returns the log probabilities of each output token returned in the `content` of\n   * `message`.\n   */\n  logprobs?: boolean | null;\n\n  /**\n   * An upper bound for the number of tokens that can be generated for a completion,\n   * including visible output tokens and\n   * [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\n   */\n  max_completion_tokens?: number | null;\n\n  /**\n   * @deprecated The maximum number of [tokens](/tokenizer) that can be generated in\n   * the chat completion. This value can be used to control\n   * [costs](https://openai.com/api/pricing/) for text generated via API.\n   *\n   * This value is now deprecated in favor of `max_completion_tokens`, and is not\n   * compatible with\n   * [o-series models](https://platform.openai.com/docs/guides/reasoning).\n   */\n  max_tokens?: number | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * Output types that you would like the model to generate. Most models are capable\n   * of generating text, which is the default:\n   *\n   * `[\"text\"]`\n   *\n   * The `gpt-4o-audio-preview` model can also be used to\n   * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n   * this model generate both text and audio responses, you can use:\n   *\n   * `[\"text\", \"audio\"]`\n   */\n  modalities?: Array<'text' | 'audio'> | null;\n\n  /**\n   * How many chat completion choices to generate for each input message. Note that\n   * you will be charged based on the number of generated tokens across all of the\n   * choices. Keep `n` as `1` to minimize costs.\n   */\n  n?: number | null;\n\n  /**\n   * Whether to enable\n   * [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n   * during tool use.\n   */\n  parallel_tool_calls?: boolean;\n\n  /**\n   * Static predicted output content, such as the content of a text file that is\n   * being regenerated.\n   */\n  prediction?: ChatCompletionPredictionContent | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates. Replaces the `user` field.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  prompt_cache_key?: string;\n\n  /**\n   * The retention policy for the prompt cache. Set to `24h` to enable extended\n   * prompt caching, which keeps cached prefixes active for longer, up to a maximum\n   * of 24 hours.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching#prompt-cache-retention).\n   */\n  prompt_cache_retention?: 'in-memory' | '24h' | null;\n\n  /**\n   * Constrains effort on reasoning for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n   * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n   * Reducing reasoning effort can result in faster responses and fewer tokens used\n   * on reasoning in a response.\n   *\n   * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n   *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n   *   calls are supported for all reasoning values in gpt-5.1.\n   * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n   *   support `none`.\n   * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n   * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n   */\n  reasoning_effort?: Shared.ReasoningEffort | null;\n\n  /**\n   * An object specifying the format that the model must output.\n   *\n   * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n   * Outputs which ensures the model will match your supplied JSON schema. Learn more\n   * in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n   * ensures the message the model generates is valid JSON. Using `json_schema` is\n   * preferred for models that support it.\n   */\n  response_format?:\n    | Shared.ResponseFormatText\n    | Shared.ResponseFormatJSONSchema\n    | Shared.ResponseFormatJSONObject;\n\n  /**\n   * A stable identifier used to help detect users of your application that may be\n   * violating OpenAI's usage policies. The IDs should be a string that uniquely\n   * identifies each user, with a maximum length of 64 characters. We recommend\n   * hashing their username or email address, in order to avoid sending us any\n   * identifying information.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  safety_identifier?: string;\n\n  /**\n   * @deprecated This feature is in Beta. If specified, our system will make a best\n   * effort to sample deterministically, such that repeated requests with the same\n   * `seed` and parameters should return the same result. Determinism is not\n   * guaranteed, and you should refer to the `system_fingerprint` response parameter\n   * to monitor changes in the backend.\n   */\n  seed?: number | null;\n\n  /**\n   * Specifies the processing type used for serving the request.\n   *\n   * - If set to 'auto', then the request will be processed with the service tier\n   *   configured in the Project settings. Unless otherwise configured, the Project\n   *   will use 'default'.\n   * - If set to 'default', then the request will be processed with the standard\n   *   pricing and performance for the selected model.\n   * - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\n   *   '[priority](https://openai.com/api-priority-processing/)', then the request\n   *   will be processed with the corresponding service tier.\n   * - When not set, the default behavior is 'auto'.\n   *\n   * When the `service_tier` parameter is set, the response body will include the\n   * `service_tier` value based on the processing mode actually used to serve the\n   * request. This response value may be different from the value set in the\n   * parameter.\n   */\n  service_tier?: 'auto' | 'default' | 'flex' | 'scale' | 'priority' | null;\n\n  /**\n   * Not supported with latest reasoning models `o3` and `o4-mini`.\n   *\n   * Up to 4 sequences where the API will stop generating further tokens. The\n   * returned text will not contain the stop sequence.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * Whether or not to store the output of this chat completion request for use in\n   * our [model distillation](https://platform.openai.com/docs/guides/distillation)\n   * or [evals](https://platform.openai.com/docs/guides/evals) products.\n   *\n   * Supports text and image inputs. Note: image inputs over 8MB will be dropped.\n   */\n  store?: boolean | null;\n\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)\n   * for more information, along with the\n   * [streaming responses](https://platform.openai.com/docs/guides/streaming-responses)\n   * guide for more information on how to handle the streaming events.\n   */\n  stream?: boolean | null;\n\n  /**\n   * Options for streaming response. Only set this when you set `stream: true`.\n   */\n  stream_options?: ChatCompletionStreamOptions | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic. We generally recommend altering this or `top_p` but\n   * not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * Controls which (if any) tool is called by the model. `none` means the model will\n   * not call any tool and instead generates a message. `auto` means the model can\n   * pick between generating a message or calling one or more tools. `required` means\n   * the model must call one or more tools. Specifying a particular tool via\n   * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n   * call that tool.\n   *\n   * `none` is the default when no tools are present. `auto` is the default if tools\n   * are present.\n   */\n  tool_choice?: ChatCompletionToolChoiceOption;\n\n  /**\n   * A list of tools the model may call. You can provide either\n   * [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n   * or [function tools](https://platform.openai.com/docs/guides/function-calling).\n   */\n  tools?: Array<ChatCompletionTool>;\n\n  /**\n   * An integer between 0 and 20 specifying the number of most likely tokens to\n   * return at each token position, each with an associated log probability.\n   * `logprobs` must be set to `true` if this parameter is used.\n   */\n  top_logprobs?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * @deprecated This field is being replaced by `safety_identifier` and\n   * `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching\n   * optimizations. A stable identifier for your end-users. Used to boost cache hit\n   * rates by better bucketing similar requests and to help OpenAI detect and prevent\n   * abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  user?: string;\n\n  /**\n   * Constrains the verbosity of the model's response. Lower values will result in\n   * more concise responses, while higher values will result in more verbose\n   * responses. Currently supported values are `low`, `medium`, and `high`.\n   */\n  verbosity?: 'low' | 'medium' | 'high' | null;\n\n  /**\n   * This tool searches the web for relevant results to use in a response. Learn more\n   * about the\n   * [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n   */\n  web_search_options?: ChatCompletionCreateParams.WebSearchOptions;\n}\n\nexport namespace ChatCompletionCreateParams {\n  /**\n   * @deprecated\n   */\n  export interface Function {\n    /**\n     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain\n     * underscores and dashes, with a maximum length of 64.\n     */\n    name: string;\n\n    /**\n     * A description of what the function does, used by the model to choose when and\n     * how to call the function.\n     */\n    description?: string;\n\n    /**\n     * The parameters the functions accepts, described as a JSON Schema object. See the\n     * [guide](https://platform.openai.com/docs/guides/function-calling) for examples,\n     * and the\n     * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n     * documentation about the format.\n     *\n     * Omitting `parameters` defines a function with an empty parameter list.\n     */\n    parameters?: Shared.FunctionParameters;\n  }\n\n  /**\n   * This tool searches the web for relevant results to use in a response. Learn more\n   * about the\n   * [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n   */\n  export interface WebSearchOptions {\n    /**\n     * High level guidance for the amount of context window space to use for the\n     * search. One of `low`, `medium`, or `high`. `medium` is the default.\n     */\n    search_context_size?: 'low' | 'medium' | 'high';\n\n    /**\n     * Approximate location parameters for the search.\n     */\n    user_location?: WebSearchOptions.UserLocation | null;\n  }\n\n  export namespace WebSearchOptions {\n    /**\n     * Approximate location parameters for the search.\n     */\n    export interface UserLocation {\n      /**\n       * Approximate location parameters for the search.\n       */\n      approximate: UserLocation.Approximate;\n\n      /**\n       * The type of location approximation. Always `approximate`.\n       */\n      type: 'approximate';\n    }\n\n    export namespace UserLocation {\n      /**\n       * Approximate location parameters for the search.\n       */\n      export interface Approximate {\n        /**\n         * Free text input for the city of the user, e.g. `San Francisco`.\n         */\n        city?: string;\n\n        /**\n         * The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of\n         * the user, e.g. `US`.\n         */\n        country?: string;\n\n        /**\n         * Free text input for the region of the user, e.g. `California`.\n         */\n        region?: string;\n\n        /**\n         * The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the\n         * user, e.g. `America/Los_Angeles`.\n         */\n        timezone?: string;\n      }\n    }\n  }\n\n  export type ChatCompletionCreateParamsNonStreaming =\n    CompletionsCompletionsAPI.ChatCompletionCreateParamsNonStreaming;\n  export type ChatCompletionCreateParamsStreaming =\n    CompletionsCompletionsAPI.ChatCompletionCreateParamsStreaming;\n}\n\nexport interface ChatCompletionCreateParamsNonStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)\n   * for more information, along with the\n   * [streaming responses](https://platform.openai.com/docs/guides/streaming-responses)\n   * guide for more information on how to handle the streaming events.\n   */\n  stream?: false | null;\n}\n\nexport interface ChatCompletionCreateParamsStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)\n   * for more information, along with the\n   * [streaming responses](https://platform.openai.com/docs/guides/streaming-responses)\n   * guide for more information on how to handle the streaming events.\n   */\n  stream: true;\n}\n\nexport interface ChatCompletionUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n}\n\nexport interface ChatCompletionListParams extends CursorPageParams {\n  /**\n   * A list of metadata keys to filter the Chat Completions by. Example:\n   *\n   * `metadata[key1]=value1&metadata[key2]=value2`\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The model used to generate the Chat Completions.\n   */\n  model?: string;\n\n  /**\n   * Sort order for Chat Completions by timestamp. Use `asc` for ascending order or\n   * `desc` for descending order. Defaults to `asc`.\n   */\n  order?: 'asc' | 'desc';\n}\n\nCompletions.Messages = Messages;\n\nexport declare namespace Completions {\n  export {\n    type ChatCompletion as ChatCompletion,\n    type ChatCompletionAllowedToolChoice as ChatCompletionAllowedToolChoice,\n    type ChatCompletionAssistantMessageParam as ChatCompletionAssistantMessageParam,\n    type ChatCompletionAudio as ChatCompletionAudio,\n    type ChatCompletionAudioParam as ChatCompletionAudioParam,\n    type ChatCompletionChunk as ChatCompletionChunk,\n    type ChatCompletionContentPart as ChatCompletionContentPart,\n    type ChatCompletionContentPartImage as ChatCompletionContentPartImage,\n    type ChatCompletionContentPartInputAudio as ChatCompletionContentPartInputAudio,\n    type ChatCompletionContentPartRefusal as ChatCompletionContentPartRefusal,\n    type ChatCompletionContentPartText as ChatCompletionContentPartText,\n    type ChatCompletionCustomTool as ChatCompletionCustomTool,\n    type ChatCompletionDeleted as ChatCompletionDeleted,\n    type ChatCompletionDeveloperMessageParam as ChatCompletionDeveloperMessageParam,\n    type ChatCompletionFunctionCallOption as ChatCompletionFunctionCallOption,\n    type ChatCompletionFunctionMessageParam as ChatCompletionFunctionMessageParam,\n    type ChatCompletionFunctionTool as ChatCompletionFunctionTool,\n    type ChatCompletionMessage as ChatCompletionMessage,\n    type ChatCompletionMessageCustomToolCall as ChatCompletionMessageCustomToolCall,\n    type ChatCompletionMessageFunctionToolCall as ChatCompletionMessageFunctionToolCall,\n    type ChatCompletionMessageParam as ChatCompletionMessageParam,\n    type ChatCompletionMessageToolCall as ChatCompletionMessageToolCall,\n    type ChatCompletionModality as ChatCompletionModality,\n    type ChatCompletionNamedToolChoice as ChatCompletionNamedToolChoice,\n    type ChatCompletionNamedToolChoiceCustom as ChatCompletionNamedToolChoiceCustom,\n    type ChatCompletionPredictionContent as ChatCompletionPredictionContent,\n    type ChatCompletionRole as ChatCompletionRole,\n    type ChatCompletionStoreMessage as ChatCompletionStoreMessage,\n    type ChatCompletionStreamOptions as ChatCompletionStreamOptions,\n    type ChatCompletionSystemMessageParam as ChatCompletionSystemMessageParam,\n    type ChatCompletionTokenLogprob as ChatCompletionTokenLogprob,\n    type ChatCompletionTool as ChatCompletionTool,\n    type ChatCompletionToolChoiceOption as ChatCompletionToolChoiceOption,\n    type ChatCompletionToolMessageParam as ChatCompletionToolMessageParam,\n    type ChatCompletionUserMessageParam as ChatCompletionUserMessageParam,\n    type ChatCompletionAllowedTools as ChatCompletionAllowedTools,\n    type ChatCompletionReasoningEffort as ChatCompletionReasoningEffort,\n    type ChatCompletionsPage as ChatCompletionsPage,\n    type ChatCompletionCreateParams as ChatCompletionCreateParams,\n    type ChatCompletionCreateParamsNonStreaming as ChatCompletionCreateParamsNonStreaming,\n    type ChatCompletionCreateParamsStreaming as ChatCompletionCreateParamsStreaming,\n    type ChatCompletionUpdateParams as ChatCompletionUpdateParams,\n    type ChatCompletionListParams as ChatCompletionListParams,\n  };\n\n  export { Messages as Messages, type MessageListParams as MessageListParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as Shared from '../shared';\nimport * as CompletionsAPI from './completions/completions';\nimport {\n  ChatCompletion,\n  ChatCompletionAllowedToolChoice,\n  ChatCompletionAllowedTools,\n  ChatCompletionAssistantMessageParam,\n  ChatCompletionAudio,\n  ChatCompletionAudioParam,\n  ChatCompletionChunk,\n  ChatCompletionContentPart,\n  ChatCompletionContentPartImage,\n  ChatCompletionContentPartInputAudio,\n  ChatCompletionContentPartRefusal,\n  ChatCompletionContentPartText,\n  ChatCompletionCreateParams,\n  ChatCompletionCreateParamsNonStreaming,\n  ChatCompletionCreateParamsStreaming,\n  ChatCompletionCustomTool,\n  ChatCompletionDeleted,\n  ChatCompletionDeveloperMessageParam,\n  ChatCompletionFunctionCallOption,\n  ChatCompletionFunctionMessageParam,\n  ChatCompletionFunctionTool,\n  ChatCompletionListParams,\n  ChatCompletionMessage,\n  ChatCompletionMessageCustomToolCall,\n  ChatCompletionMessageFunctionToolCall,\n  ChatCompletionMessageParam,\n  ChatCompletionMessageToolCall,\n  ChatCompletionModality,\n  ChatCompletionNamedToolChoice,\n  ChatCompletionNamedToolChoiceCustom,\n  ChatCompletionPredictionContent,\n  ChatCompletionReasoningEffort,\n  ChatCompletionRole,\n  ChatCompletionStoreMessage,\n  ChatCompletionStreamOptions,\n  ChatCompletionSystemMessageParam,\n  ChatCompletionTokenLogprob,\n  ChatCompletionTool,\n  ChatCompletionToolChoiceOption,\n  ChatCompletionToolMessageParam,\n  ChatCompletionUpdateParams,\n  ChatCompletionUserMessageParam,\n  ChatCompletionsPage,\n  Completions,\n} from './completions/completions';\n\nexport class Chat extends APIResource {\n  completions: CompletionsAPI.Completions = new CompletionsAPI.Completions(this._client);\n}\n\nexport type ChatModel = Shared.ChatModel;\n\nChat.Completions = Completions;\n\nexport declare namespace Chat {\n  export { type ChatModel as ChatModel };\n\n  export {\n    Completions as Completions,\n    type ChatCompletion as ChatCompletion,\n    type ChatCompletionAllowedToolChoice as ChatCompletionAllowedToolChoice,\n    type ChatCompletionAssistantMessageParam as ChatCompletionAssistantMessageParam,\n    type ChatCompletionAudio as ChatCompletionAudio,\n    type ChatCompletionAudioParam as ChatCompletionAudioParam,\n    type ChatCompletionChunk as ChatCompletionChunk,\n    type ChatCompletionContentPart as ChatCompletionContentPart,\n    type ChatCompletionContentPartImage as ChatCompletionContentPartImage,\n    type ChatCompletionContentPartInputAudio as ChatCompletionContentPartInputAudio,\n    type ChatCompletionContentPartRefusal as ChatCompletionContentPartRefusal,\n    type ChatCompletionContentPartText as ChatCompletionContentPartText,\n    type ChatCompletionCustomTool as ChatCompletionCustomTool,\n    type ChatCompletionDeleted as ChatCompletionDeleted,\n    type ChatCompletionDeveloperMessageParam as ChatCompletionDeveloperMessageParam,\n    type ChatCompletionFunctionCallOption as ChatCompletionFunctionCallOption,\n    type ChatCompletionFunctionMessageParam as ChatCompletionFunctionMessageParam,\n    type ChatCompletionFunctionTool as ChatCompletionFunctionTool,\n    type ChatCompletionMessage as ChatCompletionMessage,\n    type ChatCompletionMessageCustomToolCall as ChatCompletionMessageCustomToolCall,\n    type ChatCompletionMessageFunctionToolCall as ChatCompletionMessageFunctionToolCall,\n    type ChatCompletionMessageParam as ChatCompletionMessageParam,\n    type ChatCompletionMessageToolCall as ChatCompletionMessageToolCall,\n    type ChatCompletionModality as ChatCompletionModality,\n    type ChatCompletionNamedToolChoice as ChatCompletionNamedToolChoice,\n    type ChatCompletionNamedToolChoiceCustom as ChatCompletionNamedToolChoiceCustom,\n    type ChatCompletionPredictionContent as ChatCompletionPredictionContent,\n    type ChatCompletionRole as ChatCompletionRole,\n    type ChatCompletionStoreMessage as ChatCompletionStoreMessage,\n    type ChatCompletionStreamOptions as ChatCompletionStreamOptions,\n    type ChatCompletionSystemMessageParam as ChatCompletionSystemMessageParam,\n    type ChatCompletionTokenLogprob as ChatCompletionTokenLogprob,\n    type ChatCompletionTool as ChatCompletionTool,\n    type ChatCompletionToolChoiceOption as ChatCompletionToolChoiceOption,\n    type ChatCompletionToolMessageParam as ChatCompletionToolMessageParam,\n    type ChatCompletionUserMessageParam as ChatCompletionUserMessageParam,\n    type ChatCompletionAllowedTools as ChatCompletionAllowedTools,\n    type ChatCompletionReasoningEffort as ChatCompletionReasoningEffort,\n    type ChatCompletionsPage as ChatCompletionsPage,\n    type ChatCompletionCreateParams as ChatCompletionCreateParams,\n    type ChatCompletionCreateParamsNonStreaming as ChatCompletionCreateParamsNonStreaming,\n    type ChatCompletionCreateParamsStreaming as ChatCompletionCreateParamsStreaming,\n    type ChatCompletionUpdateParams as ChatCompletionUpdateParams,\n    type ChatCompletionListParams as ChatCompletionListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { isReadonlyArray } from './utils/values';\n\ntype HeaderValue = string | undefined | null;\nexport type HeadersLike =\n  | Headers\n  | readonly HeaderValue[][]\n  | Record<string, HeaderValue | readonly HeaderValue[]>\n  | undefined\n  | null\n  | NullableHeaders;\n\nconst brand_privateNullableHeaders = /* @__PURE__ */ Symbol('brand.privateNullableHeaders');\n\n/**\n * @internal\n * Users can pass explicit nulls to unset default headers. When we parse them\n * into a standard headers type we need to preserve that information.\n */\nexport type NullableHeaders = {\n  /** Brand check, prevent users from creating a NullableHeaders. */\n  [brand_privateNullableHeaders]: true;\n  /** Parsed headers. */\n  values: Headers;\n  /** Set of lowercase header names explicitly set to null. */\n  nulls: Set<string>;\n};\n\nfunction* iterateHeaders(headers: HeadersLike): IterableIterator<readonly [string, string | null]> {\n  if (!headers) return;\n\n  if (brand_privateNullableHeaders in headers) {\n    const { values, nulls } = headers;\n    yield* values.entries();\n    for (const name of nulls) {\n      yield [name, null];\n    }\n    return;\n  }\n\n  let shouldClear = false;\n  let iter: Iterable<readonly (HeaderValue | readonly HeaderValue[])[]>;\n  if (headers instanceof Headers) {\n    iter = headers.entries();\n  } else if (isReadonlyArray(headers)) {\n    iter = headers;\n  } else {\n    shouldClear = true;\n    iter = Object.entries(headers ?? {});\n  }\n  for (let row of iter) {\n    const name = row[0];\n    if (typeof name !== 'string') throw new TypeError('expected header name to be a string');\n    const values = isReadonlyArray(row[1]) ? row[1] : [row[1]];\n    let didClear = false;\n    for (const value of values) {\n      if (value === undefined) continue;\n\n      // Objects keys always overwrite older headers, they never append.\n      // Yield a null to clear the header before adding the new values.\n      if (shouldClear && !didClear) {\n        didClear = true;\n        yield [name, null];\n      }\n      yield [name, value];\n    }\n  }\n}\n\nexport const buildHeaders = (newHeaders: HeadersLike[]): NullableHeaders => {\n  const targetHeaders = new Headers();\n  const nullHeaders = new Set<string>();\n  for (const headers of newHeaders) {\n    const seenHeaders = new Set<string>();\n    for (const [name, value] of iterateHeaders(headers)) {\n      const lowerName = name.toLowerCase();\n      if (!seenHeaders.has(lowerName)) {\n        targetHeaders.delete(name);\n        seenHeaders.add(lowerName);\n      }\n      if (value === null) {\n        targetHeaders.delete(name);\n        nullHeaders.add(lowerName);\n      } else {\n        targetHeaders.append(name, value);\n        nullHeaders.delete(lowerName);\n      }\n    }\n  }\n  return { [brand_privateNullableHeaders]: true, values: targetHeaders, nulls: nullHeaders };\n};\n\nexport const isEmptyHeaders = (headers: HeadersLike) => {\n  for (const _ of iterateHeaders(headers)) return false;\n  return true;\n};\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport { APIPromise } from '../../core/api-promise';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\n\nexport class Speech extends APIResource {\n  /**\n   * Generates audio from the input text.\n   *\n   * Returns the audio file content, or a stream of audio events.\n   *\n   * @example\n   * ```ts\n   * const speech = await client.audio.speech.create({\n   *   input: 'input',\n   *   model: 'string',\n   *   voice: 'ash',\n   * });\n   *\n   * const content = await speech.blob();\n   * console.log(content);\n   * ```\n   */\n  create(body: SpeechCreateParams, options?: RequestOptions): APIPromise<Response> {\n    return this._client.post('/audio/speech', {\n      body,\n      ...options,\n      headers: buildHeaders([{ Accept: 'application/octet-stream' }, options?.headers]),\n      __binaryResponse: true,\n    });\n  }\n}\n\nexport type SpeechModel = 'tts-1' | 'tts-1-hd' | 'gpt-4o-mini-tts' | 'gpt-4o-mini-tts-2025-12-15';\n\nexport interface SpeechCreateParams {\n  /**\n   * The text to generate audio for. The maximum length is 4096 characters.\n   */\n  input: string;\n\n  /**\n   * One of the available [TTS models](https://platform.openai.com/docs/models#tts):\n   * `tts-1`, `tts-1-hd`, `gpt-4o-mini-tts`, or `gpt-4o-mini-tts-2025-12-15`.\n   */\n  model: (string & {}) | SpeechModel;\n\n  /**\n   * The voice to use when generating the audio. Supported built-in voices are\n   * `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`,\n   * `shimmer`, `verse`, `marin`, and `cedar`. Previews of the voices are available\n   * in the\n   * [Text to speech guide](https://platform.openai.com/docs/guides/text-to-speech#voice-options).\n   */\n  voice:\n    | (string & {})\n    | 'alloy'\n    | 'ash'\n    | 'ballad'\n    | 'coral'\n    | 'echo'\n    | 'sage'\n    | 'shimmer'\n    | 'verse'\n    | 'marin'\n    | 'cedar';\n\n  /**\n   * Control the voice of your generated audio with additional instructions. Does not\n   * work with `tts-1` or `tts-1-hd`.\n   */\n  instructions?: string;\n\n  /**\n   * The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`,\n   * `wav`, and `pcm`.\n   */\n  response_format?: 'mp3' | 'opus' | 'aac' | 'flac' | 'wav' | 'pcm';\n\n  /**\n   * The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is\n   * the default.\n   */\n  speed?: number;\n\n  /**\n   * The format to stream the audio in. Supported formats are `sse` and `audio`.\n   * `sse` is not supported for `tts-1` or `tts-1-hd`.\n   */\n  stream_format?: 'sse' | 'audio';\n}\n\nexport declare namespace Speech {\n  export { type SpeechModel as SpeechModel, type SpeechCreateParams as SpeechCreateParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as TranscriptionsAPI from './transcriptions';\nimport * as AudioAPI from './audio';\nimport { APIPromise } from '../../core/api-promise';\nimport { Stream } from '../../core/streaming';\nimport { type Uploadable } from '../../core/uploads';\nimport { RequestOptions } from '../../internal/request-options';\nimport { multipartFormRequestOptions } from '../../internal/uploads';\n\nexport class Transcriptions extends APIResource {\n  /**\n   * Transcribes audio into the input language.\n   *\n   * Returns a transcription object in `json`, `diarized_json`, or `verbose_json`\n   * format, or a stream of transcript events.\n   *\n   * @example\n   * ```ts\n   * const transcription =\n   *   await client.audio.transcriptions.create({\n   *     file: fs.createReadStream('speech.mp3'),\n   *     model: 'gpt-4o-transcribe',\n   *   });\n   * ```\n   */\n  create(\n    body: TranscriptionCreateParamsNonStreaming<'json' | undefined>,\n    options?: RequestOptions,\n  ): APIPromise<Transcription>;\n  create(\n    body: TranscriptionCreateParamsNonStreaming<'verbose_json'>,\n    options?: RequestOptions,\n  ): APIPromise<TranscriptionVerbose>;\n  create(\n    body: TranscriptionCreateParamsNonStreaming<'srt' | 'vtt' | 'text'>,\n    options?: RequestOptions,\n  ): APIPromise<string>;\n  create(body: TranscriptionCreateParamsNonStreaming, options?: RequestOptions): APIPromise<Transcription>;\n  create(\n    body: TranscriptionCreateParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<TranscriptionStreamEvent>>;\n  create(\n    body: TranscriptionCreateParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<TranscriptionCreateResponse | string | Stream<TranscriptionStreamEvent>>;\n  create(\n    body: TranscriptionCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<TranscriptionCreateResponse | string | Stream<TranscriptionStreamEvent>> {\n    return this._client.post(\n      '/audio/transcriptions',\n      multipartFormRequestOptions(\n        {\n          body,\n          ...options,\n          stream: body.stream ?? false,\n          __metadata: { model: body.model },\n        },\n        this._client,\n      ),\n    );\n  }\n}\n\n/**\n * Represents a transcription response returned by model, based on the provided\n * input.\n */\nexport interface Transcription {\n  /**\n   * The transcribed text.\n   */\n  text: string;\n\n  /**\n   * The log probabilities of the tokens in the transcription. Only returned with the\n   * models `gpt-4o-transcribe` and `gpt-4o-mini-transcribe` if `logprobs` is added\n   * to the `include` array.\n   */\n  logprobs?: Array<Transcription.Logprob>;\n\n  /**\n   * Token usage statistics for the request.\n   */\n  usage?: Transcription.Tokens | Transcription.Duration;\n}\n\nexport namespace Transcription {\n  export interface Logprob {\n    /**\n     * The token in the transcription.\n     */\n    token?: string;\n\n    /**\n     * The bytes of the token.\n     */\n    bytes?: Array<number>;\n\n    /**\n     * The log probability of the token.\n     */\n    logprob?: number;\n  }\n\n  /**\n   * Usage statistics for models billed by token usage.\n   */\n  export interface Tokens {\n    /**\n     * Number of input tokens billed for this request.\n     */\n    input_tokens: number;\n\n    /**\n     * Number of output tokens generated.\n     */\n    output_tokens: number;\n\n    /**\n     * Total number of tokens used (input + output).\n     */\n    total_tokens: number;\n\n    /**\n     * The type of the usage object. Always `tokens` for this variant.\n     */\n    type: 'tokens';\n\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    input_token_details?: Tokens.InputTokenDetails;\n  }\n\n  export namespace Tokens {\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    export interface InputTokenDetails {\n      /**\n       * Number of audio tokens billed for this request.\n       */\n      audio_tokens?: number;\n\n      /**\n       * Number of text tokens billed for this request.\n       */\n      text_tokens?: number;\n    }\n  }\n\n  /**\n   * Usage statistics for models billed by audio input duration.\n   */\n  export interface Duration {\n    /**\n     * Duration of the input audio in seconds.\n     */\n    seconds: number;\n\n    /**\n     * The type of the usage object. Always `duration` for this variant.\n     */\n    type: 'duration';\n  }\n}\n\n/**\n * Represents a diarized transcription response returned by the model, including\n * the combined transcript and speaker-segment annotations.\n */\nexport interface TranscriptionDiarized {\n  /**\n   * Duration of the input audio in seconds.\n   */\n  duration: number;\n\n  /**\n   * Segments of the transcript annotated with timestamps and speaker labels.\n   */\n  segments: Array<TranscriptionDiarizedSegment>;\n\n  /**\n   * The type of task that was run. Always `transcribe`.\n   */\n  task: 'transcribe';\n\n  /**\n   * The concatenated transcript text for the entire audio input.\n   */\n  text: string;\n\n  /**\n   * Token or duration usage statistics for the request.\n   */\n  usage?: TranscriptionDiarized.Tokens | TranscriptionDiarized.Duration;\n}\n\nexport namespace TranscriptionDiarized {\n  /**\n   * Usage statistics for models billed by token usage.\n   */\n  export interface Tokens {\n    /**\n     * Number of input tokens billed for this request.\n     */\n    input_tokens: number;\n\n    /**\n     * Number of output tokens generated.\n     */\n    output_tokens: number;\n\n    /**\n     * Total number of tokens used (input + output).\n     */\n    total_tokens: number;\n\n    /**\n     * The type of the usage object. Always `tokens` for this variant.\n     */\n    type: 'tokens';\n\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    input_token_details?: Tokens.InputTokenDetails;\n  }\n\n  export namespace Tokens {\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    export interface InputTokenDetails {\n      /**\n       * Number of audio tokens billed for this request.\n       */\n      audio_tokens?: number;\n\n      /**\n       * Number of text tokens billed for this request.\n       */\n      text_tokens?: number;\n    }\n  }\n\n  /**\n   * Usage statistics for models billed by audio input duration.\n   */\n  export interface Duration {\n    /**\n     * Duration of the input audio in seconds.\n     */\n    seconds: number;\n\n    /**\n     * The type of the usage object. Always `duration` for this variant.\n     */\n    type: 'duration';\n  }\n}\n\n/**\n * A segment of diarized transcript text with speaker metadata.\n */\nexport interface TranscriptionDiarizedSegment {\n  /**\n   * Unique identifier for the segment.\n   */\n  id: string;\n\n  /**\n   * End timestamp of the segment in seconds.\n   */\n  end: number;\n\n  /**\n   * Speaker label for this segment. When known speakers are provided, the label\n   * matches `known_speaker_names[]`. Otherwise speakers are labeled sequentially\n   * using capital letters (`A`, `B`, ...).\n   */\n  speaker: string;\n\n  /**\n   * Start timestamp of the segment in seconds.\n   */\n  start: number;\n\n  /**\n   * Transcript text for this segment.\n   */\n  text: string;\n\n  /**\n   * The type of the segment. Always `transcript.text.segment`.\n   */\n  type: 'transcript.text.segment';\n}\n\nexport type TranscriptionInclude = 'logprobs';\n\nexport interface TranscriptionSegment {\n  /**\n   * Unique identifier of the segment.\n   */\n  id: number;\n\n  /**\n   * Average logprob of the segment. If the value is lower than -1, consider the\n   * logprobs failed.\n   */\n  avg_logprob: number;\n\n  /**\n   * Compression ratio of the segment. If the value is greater than 2.4, consider the\n   * compression failed.\n   */\n  compression_ratio: number;\n\n  /**\n   * End time of the segment in seconds.\n   */\n  end: number;\n\n  /**\n   * Probability of no speech in the segment. If the value is higher than 1.0 and the\n   * `avg_logprob` is below -1, consider this segment silent.\n   */\n  no_speech_prob: number;\n\n  /**\n   * Seek offset of the segment.\n   */\n  seek: number;\n\n  /**\n   * Start time of the segment in seconds.\n   */\n  start: number;\n\n  /**\n   * Temperature parameter used for generating the segment.\n   */\n  temperature: number;\n\n  /**\n   * Text content of the segment.\n   */\n  text: string;\n\n  /**\n   * Array of token IDs for the text content.\n   */\n  tokens: Array<number>;\n}\n\n/**\n * Emitted when a diarized transcription returns a completed segment with speaker\n * information. Only emitted when you\n * [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)\n * with `stream` set to `true` and `response_format` set to `diarized_json`.\n */\nexport type TranscriptionStreamEvent =\n  | TranscriptionTextSegmentEvent\n  | TranscriptionTextDeltaEvent\n  | TranscriptionTextDoneEvent;\n\n/**\n * Emitted when there is an additional text delta. This is also the first event\n * emitted when the transcription starts. Only emitted when you\n * [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)\n * with the `Stream` parameter set to `true`.\n */\nexport interface TranscriptionTextDeltaEvent {\n  /**\n   * The text delta that was additionally transcribed.\n   */\n  delta: string;\n\n  /**\n   * The type of the event. Always `transcript.text.delta`.\n   */\n  type: 'transcript.text.delta';\n\n  /**\n   * The log probabilities of the delta. Only included if you\n   * [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)\n   * with the `include[]` parameter set to `logprobs`.\n   */\n  logprobs?: Array<TranscriptionTextDeltaEvent.Logprob>;\n\n  /**\n   * Identifier of the diarized segment that this delta belongs to. Only present when\n   * using `gpt-4o-transcribe-diarize`.\n   */\n  segment_id?: string;\n}\n\nexport namespace TranscriptionTextDeltaEvent {\n  export interface Logprob {\n    /**\n     * The token that was used to generate the log probability.\n     */\n    token?: string;\n\n    /**\n     * The bytes that were used to generate the log probability.\n     */\n    bytes?: Array<number>;\n\n    /**\n     * The log probability of the token.\n     */\n    logprob?: number;\n  }\n}\n\n/**\n * Emitted when the transcription is complete. Contains the complete transcription\n * text. Only emitted when you\n * [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)\n * with the `Stream` parameter set to `true`.\n */\nexport interface TranscriptionTextDoneEvent {\n  /**\n   * The text that was transcribed.\n   */\n  text: string;\n\n  /**\n   * The type of the event. Always `transcript.text.done`.\n   */\n  type: 'transcript.text.done';\n\n  /**\n   * The log probabilities of the individual tokens in the transcription. Only\n   * included if you\n   * [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)\n   * with the `include[]` parameter set to `logprobs`.\n   */\n  logprobs?: Array<TranscriptionTextDoneEvent.Logprob>;\n\n  /**\n   * Usage statistics for models billed by token usage.\n   */\n  usage?: TranscriptionTextDoneEvent.Usage;\n}\n\nexport namespace TranscriptionTextDoneEvent {\n  export interface Logprob {\n    /**\n     * The token that was used to generate the log probability.\n     */\n    token?: string;\n\n    /**\n     * The bytes that were used to generate the log probability.\n     */\n    bytes?: Array<number>;\n\n    /**\n     * The log probability of the token.\n     */\n    logprob?: number;\n  }\n\n  /**\n   * Usage statistics for models billed by token usage.\n   */\n  export interface Usage {\n    /**\n     * Number of input tokens billed for this request.\n     */\n    input_tokens: number;\n\n    /**\n     * Number of output tokens generated.\n     */\n    output_tokens: number;\n\n    /**\n     * Total number of tokens used (input + output).\n     */\n    total_tokens: number;\n\n    /**\n     * The type of the usage object. Always `tokens` for this variant.\n     */\n    type: 'tokens';\n\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    input_token_details?: Usage.InputTokenDetails;\n  }\n\n  export namespace Usage {\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    export interface InputTokenDetails {\n      /**\n       * Number of audio tokens billed for this request.\n       */\n      audio_tokens?: number;\n\n      /**\n       * Number of text tokens billed for this request.\n       */\n      text_tokens?: number;\n    }\n  }\n}\n\n/**\n * Emitted when a diarized transcription returns a completed segment with speaker\n * information. Only emitted when you\n * [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)\n * with `stream` set to `true` and `response_format` set to `diarized_json`.\n */\nexport interface TranscriptionTextSegmentEvent {\n  /**\n   * Unique identifier for the segment.\n   */\n  id: string;\n\n  /**\n   * End timestamp of the segment in seconds.\n   */\n  end: number;\n\n  /**\n   * Speaker label for this segment.\n   */\n  speaker: string;\n\n  /**\n   * Start timestamp of the segment in seconds.\n   */\n  start: number;\n\n  /**\n   * Transcript text for this segment.\n   */\n  text: string;\n\n  /**\n   * The type of the event. Always `transcript.text.segment`.\n   */\n  type: 'transcript.text.segment';\n}\n\n/**\n * Represents a verbose json transcription response returned by model, based on the\n * provided input.\n */\nexport interface TranscriptionVerbose {\n  /**\n   * The duration of the input audio.\n   */\n  duration: number;\n\n  /**\n   * The language of the input audio.\n   */\n  language: string;\n\n  /**\n   * The transcribed text.\n   */\n  text: string;\n\n  /**\n   * Segments of the transcribed text and their corresponding details.\n   */\n  segments?: Array<TranscriptionSegment>;\n\n  /**\n   * Usage statistics for models billed by audio input duration.\n   */\n  usage?: TranscriptionVerbose.Usage;\n\n  /**\n   * Extracted words and their corresponding timestamps.\n   */\n  words?: Array<TranscriptionWord>;\n}\n\nexport namespace TranscriptionVerbose {\n  /**\n   * Usage statistics for models billed by audio input duration.\n   */\n  export interface Usage {\n    /**\n     * Duration of the input audio in seconds.\n     */\n    seconds: number;\n\n    /**\n     * The type of the usage object. Always `duration` for this variant.\n     */\n    type: 'duration';\n  }\n}\n\nexport interface TranscriptionWord {\n  /**\n   * End time of the word in seconds.\n   */\n  end: number;\n\n  /**\n   * Start time of the word in seconds.\n   */\n  start: number;\n\n  /**\n   * The text content of the word.\n   */\n  word: string;\n}\n\n/**\n * Represents a transcription response returned by model, based on the provided\n * input.\n */\nexport type TranscriptionCreateResponse = Transcription | TranscriptionDiarized | TranscriptionVerbose;\n\nexport type TranscriptionCreateParams<\n  ResponseFormat extends AudioAPI.AudioResponseFormat | undefined = AudioAPI.AudioResponseFormat | undefined,\n> = TranscriptionCreateParamsNonStreaming<ResponseFormat> | TranscriptionCreateParamsStreaming;\n\nexport interface TranscriptionCreateParamsBase<\n  ResponseFormat extends AudioAPI.AudioResponseFormat | undefined = AudioAPI.AudioResponseFormat | undefined,\n> {\n  /**\n   * The audio file object (not file name) to transcribe, in one of these formats:\n   * flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uploadable;\n\n  /**\n   * ID of the model to use. The options are `gpt-4o-transcribe`,\n   * `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `whisper-1`\n   * (which is powered by our open source Whisper V2 model), and\n   * `gpt-4o-transcribe-diarize`.\n   */\n  model: (string & {}) | AudioAPI.AudioModel;\n\n  /**\n   * Controls how the audio is cut into chunks. When set to `\"auto\"`, the server\n   * first normalizes loudness and then uses voice activity detection (VAD) to choose\n   * boundaries. `server_vad` object can be provided to tweak VAD detection\n   * parameters manually. If unset, the audio is transcribed as a single block.\n   * Required when using `gpt-4o-transcribe-diarize` for inputs longer than 30\n   * seconds.\n   */\n  chunking_strategy?: 'auto' | TranscriptionCreateParams.VadConfig | null;\n\n  /**\n   * Additional information to include in the transcription response. `logprobs` will\n   * return the log probabilities of the tokens in the response to understand the\n   * model's confidence in the transcription. `logprobs` only works with\n   * response_format set to `json` and only with the models `gpt-4o-transcribe`,\n   * `gpt-4o-mini-transcribe`, and `gpt-4o-mini-transcribe-2025-12-15`. This field is\n   * not supported when using `gpt-4o-transcribe-diarize`.\n   */\n  include?: Array<TranscriptionInclude>;\n\n  /**\n   * Optional list of speaker names that correspond to the audio samples provided in\n   * `known_speaker_references[]`. Each entry should be a short identifier (for\n   * example `customer` or `agent`). Up to 4 speakers are supported.\n   */\n  known_speaker_names?: Array<string>;\n\n  /**\n   * Optional list of audio samples (as\n   * [data URLs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs))\n   * that contain known speaker references matching `known_speaker_names[]`. Each\n   * sample must be between 2 and 10 seconds, and can use any of the same input audio\n   * formats supported by `file`.\n   */\n  known_speaker_references?: Array<string>;\n\n  /**\n   * The language of the input audio. Supplying the input language in\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n   * format will improve accuracy and latency.\n   */\n  language?: string;\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The\n   * [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n   * should match the audio language. This field is not supported when using\n   * `gpt-4o-transcribe-diarize`.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the output, in one of these options: `json`, `text`, `srt`,\n   * `verbose_json`, `vtt`, or `diarized_json`. For `gpt-4o-transcribe` and\n   * `gpt-4o-mini-transcribe`, the only supported format is `json`. For\n   * `gpt-4o-transcribe-diarize`, the supported formats are `json`, `text`, and\n   * `diarized_json`, with `diarized_json` required to receive speaker annotations.\n   */\n  response_format?: ResponseFormat;\n\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section of the Speech-to-Text guide](https://platform.openai.com/docs/guides/speech-to-text?lang=curl#streaming-transcriptions)\n   * for more information.\n   *\n   * Note: Streaming is not supported for the `whisper-1` model and will be ignored.\n   */\n  stream?: boolean | null;\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n\n  /**\n   * The timestamp granularities to populate for this transcription.\n   * `response_format` must be set `verbose_json` to use timestamp granularities.\n   * Either or both of these options are supported: `word`, or `segment`. Note: There\n   * is no additional latency for segment timestamps, but generating word timestamps\n   * incurs additional latency. This option is not available for\n   * `gpt-4o-transcribe-diarize`.\n   */\n  timestamp_granularities?: Array<'word' | 'segment'>;\n}\n\nexport namespace TranscriptionCreateParams {\n  export interface VadConfig {\n    /**\n     * Must be set to `server_vad` to enable manual chunking using server side VAD.\n     */\n    type: 'server_vad';\n\n    /**\n     * Amount of audio to include before the VAD detected speech (in milliseconds).\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Duration of silence to detect speech stop (in milliseconds). With shorter values\n     * the model will respond more quickly, but may jump in on short pauses from the\n     * user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Sensitivity threshold (0.0 to 1.0) for voice activity detection. A higher\n     * threshold will require louder audio to activate the model, and thus might\n     * perform better in noisy environments.\n     */\n    threshold?: number;\n  }\n\n  export type TranscriptionCreateParamsNonStreaming = TranscriptionsAPI.TranscriptionCreateParamsNonStreaming;\n  export type TranscriptionCreateParamsStreaming = TranscriptionsAPI.TranscriptionCreateParamsStreaming;\n}\n\nexport interface TranscriptionCreateParamsNonStreaming<\n  ResponseFormat extends AudioAPI.AudioResponseFormat | undefined = AudioAPI.AudioResponseFormat | undefined,\n> extends TranscriptionCreateParamsBase<ResponseFormat> {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section of the Speech-to-Text guide](https://platform.openai.com/docs/guides/speech-to-text?lang=curl#streaming-transcriptions)\n   * for more information.\n   *\n   * Note: Streaming is not supported for the `whisper-1` model and will be ignored.\n   */\n  stream?: false | null;\n}\n\nexport interface TranscriptionCreateParamsStreaming extends TranscriptionCreateParamsBase {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section of the Speech-to-Text guide](https://platform.openai.com/docs/guides/speech-to-text?lang=curl#streaming-transcriptions)\n   * for more information.\n   *\n   * Note: Streaming is not supported for the `whisper-1` model and will be ignored.\n   */\n  stream: true;\n}\n\nexport declare namespace Transcriptions {\n  export {\n    type Transcription as Transcription,\n    type TranscriptionDiarized as TranscriptionDiarized,\n    type TranscriptionDiarizedSegment as TranscriptionDiarizedSegment,\n    type TranscriptionInclude as TranscriptionInclude,\n    type TranscriptionSegment as TranscriptionSegment,\n    type TranscriptionStreamEvent as TranscriptionStreamEvent,\n    type TranscriptionTextDeltaEvent as TranscriptionTextDeltaEvent,\n    type TranscriptionTextDoneEvent as TranscriptionTextDoneEvent,\n    type TranscriptionTextSegmentEvent as TranscriptionTextSegmentEvent,\n    type TranscriptionVerbose as TranscriptionVerbose,\n    type TranscriptionWord as TranscriptionWord,\n    type TranscriptionCreateResponse as TranscriptionCreateResponse,\n    type TranscriptionCreateParams as TranscriptionCreateParams,\n    type TranscriptionCreateParamsNonStreaming as TranscriptionCreateParamsNonStreaming,\n    type TranscriptionCreateParamsStreaming as TranscriptionCreateParamsStreaming,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as AudioAPI from './audio';\nimport * as TranscriptionsAPI from './transcriptions';\nimport { APIPromise } from '../../core/api-promise';\nimport { type Uploadable } from '../../core/uploads';\nimport { RequestOptions } from '../../internal/request-options';\nimport { multipartFormRequestOptions } from '../../internal/uploads';\n\nexport class Translations extends APIResource {\n  /**\n   * Translates audio into English.\n   *\n   * @example\n   * ```ts\n   * const translation = await client.audio.translations.create({\n   *   file: fs.createReadStream('speech.mp3'),\n   *   model: 'whisper-1',\n   * });\n   * ```\n   */\n  create(\n    body: TranslationCreateParams<'json' | undefined>,\n    options?: RequestOptions,\n  ): APIPromise<Translation>;\n  create(\n    body: TranslationCreateParams<'verbose_json'>,\n    options?: RequestOptions,\n  ): APIPromise<TranslationVerbose>;\n  create(body: TranslationCreateParams<'text' | 'srt' | 'vtt'>, options?: RequestOptions): APIPromise<string>;\n  create(body: TranslationCreateParams, options?: RequestOptions): APIPromise<Translation>;\n  create(\n    body: TranslationCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<TranslationCreateResponse | string> {\n    return this._client.post(\n      '/audio/translations',\n      multipartFormRequestOptions({ body, ...options, __metadata: { model: body.model } }, this._client),\n    );\n  }\n}\n\nexport interface Translation {\n  text: string;\n}\n\nexport interface TranslationVerbose {\n  /**\n   * The duration of the input audio.\n   */\n  duration: number;\n\n  /**\n   * The language of the output translation (always `english`).\n   */\n  language: string;\n\n  /**\n   * The translated text.\n   */\n  text: string;\n\n  /**\n   * Segments of the translated text and their corresponding details.\n   */\n  segments?: Array<TranscriptionsAPI.TranscriptionSegment>;\n}\n\nexport type TranslationCreateResponse = Translation | TranslationVerbose;\n\nexport interface TranslationCreateParams<\n  ResponseFormat extends AudioAPI.AudioResponseFormat | undefined = AudioAPI.AudioResponseFormat | undefined,\n> {\n  /**\n   * The audio file object (not file name) translate, in one of these formats: flac,\n   * mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uploadable;\n\n  /**\n   * ID of the model to use. Only `whisper-1` (which is powered by our open source\n   * Whisper V2 model) is currently available.\n   */\n  model: (string & {}) | AudioAPI.AudioModel;\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The\n   * [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n   * should be in English.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the output, in one of these options: `json`, `text`, `srt`,\n   * `verbose_json`, or `vtt`.\n   */\n  response_format?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n}\n\nexport declare namespace Translations {\n  export {\n    type Translation as Translation,\n    type TranslationVerbose as TranslationVerbose,\n    type TranslationCreateResponse as TranslationCreateResponse,\n    type TranslationCreateParams as TranslationCreateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as SpeechAPI from './speech';\nimport { Speech, SpeechCreateParams, SpeechModel } from './speech';\nimport * as TranscriptionsAPI from './transcriptions';\nimport {\n  Transcription,\n  TranscriptionCreateParams,\n  TranscriptionCreateParamsNonStreaming,\n  TranscriptionCreateParamsStreaming,\n  TranscriptionCreateResponse,\n  TranscriptionDiarized,\n  TranscriptionDiarizedSegment,\n  TranscriptionInclude,\n  TranscriptionSegment,\n  TranscriptionStreamEvent,\n  TranscriptionTextDeltaEvent,\n  TranscriptionTextDoneEvent,\n  TranscriptionTextSegmentEvent,\n  TranscriptionVerbose,\n  TranscriptionWord,\n  Transcriptions,\n} from './transcriptions';\nimport * as TranslationsAPI from './translations';\nimport {\n  Translation,\n  TranslationCreateParams,\n  TranslationCreateResponse,\n  TranslationVerbose,\n  Translations,\n} from './translations';\n\nexport class Audio extends APIResource {\n  transcriptions: TranscriptionsAPI.Transcriptions = new TranscriptionsAPI.Transcriptions(this._client);\n  translations: TranslationsAPI.Translations = new TranslationsAPI.Translations(this._client);\n  speech: SpeechAPI.Speech = new SpeechAPI.Speech(this._client);\n}\n\nexport type AudioModel =\n  | 'whisper-1'\n  | 'gpt-4o-transcribe'\n  | 'gpt-4o-mini-transcribe'\n  | 'gpt-4o-mini-transcribe-2025-12-15'\n  | 'gpt-4o-transcribe-diarize';\n\n/**\n * The format of the output, in one of these options: `json`, `text`, `srt`,\n * `verbose_json`, `vtt`, or `diarized_json`. For `gpt-4o-transcribe` and\n * `gpt-4o-mini-transcribe`, the only supported format is `json`. For\n * `gpt-4o-transcribe-diarize`, the supported formats are `json`, `text`, and\n * `diarized_json`, with `diarized_json` required to receive speaker annotations.\n */\nexport type AudioResponseFormat = 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt' | 'diarized_json';\n\nAudio.Transcriptions = Transcriptions;\nAudio.Translations = Translations;\nAudio.Speech = Speech;\n\nexport declare namespace Audio {\n  export { type AudioModel as AudioModel, type AudioResponseFormat as AudioResponseFormat };\n\n  export {\n    Transcriptions as Transcriptions,\n    type Transcription as Transcription,\n    type TranscriptionDiarized as TranscriptionDiarized,\n    type TranscriptionDiarizedSegment as TranscriptionDiarizedSegment,\n    type TranscriptionInclude as TranscriptionInclude,\n    type TranscriptionSegment as TranscriptionSegment,\n    type TranscriptionStreamEvent as TranscriptionStreamEvent,\n    type TranscriptionTextDeltaEvent as TranscriptionTextDeltaEvent,\n    type TranscriptionTextDoneEvent as TranscriptionTextDoneEvent,\n    type TranscriptionTextSegmentEvent as TranscriptionTextSegmentEvent,\n    type TranscriptionVerbose as TranscriptionVerbose,\n    type TranscriptionWord as TranscriptionWord,\n    type TranscriptionCreateResponse as TranscriptionCreateResponse,\n    type TranscriptionCreateParams as TranscriptionCreateParams,\n    type TranscriptionCreateParamsNonStreaming as TranscriptionCreateParamsNonStreaming,\n    type TranscriptionCreateParamsStreaming as TranscriptionCreateParamsStreaming,\n  };\n\n  export {\n    Translations as Translations,\n    type Translation as Translation,\n    type TranslationVerbose as TranslationVerbose,\n    type TranslationCreateResponse as TranslationCreateResponse,\n    type TranslationCreateParams as TranslationCreateParams,\n  };\n\n  export { Speech as Speech, type SpeechModel as SpeechModel, type SpeechCreateParams as SpeechCreateParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport * as BatchesAPI from './batches';\nimport * as Shared from './shared';\nimport { APIPromise } from '../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../core/pagination';\nimport { RequestOptions } from '../internal/request-options';\nimport { path } from '../internal/utils/path';\n\nexport class Batches extends APIResource {\n  /**\n   * Creates and executes a batch from an uploaded file of requests\n   */\n  create(body: BatchCreateParams, options?: RequestOptions): APIPromise<Batch> {\n    return this._client.post('/batches', { body, ...options });\n  }\n\n  /**\n   * Retrieves a batch.\n   */\n  retrieve(batchID: string, options?: RequestOptions): APIPromise<Batch> {\n    return this._client.get(path`/batches/${batchID}`, options);\n  }\n\n  /**\n   * List your organization's batches.\n   */\n  list(\n    query: BatchListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<BatchesPage, Batch> {\n    return this._client.getAPIList('/batches', CursorPage<Batch>, { query, ...options });\n  }\n\n  /**\n   * Cancels an in-progress batch. The batch will be in status `cancelling` for up to\n   * 10 minutes, before changing to `cancelled`, where it will have partial results\n   * (if any) available in the output file.\n   */\n  cancel(batchID: string, options?: RequestOptions): APIPromise<Batch> {\n    return this._client.post(path`/batches/${batchID}/cancel`, options);\n  }\n}\n\nexport type BatchesPage = CursorPage<Batch>;\n\nexport interface Batch {\n  id: string;\n\n  /**\n   * The time frame within which the batch should be processed.\n   */\n  completion_window: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch was created.\n   */\n  created_at: number;\n\n  /**\n   * The OpenAI API endpoint used by the batch.\n   */\n  endpoint: string;\n\n  /**\n   * The ID of the input file for the batch.\n   */\n  input_file_id: string;\n\n  /**\n   * The object type, which is always `batch`.\n   */\n  object: 'batch';\n\n  /**\n   * The current status of the batch.\n   */\n  status:\n    | 'validating'\n    | 'failed'\n    | 'in_progress'\n    | 'finalizing'\n    | 'completed'\n    | 'expired'\n    | 'cancelling'\n    | 'cancelled';\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch was cancelled.\n   */\n  cancelled_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch started cancelling.\n   */\n  cancelling_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch was completed.\n   */\n  completed_at?: number;\n\n  /**\n   * The ID of the file containing the outputs of requests with errors.\n   */\n  error_file_id?: string;\n\n  errors?: Batch.Errors;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch expired.\n   */\n  expired_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch will expire.\n   */\n  expires_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch failed.\n   */\n  failed_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch started finalizing.\n   */\n  finalizing_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch started processing.\n   */\n  in_progress_at?: number;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * Model ID used to process the batch, like `gpt-5-2025-08-07`. OpenAI offers a\n   * wide range of models with different capabilities, performance characteristics,\n   * and price points. Refer to the\n   * [model guide](https://platform.openai.com/docs/models) to browse and compare\n   * available models.\n   */\n  model?: string;\n\n  /**\n   * The ID of the file containing the outputs of successfully executed requests.\n   */\n  output_file_id?: string;\n\n  /**\n   * The request counts for different statuses within the batch.\n   */\n  request_counts?: BatchRequestCounts;\n\n  /**\n   * Represents token usage details including input tokens, output tokens, a\n   * breakdown of output tokens, and the total tokens used. Only populated on batches\n   * created after September 7, 2025.\n   */\n  usage?: BatchUsage;\n}\n\nexport namespace Batch {\n  export interface Errors {\n    data?: Array<BatchesAPI.BatchError>;\n\n    /**\n     * The object type, which is always `list`.\n     */\n    object?: string;\n  }\n}\n\nexport interface BatchError {\n  /**\n   * An error code identifying the error type.\n   */\n  code?: string;\n\n  /**\n   * The line number of the input file where the error occurred, if applicable.\n   */\n  line?: number | null;\n\n  /**\n   * A human-readable message providing more details about the error.\n   */\n  message?: string;\n\n  /**\n   * The name of the parameter that caused the error, if applicable.\n   */\n  param?: string | null;\n}\n\n/**\n * The request counts for different statuses within the batch.\n */\nexport interface BatchRequestCounts {\n  /**\n   * Number of requests that have been completed successfully.\n   */\n  completed: number;\n\n  /**\n   * Number of requests that have failed.\n   */\n  failed: number;\n\n  /**\n   * Total number of requests in the batch.\n   */\n  total: number;\n}\n\n/**\n * Represents token usage details including input tokens, output tokens, a\n * breakdown of output tokens, and the total tokens used. Only populated on batches\n * created after September 7, 2025.\n */\nexport interface BatchUsage {\n  /**\n   * The number of input tokens.\n   */\n  input_tokens: number;\n\n  /**\n   * A detailed breakdown of the input tokens.\n   */\n  input_tokens_details: BatchUsage.InputTokensDetails;\n\n  /**\n   * The number of output tokens.\n   */\n  output_tokens: number;\n\n  /**\n   * A detailed breakdown of the output tokens.\n   */\n  output_tokens_details: BatchUsage.OutputTokensDetails;\n\n  /**\n   * The total number of tokens used.\n   */\n  total_tokens: number;\n}\n\nexport namespace BatchUsage {\n  /**\n   * A detailed breakdown of the input tokens.\n   */\n  export interface InputTokensDetails {\n    /**\n     * The number of tokens that were retrieved from the cache.\n     * [More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    cached_tokens: number;\n  }\n\n  /**\n   * A detailed breakdown of the output tokens.\n   */\n  export interface OutputTokensDetails {\n    /**\n     * The number of reasoning tokens.\n     */\n    reasoning_tokens: number;\n  }\n}\n\nexport interface BatchCreateParams {\n  /**\n   * The time frame within which the batch should be processed. Currently only `24h`\n   * is supported.\n   */\n  completion_window: '24h';\n\n  /**\n   * The endpoint to be used for all requests in the batch. Currently\n   * `/v1/responses`, `/v1/chat/completions`, `/v1/embeddings`, `/v1/completions`,\n   * `/v1/moderations`, `/v1/images/generations`, and `/v1/images/edits` are\n   * supported. Note that `/v1/embeddings` batches are also restricted to a maximum\n   * of 50,000 embedding inputs across all requests in the batch.\n   */\n  endpoint:\n    | '/v1/responses'\n    | '/v1/chat/completions'\n    | '/v1/embeddings'\n    | '/v1/completions'\n    | '/v1/moderations'\n    | '/v1/images/generations'\n    | '/v1/images/edits';\n\n  /**\n   * The ID of an uploaded file that contains requests for the new batch.\n   *\n   * See [upload file](https://platform.openai.com/docs/api-reference/files/create)\n   * for how to upload a file.\n   *\n   * Your input file must be formatted as a\n   * [JSONL file](https://platform.openai.com/docs/api-reference/batch/request-input),\n   * and must be uploaded with the purpose `batch`. The file can contain up to 50,000\n   * requests, and can be up to 200 MB in size.\n   */\n  input_file_id: string;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The expiration policy for the output and/or error file that are generated for a\n   * batch.\n   */\n  output_expires_after?: BatchCreateParams.OutputExpiresAfter;\n}\n\nexport namespace BatchCreateParams {\n  /**\n   * The expiration policy for the output and/or error file that are generated for a\n   * batch.\n   */\n  export interface OutputExpiresAfter {\n    /**\n     * Anchor timestamp after which the expiration policy applies. Supported anchors:\n     * `created_at`. Note that the anchor is the file creation time, not the time the\n     * batch is created.\n     */\n    anchor: 'created_at';\n\n    /**\n     * The number of seconds after the anchor time that the file will expire. Must be\n     * between 3600 (1 hour) and 2592000 (30 days).\n     */\n    seconds: number;\n  }\n}\n\nexport interface BatchListParams extends CursorPageParams {}\n\nexport declare namespace Batches {\n  export {\n    type Batch as Batch,\n    type BatchError as BatchError,\n    type BatchRequestCounts as BatchRequestCounts,\n    type BatchUsage as BatchUsage,\n    type BatchesPage as BatchesPage,\n    type BatchCreateParams as BatchCreateParams,\n    type BatchListParams as BatchListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as Shared from '../shared';\nimport * as MessagesAPI from './threads/messages';\nimport * as ThreadsAPI from './threads/threads';\nimport * as RunsAPI from './threads/runs/runs';\nimport * as StepsAPI from './threads/runs/steps';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\nimport { AssistantStream } from '../../lib/AssistantStream';\n\nexport class Assistants extends APIResource {\n  /**\n   * Create an assistant with a model and instructions.\n   *\n   * @deprecated\n   */\n  create(body: AssistantCreateParams, options?: RequestOptions): APIPromise<Assistant> {\n    return this._client.post('/assistants', {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Retrieves an assistant.\n   *\n   * @deprecated\n   */\n  retrieve(assistantID: string, options?: RequestOptions): APIPromise<Assistant> {\n    return this._client.get(path`/assistants/${assistantID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Modifies an assistant.\n   *\n   * @deprecated\n   */\n  update(assistantID: string, body: AssistantUpdateParams, options?: RequestOptions): APIPromise<Assistant> {\n    return this._client.post(path`/assistants/${assistantID}`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Returns a list of assistants.\n   *\n   * @deprecated\n   */\n  list(\n    query: AssistantListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<AssistantsPage, Assistant> {\n    return this._client.getAPIList('/assistants', CursorPage<Assistant>, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Delete an assistant.\n   *\n   * @deprecated\n   */\n  delete(assistantID: string, options?: RequestOptions): APIPromise<AssistantDeleted> {\n    return this._client.delete(path`/assistants/${assistantID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n}\n\nexport type AssistantsPage = CursorPage<Assistant>;\n\n/**\n * @deprecated Represents an `assistant` that can call the model and use tools.\n */\nexport interface Assistant {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the assistant was created.\n   */\n  created_at: number;\n\n  /**\n   * The description of the assistant. The maximum length is 512 characters.\n   */\n  description: string | null;\n\n  /**\n   * The system instructions that the assistant uses. The maximum length is 256,000\n   * characters.\n   */\n  instructions: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models) for descriptions of\n   * them.\n   */\n  model: string;\n\n  /**\n   * The name of the assistant. The maximum length is 256 characters.\n   */\n  name: string | null;\n\n  /**\n   * The object type, which is always `assistant`.\n   */\n  object: 'assistant';\n\n  /**\n   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per\n   * assistant. Tools can be of types `code_interpreter`, `file_search`, or\n   * `function`.\n   */\n  tools: Array<AssistantTool>;\n\n  /**\n   * Specifies the format that the model must output. Compatible with\n   * [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),\n   * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n   * Outputs which ensures the model will match your supplied JSON schema. Learn more\n   * in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: ThreadsAPI.AssistantResponseFormatOption | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   */\n  temperature?: number | null;\n\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  tool_resources?: Assistant.ToolResources | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number | null;\n}\n\nexport namespace Assistant {\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n       * available to the `code_interpreter`` tool. There can be a maximum of 20 files\n       * associated with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * The ID of the\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this assistant. There can be a maximum of 1 vector store attached to\n       * the assistant.\n       */\n      vector_store_ids?: Array<string>;\n    }\n  }\n}\n\nexport interface AssistantDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'assistant.deleted';\n}\n\n/**\n * Represents an event emitted when streaming a Run.\n *\n * Each event in a server-sent events stream has an `event` and `data` property:\n *\n * ```\n * event: thread.created\n * data: {\"id\": \"thread_123\", \"object\": \"thread\", ...}\n * ```\n *\n * We emit events whenever a new object is created, transitions to a new state, or\n * is being streamed in parts (deltas). For example, we emit `thread.run.created`\n * when a new run is created, `thread.run.completed` when a run completes, and so\n * on. When an Assistant chooses to create a message during a run, we emit a\n * `thread.message.created event`, a `thread.message.in_progress` event, many\n * `thread.message.delta` events, and finally a `thread.message.completed` event.\n *\n * We may add additional events over time, so we recommend handling unknown events\n * gracefully in your code. See the\n * [Assistants API quickstart](https://platform.openai.com/docs/assistants/overview)\n * to learn how to integrate the Assistants API with streaming.\n */\nexport type AssistantStreamEvent =\n  | AssistantStreamEvent.ThreadCreated\n  | AssistantStreamEvent.ThreadRunCreated\n  | AssistantStreamEvent.ThreadRunQueued\n  | AssistantStreamEvent.ThreadRunInProgress\n  | AssistantStreamEvent.ThreadRunRequiresAction\n  | AssistantStreamEvent.ThreadRunCompleted\n  | AssistantStreamEvent.ThreadRunIncomplete\n  | AssistantStreamEvent.ThreadRunFailed\n  | AssistantStreamEvent.ThreadRunCancelling\n  | AssistantStreamEvent.ThreadRunCancelled\n  | AssistantStreamEvent.ThreadRunExpired\n  | AssistantStreamEvent.ThreadRunStepCreated\n  | AssistantStreamEvent.ThreadRunStepInProgress\n  | AssistantStreamEvent.ThreadRunStepDelta\n  | AssistantStreamEvent.ThreadRunStepCompleted\n  | AssistantStreamEvent.ThreadRunStepFailed\n  | AssistantStreamEvent.ThreadRunStepCancelled\n  | AssistantStreamEvent.ThreadRunStepExpired\n  | AssistantStreamEvent.ThreadMessageCreated\n  | AssistantStreamEvent.ThreadMessageInProgress\n  | AssistantStreamEvent.ThreadMessageDelta\n  | AssistantStreamEvent.ThreadMessageCompleted\n  | AssistantStreamEvent.ThreadMessageIncomplete\n  | AssistantStreamEvent.ErrorEvent;\n\nexport namespace AssistantStreamEvent {\n  /**\n   * Occurs when a new\n   * [thread](https://platform.openai.com/docs/api-reference/threads/object) is\n   * created.\n   */\n  export interface ThreadCreated {\n    /**\n     * Represents a thread that contains\n     * [messages](https://platform.openai.com/docs/api-reference/messages).\n     */\n    data: ThreadsAPI.Thread;\n\n    event: 'thread.created';\n\n    /**\n     * Whether to enable input audio transcription.\n     */\n    enabled?: boolean;\n  }\n\n  /**\n   * Occurs when a new\n   * [run](https://platform.openai.com/docs/api-reference/runs/object) is created.\n   */\n  export interface ThreadRunCreated {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.created';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to a `queued` status.\n   */\n  export interface ThreadRunQueued {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.queued';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to an `in_progress` status.\n   */\n  export interface ThreadRunInProgress {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.in_progress';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to a `requires_action` status.\n   */\n  export interface ThreadRunRequiresAction {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.requires_action';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * is completed.\n   */\n  export interface ThreadRunCompleted {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.completed';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * ends with status `incomplete`.\n   */\n  export interface ThreadRunIncomplete {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.incomplete';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * fails.\n   */\n  export interface ThreadRunFailed {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.failed';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to a `cancelling` status.\n   */\n  export interface ThreadRunCancelling {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.cancelling';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * is cancelled.\n   */\n  export interface ThreadRunCancelled {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.cancelled';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * expires.\n   */\n  export interface ThreadRunExpired {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.expired';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * is created.\n   */\n  export interface ThreadRunStepCreated {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.created';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * moves to an `in_progress` state.\n   */\n  export interface ThreadRunStepInProgress {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.in_progress';\n  }\n\n  /**\n   * Occurs when parts of a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * are being streamed.\n   */\n  export interface ThreadRunStepDelta {\n    /**\n     * Represents a run step delta i.e. any changed fields on a run step during\n     * streaming.\n     */\n    data: StepsAPI.RunStepDeltaEvent;\n\n    event: 'thread.run.step.delta';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * is completed.\n   */\n  export interface ThreadRunStepCompleted {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.completed';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * fails.\n   */\n  export interface ThreadRunStepFailed {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.failed';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * is cancelled.\n   */\n  export interface ThreadRunStepCancelled {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.cancelled';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * expires.\n   */\n  export interface ThreadRunStepExpired {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.expired';\n  }\n\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) is\n   * created.\n   */\n  export interface ThreadMessageCreated {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.created';\n  }\n\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) moves\n   * to an `in_progress` state.\n   */\n  export interface ThreadMessageInProgress {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.in_progress';\n  }\n\n  /**\n   * Occurs when parts of a\n   * [Message](https://platform.openai.com/docs/api-reference/messages/object) are\n   * being streamed.\n   */\n  export interface ThreadMessageDelta {\n    /**\n     * Represents a message delta i.e. any changed fields on a message during\n     * streaming.\n     */\n    data: MessagesAPI.MessageDeltaEvent;\n\n    event: 'thread.message.delta';\n  }\n\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) is\n   * completed.\n   */\n  export interface ThreadMessageCompleted {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.completed';\n  }\n\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) ends\n   * before it is completed.\n   */\n  export interface ThreadMessageIncomplete {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.incomplete';\n  }\n\n  /**\n   * Occurs when an\n   * [error](https://platform.openai.com/docs/guides/error-codes#api-errors) occurs.\n   * This can happen due to an internal server error or a timeout.\n   */\n  export interface ErrorEvent {\n    data: Shared.ErrorObject;\n\n    event: 'error';\n  }\n}\n\nexport type AssistantTool = CodeInterpreterTool | FileSearchTool | FunctionTool;\n\nexport interface CodeInterpreterTool {\n  /**\n   * The type of tool being defined: `code_interpreter`\n   */\n  type: 'code_interpreter';\n}\n\nexport interface FileSearchTool {\n  /**\n   * The type of tool being defined: `file_search`\n   */\n  type: 'file_search';\n\n  /**\n   * Overrides for the file search tool.\n   */\n  file_search?: FileSearchTool.FileSearch;\n}\n\nexport namespace FileSearchTool {\n  /**\n   * Overrides for the file search tool.\n   */\n  export interface FileSearch {\n    /**\n     * The maximum number of results the file search tool should output. The default is\n     * 20 for `gpt-4*` models and 5 for `gpt-3.5-turbo`. This number should be between\n     * 1 and 50 inclusive.\n     *\n     * Note that the file search tool may output fewer than `max_num_results` results.\n     * See the\n     * [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)\n     * for more information.\n     */\n    max_num_results?: number;\n\n    /**\n     * The ranking options for the file search. If not specified, the file search tool\n     * will use the `auto` ranker and a score_threshold of 0.\n     *\n     * See the\n     * [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)\n     * for more information.\n     */\n    ranking_options?: FileSearch.RankingOptions;\n  }\n\n  export namespace FileSearch {\n    /**\n     * The ranking options for the file search. If not specified, the file search tool\n     * will use the `auto` ranker and a score_threshold of 0.\n     *\n     * See the\n     * [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)\n     * for more information.\n     */\n    export interface RankingOptions {\n      /**\n       * The score threshold for the file search. All values must be a floating point\n       * number between 0 and 1.\n       */\n      score_threshold: number;\n\n      /**\n       * The ranker to use for the file search. If not specified will use the `auto`\n       * ranker.\n       */\n      ranker?: 'auto' | 'default_2024_08_21';\n    }\n  }\n}\n\nexport interface FunctionTool {\n  function: Shared.FunctionDefinition;\n\n  /**\n   * The type of tool being defined: `function`\n   */\n  type: 'function';\n}\n\n/**\n * Occurs when a\n * [message](https://platform.openai.com/docs/api-reference/messages/object) is\n * created.\n */\nexport type MessageStreamEvent =\n  | MessageStreamEvent.ThreadMessageCreated\n  | MessageStreamEvent.ThreadMessageInProgress\n  | MessageStreamEvent.ThreadMessageDelta\n  | MessageStreamEvent.ThreadMessageCompleted\n  | MessageStreamEvent.ThreadMessageIncomplete;\n\nexport namespace MessageStreamEvent {\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) is\n   * created.\n   */\n  export interface ThreadMessageCreated {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.created';\n  }\n\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) moves\n   * to an `in_progress` state.\n   */\n  export interface ThreadMessageInProgress {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.in_progress';\n  }\n\n  /**\n   * Occurs when parts of a\n   * [Message](https://platform.openai.com/docs/api-reference/messages/object) are\n   * being streamed.\n   */\n  export interface ThreadMessageDelta {\n    /**\n     * Represents a message delta i.e. any changed fields on a message during\n     * streaming.\n     */\n    data: MessagesAPI.MessageDeltaEvent;\n\n    event: 'thread.message.delta';\n  }\n\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) is\n   * completed.\n   */\n  export interface ThreadMessageCompleted {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.completed';\n  }\n\n  /**\n   * Occurs when a\n   * [message](https://platform.openai.com/docs/api-reference/messages/object) ends\n   * before it is completed.\n   */\n  export interface ThreadMessageIncomplete {\n    /**\n     * Represents a message within a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: MessagesAPI.Message;\n\n    event: 'thread.message.incomplete';\n  }\n}\n\n/**\n * Occurs when a\n * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n * is created.\n */\nexport type RunStepStreamEvent =\n  | RunStepStreamEvent.ThreadRunStepCreated\n  | RunStepStreamEvent.ThreadRunStepInProgress\n  | RunStepStreamEvent.ThreadRunStepDelta\n  | RunStepStreamEvent.ThreadRunStepCompleted\n  | RunStepStreamEvent.ThreadRunStepFailed\n  | RunStepStreamEvent.ThreadRunStepCancelled\n  | RunStepStreamEvent.ThreadRunStepExpired;\n\nexport namespace RunStepStreamEvent {\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * is created.\n   */\n  export interface ThreadRunStepCreated {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.created';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * moves to an `in_progress` state.\n   */\n  export interface ThreadRunStepInProgress {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.in_progress';\n  }\n\n  /**\n   * Occurs when parts of a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * are being streamed.\n   */\n  export interface ThreadRunStepDelta {\n    /**\n     * Represents a run step delta i.e. any changed fields on a run step during\n     * streaming.\n     */\n    data: StepsAPI.RunStepDeltaEvent;\n\n    event: 'thread.run.step.delta';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * is completed.\n   */\n  export interface ThreadRunStepCompleted {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.completed';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * fails.\n   */\n  export interface ThreadRunStepFailed {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.failed';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * is cancelled.\n   */\n  export interface ThreadRunStepCancelled {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.cancelled';\n  }\n\n  /**\n   * Occurs when a\n   * [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)\n   * expires.\n   */\n  export interface ThreadRunStepExpired {\n    /**\n     * Represents a step in execution of a run.\n     */\n    data: StepsAPI.RunStep;\n\n    event: 'thread.run.step.expired';\n  }\n}\n\n/**\n * Occurs when a new\n * [run](https://platform.openai.com/docs/api-reference/runs/object) is created.\n */\nexport type RunStreamEvent =\n  | RunStreamEvent.ThreadRunCreated\n  | RunStreamEvent.ThreadRunQueued\n  | RunStreamEvent.ThreadRunInProgress\n  | RunStreamEvent.ThreadRunRequiresAction\n  | RunStreamEvent.ThreadRunCompleted\n  | RunStreamEvent.ThreadRunIncomplete\n  | RunStreamEvent.ThreadRunFailed\n  | RunStreamEvent.ThreadRunCancelling\n  | RunStreamEvent.ThreadRunCancelled\n  | RunStreamEvent.ThreadRunExpired;\n\nexport namespace RunStreamEvent {\n  /**\n   * Occurs when a new\n   * [run](https://platform.openai.com/docs/api-reference/runs/object) is created.\n   */\n  export interface ThreadRunCreated {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.created';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to a `queued` status.\n   */\n  export interface ThreadRunQueued {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.queued';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to an `in_progress` status.\n   */\n  export interface ThreadRunInProgress {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.in_progress';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to a `requires_action` status.\n   */\n  export interface ThreadRunRequiresAction {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.requires_action';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * is completed.\n   */\n  export interface ThreadRunCompleted {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.completed';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * ends with status `incomplete`.\n   */\n  export interface ThreadRunIncomplete {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.incomplete';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * fails.\n   */\n  export interface ThreadRunFailed {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.failed';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * moves to a `cancelling` status.\n   */\n  export interface ThreadRunCancelling {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.cancelling';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * is cancelled.\n   */\n  export interface ThreadRunCancelled {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.cancelled';\n  }\n\n  /**\n   * Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)\n   * expires.\n   */\n  export interface ThreadRunExpired {\n    /**\n     * Represents an execution run on a\n     * [thread](https://platform.openai.com/docs/api-reference/threads).\n     */\n    data: RunsAPI.Run;\n\n    event: 'thread.run.expired';\n  }\n}\n\n/**\n * Occurs when a new\n * [thread](https://platform.openai.com/docs/api-reference/threads/object) is\n * created.\n */\nexport interface ThreadStreamEvent {\n  /**\n   * Represents a thread that contains\n   * [messages](https://platform.openai.com/docs/api-reference/messages).\n   */\n  data: ThreadsAPI.Thread;\n\n  event: 'thread.created';\n\n  /**\n   * Whether to enable input audio transcription.\n   */\n  enabled?: boolean;\n}\n\nexport interface AssistantCreateParams {\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models) for descriptions of\n   * them.\n   */\n  model: (string & {}) | Shared.ChatModel;\n\n  /**\n   * The description of the assistant. The maximum length is 512 characters.\n   */\n  description?: string | null;\n\n  /**\n   * The system instructions that the assistant uses. The maximum length is 256,000\n   * characters.\n   */\n  instructions?: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The name of the assistant. The maximum length is 256 characters.\n   */\n  name?: string | null;\n\n  /**\n   * Constrains effort on reasoning for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n   * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n   * Reducing reasoning effort can result in faster responses and fewer tokens used\n   * on reasoning in a response.\n   *\n   * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n   *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n   *   calls are supported for all reasoning values in gpt-5.1.\n   * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n   *   support `none`.\n   * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n   * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n   */\n  reasoning_effort?: Shared.ReasoningEffort | null;\n\n  /**\n   * Specifies the format that the model must output. Compatible with\n   * [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),\n   * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n   * Outputs which ensures the model will match your supplied JSON schema. Learn more\n   * in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: ThreadsAPI.AssistantResponseFormatOption | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   */\n  temperature?: number | null;\n\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  tool_resources?: AssistantCreateParams.ToolResources | null;\n\n  /**\n   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per\n   * assistant. Tools can be of types `code_interpreter`, `file_search`, or\n   * `function`.\n   */\n  tools?: Array<AssistantTool>;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number | null;\n}\n\nexport namespace AssistantCreateParams {\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n       * available to the `code_interpreter` tool. There can be a maximum of 20 files\n       * associated with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * The\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this assistant. There can be a maximum of 1 vector store attached to\n       * the assistant.\n       */\n      vector_store_ids?: Array<string>;\n\n      /**\n       * A helper to create a\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * with file_ids and attach it to this assistant. There can be a maximum of 1\n       * vector store attached to the assistant.\n       */\n      vector_stores?: Array<FileSearch.VectorStore>;\n    }\n\n    export namespace FileSearch {\n      export interface VectorStore {\n        /**\n         * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n         * strategy.\n         */\n        chunking_strategy?: VectorStore.Auto | VectorStore.Static;\n\n        /**\n         * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to\n         * add to the vector store. There can be a maximum of 10000 files in a vector\n         * store.\n         */\n        file_ids?: Array<string>;\n\n        /**\n         * Set of 16 key-value pairs that can be attached to an object. This can be useful\n         * for storing additional information about the object in a structured format, and\n         * querying for objects via API or the dashboard.\n         *\n         * Keys are strings with a maximum length of 64 characters. Values are strings with\n         * a maximum length of 512 characters.\n         */\n        metadata?: Shared.Metadata | null;\n      }\n\n      export namespace VectorStore {\n        /**\n         * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of\n         * `800` and `chunk_overlap_tokens` of `400`.\n         */\n        export interface Auto {\n          /**\n           * Always `auto`.\n           */\n          type: 'auto';\n        }\n\n        export interface Static {\n          static: Static.Static;\n\n          /**\n           * Always `static`.\n           */\n          type: 'static';\n        }\n\n        export namespace Static {\n          export interface Static {\n            /**\n             * The number of tokens that overlap between chunks. The default value is `400`.\n             *\n             * Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n             */\n            chunk_overlap_tokens: number;\n\n            /**\n             * The maximum number of tokens in each chunk. The default value is `800`. The\n             * minimum value is `100` and the maximum value is `4096`.\n             */\n            max_chunk_size_tokens: number;\n          }\n        }\n      }\n    }\n  }\n}\n\nexport interface AssistantUpdateParams {\n  /**\n   * The description of the assistant. The maximum length is 512 characters.\n   */\n  description?: string | null;\n\n  /**\n   * The system instructions that the assistant uses. The maximum length is 256,000\n   * characters.\n   */\n  instructions?: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models) for descriptions of\n   * them.\n   */\n  model?:\n    | (string & {})\n    | 'gpt-5'\n    | 'gpt-5-mini'\n    | 'gpt-5-nano'\n    | 'gpt-5-2025-08-07'\n    | 'gpt-5-mini-2025-08-07'\n    | 'gpt-5-nano-2025-08-07'\n    | 'gpt-4.1'\n    | 'gpt-4.1-mini'\n    | 'gpt-4.1-nano'\n    | 'gpt-4.1-2025-04-14'\n    | 'gpt-4.1-mini-2025-04-14'\n    | 'gpt-4.1-nano-2025-04-14'\n    | 'o3-mini'\n    | 'o3-mini-2025-01-31'\n    | 'o1'\n    | 'o1-2024-12-17'\n    | 'gpt-4o'\n    | 'gpt-4o-2024-11-20'\n    | 'gpt-4o-2024-08-06'\n    | 'gpt-4o-2024-05-13'\n    | 'gpt-4o-mini'\n    | 'gpt-4o-mini-2024-07-18'\n    | 'gpt-4.5-preview'\n    | 'gpt-4.5-preview-2025-02-27'\n    | 'gpt-4-turbo'\n    | 'gpt-4-turbo-2024-04-09'\n    | 'gpt-4-0125-preview'\n    | 'gpt-4-turbo-preview'\n    | 'gpt-4-1106-preview'\n    | 'gpt-4-vision-preview'\n    | 'gpt-4'\n    | 'gpt-4-0314'\n    | 'gpt-4-0613'\n    | 'gpt-4-32k'\n    | 'gpt-4-32k-0314'\n    | 'gpt-4-32k-0613'\n    | 'gpt-3.5-turbo'\n    | 'gpt-3.5-turbo-16k'\n    | 'gpt-3.5-turbo-0613'\n    | 'gpt-3.5-turbo-1106'\n    | 'gpt-3.5-turbo-0125'\n    | 'gpt-3.5-turbo-16k-0613';\n\n  /**\n   * The name of the assistant. The maximum length is 256 characters.\n   */\n  name?: string | null;\n\n  /**\n   * Constrains effort on reasoning for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n   * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n   * Reducing reasoning effort can result in faster responses and fewer tokens used\n   * on reasoning in a response.\n   *\n   * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n   *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n   *   calls are supported for all reasoning values in gpt-5.1.\n   * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n   *   support `none`.\n   * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n   * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n   */\n  reasoning_effort?: Shared.ReasoningEffort | null;\n\n  /**\n   * Specifies the format that the model must output. Compatible with\n   * [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),\n   * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n   * Outputs which ensures the model will match your supplied JSON schema. Learn more\n   * in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: ThreadsAPI.AssistantResponseFormatOption | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   */\n  temperature?: number | null;\n\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  tool_resources?: AssistantUpdateParams.ToolResources | null;\n\n  /**\n   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per\n   * assistant. Tools can be of types `code_interpreter`, `file_search`, or\n   * `function`.\n   */\n  tools?: Array<AssistantTool>;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number | null;\n}\n\nexport namespace AssistantUpdateParams {\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * Overrides the list of\n       * [file](https://platform.openai.com/docs/api-reference/files) IDs made available\n       * to the `code_interpreter` tool. There can be a maximum of 20 files associated\n       * with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * Overrides the\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this assistant. There can be a maximum of 1 vector store attached to\n       * the assistant.\n       */\n      vector_store_ids?: Array<string>;\n    }\n  }\n}\n\nexport interface AssistantListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * starting with obj_foo, your subsequent call can include before=obj_foo in order\n   * to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport declare namespace Assistants {\n  export {\n    type Assistant as Assistant,\n    type AssistantDeleted as AssistantDeleted,\n    type AssistantStreamEvent as AssistantStreamEvent,\n    type AssistantTool as AssistantTool,\n    type CodeInterpreterTool as CodeInterpreterTool,\n    type FileSearchTool as FileSearchTool,\n    type FunctionTool as FunctionTool,\n    type MessageStreamEvent as MessageStreamEvent,\n    type RunStepStreamEvent as RunStepStreamEvent,\n    type RunStreamEvent as RunStreamEvent,\n    type ThreadStreamEvent as ThreadStreamEvent,\n    type AssistantsPage as AssistantsPage,\n    type AssistantCreateParams as AssistantCreateParams,\n    type AssistantUpdateParams as AssistantUpdateParams,\n    type AssistantListParams as AssistantListParams,\n  };\n\n  export { AssistantStream };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport { APIPromise } from '../../../core/api-promise';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\n\nexport class Sessions extends APIResource {\n  /**\n   * Create an ephemeral API token for use in client-side applications with the\n   * Realtime API. Can be configured with the same session parameters as the\n   * `session.update` client event.\n   *\n   * It responds with a session object, plus a `client_secret` key which contains a\n   * usable ephemeral API token that can be used to authenticate browser clients for\n   * the Realtime API.\n   *\n   * @example\n   * ```ts\n   * const session =\n   *   await client.beta.realtime.sessions.create();\n   * ```\n   */\n  create(body: SessionCreateParams, options?: RequestOptions): APIPromise<SessionCreateResponse> {\n    return this._client.post('/realtime/sessions', {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n}\n\n/**\n * Realtime session object configuration.\n */\nexport interface Session {\n  /**\n   * Unique identifier for the session that looks like `sess_1234567890abcdef`.\n   */\n  id?: string;\n\n  /**\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\n   * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\n   * (mono), and little-endian byte order.\n   */\n  input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  input_audio_noise_reduction?: Session.InputAudioNoiseReduction;\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously through\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n   * and should be treated as guidance of input audio content rather than precisely\n   * what the model heard. The client can optionally set the language and prompt for\n   * transcription, these offer additional guidance to the transcription service.\n   */\n  input_audio_transcription?: Session.InputAudioTranscription;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior.\n   *\n   * Note that the server sets default instructions which will be used if this field\n   * is not set and are visible in the `session.created` event at the start of the\n   * session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_response_output_tokens?: number | 'inf';\n\n  /**\n   * The set of modalities the model can respond with. To disable audio, set this to\n   * [\"text\"].\n   */\n  modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * The Realtime model used for this session.\n   */\n  model?:\n    | 'gpt-4o-realtime-preview'\n    | 'gpt-4o-realtime-preview-2024-10-01'\n    | 'gpt-4o-realtime-preview-2024-12-17'\n    | 'gpt-4o-realtime-preview-2025-06-03'\n    | 'gpt-4o-mini-realtime-preview'\n    | 'gpt-4o-mini-realtime-preview-2024-12-17';\n\n  /**\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n   * For `pcm16`, output audio is sampled at a rate of 24kHz.\n   */\n  output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\n   * minimum speed. 1.5 is the maximum speed. This value can only be changed in\n   * between model turns, not while a response is in progress.\n   */\n  speed?: number;\n\n  /**\n   * Sampling temperature for the model, limited to [0.6, 1.2]. For audio models a\n   * temperature of 0.8 is highly recommended for best performance.\n   */\n  temperature?: number;\n\n  /**\n   * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\n   * a function.\n   */\n  tool_choice?: string;\n\n  /**\n   * Tools (functions) available to the model.\n   */\n  tools?: Array<Session.Tool>;\n\n  /**\n   * Configuration options for tracing. Set to null to disable tracing. Once tracing\n   * is enabled for a session, the configuration cannot be modified.\n   *\n   * `auto` will create a trace for the session with default values for the workflow\n   * name, group id, and metadata.\n   */\n  tracing?: 'auto' | Session.TracingConfiguration;\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response. Server VAD means that the model will detect the start and end of\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\n   * is more advanced and uses a turn detection model (in conjunction with VAD) to\n   * semantically estimate whether the user has finished speaking, then dynamically\n   * sets a timeout based on this probability. For example, if user audio trails off\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\n   * for the user to continue speaking. This can be useful for more natural\n   * conversations, but may have a higher latency.\n   */\n  turn_detection?: Session.TurnDetection;\n\n  /**\n   * The voice the model uses to respond. Voice cannot be changed during the session\n   * once the model has responded with audio at least once. Current voice options are\n   * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n   */\n  voice?: (string & {}) | 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';\n}\n\nexport namespace Session {\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  export interface InputAudioNoiseReduction {\n    /**\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\n     * headphones, `far_field` is for far-field microphones such as laptop or\n     * conference room microphones.\n     */\n    type?: 'near_field' | 'far_field';\n  }\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously through\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n   * and should be treated as guidance of input audio content rather than precisely\n   * what the model heard. The client can optionally set the language and prompt for\n   * transcription, these offer additional guidance to the transcription service.\n   */\n  export interface InputAudioTranscription {\n    /**\n     * The language of the input audio. Supplying the input language in\n     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n     * format will improve accuracy and latency.\n     */\n    language?: string;\n\n    /**\n     * The model to use for transcription, current options are `gpt-4o-transcribe`,\n     * `gpt-4o-mini-transcribe`, and `whisper-1`.\n     */\n    model?: string;\n\n    /**\n     * An optional text to guide the model's style or continue a previous audio\n     * segment. For `whisper-1`, the\n     * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n     * For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n     * \"expect words related to technology\".\n     */\n    prompt?: string;\n  }\n\n  export interface Tool {\n    /**\n     * The description of the function, including guidance on when and how to call it,\n     * and guidance about what to tell the user when calling (if anything).\n     */\n    description?: string;\n\n    /**\n     * The name of the function.\n     */\n    name?: string;\n\n    /**\n     * Parameters of the function in JSON Schema.\n     */\n    parameters?: unknown;\n\n    /**\n     * The type of the tool, i.e. `function`.\n     */\n    type?: 'function';\n  }\n\n  /**\n   * Granular configuration for tracing.\n   */\n  export interface TracingConfiguration {\n    /**\n     * The group id to attach to this trace to enable filtering and grouping in the\n     * traces dashboard.\n     */\n    group_id?: string;\n\n    /**\n     * The arbitrary metadata to attach to this trace to enable filtering in the traces\n     * dashboard.\n     */\n    metadata?: unknown;\n\n    /**\n     * The name of the workflow to attach to this trace. This is used to name the trace\n     * in the traces dashboard.\n     */\n    workflow_name?: string;\n  }\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response. Server VAD means that the model will detect the start and end of\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\n   * is more advanced and uses a turn detection model (in conjunction with VAD) to\n   * semantically estimate whether the user has finished speaking, then dynamically\n   * sets a timeout based on this probability. For example, if user audio trails off\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\n   * for the user to continue speaking. This can be useful for more natural\n   * conversations, but may have a higher latency.\n   */\n  export interface TurnDetection {\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs.\n     */\n    create_response?: boolean;\n\n    /**\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n     * will wait longer for the user to continue speaking, `high` will respond more\n     * quickly. `auto` is the default and is equivalent to `medium`.\n     */\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n    /**\n     * Whether or not to automatically interrupt any ongoing response with output to\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n     * occurs.\n     */\n    interrupt_response?: boolean;\n\n    /**\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\n     * detected speech (in milliseconds). Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\n     * more quickly, but may jump in on short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\n     * model, and thus might perform better in noisy environments.\n     */\n    threshold?: number;\n\n    /**\n     * Type of turn detection.\n     */\n    type?: 'server_vad' | 'semantic_vad';\n  }\n}\n\n/**\n * A new Realtime session configuration, with an ephemeral key. Default TTL for\n * keys is one minute.\n */\nexport interface SessionCreateResponse {\n  /**\n   * Ephemeral key returned by the API.\n   */\n  client_secret: SessionCreateResponse.ClientSecret;\n\n  /**\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n   */\n  input_audio_format?: string;\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously and should be treated as rough guidance rather than the\n   * representation understood by the model.\n   */\n  input_audio_transcription?: SessionCreateResponse.InputAudioTranscription;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior.\n   *\n   * Note that the server sets default instructions which will be used if this field\n   * is not set and are visible in the `session.created` event at the start of the\n   * session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_response_output_tokens?: number | 'inf';\n\n  /**\n   * The set of modalities the model can respond with. To disable audio, set this to\n   * [\"text\"].\n   */\n  modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n   */\n  output_audio_format?: string;\n\n  /**\n   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\n   * minimum speed. 1.5 is the maximum speed. This value can only be changed in\n   * between model turns, not while a response is in progress.\n   */\n  speed?: number;\n\n  /**\n   * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\n   */\n  temperature?: number;\n\n  /**\n   * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\n   * a function.\n   */\n  tool_choice?: string;\n\n  /**\n   * Tools (functions) available to the model.\n   */\n  tools?: Array<SessionCreateResponse.Tool>;\n\n  /**\n   * Configuration options for tracing. Set to null to disable tracing. Once tracing\n   * is enabled for a session, the configuration cannot be modified.\n   *\n   * `auto` will create a trace for the session with default values for the workflow\n   * name, group id, and metadata.\n   */\n  tracing?: 'auto' | SessionCreateResponse.TracingConfiguration;\n\n  /**\n   * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n   * means that the model will detect the start and end of speech based on audio\n   * volume and respond at the end of user speech.\n   */\n  turn_detection?: SessionCreateResponse.TurnDetection;\n\n  /**\n   * The voice the model uses to respond. Voice cannot be changed during the session\n   * once the model has responded with audio at least once. Current voice options are\n   * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n   */\n  voice?: (string & {}) | 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';\n}\n\nexport namespace SessionCreateResponse {\n  /**\n   * Ephemeral key returned by the API.\n   */\n  export interface ClientSecret {\n    /**\n     * Timestamp for when the token expires. Currently, all tokens expire after one\n     * minute.\n     */\n    expires_at: number;\n\n    /**\n     * Ephemeral key usable in client environments to authenticate connections to the\n     * Realtime API. Use this in client-side environments rather than a standard API\n     * token, which should only be used server-side.\n     */\n    value: string;\n  }\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously and should be treated as rough guidance rather than the\n   * representation understood by the model.\n   */\n  export interface InputAudioTranscription {\n    /**\n     * The model to use for transcription.\n     */\n    model?: string;\n  }\n\n  export interface Tool {\n    /**\n     * The description of the function, including guidance on when and how to call it,\n     * and guidance about what to tell the user when calling (if anything).\n     */\n    description?: string;\n\n    /**\n     * The name of the function.\n     */\n    name?: string;\n\n    /**\n     * Parameters of the function in JSON Schema.\n     */\n    parameters?: unknown;\n\n    /**\n     * The type of the tool, i.e. `function`.\n     */\n    type?: 'function';\n  }\n\n  /**\n   * Granular configuration for tracing.\n   */\n  export interface TracingConfiguration {\n    /**\n     * The group id to attach to this trace to enable filtering and grouping in the\n     * traces dashboard.\n     */\n    group_id?: string;\n\n    /**\n     * The arbitrary metadata to attach to this trace to enable filtering in the traces\n     * dashboard.\n     */\n    metadata?: unknown;\n\n    /**\n     * The name of the workflow to attach to this trace. This is used to name the trace\n     * in the traces dashboard.\n     */\n    workflow_name?: string;\n  }\n\n  /**\n   * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n   * means that the model will detect the start and end of speech based on audio\n   * volume and respond at the end of user speech.\n   */\n  export interface TurnDetection {\n    /**\n     * Amount of audio to include before the VAD detected speech (in milliseconds).\n     * Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n     * With shorter values the model will respond more quickly, but may jump in on\n     * short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n     * threshold will require louder audio to activate the model, and thus might\n     * perform better in noisy environments.\n     */\n    threshold?: number;\n\n    /**\n     * Type of turn detection, only `server_vad` is currently supported.\n     */\n    type?: string;\n  }\n}\n\nexport interface SessionCreateParams {\n  /**\n   * Configuration options for the generated client secret.\n   */\n  client_secret?: SessionCreateParams.ClientSecret;\n\n  /**\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\n   * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\n   * (mono), and little-endian byte order.\n   */\n  input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  input_audio_noise_reduction?: SessionCreateParams.InputAudioNoiseReduction;\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously through\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n   * and should be treated as guidance of input audio content rather than precisely\n   * what the model heard. The client can optionally set the language and prompt for\n   * transcription, these offer additional guidance to the transcription service.\n   */\n  input_audio_transcription?: SessionCreateParams.InputAudioTranscription;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior.\n   *\n   * Note that the server sets default instructions which will be used if this field\n   * is not set and are visible in the `session.created` event at the start of the\n   * session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_response_output_tokens?: number | 'inf';\n\n  /**\n   * The set of modalities the model can respond with. To disable audio, set this to\n   * [\"text\"].\n   */\n  modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * The Realtime model used for this session.\n   */\n  model?:\n    | 'gpt-4o-realtime-preview'\n    | 'gpt-4o-realtime-preview-2024-10-01'\n    | 'gpt-4o-realtime-preview-2024-12-17'\n    | 'gpt-4o-realtime-preview-2025-06-03'\n    | 'gpt-4o-mini-realtime-preview'\n    | 'gpt-4o-mini-realtime-preview-2024-12-17';\n\n  /**\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n   * For `pcm16`, output audio is sampled at a rate of 24kHz.\n   */\n  output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\n   * minimum speed. 1.5 is the maximum speed. This value can only be changed in\n   * between model turns, not while a response is in progress.\n   */\n  speed?: number;\n\n  /**\n   * Sampling temperature for the model, limited to [0.6, 1.2]. For audio models a\n   * temperature of 0.8 is highly recommended for best performance.\n   */\n  temperature?: number;\n\n  /**\n   * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\n   * a function.\n   */\n  tool_choice?: string;\n\n  /**\n   * Tools (functions) available to the model.\n   */\n  tools?: Array<SessionCreateParams.Tool>;\n\n  /**\n   * Configuration options for tracing. Set to null to disable tracing. Once tracing\n   * is enabled for a session, the configuration cannot be modified.\n   *\n   * `auto` will create a trace for the session with default values for the workflow\n   * name, group id, and metadata.\n   */\n  tracing?: 'auto' | SessionCreateParams.TracingConfiguration;\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response. Server VAD means that the model will detect the start and end of\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\n   * is more advanced and uses a turn detection model (in conjunction with VAD) to\n   * semantically estimate whether the user has finished speaking, then dynamically\n   * sets a timeout based on this probability. For example, if user audio trails off\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\n   * for the user to continue speaking. This can be useful for more natural\n   * conversations, but may have a higher latency.\n   */\n  turn_detection?: SessionCreateParams.TurnDetection;\n\n  /**\n   * The voice the model uses to respond. Voice cannot be changed during the session\n   * once the model has responded with audio at least once. Current voice options are\n   * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n   */\n  voice?: (string & {}) | 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';\n}\n\nexport namespace SessionCreateParams {\n  /**\n   * Configuration options for the generated client secret.\n   */\n  export interface ClientSecret {\n    /**\n     * Configuration for the ephemeral token expiration.\n     */\n    expires_after?: ClientSecret.ExpiresAfter;\n  }\n\n  export namespace ClientSecret {\n    /**\n     * Configuration for the ephemeral token expiration.\n     */\n    export interface ExpiresAfter {\n      /**\n       * The anchor point for the ephemeral token expiration. Only `created_at` is\n       * currently supported.\n       */\n      anchor: 'created_at';\n\n      /**\n       * The number of seconds from the anchor point to the expiration. Select a value\n       * between `10` and `7200`.\n       */\n      seconds?: number;\n    }\n  }\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  export interface InputAudioNoiseReduction {\n    /**\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\n     * headphones, `far_field` is for far-field microphones such as laptop or\n     * conference room microphones.\n     */\n    type?: 'near_field' | 'far_field';\n  }\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously through\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n   * and should be treated as guidance of input audio content rather than precisely\n   * what the model heard. The client can optionally set the language and prompt for\n   * transcription, these offer additional guidance to the transcription service.\n   */\n  export interface InputAudioTranscription {\n    /**\n     * The language of the input audio. Supplying the input language in\n     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n     * format will improve accuracy and latency.\n     */\n    language?: string;\n\n    /**\n     * The model to use for transcription, current options are `gpt-4o-transcribe`,\n     * `gpt-4o-mini-transcribe`, and `whisper-1`.\n     */\n    model?: string;\n\n    /**\n     * An optional text to guide the model's style or continue a previous audio\n     * segment. For `whisper-1`, the\n     * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n     * For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n     * \"expect words related to technology\".\n     */\n    prompt?: string;\n  }\n\n  export interface Tool {\n    /**\n     * The description of the function, including guidance on when and how to call it,\n     * and guidance about what to tell the user when calling (if anything).\n     */\n    description?: string;\n\n    /**\n     * The name of the function.\n     */\n    name?: string;\n\n    /**\n     * Parameters of the function in JSON Schema.\n     */\n    parameters?: unknown;\n\n    /**\n     * The type of the tool, i.e. `function`.\n     */\n    type?: 'function';\n  }\n\n  /**\n   * Granular configuration for tracing.\n   */\n  export interface TracingConfiguration {\n    /**\n     * The group id to attach to this trace to enable filtering and grouping in the\n     * traces dashboard.\n     */\n    group_id?: string;\n\n    /**\n     * The arbitrary metadata to attach to this trace to enable filtering in the traces\n     * dashboard.\n     */\n    metadata?: unknown;\n\n    /**\n     * The name of the workflow to attach to this trace. This is used to name the trace\n     * in the traces dashboard.\n     */\n    workflow_name?: string;\n  }\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response. Server VAD means that the model will detect the start and end of\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\n   * is more advanced and uses a turn detection model (in conjunction with VAD) to\n   * semantically estimate whether the user has finished speaking, then dynamically\n   * sets a timeout based on this probability. For example, if user audio trails off\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\n   * for the user to continue speaking. This can be useful for more natural\n   * conversations, but may have a higher latency.\n   */\n  export interface TurnDetection {\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs.\n     */\n    create_response?: boolean;\n\n    /**\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n     * will wait longer for the user to continue speaking, `high` will respond more\n     * quickly. `auto` is the default and is equivalent to `medium`.\n     */\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n    /**\n     * Whether or not to automatically interrupt any ongoing response with output to\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n     * occurs.\n     */\n    interrupt_response?: boolean;\n\n    /**\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\n     * detected speech (in milliseconds). Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\n     * more quickly, but may jump in on short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\n     * model, and thus might perform better in noisy environments.\n     */\n    threshold?: number;\n\n    /**\n     * Type of turn detection.\n     */\n    type?: 'server_vad' | 'semantic_vad';\n  }\n}\n\nexport declare namespace Sessions {\n  export {\n    type Session as Session,\n    type SessionCreateResponse as SessionCreateResponse,\n    type SessionCreateParams as SessionCreateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport { APIPromise } from '../../../core/api-promise';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\n\nexport class TranscriptionSessions extends APIResource {\n  /**\n   * Create an ephemeral API token for use in client-side applications with the\n   * Realtime API specifically for realtime transcriptions. Can be configured with\n   * the same session parameters as the `transcription_session.update` client event.\n   *\n   * It responds with a session object, plus a `client_secret` key which contains a\n   * usable ephemeral API token that can be used to authenticate browser clients for\n   * the Realtime API.\n   *\n   * @example\n   * ```ts\n   * const transcriptionSession =\n   *   await client.beta.realtime.transcriptionSessions.create();\n   * ```\n   */\n  create(body: TranscriptionSessionCreateParams, options?: RequestOptions): APIPromise<TranscriptionSession> {\n    return this._client.post('/realtime/transcription_sessions', {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n}\n\n/**\n * A new Realtime transcription session configuration.\n *\n * When a session is created on the server via REST API, the session object also\n * contains an ephemeral key. Default TTL for keys is 10 minutes. This property is\n * not present when a session is updated via the WebSocket API.\n */\nexport interface TranscriptionSession {\n  /**\n   * Ephemeral key returned by the API. Only present when the session is created on\n   * the server via REST API.\n   */\n  client_secret: TranscriptionSession.ClientSecret;\n\n  /**\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n   */\n  input_audio_format?: string;\n\n  /**\n   * Configuration of the transcription model.\n   */\n  input_audio_transcription?: TranscriptionSession.InputAudioTranscription;\n\n  /**\n   * The set of modalities the model can respond with. To disable audio, set this to\n   * [\"text\"].\n   */\n  modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n   * means that the model will detect the start and end of speech based on audio\n   * volume and respond at the end of user speech.\n   */\n  turn_detection?: TranscriptionSession.TurnDetection;\n}\n\nexport namespace TranscriptionSession {\n  /**\n   * Ephemeral key returned by the API. Only present when the session is created on\n   * the server via REST API.\n   */\n  export interface ClientSecret {\n    /**\n     * Timestamp for when the token expires. Currently, all tokens expire after one\n     * minute.\n     */\n    expires_at: number;\n\n    /**\n     * Ephemeral key usable in client environments to authenticate connections to the\n     * Realtime API. Use this in client-side environments rather than a standard API\n     * token, which should only be used server-side.\n     */\n    value: string;\n  }\n\n  /**\n   * Configuration of the transcription model.\n   */\n  export interface InputAudioTranscription {\n    /**\n     * The language of the input audio. Supplying the input language in\n     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n     * format will improve accuracy and latency.\n     */\n    language?: string;\n\n    /**\n     * The model to use for transcription. Can be `gpt-4o-transcribe`,\n     * `gpt-4o-mini-transcribe`, or `whisper-1`.\n     */\n    model?: 'gpt-4o-transcribe' | 'gpt-4o-mini-transcribe' | 'whisper-1';\n\n    /**\n     * An optional text to guide the model's style or continue a previous audio\n     * segment. The\n     * [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n     * should match the audio language.\n     */\n    prompt?: string;\n  }\n\n  /**\n   * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n   * means that the model will detect the start and end of speech based on audio\n   * volume and respond at the end of user speech.\n   */\n  export interface TurnDetection {\n    /**\n     * Amount of audio to include before the VAD detected speech (in milliseconds).\n     * Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n     * With shorter values the model will respond more quickly, but may jump in on\n     * short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n     * threshold will require louder audio to activate the model, and thus might\n     * perform better in noisy environments.\n     */\n    threshold?: number;\n\n    /**\n     * Type of turn detection, only `server_vad` is currently supported.\n     */\n    type?: string;\n  }\n}\n\nexport interface TranscriptionSessionCreateParams {\n  /**\n   * Configuration options for the generated client secret.\n   */\n  client_secret?: TranscriptionSessionCreateParams.ClientSecret;\n\n  /**\n   * The set of items to include in the transcription. Current available items are:\n   *\n   * - `item.input_audio_transcription.logprobs`\n   */\n  include?: Array<string>;\n\n  /**\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\n   * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\n   * (mono), and little-endian byte order.\n   */\n  input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  input_audio_noise_reduction?: TranscriptionSessionCreateParams.InputAudioNoiseReduction;\n\n  /**\n   * Configuration for input audio transcription. The client can optionally set the\n   * language and prompt for transcription, these offer additional guidance to the\n   * transcription service.\n   */\n  input_audio_transcription?: TranscriptionSessionCreateParams.InputAudioTranscription;\n\n  /**\n   * The set of modalities the model can respond with. To disable audio, set this to\n   * [\"text\"].\n   */\n  modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response. Server VAD means that the model will detect the start and end of\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\n   * is more advanced and uses a turn detection model (in conjunction with VAD) to\n   * semantically estimate whether the user has finished speaking, then dynamically\n   * sets a timeout based on this probability. For example, if user audio trails off\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\n   * for the user to continue speaking. This can be useful for more natural\n   * conversations, but may have a higher latency.\n   */\n  turn_detection?: TranscriptionSessionCreateParams.TurnDetection;\n}\n\nexport namespace TranscriptionSessionCreateParams {\n  /**\n   * Configuration options for the generated client secret.\n   */\n  export interface ClientSecret {\n    /**\n     * Configuration for the ephemeral token expiration.\n     */\n    expires_at?: ClientSecret.ExpiresAt;\n  }\n\n  export namespace ClientSecret {\n    /**\n     * Configuration for the ephemeral token expiration.\n     */\n    export interface ExpiresAt {\n      /**\n       * The anchor point for the ephemeral token expiration. Only `created_at` is\n       * currently supported.\n       */\n      anchor?: 'created_at';\n\n      /**\n       * The number of seconds from the anchor point to the expiration. Select a value\n       * between `10` and `7200`.\n       */\n      seconds?: number;\n    }\n  }\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  export interface InputAudioNoiseReduction {\n    /**\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\n     * headphones, `far_field` is for far-field microphones such as laptop or\n     * conference room microphones.\n     */\n    type?: 'near_field' | 'far_field';\n  }\n\n  /**\n   * Configuration for input audio transcription. The client can optionally set the\n   * language and prompt for transcription, these offer additional guidance to the\n   * transcription service.\n   */\n  export interface InputAudioTranscription {\n    /**\n     * The language of the input audio. Supplying the input language in\n     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n     * format will improve accuracy and latency.\n     */\n    language?: string;\n\n    /**\n     * The model to use for transcription, current options are `gpt-4o-transcribe`,\n     * `gpt-4o-mini-transcribe`, and `whisper-1`.\n     */\n    model?: 'gpt-4o-transcribe' | 'gpt-4o-mini-transcribe' | 'whisper-1';\n\n    /**\n     * An optional text to guide the model's style or continue a previous audio\n     * segment. For `whisper-1`, the\n     * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n     * For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n     * \"expect words related to technology\".\n     */\n    prompt?: string;\n  }\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response. Server VAD means that the model will detect the start and end of\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\n   * is more advanced and uses a turn detection model (in conjunction with VAD) to\n   * semantically estimate whether the user has finished speaking, then dynamically\n   * sets a timeout based on this probability. For example, if user audio trails off\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\n   * for the user to continue speaking. This can be useful for more natural\n   * conversations, but may have a higher latency.\n   */\n  export interface TurnDetection {\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs. Not available for transcription sessions.\n     */\n    create_response?: boolean;\n\n    /**\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n     * will wait longer for the user to continue speaking, `high` will respond more\n     * quickly. `auto` is the default and is equivalent to `medium`.\n     */\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n    /**\n     * Whether or not to automatically interrupt any ongoing response with output to\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n     * occurs. Not available for transcription sessions.\n     */\n    interrupt_response?: boolean;\n\n    /**\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\n     * detected speech (in milliseconds). Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\n     * more quickly, but may jump in on short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\n     * model, and thus might perform better in noisy environments.\n     */\n    threshold?: number;\n\n    /**\n     * Type of turn detection.\n     */\n    type?: 'server_vad' | 'semantic_vad';\n  }\n}\n\nexport declare namespace TranscriptionSessions {\n  export {\n    type TranscriptionSession as TranscriptionSession,\n    type TranscriptionSessionCreateParams as TranscriptionSessionCreateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as RealtimeAPI from './realtime';\nimport * as Shared from '../../shared';\nimport * as SessionsAPI from './sessions';\nimport {\n  Session as SessionsAPISession,\n  SessionCreateParams,\n  SessionCreateResponse,\n  Sessions,\n} from './sessions';\nimport * as TranscriptionSessionsAPI from './transcription-sessions';\nimport {\n  TranscriptionSession,\n  TranscriptionSessionCreateParams,\n  TranscriptionSessions,\n} from './transcription-sessions';\n\n/**\n * @deprecated Realtime has now launched and is generally available. The old beta API is now deprecated.\n */\nexport class Realtime extends APIResource {\n  sessions: SessionsAPI.Sessions = new SessionsAPI.Sessions(this._client);\n  transcriptionSessions: TranscriptionSessionsAPI.TranscriptionSessions =\n    new TranscriptionSessionsAPI.TranscriptionSessions(this._client);\n}\n\n/**\n * Returned when a conversation is created. Emitted right after session creation.\n */\nexport interface ConversationCreatedEvent {\n  /**\n   * The conversation resource.\n   */\n  conversation: ConversationCreatedEvent.Conversation;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The event type, must be `conversation.created`.\n   */\n  type: 'conversation.created';\n}\n\nexport namespace ConversationCreatedEvent {\n  /**\n   * The conversation resource.\n   */\n  export interface Conversation {\n    /**\n     * The unique ID of the conversation.\n     */\n    id?: string;\n\n    /**\n     * The object type, must be `realtime.conversation`.\n     */\n    object?: 'realtime.conversation';\n  }\n}\n\n/**\n * The item to add to the conversation.\n */\nexport interface ConversationItem {\n  /**\n   * The unique ID of the item, this can be generated by the client to help manage\n   * server-side context, but is not required because the server will generate one if\n   * not provided.\n   */\n  id?: string;\n\n  /**\n   * The arguments of the function call (for `function_call` items).\n   */\n  arguments?: string;\n\n  /**\n   * The ID of the function call (for `function_call` and `function_call_output`\n   * items). If passed on a `function_call_output` item, the server will check that a\n   * `function_call` item with the same ID exists in the conversation history.\n   */\n  call_id?: string;\n\n  /**\n   * The content of the message, applicable for `message` items.\n   *\n   * - Message items of role `system` support only `input_text` content\n   * - Message items of role `user` support `input_text` and `input_audio` content\n   * - Message items of role `assistant` support `text` content.\n   */\n  content?: Array<ConversationItemContent>;\n\n  /**\n   * The name of the function being called (for `function_call` items).\n   */\n  name?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The output of the function call (for `function_call_output` items).\n   */\n  output?: string;\n\n  /**\n   * The role of the message sender (`user`, `assistant`, `system`), only applicable\n   * for `message` items.\n   */\n  role?: 'user' | 'assistant' | 'system';\n\n  /**\n   * The status of the item (`completed`, `incomplete`, `in_progress`). These have no\n   * effect on the conversation, but are accepted for consistency with the\n   * `conversation.item.created` event.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n\n  /**\n   * The type of the item (`message`, `function_call`, `function_call_output`).\n   */\n  type?: 'message' | 'function_call' | 'function_call_output';\n}\n\nexport interface ConversationItemContent {\n  /**\n   * ID of a previous conversation item to reference (for `item_reference` content\n   * types in `response.create` events). These can reference both client and server\n   * created items.\n   */\n  id?: string;\n\n  /**\n   * Base64-encoded audio bytes, used for `input_audio` content type.\n   */\n  audio?: string;\n\n  /**\n   * The text content, used for `input_text` and `text` content types.\n   */\n  text?: string;\n\n  /**\n   * The transcript of the audio, used for `input_audio` and `audio` content types.\n   */\n  transcript?: string;\n\n  /**\n   * The content type (`input_text`, `input_audio`, `item_reference`, `text`,\n   * `audio`).\n   */\n  type?: 'input_text' | 'input_audio' | 'item_reference' | 'text' | 'audio';\n}\n\n/**\n * Add a new Item to the Conversation's context, including messages, function\n * calls, and function call responses. This event can be used both to populate a\n * \"history\" of the conversation and to add new items mid-stream, but has the\n * current limitation that it cannot populate assistant audio messages.\n *\n * If successful, the server will respond with a `conversation.item.created` event,\n * otherwise an `error` event will be sent.\n */\nexport interface ConversationItemCreateEvent {\n  /**\n   * The item to add to the conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The event type, must be `conversation.item.create`.\n   */\n  type: 'conversation.item.create';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n\n  /**\n   * The ID of the preceding item after which the new item will be inserted. If not\n   * set, the new item will be appended to the end of the conversation. If set to\n   * `root`, the new item will be added to the beginning of the conversation. If set\n   * to an existing ID, it allows an item to be inserted mid-conversation. If the ID\n   * cannot be found, an error will be returned and the item will not be added.\n   */\n  previous_item_id?: string;\n}\n\n/**\n * Returned when a conversation item is created. There are several scenarios that\n * produce this event:\n *\n * - The server is generating a Response, which if successful will produce either\n *   one or two Items, which will be of type `message` (role `assistant`) or type\n *   `function_call`.\n * - The input audio buffer has been committed, either by the client or the server\n *   (in `server_vad` mode). The server will take the content of the input audio\n *   buffer and add it to a new user message Item.\n * - The client has sent a `conversation.item.create` event to add a new Item to\n *   the Conversation.\n */\nexport interface ConversationItemCreatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The item to add to the conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The event type, must be `conversation.item.created`.\n   */\n  type: 'conversation.item.created';\n\n  /**\n   * The ID of the preceding item in the Conversation context, allows the client to\n   * understand the order of the conversation. Can be `null` if the item has no\n   * predecessor.\n   */\n  previous_item_id?: string | null;\n}\n\n/**\n * Send this event when you want to remove any item from the conversation history.\n * The server will respond with a `conversation.item.deleted` event, unless the\n * item does not exist in the conversation history, in which case the server will\n * respond with an error.\n */\nexport interface ConversationItemDeleteEvent {\n  /**\n   * The ID of the item to delete.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.delete`.\n   */\n  type: 'conversation.item.delete';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when an item in the conversation is deleted by the client with a\n * `conversation.item.delete` event. This event is used to synchronize the server's\n * understanding of the conversation history with the client's view.\n */\nexport interface ConversationItemDeletedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item that was deleted.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.deleted`.\n   */\n  type: 'conversation.item.deleted';\n}\n\n/**\n * This event is the output of audio transcription for user audio written to the\n * user audio buffer. Transcription begins when the input audio buffer is committed\n * by the client or server (in `server_vad` mode). Transcription runs\n * asynchronously with Response creation, so this event may come before or after\n * the Response events.\n *\n * Realtime API models accept audio natively, and thus input transcription is a\n * separate process run on a separate ASR (Automatic Speech Recognition) model. The\n * transcript may diverge somewhat from the model's interpretation, and should be\n * treated as a rough guide.\n */\nexport interface ConversationItemInputAudioTranscriptionCompletedEvent {\n  /**\n   * The index of the content part containing the audio.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item containing the audio.\n   */\n  item_id: string;\n\n  /**\n   * The transcribed text.\n   */\n  transcript: string;\n\n  /**\n   * The event type, must be `conversation.item.input_audio_transcription.completed`.\n   */\n  type: 'conversation.item.input_audio_transcription.completed';\n\n  /**\n   * Usage statistics for the transcription.\n   */\n  usage:\n    | ConversationItemInputAudioTranscriptionCompletedEvent.TranscriptTextUsageTokens\n    | ConversationItemInputAudioTranscriptionCompletedEvent.TranscriptTextUsageDuration;\n\n  /**\n   * The log probabilities of the transcription.\n   */\n  logprobs?: Array<ConversationItemInputAudioTranscriptionCompletedEvent.Logprob> | null;\n}\n\nexport namespace ConversationItemInputAudioTranscriptionCompletedEvent {\n  /**\n   * Usage statistics for models billed by token usage.\n   */\n  export interface TranscriptTextUsageTokens {\n    /**\n     * Number of input tokens billed for this request.\n     */\n    input_tokens: number;\n\n    /**\n     * Number of output tokens generated.\n     */\n    output_tokens: number;\n\n    /**\n     * Total number of tokens used (input + output).\n     */\n    total_tokens: number;\n\n    /**\n     * The type of the usage object. Always `tokens` for this variant.\n     */\n    type: 'tokens';\n\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    input_token_details?: TranscriptTextUsageTokens.InputTokenDetails;\n  }\n\n  export namespace TranscriptTextUsageTokens {\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    export interface InputTokenDetails {\n      /**\n       * Number of audio tokens billed for this request.\n       */\n      audio_tokens?: number;\n\n      /**\n       * Number of text tokens billed for this request.\n       */\n      text_tokens?: number;\n    }\n  }\n\n  /**\n   * Usage statistics for models billed by audio input duration.\n   */\n  export interface TranscriptTextUsageDuration {\n    /**\n     * Duration of the input audio in seconds.\n     */\n    seconds: number;\n\n    /**\n     * The type of the usage object. Always `duration` for this variant.\n     */\n    type: 'duration';\n  }\n\n  /**\n   * A log probability object.\n   */\n  export interface Logprob {\n    /**\n     * The token that was used to generate the log probability.\n     */\n    token: string;\n\n    /**\n     * The bytes that were used to generate the log probability.\n     */\n    bytes: Array<number>;\n\n    /**\n     * The log probability of the token.\n     */\n    logprob: number;\n  }\n}\n\n/**\n * Returned when the text value of an input audio transcription content part is\n * updated.\n */\nexport interface ConversationItemInputAudioTranscriptionDeltaEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.input_audio_transcription.delta`.\n   */\n  type: 'conversation.item.input_audio_transcription.delta';\n\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index?: number;\n\n  /**\n   * The text delta.\n   */\n  delta?: string;\n\n  /**\n   * The log probabilities of the transcription.\n   */\n  logprobs?: Array<ConversationItemInputAudioTranscriptionDeltaEvent.Logprob> | null;\n}\n\nexport namespace ConversationItemInputAudioTranscriptionDeltaEvent {\n  /**\n   * A log probability object.\n   */\n  export interface Logprob {\n    /**\n     * The token that was used to generate the log probability.\n     */\n    token: string;\n\n    /**\n     * The bytes that were used to generate the log probability.\n     */\n    bytes: Array<number>;\n\n    /**\n     * The log probability of the token.\n     */\n    logprob: number;\n  }\n}\n\n/**\n * Returned when input audio transcription is configured, and a transcription\n * request for a user message failed. These events are separate from other `error`\n * events so that the client can identify the related Item.\n */\nexport interface ConversationItemInputAudioTranscriptionFailedEvent {\n  /**\n   * The index of the content part containing the audio.\n   */\n  content_index: number;\n\n  /**\n   * Details of the transcription error.\n   */\n  error: ConversationItemInputAudioTranscriptionFailedEvent.Error;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.input_audio_transcription.failed`.\n   */\n  type: 'conversation.item.input_audio_transcription.failed';\n}\n\nexport namespace ConversationItemInputAudioTranscriptionFailedEvent {\n  /**\n   * Details of the transcription error.\n   */\n  export interface Error {\n    /**\n     * Error code, if any.\n     */\n    code?: string;\n\n    /**\n     * A human-readable error message.\n     */\n    message?: string;\n\n    /**\n     * Parameter related to the error, if any.\n     */\n    param?: string;\n\n    /**\n     * The type of error.\n     */\n    type?: string;\n  }\n}\n\n/**\n * Send this event when you want to retrieve the server's representation of a\n * specific item in the conversation history. This is useful, for example, to\n * inspect user audio after noise cancellation and VAD. The server will respond\n * with a `conversation.item.retrieved` event, unless the item does not exist in\n * the conversation history, in which case the server will respond with an error.\n */\nexport interface ConversationItemRetrieveEvent {\n  /**\n   * The ID of the item to retrieve.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.retrieve`.\n   */\n  type: 'conversation.item.retrieve';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Send this event to truncate a previous assistant messages audio. The server\n * will produce audio faster than realtime, so this event is useful when the user\n * interrupts to truncate audio that has already been sent to the client but not\n * yet played. This will synchronize the server's understanding of the audio with\n * the client's playback.\n *\n * Truncating audio will delete the server-side text transcript to ensure there is\n * not text in the context that hasn't been heard by the user.\n *\n * If successful, the server will respond with a `conversation.item.truncated`\n * event.\n */\nexport interface ConversationItemTruncateEvent {\n  /**\n   * Inclusive duration up to which audio is truncated, in milliseconds. If the\n   * audio_end_ms is greater than the actual audio duration, the server will respond\n   * with an error.\n   */\n  audio_end_ms: number;\n\n  /**\n   * The index of the content part to truncate. Set this to 0.\n   */\n  content_index: number;\n\n  /**\n   * The ID of the assistant message item to truncate. Only assistant message items\n   * can be truncated.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.truncate`.\n   */\n  type: 'conversation.item.truncate';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when an earlier assistant audio message item is truncated by the client\n * with a `conversation.item.truncate` event. This event is used to synchronize the\n * server's understanding of the audio with the client's playback.\n *\n * This action will truncate the audio and remove the server-side text transcript\n * to ensure there is no text in the context that hasn't been heard by the user.\n */\nexport interface ConversationItemTruncatedEvent {\n  /**\n   * The duration up to which the audio was truncated, in milliseconds.\n   */\n  audio_end_ms: number;\n\n  /**\n   * The index of the content part that was truncated.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the assistant message item that was truncated.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.truncated`.\n   */\n  type: 'conversation.item.truncated';\n}\n\n/**\n * The item to add to the conversation.\n */\nexport interface ConversationItemWithReference {\n  /**\n   * For an item of type (`message` | `function_call` | `function_call_output`) this\n   * field allows the client to assign the unique ID of the item. It is not required\n   * because the server will generate one if not provided.\n   *\n   * For an item of type `item_reference`, this field is required and is a reference\n   * to any item that has previously existed in the conversation.\n   */\n  id?: string;\n\n  /**\n   * The arguments of the function call (for `function_call` items).\n   */\n  arguments?: string;\n\n  /**\n   * The ID of the function call (for `function_call` and `function_call_output`\n   * items). If passed on a `function_call_output` item, the server will check that a\n   * `function_call` item with the same ID exists in the conversation history.\n   */\n  call_id?: string;\n\n  /**\n   * The content of the message, applicable for `message` items.\n   *\n   * - Message items of role `system` support only `input_text` content\n   * - Message items of role `user` support `input_text` and `input_audio` content\n   * - Message items of role `assistant` support `text` content.\n   */\n  content?: Array<ConversationItemWithReference.Content>;\n\n  /**\n   * The name of the function being called (for `function_call` items).\n   */\n  name?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The output of the function call (for `function_call_output` items).\n   */\n  output?: string;\n\n  /**\n   * The role of the message sender (`user`, `assistant`, `system`), only applicable\n   * for `message` items.\n   */\n  role?: 'user' | 'assistant' | 'system';\n\n  /**\n   * The status of the item (`completed`, `incomplete`, `in_progress`). These have no\n   * effect on the conversation, but are accepted for consistency with the\n   * `conversation.item.created` event.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n\n  /**\n   * The type of the item (`message`, `function_call`, `function_call_output`,\n   * `item_reference`).\n   */\n  type?: 'message' | 'function_call' | 'function_call_output' | 'item_reference';\n}\n\nexport namespace ConversationItemWithReference {\n  export interface Content {\n    /**\n     * ID of a previous conversation item to reference (for `item_reference` content\n     * types in `response.create` events). These can reference both client and server\n     * created items.\n     */\n    id?: string;\n\n    /**\n     * Base64-encoded audio bytes, used for `input_audio` content type.\n     */\n    audio?: string;\n\n    /**\n     * The text content, used for `input_text` and `text` content types.\n     */\n    text?: string;\n\n    /**\n     * The transcript of the audio, used for `input_audio` content type.\n     */\n    transcript?: string;\n\n    /**\n     * The content type (`input_text`, `input_audio`, `item_reference`, `text`).\n     */\n    type?: 'input_text' | 'input_audio' | 'item_reference' | 'text';\n  }\n}\n\n/**\n * Returned when an error occurs, which could be a client problem or a server\n * problem. Most errors are recoverable and the session will stay open, we\n * recommend to implementors to monitor and log error messages by default.\n */\nexport interface ErrorEvent {\n  /**\n   * Details of the error.\n   */\n  error: ErrorEvent.Error;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The event type, must be `error`.\n   */\n  type: 'error';\n}\n\nexport namespace ErrorEvent {\n  /**\n   * Details of the error.\n   */\n  export interface Error {\n    /**\n     * A human-readable error message.\n     */\n    message: string;\n\n    /**\n     * The type of error (e.g., \"invalid_request_error\", \"server_error\").\n     */\n    type: string;\n\n    /**\n     * Error code, if any.\n     */\n    code?: string | null;\n\n    /**\n     * The event_id of the client event that caused the error, if applicable.\n     */\n    event_id?: string | null;\n\n    /**\n     * Parameter related to the error, if any.\n     */\n    param?: string | null;\n  }\n}\n\n/**\n * Send this event to append audio bytes to the input audio buffer. The audio\n * buffer is temporary storage you can write to and later commit. In Server VAD\n * mode, the audio buffer is used to detect speech and the server will decide when\n * to commit. When Server VAD is disabled, you must commit the audio buffer\n * manually.\n *\n * The client may choose how much audio to place in each event up to a maximum of\n * 15 MiB, for example streaming smaller chunks from the client may allow the VAD\n * to be more responsive. Unlike made other client events, the server will not send\n * a confirmation response to this event.\n */\nexport interface InputAudioBufferAppendEvent {\n  /**\n   * Base64-encoded audio bytes. This must be in the format specified by the\n   * `input_audio_format` field in the session configuration.\n   */\n  audio: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.append`.\n   */\n  type: 'input_audio_buffer.append';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Send this event to clear the audio bytes in the buffer. The server will respond\n * with an `input_audio_buffer.cleared` event.\n */\nexport interface InputAudioBufferClearEvent {\n  /**\n   * The event type, must be `input_audio_buffer.clear`.\n   */\n  type: 'input_audio_buffer.clear';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when the input audio buffer is cleared by the client with a\n * `input_audio_buffer.clear` event.\n */\nexport interface InputAudioBufferClearedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.cleared`.\n   */\n  type: 'input_audio_buffer.cleared';\n}\n\n/**\n * Send this event to commit the user input audio buffer, which will create a new\n * user message item in the conversation. This event will produce an error if the\n * input audio buffer is empty. When in Server VAD mode, the client does not need\n * to send this event, the server will commit the audio buffer automatically.\n *\n * Committing the input audio buffer will trigger input audio transcription (if\n * enabled in session configuration), but it will not create a response from the\n * model. The server will respond with an `input_audio_buffer.committed` event.\n */\nexport interface InputAudioBufferCommitEvent {\n  /**\n   * The event type, must be `input_audio_buffer.commit`.\n   */\n  type: 'input_audio_buffer.commit';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when an input audio buffer is committed, either by the client or\n * automatically in server VAD mode. The `item_id` property is the ID of the user\n * message item that will be created, thus a `conversation.item.created` event will\n * also be sent to the client.\n */\nexport interface InputAudioBufferCommittedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item that will be created.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.committed`.\n   */\n  type: 'input_audio_buffer.committed';\n\n  /**\n   * The ID of the preceding item after which the new item will be inserted. Can be\n   * `null` if the item has no predecessor.\n   */\n  previous_item_id?: string | null;\n}\n\n/**\n * Sent by the server when in `server_vad` mode to indicate that speech has been\n * detected in the audio buffer. This can happen any time audio is added to the\n * buffer (unless speech is already detected). The client may want to use this\n * event to interrupt audio playback or provide visual feedback to the user.\n *\n * The client should expect to receive a `input_audio_buffer.speech_stopped` event\n * when speech stops. The `item_id` property is the ID of the user message item\n * that will be created when speech stops and will also be included in the\n * `input_audio_buffer.speech_stopped` event (unless the client manually commits\n * the audio buffer during VAD activation).\n */\nexport interface InputAudioBufferSpeechStartedEvent {\n  /**\n   * Milliseconds from the start of all audio written to the buffer during the\n   * session when speech was first detected. This will correspond to the beginning of\n   * audio sent to the model, and thus includes the `prefix_padding_ms` configured in\n   * the Session.\n   */\n  audio_start_ms: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item that will be created when speech stops.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.speech_started`.\n   */\n  type: 'input_audio_buffer.speech_started';\n}\n\n/**\n * Returned in `server_vad` mode when the server detects the end of speech in the\n * audio buffer. The server will also send an `conversation.item.created` event\n * with the user message item that is created from the audio buffer.\n */\nexport interface InputAudioBufferSpeechStoppedEvent {\n  /**\n   * Milliseconds since the session started when speech stopped. This will correspond\n   * to the end of audio sent to the model, and thus includes the\n   * `min_silence_duration_ms` configured in the Session.\n   */\n  audio_end_ms: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item that will be created.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.speech_stopped`.\n   */\n  type: 'input_audio_buffer.speech_stopped';\n}\n\n/**\n * Emitted at the beginning of a Response to indicate the updated rate limits. When\n * a Response is created some tokens will be \"reserved\" for the output tokens, the\n * rate limits shown here reflect that reservation, which is then adjusted\n * accordingly once the Response is completed.\n */\nexport interface RateLimitsUpdatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * List of rate limit information.\n   */\n  rate_limits: Array<RateLimitsUpdatedEvent.RateLimit>;\n\n  /**\n   * The event type, must be `rate_limits.updated`.\n   */\n  type: 'rate_limits.updated';\n}\n\nexport namespace RateLimitsUpdatedEvent {\n  export interface RateLimit {\n    /**\n     * The maximum allowed value for the rate limit.\n     */\n    limit?: number;\n\n    /**\n     * The name of the rate limit (`requests`, `tokens`).\n     */\n    name?: 'requests' | 'tokens';\n\n    /**\n     * The remaining value before the limit is reached.\n     */\n    remaining?: number;\n\n    /**\n     * Seconds until the rate limit resets.\n     */\n    reset_seconds?: number;\n  }\n}\n\n/**\n * A realtime client event.\n */\nexport type RealtimeClientEvent =\n  | ConversationItemCreateEvent\n  | ConversationItemDeleteEvent\n  | ConversationItemRetrieveEvent\n  | ConversationItemTruncateEvent\n  | InputAudioBufferAppendEvent\n  | InputAudioBufferClearEvent\n  | RealtimeClientEvent.OutputAudioBufferClear\n  | InputAudioBufferCommitEvent\n  | ResponseCancelEvent\n  | ResponseCreateEvent\n  | SessionUpdateEvent\n  | TranscriptionSessionUpdate;\n\nexport namespace RealtimeClientEvent {\n  /**\n   * **WebRTC Only:** Emit to cut off the current audio response. This will trigger\n   * the server to stop generating audio and emit a `output_audio_buffer.cleared`\n   * event. This event should be preceded by a `response.cancel` client event to stop\n   * the generation of the current response.\n   * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n   */\n  export interface OutputAudioBufferClear {\n    /**\n     * The event type, must be `output_audio_buffer.clear`.\n     */\n    type: 'output_audio_buffer.clear';\n\n    /**\n     * The unique ID of the client event used for error handling.\n     */\n    event_id?: string;\n  }\n}\n\n/**\n * The response resource.\n */\nexport interface RealtimeResponse {\n  /**\n   * The unique ID of the response.\n   */\n  id?: string;\n\n  /**\n   * Which conversation the response is added to, determined by the `conversation`\n   * field in the `response.create` event. If `auto`, the response will be added to\n   * the default conversation and the value of `conversation_id` will be an id like\n   * `conv_1234`. If `none`, the response will not be added to any conversation and\n   * the value of `conversation_id` will be `null`. If responses are being triggered\n   * by server VAD, the response will be added to the default conversation, thus the\n   * `conversation_id` will be an id like `conv_1234`.\n   */\n  conversation_id?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls, that was used in this response.\n   */\n  max_output_tokens?: number | 'inf';\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The set of modalities the model used to respond. If there are multiple\n   * modalities, the model will pick one, for example if `modalities` is\n   * `[\"text\", \"audio\"]`, the model could be responding in either text or audio.\n   */\n  modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * The object type, must be `realtime.response`.\n   */\n  object?: 'realtime.response';\n\n  /**\n   * The list of output items generated by the response.\n   */\n  output?: Array<ConversationItem>;\n\n  /**\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n   */\n  output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * The final status of the response (`completed`, `cancelled`, `failed`, or\n   * `incomplete`, `in_progress`).\n   */\n  status?: 'completed' | 'cancelled' | 'failed' | 'incomplete' | 'in_progress';\n\n  /**\n   * Additional details about the status.\n   */\n  status_details?: RealtimeResponseStatus;\n\n  /**\n   * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\n   */\n  temperature?: number;\n\n  /**\n   * Usage statistics for the Response, this will correspond to billing. A Realtime\n   * API session will maintain a conversation context and append new Items to the\n   * Conversation, thus output from previous turns (text and audio tokens) will\n   * become the input for later turns.\n   */\n  usage?: RealtimeResponseUsage;\n\n  /**\n   * The voice the model used to respond. Current voice options are `alloy`, `ash`,\n   * `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n   */\n  voice?: (string & {}) | 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';\n}\n\n/**\n * Additional details about the status.\n */\nexport interface RealtimeResponseStatus {\n  /**\n   * A description of the error that caused the response to fail, populated when the\n   * `status` is `failed`.\n   */\n  error?: RealtimeResponseStatus.Error;\n\n  /**\n   * The reason the Response did not complete. For a `cancelled` Response, one of\n   * `turn_detected` (the server VAD detected a new start of speech) or\n   * `client_cancelled` (the client sent a cancel event). For an `incomplete`\n   * Response, one of `max_output_tokens` or `content_filter` (the server-side safety\n   * filter activated and cut off the response).\n   */\n  reason?: 'turn_detected' | 'client_cancelled' | 'max_output_tokens' | 'content_filter';\n\n  /**\n   * The type of error that caused the response to fail, corresponding with the\n   * `status` field (`completed`, `cancelled`, `incomplete`, `failed`).\n   */\n  type?: 'completed' | 'cancelled' | 'incomplete' | 'failed';\n}\n\nexport namespace RealtimeResponseStatus {\n  /**\n   * A description of the error that caused the response to fail, populated when the\n   * `status` is `failed`.\n   */\n  export interface Error {\n    /**\n     * Error code, if any.\n     */\n    code?: string;\n\n    /**\n     * The type of error.\n     */\n    type?: string;\n  }\n}\n\n/**\n * Usage statistics for the Response, this will correspond to billing. A Realtime\n * API session will maintain a conversation context and append new Items to the\n * Conversation, thus output from previous turns (text and audio tokens) will\n * become the input for later turns.\n */\nexport interface RealtimeResponseUsage {\n  /**\n   * Details about the input tokens used in the Response.\n   */\n  input_token_details?: RealtimeResponseUsage.InputTokenDetails;\n\n  /**\n   * The number of input tokens used in the Response, including text and audio\n   * tokens.\n   */\n  input_tokens?: number;\n\n  /**\n   * Details about the output tokens used in the Response.\n   */\n  output_token_details?: RealtimeResponseUsage.OutputTokenDetails;\n\n  /**\n   * The number of output tokens sent in the Response, including text and audio\n   * tokens.\n   */\n  output_tokens?: number;\n\n  /**\n   * The total number of tokens in the Response including input and output text and\n   * audio tokens.\n   */\n  total_tokens?: number;\n}\n\nexport namespace RealtimeResponseUsage {\n  /**\n   * Details about the input tokens used in the Response.\n   */\n  export interface InputTokenDetails {\n    /**\n     * The number of audio tokens used in the Response.\n     */\n    audio_tokens?: number;\n\n    /**\n     * The number of cached tokens used in the Response.\n     */\n    cached_tokens?: number;\n\n    /**\n     * The number of text tokens used in the Response.\n     */\n    text_tokens?: number;\n  }\n\n  /**\n   * Details about the output tokens used in the Response.\n   */\n  export interface OutputTokenDetails {\n    /**\n     * The number of audio tokens used in the Response.\n     */\n    audio_tokens?: number;\n\n    /**\n     * The number of text tokens used in the Response.\n     */\n    text_tokens?: number;\n  }\n}\n\n/**\n * A realtime server event.\n */\nexport type RealtimeServerEvent =\n  | ConversationCreatedEvent\n  | ConversationItemCreatedEvent\n  | ConversationItemDeletedEvent\n  | ConversationItemInputAudioTranscriptionCompletedEvent\n  | ConversationItemInputAudioTranscriptionDeltaEvent\n  | ConversationItemInputAudioTranscriptionFailedEvent\n  | RealtimeServerEvent.ConversationItemRetrieved\n  | ConversationItemTruncatedEvent\n  | ErrorEvent\n  | InputAudioBufferClearedEvent\n  | InputAudioBufferCommittedEvent\n  | InputAudioBufferSpeechStartedEvent\n  | InputAudioBufferSpeechStoppedEvent\n  | RateLimitsUpdatedEvent\n  | ResponseAudioDeltaEvent\n  | ResponseAudioDoneEvent\n  | ResponseAudioTranscriptDeltaEvent\n  | ResponseAudioTranscriptDoneEvent\n  | ResponseContentPartAddedEvent\n  | ResponseContentPartDoneEvent\n  | ResponseCreatedEvent\n  | ResponseDoneEvent\n  | ResponseFunctionCallArgumentsDeltaEvent\n  | ResponseFunctionCallArgumentsDoneEvent\n  | ResponseOutputItemAddedEvent\n  | ResponseOutputItemDoneEvent\n  | ResponseTextDeltaEvent\n  | ResponseTextDoneEvent\n  | SessionCreatedEvent\n  | SessionUpdatedEvent\n  | TranscriptionSessionUpdatedEvent\n  | RealtimeServerEvent.OutputAudioBufferStarted\n  | RealtimeServerEvent.OutputAudioBufferStopped\n  | RealtimeServerEvent.OutputAudioBufferCleared;\n\nexport namespace RealtimeServerEvent {\n  /**\n   * Returned when a conversation item is retrieved with\n   * `conversation.item.retrieve`.\n   */\n  export interface ConversationItemRetrieved {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * The item to add to the conversation.\n     */\n    item: RealtimeAPI.ConversationItem;\n\n    /**\n     * The event type, must be `conversation.item.retrieved`.\n     */\n    type: 'conversation.item.retrieved';\n  }\n\n  /**\n   * **WebRTC Only:** Emitted when the server begins streaming audio to the client.\n   * This event is emitted after an audio content part has been added\n   * (`response.content_part.added`) to the response.\n   * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n   */\n  export interface OutputAudioBufferStarted {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * The unique ID of the response that produced the audio.\n     */\n    response_id: string;\n\n    /**\n     * The event type, must be `output_audio_buffer.started`.\n     */\n    type: 'output_audio_buffer.started';\n  }\n\n  /**\n   * **WebRTC Only:** Emitted when the output audio buffer has been completely\n   * drained on the server, and no more audio is forthcoming. This event is emitted\n   * after the full response data has been sent to the client (`response.done`).\n   * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n   */\n  export interface OutputAudioBufferStopped {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * The unique ID of the response that produced the audio.\n     */\n    response_id: string;\n\n    /**\n     * The event type, must be `output_audio_buffer.stopped`.\n     */\n    type: 'output_audio_buffer.stopped';\n  }\n\n  /**\n   * **WebRTC Only:** Emitted when the output audio buffer is cleared. This happens\n   * either in VAD mode when the user has interrupted\n   * (`input_audio_buffer.speech_started`), or when the client has emitted the\n   * `output_audio_buffer.clear` event to manually cut off the current audio\n   * response.\n   * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n   */\n  export interface OutputAudioBufferCleared {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * The unique ID of the response that produced the audio.\n     */\n    response_id: string;\n\n    /**\n     * The event type, must be `output_audio_buffer.cleared`.\n     */\n    type: 'output_audio_buffer.cleared';\n  }\n}\n\n/**\n * Returned when the model-generated audio is updated.\n */\nexport interface ResponseAudioDeltaEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * Base64-encoded audio data delta.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.audio.delta`.\n   */\n  type: 'response.audio.delta';\n}\n\n/**\n * Returned when the model-generated audio is done. Also emitted when a Response is\n * interrupted, incomplete, or cancelled.\n */\nexport interface ResponseAudioDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.audio.done`.\n   */\n  type: 'response.audio.done';\n}\n\n/**\n * Returned when the model-generated transcription of audio output is updated.\n */\nexport interface ResponseAudioTranscriptDeltaEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The transcript delta.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.audio_transcript.delta`.\n   */\n  type: 'response.audio_transcript.delta';\n}\n\n/**\n * Returned when the model-generated transcription of audio output is done\n * streaming. Also emitted when a Response is interrupted, incomplete, or\n * cancelled.\n */\nexport interface ResponseAudioTranscriptDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The final transcript of the audio.\n   */\n  transcript: string;\n\n  /**\n   * The event type, must be `response.audio_transcript.done`.\n   */\n  type: 'response.audio_transcript.done';\n}\n\n/**\n * Send this event to cancel an in-progress response. The server will respond with\n * a `response.done` event with a status of `response.status=cancelled`. If there\n * is no response to cancel, the server will respond with an error.\n */\nexport interface ResponseCancelEvent {\n  /**\n   * The event type, must be `response.cancel`.\n   */\n  type: 'response.cancel';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n\n  /**\n   * A specific response ID to cancel - if not provided, will cancel an in-progress\n   * response in the default conversation.\n   */\n  response_id?: string;\n}\n\n/**\n * Returned when a new content part is added to an assistant message item during\n * response generation.\n */\nexport interface ResponseContentPartAddedEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item to which the content part was added.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The content part that was added.\n   */\n  part: ResponseContentPartAddedEvent.Part;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.content_part.added`.\n   */\n  type: 'response.content_part.added';\n}\n\nexport namespace ResponseContentPartAddedEvent {\n  /**\n   * The content part that was added.\n   */\n  export interface Part {\n    /**\n     * Base64-encoded audio data (if type is \"audio\").\n     */\n    audio?: string;\n\n    /**\n     * The text content (if type is \"text\").\n     */\n    text?: string;\n\n    /**\n     * The transcript of the audio (if type is \"audio\").\n     */\n    transcript?: string;\n\n    /**\n     * The content type (\"text\", \"audio\").\n     */\n    type?: 'text' | 'audio';\n  }\n}\n\n/**\n * Returned when a content part is done streaming in an assistant message item.\n * Also emitted when a Response is interrupted, incomplete, or cancelled.\n */\nexport interface ResponseContentPartDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The content part that is done.\n   */\n  part: ResponseContentPartDoneEvent.Part;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.content_part.done`.\n   */\n  type: 'response.content_part.done';\n}\n\nexport namespace ResponseContentPartDoneEvent {\n  /**\n   * The content part that is done.\n   */\n  export interface Part {\n    /**\n     * Base64-encoded audio data (if type is \"audio\").\n     */\n    audio?: string;\n\n    /**\n     * The text content (if type is \"text\").\n     */\n    text?: string;\n\n    /**\n     * The transcript of the audio (if type is \"audio\").\n     */\n    transcript?: string;\n\n    /**\n     * The content type (\"text\", \"audio\").\n     */\n    type?: 'text' | 'audio';\n  }\n}\n\n/**\n * This event instructs the server to create a Response, which means triggering\n * model inference. When in Server VAD mode, the server will create Responses\n * automatically.\n *\n * A Response will include at least one Item, and may have two, in which case the\n * second will be a function call. These Items will be appended to the conversation\n * history.\n *\n * The server will respond with a `response.created` event, events for Items and\n * content created, and finally a `response.done` event to indicate the Response is\n * complete.\n *\n * The `response.create` event includes inference configuration like\n * `instructions`, and `temperature`. These fields will override the Session's\n * configuration for this Response only.\n */\nexport interface ResponseCreateEvent {\n  /**\n   * The event type, must be `response.create`.\n   */\n  type: 'response.create';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n\n  /**\n   * Create a new Realtime response with these parameters\n   */\n  response?: ResponseCreateEvent.Response;\n}\n\nexport namespace ResponseCreateEvent {\n  /**\n   * Create a new Realtime response with these parameters\n   */\n  export interface Response {\n    /**\n     * Controls which conversation the response is added to. Currently supports `auto`\n     * and `none`, with `auto` as the default value. The `auto` value means that the\n     * contents of the response will be added to the default conversation. Set this to\n     * `none` to create an out-of-band response which will not add items to default\n     * conversation.\n     */\n    conversation?: (string & {}) | 'auto' | 'none';\n\n    /**\n     * Input items to include in the prompt for the model. Using this field creates a\n     * new context for this Response instead of using the default conversation. An\n     * empty array `[]` will clear the context for this Response. Note that this can\n     * include references to items from the default conversation.\n     */\n    input?: Array<RealtimeAPI.ConversationItemWithReference>;\n\n    /**\n     * The default system instructions (i.e. system message) prepended to model calls.\n     * This field allows the client to guide the model on desired responses. The model\n     * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n     * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n     * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n     * instructions are not guaranteed to be followed by the model, but they provide\n     * guidance to the model on the desired behavior.\n     *\n     * Note that the server sets default instructions which will be used if this field\n     * is not set and are visible in the `session.created` event at the start of the\n     * session.\n     */\n    instructions?: string;\n\n    /**\n     * Maximum number of output tokens for a single assistant response, inclusive of\n     * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n     * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n     */\n    max_response_output_tokens?: number | 'inf';\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n\n    /**\n     * The set of modalities the model can respond with. To disable audio, set this to\n     * [\"text\"].\n     */\n    modalities?: Array<'text' | 'audio'>;\n\n    /**\n     * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n     */\n    output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n    /**\n     * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\n     */\n    temperature?: number;\n\n    /**\n     * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\n     * a function, like `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n     */\n    tool_choice?: string;\n\n    /**\n     * Tools (functions) available to the model.\n     */\n    tools?: Array<Response.Tool>;\n\n    /**\n     * The voice the model uses to respond. Voice cannot be changed during the session\n     * once the model has responded with audio at least once. Current voice options are\n     * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n     */\n    voice?: (string & {}) | 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';\n  }\n\n  export namespace Response {\n    export interface Tool {\n      /**\n       * The description of the function, including guidance on when and how to call it,\n       * and guidance about what to tell the user when calling (if anything).\n       */\n      description?: string;\n\n      /**\n       * The name of the function.\n       */\n      name?: string;\n\n      /**\n       * Parameters of the function in JSON Schema.\n       */\n      parameters?: unknown;\n\n      /**\n       * The type of the tool, i.e. `function`.\n       */\n      type?: 'function';\n    }\n  }\n}\n\n/**\n * Returned when a new Response is created. The first event of response creation,\n * where the response is in an initial state of `in_progress`.\n */\nexport interface ResponseCreatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The response resource.\n   */\n  response: RealtimeResponse;\n\n  /**\n   * The event type, must be `response.created`.\n   */\n  type: 'response.created';\n}\n\n/**\n * Returned when a Response is done streaming. Always emitted, no matter the final\n * state. The Response object included in the `response.done` event will include\n * all output Items in the Response but will omit the raw audio data.\n */\nexport interface ResponseDoneEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The response resource.\n   */\n  response: RealtimeResponse;\n\n  /**\n   * The event type, must be `response.done`.\n   */\n  type: 'response.done';\n}\n\n/**\n * Returned when the model-generated function call arguments are updated.\n */\nexport interface ResponseFunctionCallArgumentsDeltaEvent {\n  /**\n   * The ID of the function call.\n   */\n  call_id: string;\n\n  /**\n   * The arguments delta as a JSON string.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the function call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.function_call_arguments.delta`.\n   */\n  type: 'response.function_call_arguments.delta';\n}\n\n/**\n * Returned when the model-generated function call arguments are done streaming.\n * Also emitted when a Response is interrupted, incomplete, or cancelled.\n */\nexport interface ResponseFunctionCallArgumentsDoneEvent {\n  /**\n   * The final arguments as a JSON string.\n   */\n  arguments: string;\n\n  /**\n   * The ID of the function call.\n   */\n  call_id: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the function call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.function_call_arguments.done`.\n   */\n  type: 'response.function_call_arguments.done';\n}\n\n/**\n * Returned when a new Item is created during Response generation.\n */\nexport interface ResponseOutputItemAddedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The item to add to the conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The index of the output item in the Response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the Response to which the item belongs.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_item.added`.\n   */\n  type: 'response.output_item.added';\n}\n\n/**\n * Returned when an Item is done streaming. Also emitted when a Response is\n * interrupted, incomplete, or cancelled.\n */\nexport interface ResponseOutputItemDoneEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The item to add to the conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The index of the output item in the Response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the Response to which the item belongs.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_item.done`.\n   */\n  type: 'response.output_item.done';\n}\n\n/**\n * Returned when the text value of a \"text\" content part is updated.\n */\nexport interface ResponseTextDeltaEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The text delta.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.text.delta`.\n   */\n  type: 'response.text.delta';\n}\n\n/**\n * Returned when the text value of a \"text\" content part is done streaming. Also\n * emitted when a Response is interrupted, incomplete, or cancelled.\n */\nexport interface ResponseTextDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The final text content.\n   */\n  text: string;\n\n  /**\n   * The event type, must be `response.text.done`.\n   */\n  type: 'response.text.done';\n}\n\n/**\n * Returned when a Session is created. Emitted automatically when a new connection\n * is established as the first server event. This event will contain the default\n * Session configuration.\n */\nexport interface SessionCreatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * Realtime session object configuration.\n   */\n  session: SessionsAPI.Session;\n\n  /**\n   * The event type, must be `session.created`.\n   */\n  type: 'session.created';\n}\n\n/**\n * Send this event to update the sessions default configuration. The client may\n * send this event at any time to update any field, except for `voice`. However,\n * note that once a session has been initialized with a particular `model`, it\n * cant be changed to another model using `session.update`.\n *\n * When the server receives a `session.update`, it will respond with a\n * `session.updated` event showing the full, effective configuration. Only the\n * fields that are present are updated. To clear a field like `instructions`, pass\n * an empty string.\n */\nexport interface SessionUpdateEvent {\n  /**\n   * Realtime session object configuration.\n   */\n  session: SessionUpdateEvent.Session;\n\n  /**\n   * The event type, must be `session.update`.\n   */\n  type: 'session.update';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\nexport namespace SessionUpdateEvent {\n  /**\n   * Realtime session object configuration.\n   */\n  export interface Session {\n    /**\n     * Configuration options for the generated client secret.\n     */\n    client_secret?: Session.ClientSecret;\n\n    /**\n     * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\n     * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\n     * (mono), and little-endian byte order.\n     */\n    input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n    /**\n     * Configuration for input audio noise reduction. This can be set to `null` to turn\n     * off. Noise reduction filters audio added to the input audio buffer before it is\n     * sent to VAD and the model. Filtering the audio can improve VAD and turn\n     * detection accuracy (reducing false positives) and model performance by improving\n     * perception of the input audio.\n     */\n    input_audio_noise_reduction?: Session.InputAudioNoiseReduction;\n\n    /**\n     * Configuration for input audio transcription, defaults to off and can be set to\n     * `null` to turn off once on. Input audio transcription is not native to the\n     * model, since the model consumes audio directly. Transcription runs\n     * asynchronously through\n     * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n     * and should be treated as guidance of input audio content rather than precisely\n     * what the model heard. The client can optionally set the language and prompt for\n     * transcription, these offer additional guidance to the transcription service.\n     */\n    input_audio_transcription?: Session.InputAudioTranscription;\n\n    /**\n     * The default system instructions (i.e. system message) prepended to model calls.\n     * This field allows the client to guide the model on desired responses. The model\n     * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n     * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n     * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n     * instructions are not guaranteed to be followed by the model, but they provide\n     * guidance to the model on the desired behavior.\n     *\n     * Note that the server sets default instructions which will be used if this field\n     * is not set and are visible in the `session.created` event at the start of the\n     * session.\n     */\n    instructions?: string;\n\n    /**\n     * Maximum number of output tokens for a single assistant response, inclusive of\n     * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n     * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n     */\n    max_response_output_tokens?: number | 'inf';\n\n    /**\n     * The set of modalities the model can respond with. To disable audio, set this to\n     * [\"text\"].\n     */\n    modalities?: Array<'text' | 'audio'>;\n\n    /**\n     * The Realtime model used for this session.\n     */\n    model?:\n      | 'gpt-4o-realtime-preview'\n      | 'gpt-4o-realtime-preview-2024-10-01'\n      | 'gpt-4o-realtime-preview-2024-12-17'\n      | 'gpt-4o-realtime-preview-2025-06-03'\n      | 'gpt-4o-mini-realtime-preview'\n      | 'gpt-4o-mini-realtime-preview-2024-12-17';\n\n    /**\n     * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n     * For `pcm16`, output audio is sampled at a rate of 24kHz.\n     */\n    output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n    /**\n     * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\n     * minimum speed. 1.5 is the maximum speed. This value can only be changed in\n     * between model turns, not while a response is in progress.\n     */\n    speed?: number;\n\n    /**\n     * Sampling temperature for the model, limited to [0.6, 1.2]. For audio models a\n     * temperature of 0.8 is highly recommended for best performance.\n     */\n    temperature?: number;\n\n    /**\n     * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\n     * a function.\n     */\n    tool_choice?: string;\n\n    /**\n     * Tools (functions) available to the model.\n     */\n    tools?: Array<Session.Tool>;\n\n    /**\n     * Configuration options for tracing. Set to null to disable tracing. Once tracing\n     * is enabled for a session, the configuration cannot be modified.\n     *\n     * `auto` will create a trace for the session with default values for the workflow\n     * name, group id, and metadata.\n     */\n    tracing?: 'auto' | Session.TracingConfiguration;\n\n    /**\n     * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n     * set to `null` to turn off, in which case the client must manually trigger model\n     * response. Server VAD means that the model will detect the start and end of\n     * speech based on audio volume and respond at the end of user speech. Semantic VAD\n     * is more advanced and uses a turn detection model (in conjunction with VAD) to\n     * semantically estimate whether the user has finished speaking, then dynamically\n     * sets a timeout based on this probability. For example, if user audio trails off\n     * with \"uhhm\", the model will score a low probability of turn end and wait longer\n     * for the user to continue speaking. This can be useful for more natural\n     * conversations, but may have a higher latency.\n     */\n    turn_detection?: Session.TurnDetection;\n\n    /**\n     * The voice the model uses to respond. Voice cannot be changed during the session\n     * once the model has responded with audio at least once. Current voice options are\n     * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n     */\n    voice?: (string & {}) | 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';\n  }\n\n  export namespace Session {\n    /**\n     * Configuration options for the generated client secret.\n     */\n    export interface ClientSecret {\n      /**\n       * Configuration for the ephemeral token expiration.\n       */\n      expires_after?: ClientSecret.ExpiresAfter;\n    }\n\n    export namespace ClientSecret {\n      /**\n       * Configuration for the ephemeral token expiration.\n       */\n      export interface ExpiresAfter {\n        /**\n         * The anchor point for the ephemeral token expiration. Only `created_at` is\n         * currently supported.\n         */\n        anchor: 'created_at';\n\n        /**\n         * The number of seconds from the anchor point to the expiration. Select a value\n         * between `10` and `7200`.\n         */\n        seconds?: number;\n      }\n    }\n\n    /**\n     * Configuration for input audio noise reduction. This can be set to `null` to turn\n     * off. Noise reduction filters audio added to the input audio buffer before it is\n     * sent to VAD and the model. Filtering the audio can improve VAD and turn\n     * detection accuracy (reducing false positives) and model performance by improving\n     * perception of the input audio.\n     */\n    export interface InputAudioNoiseReduction {\n      /**\n       * Type of noise reduction. `near_field` is for close-talking microphones such as\n       * headphones, `far_field` is for far-field microphones such as laptop or\n       * conference room microphones.\n       */\n      type?: 'near_field' | 'far_field';\n    }\n\n    /**\n     * Configuration for input audio transcription, defaults to off and can be set to\n     * `null` to turn off once on. Input audio transcription is not native to the\n     * model, since the model consumes audio directly. Transcription runs\n     * asynchronously through\n     * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n     * and should be treated as guidance of input audio content rather than precisely\n     * what the model heard. The client can optionally set the language and prompt for\n     * transcription, these offer additional guidance to the transcription service.\n     */\n    export interface InputAudioTranscription {\n      /**\n       * The language of the input audio. Supplying the input language in\n       * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n       * format will improve accuracy and latency.\n       */\n      language?: string;\n\n      /**\n       * The model to use for transcription, current options are `gpt-4o-transcribe`,\n       * `gpt-4o-mini-transcribe`, and `whisper-1`.\n       */\n      model?: string;\n\n      /**\n       * An optional text to guide the model's style or continue a previous audio\n       * segment. For `whisper-1`, the\n       * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n       * For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n       * \"expect words related to technology\".\n       */\n      prompt?: string;\n    }\n\n    export interface Tool {\n      /**\n       * The description of the function, including guidance on when and how to call it,\n       * and guidance about what to tell the user when calling (if anything).\n       */\n      description?: string;\n\n      /**\n       * The name of the function.\n       */\n      name?: string;\n\n      /**\n       * Parameters of the function in JSON Schema.\n       */\n      parameters?: unknown;\n\n      /**\n       * The type of the tool, i.e. `function`.\n       */\n      type?: 'function';\n    }\n\n    /**\n     * Granular configuration for tracing.\n     */\n    export interface TracingConfiguration {\n      /**\n       * The group id to attach to this trace to enable filtering and grouping in the\n       * traces dashboard.\n       */\n      group_id?: string;\n\n      /**\n       * The arbitrary metadata to attach to this trace to enable filtering in the traces\n       * dashboard.\n       */\n      metadata?: unknown;\n\n      /**\n       * The name of the workflow to attach to this trace. This is used to name the trace\n       * in the traces dashboard.\n       */\n      workflow_name?: string;\n    }\n\n    /**\n     * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n     * set to `null` to turn off, in which case the client must manually trigger model\n     * response. Server VAD means that the model will detect the start and end of\n     * speech based on audio volume and respond at the end of user speech. Semantic VAD\n     * is more advanced and uses a turn detection model (in conjunction with VAD) to\n     * semantically estimate whether the user has finished speaking, then dynamically\n     * sets a timeout based on this probability. For example, if user audio trails off\n     * with \"uhhm\", the model will score a low probability of turn end and wait longer\n     * for the user to continue speaking. This can be useful for more natural\n     * conversations, but may have a higher latency.\n     */\n    export interface TurnDetection {\n      /**\n       * Whether or not to automatically generate a response when a VAD stop event\n       * occurs.\n       */\n      create_response?: boolean;\n\n      /**\n       * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n       * will wait longer for the user to continue speaking, `high` will respond more\n       * quickly. `auto` is the default and is equivalent to `medium`.\n       */\n      eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n      /**\n       * Whether or not to automatically interrupt any ongoing response with output to\n       * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n       * occurs.\n       */\n      interrupt_response?: boolean;\n\n      /**\n       * Used only for `server_vad` mode. Amount of audio to include before the VAD\n       * detected speech (in milliseconds). Defaults to 300ms.\n       */\n      prefix_padding_ms?: number;\n\n      /**\n       * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n       * milliseconds). Defaults to 500ms. With shorter values the model will respond\n       * more quickly, but may jump in on short pauses from the user.\n       */\n      silence_duration_ms?: number;\n\n      /**\n       * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n       * defaults to 0.5. A higher threshold will require louder audio to activate the\n       * model, and thus might perform better in noisy environments.\n       */\n      threshold?: number;\n\n      /**\n       * Type of turn detection.\n       */\n      type?: 'server_vad' | 'semantic_vad';\n    }\n  }\n}\n\n/**\n * Returned when a session is updated with a `session.update` event, unless there\n * is an error.\n */\nexport interface SessionUpdatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * Realtime session object configuration.\n   */\n  session: SessionsAPI.Session;\n\n  /**\n   * The event type, must be `session.updated`.\n   */\n  type: 'session.updated';\n}\n\n/**\n * Send this event to update a transcription session.\n */\nexport interface TranscriptionSessionUpdate {\n  /**\n   * Realtime transcription session object configuration.\n   */\n  session: TranscriptionSessionUpdate.Session;\n\n  /**\n   * The event type, must be `transcription_session.update`.\n   */\n  type: 'transcription_session.update';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\nexport namespace TranscriptionSessionUpdate {\n  /**\n   * Realtime transcription session object configuration.\n   */\n  export interface Session {\n    /**\n     * Configuration options for the generated client secret.\n     */\n    client_secret?: Session.ClientSecret;\n\n    /**\n     * The set of items to include in the transcription. Current available items are:\n     *\n     * - `item.input_audio_transcription.logprobs`\n     */\n    include?: Array<string>;\n\n    /**\n     * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\n     * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\n     * (mono), and little-endian byte order.\n     */\n    input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n    /**\n     * Configuration for input audio noise reduction. This can be set to `null` to turn\n     * off. Noise reduction filters audio added to the input audio buffer before it is\n     * sent to VAD and the model. Filtering the audio can improve VAD and turn\n     * detection accuracy (reducing false positives) and model performance by improving\n     * perception of the input audio.\n     */\n    input_audio_noise_reduction?: Session.InputAudioNoiseReduction;\n\n    /**\n     * Configuration for input audio transcription. The client can optionally set the\n     * language and prompt for transcription, these offer additional guidance to the\n     * transcription service.\n     */\n    input_audio_transcription?: Session.InputAudioTranscription;\n\n    /**\n     * The set of modalities the model can respond with. To disable audio, set this to\n     * [\"text\"].\n     */\n    modalities?: Array<'text' | 'audio'>;\n\n    /**\n     * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n     * set to `null` to turn off, in which case the client must manually trigger model\n     * response. Server VAD means that the model will detect the start and end of\n     * speech based on audio volume and respond at the end of user speech. Semantic VAD\n     * is more advanced and uses a turn detection model (in conjunction with VAD) to\n     * semantically estimate whether the user has finished speaking, then dynamically\n     * sets a timeout based on this probability. For example, if user audio trails off\n     * with \"uhhm\", the model will score a low probability of turn end and wait longer\n     * for the user to continue speaking. This can be useful for more natural\n     * conversations, but may have a higher latency.\n     */\n    turn_detection?: Session.TurnDetection;\n  }\n\n  export namespace Session {\n    /**\n     * Configuration options for the generated client secret.\n     */\n    export interface ClientSecret {\n      /**\n       * Configuration for the ephemeral token expiration.\n       */\n      expires_at?: ClientSecret.ExpiresAt;\n    }\n\n    export namespace ClientSecret {\n      /**\n       * Configuration for the ephemeral token expiration.\n       */\n      export interface ExpiresAt {\n        /**\n         * The anchor point for the ephemeral token expiration. Only `created_at` is\n         * currently supported.\n         */\n        anchor?: 'created_at';\n\n        /**\n         * The number of seconds from the anchor point to the expiration. Select a value\n         * between `10` and `7200`.\n         */\n        seconds?: number;\n      }\n    }\n\n    /**\n     * Configuration for input audio noise reduction. This can be set to `null` to turn\n     * off. Noise reduction filters audio added to the input audio buffer before it is\n     * sent to VAD and the model. Filtering the audio can improve VAD and turn\n     * detection accuracy (reducing false positives) and model performance by improving\n     * perception of the input audio.\n     */\n    export interface InputAudioNoiseReduction {\n      /**\n       * Type of noise reduction. `near_field` is for close-talking microphones such as\n       * headphones, `far_field` is for far-field microphones such as laptop or\n       * conference room microphones.\n       */\n      type?: 'near_field' | 'far_field';\n    }\n\n    /**\n     * Configuration for input audio transcription. The client can optionally set the\n     * language and prompt for transcription, these offer additional guidance to the\n     * transcription service.\n     */\n    export interface InputAudioTranscription {\n      /**\n       * The language of the input audio. Supplying the input language in\n       * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n       * format will improve accuracy and latency.\n       */\n      language?: string;\n\n      /**\n       * The model to use for transcription, current options are `gpt-4o-transcribe`,\n       * `gpt-4o-mini-transcribe`, and `whisper-1`.\n       */\n      model?: 'gpt-4o-transcribe' | 'gpt-4o-mini-transcribe' | 'whisper-1';\n\n      /**\n       * An optional text to guide the model's style or continue a previous audio\n       * segment. For `whisper-1`, the\n       * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n       * For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n       * \"expect words related to technology\".\n       */\n      prompt?: string;\n    }\n\n    /**\n     * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n     * set to `null` to turn off, in which case the client must manually trigger model\n     * response. Server VAD means that the model will detect the start and end of\n     * speech based on audio volume and respond at the end of user speech. Semantic VAD\n     * is more advanced and uses a turn detection model (in conjunction with VAD) to\n     * semantically estimate whether the user has finished speaking, then dynamically\n     * sets a timeout based on this probability. For example, if user audio trails off\n     * with \"uhhm\", the model will score a low probability of turn end and wait longer\n     * for the user to continue speaking. This can be useful for more natural\n     * conversations, but may have a higher latency.\n     */\n    export interface TurnDetection {\n      /**\n       * Whether or not to automatically generate a response when a VAD stop event\n       * occurs. Not available for transcription sessions.\n       */\n      create_response?: boolean;\n\n      /**\n       * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n       * will wait longer for the user to continue speaking, `high` will respond more\n       * quickly. `auto` is the default and is equivalent to `medium`.\n       */\n      eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n      /**\n       * Whether or not to automatically interrupt any ongoing response with output to\n       * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n       * occurs. Not available for transcription sessions.\n       */\n      interrupt_response?: boolean;\n\n      /**\n       * Used only for `server_vad` mode. Amount of audio to include before the VAD\n       * detected speech (in milliseconds). Defaults to 300ms.\n       */\n      prefix_padding_ms?: number;\n\n      /**\n       * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n       * milliseconds). Defaults to 500ms. With shorter values the model will respond\n       * more quickly, but may jump in on short pauses from the user.\n       */\n      silence_duration_ms?: number;\n\n      /**\n       * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n       * defaults to 0.5. A higher threshold will require louder audio to activate the\n       * model, and thus might perform better in noisy environments.\n       */\n      threshold?: number;\n\n      /**\n       * Type of turn detection.\n       */\n      type?: 'server_vad' | 'semantic_vad';\n    }\n  }\n}\n\n/**\n * Returned when a transcription session is updated with a\n * `transcription_session.update` event, unless there is an error.\n */\nexport interface TranscriptionSessionUpdatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * A new Realtime transcription session configuration.\n   *\n   * When a session is created on the server via REST API, the session object also\n   * contains an ephemeral key. Default TTL for keys is 10 minutes. This property is\n   * not present when a session is updated via the WebSocket API.\n   */\n  session: TranscriptionSessionsAPI.TranscriptionSession;\n\n  /**\n   * The event type, must be `transcription_session.updated`.\n   */\n  type: 'transcription_session.updated';\n}\n\nRealtime.Sessions = Sessions;\nRealtime.TranscriptionSessions = TranscriptionSessions;\n\nexport declare namespace Realtime {\n  export {\n    type ConversationCreatedEvent as ConversationCreatedEvent,\n    type ConversationItem as ConversationItem,\n    type ConversationItemContent as ConversationItemContent,\n    type ConversationItemCreateEvent as ConversationItemCreateEvent,\n    type ConversationItemCreatedEvent as ConversationItemCreatedEvent,\n    type ConversationItemDeleteEvent as ConversationItemDeleteEvent,\n    type ConversationItemDeletedEvent as ConversationItemDeletedEvent,\n    type ConversationItemInputAudioTranscriptionCompletedEvent as ConversationItemInputAudioTranscriptionCompletedEvent,\n    type ConversationItemInputAudioTranscriptionDeltaEvent as ConversationItemInputAudioTranscriptionDeltaEvent,\n    type ConversationItemInputAudioTranscriptionFailedEvent as ConversationItemInputAudioTranscriptionFailedEvent,\n    type ConversationItemRetrieveEvent as ConversationItemRetrieveEvent,\n    type ConversationItemTruncateEvent as ConversationItemTruncateEvent,\n    type ConversationItemTruncatedEvent as ConversationItemTruncatedEvent,\n    type ConversationItemWithReference as ConversationItemWithReference,\n    type ErrorEvent as ErrorEvent,\n    type InputAudioBufferAppendEvent as InputAudioBufferAppendEvent,\n    type InputAudioBufferClearEvent as InputAudioBufferClearEvent,\n    type InputAudioBufferClearedEvent as InputAudioBufferClearedEvent,\n    type InputAudioBufferCommitEvent as InputAudioBufferCommitEvent,\n    type InputAudioBufferCommittedEvent as InputAudioBufferCommittedEvent,\n    type InputAudioBufferSpeechStartedEvent as InputAudioBufferSpeechStartedEvent,\n    type InputAudioBufferSpeechStoppedEvent as InputAudioBufferSpeechStoppedEvent,\n    type RateLimitsUpdatedEvent as RateLimitsUpdatedEvent,\n    type RealtimeClientEvent as RealtimeClientEvent,\n    type RealtimeResponse as RealtimeResponse,\n    type RealtimeResponseStatus as RealtimeResponseStatus,\n    type RealtimeResponseUsage as RealtimeResponseUsage,\n    type RealtimeServerEvent as RealtimeServerEvent,\n    type ResponseAudioDeltaEvent as ResponseAudioDeltaEvent,\n    type ResponseAudioDoneEvent as ResponseAudioDoneEvent,\n    type ResponseAudioTranscriptDeltaEvent as ResponseAudioTranscriptDeltaEvent,\n    type ResponseAudioTranscriptDoneEvent as ResponseAudioTranscriptDoneEvent,\n    type ResponseCancelEvent as ResponseCancelEvent,\n    type ResponseContentPartAddedEvent as ResponseContentPartAddedEvent,\n    type ResponseContentPartDoneEvent as ResponseContentPartDoneEvent,\n    type ResponseCreateEvent as ResponseCreateEvent,\n    type ResponseCreatedEvent as ResponseCreatedEvent,\n    type ResponseDoneEvent as ResponseDoneEvent,\n    type ResponseFunctionCallArgumentsDeltaEvent as ResponseFunctionCallArgumentsDeltaEvent,\n    type ResponseFunctionCallArgumentsDoneEvent as ResponseFunctionCallArgumentsDoneEvent,\n    type ResponseOutputItemAddedEvent as ResponseOutputItemAddedEvent,\n    type ResponseOutputItemDoneEvent as ResponseOutputItemDoneEvent,\n    type ResponseTextDeltaEvent as ResponseTextDeltaEvent,\n    type ResponseTextDoneEvent as ResponseTextDoneEvent,\n    type SessionCreatedEvent as SessionCreatedEvent,\n    type SessionUpdateEvent as SessionUpdateEvent,\n    type SessionUpdatedEvent as SessionUpdatedEvent,\n    type TranscriptionSessionUpdate as TranscriptionSessionUpdate,\n    type TranscriptionSessionUpdatedEvent as TranscriptionSessionUpdatedEvent,\n  };\n\n  export {\n    Sessions as Sessions,\n    type SessionsAPISession as Session,\n    type SessionCreateResponse as SessionCreateResponse,\n    type SessionCreateParams as SessionCreateParams,\n  };\n\n  export {\n    TranscriptionSessions as TranscriptionSessions,\n    type TranscriptionSession as TranscriptionSession,\n    type TranscriptionSessionCreateParams as TranscriptionSessionCreateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as ThreadsAPI from './threads';\nimport { APIPromise } from '../../../core/api-promise';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Sessions extends APIResource {\n  /**\n   * Create a ChatKit session.\n   *\n   * @example\n   * ```ts\n   * const chatSession =\n   *   await client.beta.chatkit.sessions.create({\n   *     user: 'x',\n   *     workflow: { id: 'id' },\n   *   });\n   * ```\n   */\n  create(body: SessionCreateParams, options?: RequestOptions): APIPromise<ThreadsAPI.ChatSession> {\n    return this._client.post('/chatkit/sessions', {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'chatkit_beta=v1' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Cancel an active ChatKit session and return its most recent metadata.\n   *\n   * Cancelling prevents new requests from using the issued client secret.\n   *\n   * @example\n   * ```ts\n   * const chatSession =\n   *   await client.beta.chatkit.sessions.cancel('cksess_123');\n   * ```\n   */\n  cancel(sessionID: string, options?: RequestOptions): APIPromise<ThreadsAPI.ChatSession> {\n    return this._client.post(path`/chatkit/sessions/${sessionID}/cancel`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'chatkit_beta=v1' }, options?.headers]),\n    });\n  }\n}\n\nexport interface SessionCreateParams {\n  /**\n   * A free-form string that identifies your end user; ensures this Session can\n   * access other objects that have the same `user` scope.\n   */\n  user: string;\n\n  /**\n   * Workflow that powers the session.\n   */\n  workflow: ThreadsAPI.ChatSessionWorkflowParam;\n\n  /**\n   * Optional overrides for ChatKit runtime configuration features\n   */\n  chatkit_configuration?: ThreadsAPI.ChatSessionChatKitConfigurationParam;\n\n  /**\n   * Optional override for session expiration timing in seconds from creation.\n   * Defaults to 10 minutes.\n   */\n  expires_after?: ThreadsAPI.ChatSessionExpiresAfterParam;\n\n  /**\n   * Optional override for per-minute request limits. When omitted, defaults to 10.\n   */\n  rate_limits?: ThreadsAPI.ChatSessionRateLimitsParam;\n}\n\nexport declare namespace Sessions {\n  export { type SessionCreateParams as SessionCreateParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as ChatKitAPI from './chatkit';\nimport { APIPromise } from '../../../core/api-promise';\nimport {\n  ConversationCursorPage,\n  type ConversationCursorPageParams,\n  PagePromise,\n} from '../../../core/pagination';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Threads extends APIResource {\n  /**\n   * Retrieve a ChatKit thread by its identifier.\n   *\n   * @example\n   * ```ts\n   * const chatkitThread =\n   *   await client.beta.chatkit.threads.retrieve('cthr_123');\n   * ```\n   */\n  retrieve(threadID: string, options?: RequestOptions): APIPromise<ChatKitThread> {\n    return this._client.get(path`/chatkit/threads/${threadID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'chatkit_beta=v1' }, options?.headers]),\n    });\n  }\n\n  /**\n   * List ChatKit threads with optional pagination and user filters.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const chatkitThread of client.beta.chatkit.threads.list()) {\n   *   // ...\n   * }\n   * ```\n   */\n  list(\n    query: ThreadListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<ChatKitThreadsPage, ChatKitThread> {\n    return this._client.getAPIList('/chatkit/threads', ConversationCursorPage<ChatKitThread>, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'chatkit_beta=v1' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Delete a ChatKit thread along with its items and stored attachments.\n   *\n   * @example\n   * ```ts\n   * const thread = await client.beta.chatkit.threads.delete(\n   *   'cthr_123',\n   * );\n   * ```\n   */\n  delete(threadID: string, options?: RequestOptions): APIPromise<ThreadDeleteResponse> {\n    return this._client.delete(path`/chatkit/threads/${threadID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'chatkit_beta=v1' }, options?.headers]),\n    });\n  }\n\n  /**\n   * List items that belong to a ChatKit thread.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const thread of client.beta.chatkit.threads.listItems(\n   *   'cthr_123',\n   * )) {\n   *   // ...\n   * }\n   * ```\n   */\n  listItems(\n    threadID: string,\n    query: ThreadListItemsParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<\n    ChatKitThreadItemListDataPage,\n    | ChatKitThreadUserMessageItem\n    | ChatKitThreadAssistantMessageItem\n    | ChatKitWidgetItem\n    | ChatKitThreadItemList.ChatKitClientToolCall\n    | ChatKitThreadItemList.ChatKitTask\n    | ChatKitThreadItemList.ChatKitTaskGroup\n  > {\n    return this._client.getAPIList(\n      path`/chatkit/threads/${threadID}/items`,\n      ConversationCursorPage<\n        | ChatKitThreadUserMessageItem\n        | ChatKitThreadAssistantMessageItem\n        | ChatKitWidgetItem\n        | ChatKitThreadItemList.ChatKitClientToolCall\n        | ChatKitThreadItemList.ChatKitTask\n        | ChatKitThreadItemList.ChatKitTaskGroup\n      >,\n      { query, ...options, headers: buildHeaders([{ 'OpenAI-Beta': 'chatkit_beta=v1' }, options?.headers]) },\n    );\n  }\n}\n\nexport type ChatKitThreadsPage = ConversationCursorPage<ChatKitThread>;\n\nexport type ChatKitThreadItemListDataPage = ConversationCursorPage<\n  | ChatKitThreadUserMessageItem\n  | ChatKitThreadAssistantMessageItem\n  | ChatKitWidgetItem\n  | ChatKitThreadItemList.ChatKitClientToolCall\n  | ChatKitThreadItemList.ChatKitTask\n  | ChatKitThreadItemList.ChatKitTaskGroup\n>;\n\n/**\n * Represents a ChatKit session and its resolved configuration.\n */\nexport interface ChatSession {\n  /**\n   * Identifier for the ChatKit session.\n   */\n  id: string;\n\n  /**\n   * Resolved ChatKit feature configuration for the session.\n   */\n  chatkit_configuration: ChatSessionChatKitConfiguration;\n\n  /**\n   * Ephemeral client secret that authenticates session requests.\n   */\n  client_secret: string;\n\n  /**\n   * Unix timestamp (in seconds) for when the session expires.\n   */\n  expires_at: number;\n\n  /**\n   * Convenience copy of the per-minute request limit.\n   */\n  max_requests_per_1_minute: number;\n\n  /**\n   * Type discriminator that is always `chatkit.session`.\n   */\n  object: 'chatkit.session';\n\n  /**\n   * Resolved rate limit values.\n   */\n  rate_limits: ChatSessionRateLimits;\n\n  /**\n   * Current lifecycle state of the session.\n   */\n  status: ChatSessionStatus;\n\n  /**\n   * User identifier associated with the session.\n   */\n  user: string;\n\n  /**\n   * Workflow metadata for the session.\n   */\n  workflow: ChatKitAPI.ChatKitWorkflow;\n}\n\n/**\n * Automatic thread title preferences for the session.\n */\nexport interface ChatSessionAutomaticThreadTitling {\n  /**\n   * Whether automatic thread titling is enabled.\n   */\n  enabled: boolean;\n}\n\n/**\n * ChatKit configuration for the session.\n */\nexport interface ChatSessionChatKitConfiguration {\n  /**\n   * Automatic thread titling preferences.\n   */\n  automatic_thread_titling: ChatSessionAutomaticThreadTitling;\n\n  /**\n   * Upload settings for the session.\n   */\n  file_upload: ChatSessionFileUpload;\n\n  /**\n   * History retention configuration.\n   */\n  history: ChatSessionHistory;\n}\n\n/**\n * Optional per-session configuration settings for ChatKit behavior.\n */\nexport interface ChatSessionChatKitConfigurationParam {\n  /**\n   * Configuration for automatic thread titling. When omitted, automatic thread\n   * titling is enabled by default.\n   */\n  automatic_thread_titling?: ChatSessionChatKitConfigurationParam.AutomaticThreadTitling;\n\n  /**\n   * Configuration for upload enablement and limits. When omitted, uploads are\n   * disabled by default (max_files 10, max_file_size 512 MB).\n   */\n  file_upload?: ChatSessionChatKitConfigurationParam.FileUpload;\n\n  /**\n   * Configuration for chat history retention. When omitted, history is enabled by\n   * default with no limit on recent_threads (null).\n   */\n  history?: ChatSessionChatKitConfigurationParam.History;\n}\n\nexport namespace ChatSessionChatKitConfigurationParam {\n  /**\n   * Configuration for automatic thread titling. When omitted, automatic thread\n   * titling is enabled by default.\n   */\n  export interface AutomaticThreadTitling {\n    /**\n     * Enable automatic thread title generation. Defaults to true.\n     */\n    enabled?: boolean;\n  }\n\n  /**\n   * Configuration for upload enablement and limits. When omitted, uploads are\n   * disabled by default (max_files 10, max_file_size 512 MB).\n   */\n  export interface FileUpload {\n    /**\n     * Enable uploads for this session. Defaults to false.\n     */\n    enabled?: boolean;\n\n    /**\n     * Maximum size in megabytes for each uploaded file. Defaults to 512 MB, which is\n     * the maximum allowable size.\n     */\n    max_file_size?: number;\n\n    /**\n     * Maximum number of files that can be uploaded to the session. Defaults to 10.\n     */\n    max_files?: number;\n  }\n\n  /**\n   * Configuration for chat history retention. When omitted, history is enabled by\n   * default with no limit on recent_threads (null).\n   */\n  export interface History {\n    /**\n     * Enables chat users to access previous ChatKit threads. Defaults to true.\n     */\n    enabled?: boolean;\n\n    /**\n     * Number of recent ChatKit threads users have access to. Defaults to unlimited\n     * when unset.\n     */\n    recent_threads?: number;\n  }\n}\n\n/**\n * Controls when the session expires relative to an anchor timestamp.\n */\nexport interface ChatSessionExpiresAfterParam {\n  /**\n   * Base timestamp used to calculate expiration. Currently fixed to `created_at`.\n   */\n  anchor: 'created_at';\n\n  /**\n   * Number of seconds after the anchor when the session expires.\n   */\n  seconds: number;\n}\n\n/**\n * Upload permissions and limits applied to the session.\n */\nexport interface ChatSessionFileUpload {\n  /**\n   * Indicates if uploads are enabled for the session.\n   */\n  enabled: boolean;\n\n  /**\n   * Maximum upload size in megabytes.\n   */\n  max_file_size: number | null;\n\n  /**\n   * Maximum number of uploads allowed during the session.\n   */\n  max_files: number | null;\n}\n\n/**\n * History retention preferences returned for the session.\n */\nexport interface ChatSessionHistory {\n  /**\n   * Indicates if chat history is persisted for the session.\n   */\n  enabled: boolean;\n\n  /**\n   * Number of prior threads surfaced in history views. Defaults to null when all\n   * history is retained.\n   */\n  recent_threads: number | null;\n}\n\n/**\n * Active per-minute request limit for the session.\n */\nexport interface ChatSessionRateLimits {\n  /**\n   * Maximum allowed requests per one-minute window.\n   */\n  max_requests_per_1_minute: number;\n}\n\n/**\n * Controls request rate limits for the session.\n */\nexport interface ChatSessionRateLimitsParam {\n  /**\n   * Maximum number of requests allowed per minute for the session. Defaults to 10.\n   */\n  max_requests_per_1_minute?: number;\n}\n\nexport type ChatSessionStatus = 'active' | 'expired' | 'cancelled';\n\n/**\n * Workflow reference and overrides applied to the chat session.\n */\nexport interface ChatSessionWorkflowParam {\n  /**\n   * Identifier for the workflow invoked by the session.\n   */\n  id: string;\n\n  /**\n   * State variables forwarded to the workflow. Keys may be up to 64 characters,\n   * values must be primitive types, and the map defaults to an empty object.\n   */\n  state_variables?: { [key: string]: string | boolean | number };\n\n  /**\n   * Optional tracing overrides for the workflow invocation. When omitted, tracing is\n   * enabled by default.\n   */\n  tracing?: ChatSessionWorkflowParam.Tracing;\n\n  /**\n   * Specific workflow version to run. Defaults to the latest deployed version.\n   */\n  version?: string;\n}\n\nexport namespace ChatSessionWorkflowParam {\n  /**\n   * Optional tracing overrides for the workflow invocation. When omitted, tracing is\n   * enabled by default.\n   */\n  export interface Tracing {\n    /**\n     * Whether tracing is enabled during the session. Defaults to true.\n     */\n    enabled?: boolean;\n  }\n}\n\n/**\n * Attachment metadata included on thread items.\n */\nexport interface ChatKitAttachment {\n  /**\n   * Identifier for the attachment.\n   */\n  id: string;\n\n  /**\n   * MIME type of the attachment.\n   */\n  mime_type: string;\n\n  /**\n   * Original display name for the attachment.\n   */\n  name: string;\n\n  /**\n   * Preview URL for rendering the attachment inline.\n   */\n  preview_url: string | null;\n\n  /**\n   * Attachment discriminator.\n   */\n  type: 'image' | 'file';\n}\n\n/**\n * Assistant response text accompanied by optional annotations.\n */\nexport interface ChatKitResponseOutputText {\n  /**\n   * Ordered list of annotations attached to the response text.\n   */\n  annotations: Array<ChatKitResponseOutputText.File | ChatKitResponseOutputText.URL>;\n\n  /**\n   * Assistant generated text.\n   */\n  text: string;\n\n  /**\n   * Type discriminator that is always `output_text`.\n   */\n  type: 'output_text';\n}\n\nexport namespace ChatKitResponseOutputText {\n  /**\n   * Annotation that references an uploaded file.\n   */\n  export interface File {\n    /**\n     * File attachment referenced by the annotation.\n     */\n    source: File.Source;\n\n    /**\n     * Type discriminator that is always `file` for this annotation.\n     */\n    type: 'file';\n  }\n\n  export namespace File {\n    /**\n     * File attachment referenced by the annotation.\n     */\n    export interface Source {\n      /**\n       * Filename referenced by the annotation.\n       */\n      filename: string;\n\n      /**\n       * Type discriminator that is always `file`.\n       */\n      type: 'file';\n    }\n  }\n\n  /**\n   * Annotation that references a URL.\n   */\n  export interface URL {\n    /**\n     * URL referenced by the annotation.\n     */\n    source: URL.Source;\n\n    /**\n     * Type discriminator that is always `url` for this annotation.\n     */\n    type: 'url';\n  }\n\n  export namespace URL {\n    /**\n     * URL referenced by the annotation.\n     */\n    export interface Source {\n      /**\n       * Type discriminator that is always `url`.\n       */\n      type: 'url';\n\n      /**\n       * URL referenced by the annotation.\n       */\n      url: string;\n    }\n  }\n}\n\n/**\n * Represents a ChatKit thread and its current status.\n */\nexport interface ChatKitThread {\n  /**\n   * Identifier of the thread.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) for when the thread was created.\n   */\n  created_at: number;\n\n  /**\n   * Type discriminator that is always `chatkit.thread`.\n   */\n  object: 'chatkit.thread';\n\n  /**\n   * Current status for the thread. Defaults to `active` for newly created threads.\n   */\n  status: ChatKitThread.Active | ChatKitThread.Locked | ChatKitThread.Closed;\n\n  /**\n   * Optional human-readable title for the thread. Defaults to null when no title has\n   * been generated.\n   */\n  title: string | null;\n\n  /**\n   * Free-form string that identifies your end user who owns the thread.\n   */\n  user: string;\n}\n\nexport namespace ChatKitThread {\n  /**\n   * Indicates that a thread is active.\n   */\n  export interface Active {\n    /**\n     * Status discriminator that is always `active`.\n     */\n    type: 'active';\n  }\n\n  /**\n   * Indicates that a thread is locked and cannot accept new input.\n   */\n  export interface Locked {\n    /**\n     * Reason that the thread was locked. Defaults to null when no reason is recorded.\n     */\n    reason: string | null;\n\n    /**\n     * Status discriminator that is always `locked`.\n     */\n    type: 'locked';\n  }\n\n  /**\n   * Indicates that a thread has been closed.\n   */\n  export interface Closed {\n    /**\n     * Reason that the thread was closed. Defaults to null when no reason is recorded.\n     */\n    reason: string | null;\n\n    /**\n     * Status discriminator that is always `closed`.\n     */\n    type: 'closed';\n  }\n}\n\n/**\n * Assistant-authored message within a thread.\n */\nexport interface ChatKitThreadAssistantMessageItem {\n  /**\n   * Identifier of the thread item.\n   */\n  id: string;\n\n  /**\n   * Ordered assistant response segments.\n   */\n  content: Array<ChatKitResponseOutputText>;\n\n  /**\n   * Unix timestamp (in seconds) for when the item was created.\n   */\n  created_at: number;\n\n  /**\n   * Type discriminator that is always `chatkit.thread_item`.\n   */\n  object: 'chatkit.thread_item';\n\n  /**\n   * Identifier of the parent thread.\n   */\n  thread_id: string;\n\n  /**\n   * Type discriminator that is always `chatkit.assistant_message`.\n   */\n  type: 'chatkit.assistant_message';\n}\n\n/**\n * A paginated list of thread items rendered for the ChatKit API.\n */\nexport interface ChatKitThreadItemList {\n  /**\n   * A list of items\n   */\n  data: Array<\n    | ChatKitThreadUserMessageItem\n    | ChatKitThreadAssistantMessageItem\n    | ChatKitWidgetItem\n    | ChatKitThreadItemList.ChatKitClientToolCall\n    | ChatKitThreadItemList.ChatKitTask\n    | ChatKitThreadItemList.ChatKitTaskGroup\n  >;\n\n  /**\n   * The ID of the first item in the list.\n   */\n  first_id: string | null;\n\n  /**\n   * Whether there are more items available.\n   */\n  has_more: boolean;\n\n  /**\n   * The ID of the last item in the list.\n   */\n  last_id: string | null;\n\n  /**\n   * The type of object returned, must be `list`.\n   */\n  object: 'list';\n}\n\nexport namespace ChatKitThreadItemList {\n  /**\n   * Record of a client side tool invocation initiated by the assistant.\n   */\n  export interface ChatKitClientToolCall {\n    /**\n     * Identifier of the thread item.\n     */\n    id: string;\n\n    /**\n     * JSON-encoded arguments that were sent to the tool.\n     */\n    arguments: string;\n\n    /**\n     * Identifier for the client tool call.\n     */\n    call_id: string;\n\n    /**\n     * Unix timestamp (in seconds) for when the item was created.\n     */\n    created_at: number;\n\n    /**\n     * Tool name that was invoked.\n     */\n    name: string;\n\n    /**\n     * Type discriminator that is always `chatkit.thread_item`.\n     */\n    object: 'chatkit.thread_item';\n\n    /**\n     * JSON-encoded output captured from the tool. Defaults to null while execution is\n     * in progress.\n     */\n    output: string | null;\n\n    /**\n     * Execution status for the tool call.\n     */\n    status: 'in_progress' | 'completed';\n\n    /**\n     * Identifier of the parent thread.\n     */\n    thread_id: string;\n\n    /**\n     * Type discriminator that is always `chatkit.client_tool_call`.\n     */\n    type: 'chatkit.client_tool_call';\n  }\n\n  /**\n   * Task emitted by the workflow to show progress and status updates.\n   */\n  export interface ChatKitTask {\n    /**\n     * Identifier of the thread item.\n     */\n    id: string;\n\n    /**\n     * Unix timestamp (in seconds) for when the item was created.\n     */\n    created_at: number;\n\n    /**\n     * Optional heading for the task. Defaults to null when not provided.\n     */\n    heading: string | null;\n\n    /**\n     * Type discriminator that is always `chatkit.thread_item`.\n     */\n    object: 'chatkit.thread_item';\n\n    /**\n     * Optional summary that describes the task. Defaults to null when omitted.\n     */\n    summary: string | null;\n\n    /**\n     * Subtype for the task.\n     */\n    task_type: 'custom' | 'thought';\n\n    /**\n     * Identifier of the parent thread.\n     */\n    thread_id: string;\n\n    /**\n     * Type discriminator that is always `chatkit.task`.\n     */\n    type: 'chatkit.task';\n  }\n\n  /**\n   * Collection of workflow tasks grouped together in the thread.\n   */\n  export interface ChatKitTaskGroup {\n    /**\n     * Identifier of the thread item.\n     */\n    id: string;\n\n    /**\n     * Unix timestamp (in seconds) for when the item was created.\n     */\n    created_at: number;\n\n    /**\n     * Type discriminator that is always `chatkit.thread_item`.\n     */\n    object: 'chatkit.thread_item';\n\n    /**\n     * Tasks included in the group.\n     */\n    tasks: Array<ChatKitTaskGroup.Task>;\n\n    /**\n     * Identifier of the parent thread.\n     */\n    thread_id: string;\n\n    /**\n     * Type discriminator that is always `chatkit.task_group`.\n     */\n    type: 'chatkit.task_group';\n  }\n\n  export namespace ChatKitTaskGroup {\n    /**\n     * Task entry that appears within a TaskGroup.\n     */\n    export interface Task {\n      /**\n       * Optional heading for the grouped task. Defaults to null when not provided.\n       */\n      heading: string | null;\n\n      /**\n       * Optional summary that describes the grouped task. Defaults to null when omitted.\n       */\n      summary: string | null;\n\n      /**\n       * Subtype for the grouped task.\n       */\n      type: 'custom' | 'thought';\n    }\n  }\n}\n\n/**\n * User-authored messages within a thread.\n */\nexport interface ChatKitThreadUserMessageItem {\n  /**\n   * Identifier of the thread item.\n   */\n  id: string;\n\n  /**\n   * Attachments associated with the user message. Defaults to an empty list.\n   */\n  attachments: Array<ChatKitAttachment>;\n\n  /**\n   * Ordered content elements supplied by the user.\n   */\n  content: Array<ChatKitThreadUserMessageItem.InputText | ChatKitThreadUserMessageItem.QuotedText>;\n\n  /**\n   * Unix timestamp (in seconds) for when the item was created.\n   */\n  created_at: number;\n\n  /**\n   * Inference overrides applied to the message. Defaults to null when unset.\n   */\n  inference_options: ChatKitThreadUserMessageItem.InferenceOptions | null;\n\n  /**\n   * Type discriminator that is always `chatkit.thread_item`.\n   */\n  object: 'chatkit.thread_item';\n\n  /**\n   * Identifier of the parent thread.\n   */\n  thread_id: string;\n\n  type: 'chatkit.user_message';\n}\n\nexport namespace ChatKitThreadUserMessageItem {\n  /**\n   * Text block that a user contributed to the thread.\n   */\n  export interface InputText {\n    /**\n     * Plain-text content supplied by the user.\n     */\n    text: string;\n\n    /**\n     * Type discriminator that is always `input_text`.\n     */\n    type: 'input_text';\n  }\n\n  /**\n   * Quoted snippet that the user referenced in their message.\n   */\n  export interface QuotedText {\n    /**\n     * Quoted text content.\n     */\n    text: string;\n\n    /**\n     * Type discriminator that is always `quoted_text`.\n     */\n    type: 'quoted_text';\n  }\n\n  /**\n   * Inference overrides applied to the message. Defaults to null when unset.\n   */\n  export interface InferenceOptions {\n    /**\n     * Model name that generated the response. Defaults to null when using the session\n     * default.\n     */\n    model: string | null;\n\n    /**\n     * Preferred tool to invoke. Defaults to null when ChatKit should auto-select.\n     */\n    tool_choice: InferenceOptions.ToolChoice | null;\n  }\n\n  export namespace InferenceOptions {\n    /**\n     * Preferred tool to invoke. Defaults to null when ChatKit should auto-select.\n     */\n    export interface ToolChoice {\n      /**\n       * Identifier of the requested tool.\n       */\n      id: string;\n    }\n  }\n}\n\n/**\n * Thread item that renders a widget payload.\n */\nexport interface ChatKitWidgetItem {\n  /**\n   * Identifier of the thread item.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) for when the item was created.\n   */\n  created_at: number;\n\n  /**\n   * Type discriminator that is always `chatkit.thread_item`.\n   */\n  object: 'chatkit.thread_item';\n\n  /**\n   * Identifier of the parent thread.\n   */\n  thread_id: string;\n\n  /**\n   * Type discriminator that is always `chatkit.widget`.\n   */\n  type: 'chatkit.widget';\n\n  /**\n   * Serialized widget payload rendered in the UI.\n   */\n  widget: string;\n}\n\n/**\n * Confirmation payload returned after deleting a thread.\n */\nexport interface ThreadDeleteResponse {\n  /**\n   * Identifier of the deleted thread.\n   */\n  id: string;\n\n  /**\n   * Indicates that the thread has been deleted.\n   */\n  deleted: boolean;\n\n  /**\n   * Type discriminator that is always `chatkit.thread.deleted`.\n   */\n  object: 'chatkit.thread.deleted';\n}\n\nexport interface ThreadListParams extends ConversationCursorPageParams {\n  /**\n   * List items created before this thread item ID. Defaults to null for the newest\n   * results.\n   */\n  before?: string;\n\n  /**\n   * Sort order for results by creation time. Defaults to `desc`.\n   */\n  order?: 'asc' | 'desc';\n\n  /**\n   * Filter threads that belong to this user identifier. Defaults to null to return\n   * all users.\n   */\n  user?: string;\n}\n\nexport interface ThreadListItemsParams extends ConversationCursorPageParams {\n  /**\n   * List items created before this thread item ID. Defaults to null for the newest\n   * results.\n   */\n  before?: string;\n\n  /**\n   * Sort order for results by creation time. Defaults to `desc`.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport declare namespace Threads {\n  export {\n    type ChatSession as ChatSession,\n    type ChatSessionAutomaticThreadTitling as ChatSessionAutomaticThreadTitling,\n    type ChatSessionChatKitConfiguration as ChatSessionChatKitConfiguration,\n    type ChatSessionChatKitConfigurationParam as ChatSessionChatKitConfigurationParam,\n    type ChatSessionExpiresAfterParam as ChatSessionExpiresAfterParam,\n    type ChatSessionFileUpload as ChatSessionFileUpload,\n    type ChatSessionHistory as ChatSessionHistory,\n    type ChatSessionRateLimits as ChatSessionRateLimits,\n    type ChatSessionRateLimitsParam as ChatSessionRateLimitsParam,\n    type ChatSessionStatus as ChatSessionStatus,\n    type ChatSessionWorkflowParam as ChatSessionWorkflowParam,\n    type ChatKitAttachment as ChatKitAttachment,\n    type ChatKitResponseOutputText as ChatKitResponseOutputText,\n    type ChatKitThread as ChatKitThread,\n    type ChatKitThreadAssistantMessageItem as ChatKitThreadAssistantMessageItem,\n    type ChatKitThreadItemList as ChatKitThreadItemList,\n    type ChatKitThreadUserMessageItem as ChatKitThreadUserMessageItem,\n    type ChatKitWidgetItem as ChatKitWidgetItem,\n    type ThreadDeleteResponse as ThreadDeleteResponse,\n    type ChatKitThreadsPage as ChatKitThreadsPage,\n    type ChatKitThreadItemListDataPage as ChatKitThreadItemListDataPage,\n    type ThreadListParams as ThreadListParams,\n    type ThreadListItemsParams as ThreadListItemsParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as SessionsAPI from './sessions';\nimport { SessionCreateParams, Sessions } from './sessions';\nimport * as ThreadsAPI from './threads';\nimport {\n  ChatKitAttachment,\n  ChatKitResponseOutputText,\n  ChatKitThread,\n  ChatKitThreadAssistantMessageItem,\n  ChatKitThreadItemList,\n  ChatKitThreadItemListDataPage,\n  ChatKitThreadUserMessageItem,\n  ChatKitThreadsPage,\n  ChatKitWidgetItem,\n  ChatSession,\n  ChatSessionAutomaticThreadTitling,\n  ChatSessionChatKitConfiguration,\n  ChatSessionChatKitConfigurationParam,\n  ChatSessionExpiresAfterParam,\n  ChatSessionFileUpload,\n  ChatSessionHistory,\n  ChatSessionRateLimits,\n  ChatSessionRateLimitsParam,\n  ChatSessionStatus,\n  ChatSessionWorkflowParam,\n  ThreadDeleteResponse,\n  ThreadListItemsParams,\n  ThreadListParams,\n  Threads,\n} from './threads';\n\nexport class ChatKit extends APIResource {\n  sessions: SessionsAPI.Sessions = new SessionsAPI.Sessions(this._client);\n  threads: ThreadsAPI.Threads = new ThreadsAPI.Threads(this._client);\n}\n\n/**\n * Workflow metadata and state returned for the session.\n */\nexport interface ChatKitWorkflow {\n  /**\n   * Identifier of the workflow backing the session.\n   */\n  id: string;\n\n  /**\n   * State variable key-value pairs applied when invoking the workflow. Defaults to\n   * null when no overrides were provided.\n   */\n  state_variables: { [key: string]: string | boolean | number } | null;\n\n  /**\n   * Tracing settings applied to the workflow.\n   */\n  tracing: ChatKitWorkflow.Tracing;\n\n  /**\n   * Specific workflow version used for the session. Defaults to null when using the\n   * latest deployment.\n   */\n  version: string | null;\n}\n\nexport namespace ChatKitWorkflow {\n  /**\n   * Tracing settings applied to the workflow.\n   */\n  export interface Tracing {\n    /**\n     * Indicates whether tracing is enabled.\n     */\n    enabled: boolean;\n  }\n}\n\nChatKit.Sessions = Sessions;\nChatKit.Threads = Threads;\n\nexport declare namespace ChatKit {\n  export { type ChatKitWorkflow as ChatKitWorkflow };\n\n  export { Sessions as Sessions, type SessionCreateParams as SessionCreateParams };\n\n  export {\n    Threads as Threads,\n    type ChatSession as ChatSession,\n    type ChatSessionAutomaticThreadTitling as ChatSessionAutomaticThreadTitling,\n    type ChatSessionChatKitConfiguration as ChatSessionChatKitConfiguration,\n    type ChatSessionChatKitConfigurationParam as ChatSessionChatKitConfigurationParam,\n    type ChatSessionExpiresAfterParam as ChatSessionExpiresAfterParam,\n    type ChatSessionFileUpload as ChatSessionFileUpload,\n    type ChatSessionHistory as ChatSessionHistory,\n    type ChatSessionRateLimits as ChatSessionRateLimits,\n    type ChatSessionRateLimitsParam as ChatSessionRateLimitsParam,\n    type ChatSessionStatus as ChatSessionStatus,\n    type ChatSessionWorkflowParam as ChatSessionWorkflowParam,\n    type ChatKitAttachment as ChatKitAttachment,\n    type ChatKitResponseOutputText as ChatKitResponseOutputText,\n    type ChatKitThread as ChatKitThread,\n    type ChatKitThreadAssistantMessageItem as ChatKitThreadAssistantMessageItem,\n    type ChatKitThreadItemList as ChatKitThreadItemList,\n    type ChatKitThreadUserMessageItem as ChatKitThreadUserMessageItem,\n    type ChatKitWidgetItem as ChatKitWidgetItem,\n    type ThreadDeleteResponse as ThreadDeleteResponse,\n    type ChatKitThreadsPage as ChatKitThreadsPage,\n    type ChatKitThreadItemListDataPage as ChatKitThreadItemListDataPage,\n    type ThreadListParams as ThreadListParams,\n    type ThreadListItemsParams as ThreadListItemsParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as Shared from '../../shared';\nimport * as AssistantsAPI from '../assistants';\nimport { APIPromise } from '../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\n/**\n * @deprecated The Assistants API is deprecated in favor of the Responses API\n */\nexport class Messages extends APIResource {\n  /**\n   * Create a message.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  create(threadID: string, body: MessageCreateParams, options?: RequestOptions): APIPromise<Message> {\n    return this._client.post(path`/threads/${threadID}/messages`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Retrieve a message.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  retrieve(messageID: string, params: MessageRetrieveParams, options?: RequestOptions): APIPromise<Message> {\n    const { thread_id } = params;\n    return this._client.get(path`/threads/${thread_id}/messages/${messageID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Modifies a message.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  update(messageID: string, params: MessageUpdateParams, options?: RequestOptions): APIPromise<Message> {\n    const { thread_id, ...body } = params;\n    return this._client.post(path`/threads/${thread_id}/messages/${messageID}`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Returns a list of messages for a given thread.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  list(\n    threadID: string,\n    query: MessageListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<MessagesPage, Message> {\n    return this._client.getAPIList(path`/threads/${threadID}/messages`, CursorPage<Message>, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Deletes a message.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  delete(\n    messageID: string,\n    params: MessageDeleteParams,\n    options?: RequestOptions,\n  ): APIPromise<MessageDeleted> {\n    const { thread_id } = params;\n    return this._client.delete(path`/threads/${thread_id}/messages/${messageID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n}\n\nexport type MessagesPage = CursorPage<Message>;\n\n/**\n * A citation within the message that points to a specific quote from a specific\n * File associated with the assistant or the message. Generated when the assistant\n * uses the \"file_search\" tool to search files.\n */\nexport type Annotation = FileCitationAnnotation | FilePathAnnotation;\n\n/**\n * A citation within the message that points to a specific quote from a specific\n * File associated with the assistant or the message. Generated when the assistant\n * uses the \"file_search\" tool to search files.\n */\nexport type AnnotationDelta = FileCitationDeltaAnnotation | FilePathDeltaAnnotation;\n\n/**\n * A citation within the message that points to a specific quote from a specific\n * File associated with the assistant or the message. Generated when the assistant\n * uses the \"file_search\" tool to search files.\n */\nexport interface FileCitationAnnotation {\n  end_index: number;\n\n  file_citation: FileCitationAnnotation.FileCitation;\n\n  start_index: number;\n\n  /**\n   * The text in the message content that needs to be replaced.\n   */\n  text: string;\n\n  /**\n   * Always `file_citation`.\n   */\n  type: 'file_citation';\n}\n\nexport namespace FileCitationAnnotation {\n  export interface FileCitation {\n    /**\n     * The ID of the specific File the citation is from.\n     */\n    file_id: string;\n  }\n}\n\n/**\n * A citation within the message that points to a specific quote from a specific\n * File associated with the assistant or the message. Generated when the assistant\n * uses the \"file_search\" tool to search files.\n */\nexport interface FileCitationDeltaAnnotation {\n  /**\n   * The index of the annotation in the text content part.\n   */\n  index: number;\n\n  /**\n   * Always `file_citation`.\n   */\n  type: 'file_citation';\n\n  end_index?: number;\n\n  file_citation?: FileCitationDeltaAnnotation.FileCitation;\n\n  start_index?: number;\n\n  /**\n   * The text in the message content that needs to be replaced.\n   */\n  text?: string;\n}\n\nexport namespace FileCitationDeltaAnnotation {\n  export interface FileCitation {\n    /**\n     * The ID of the specific File the citation is from.\n     */\n    file_id?: string;\n\n    /**\n     * The specific quote in the file.\n     */\n    quote?: string;\n  }\n}\n\n/**\n * A URL for the file that's generated when the assistant used the\n * `code_interpreter` tool to generate a file.\n */\nexport interface FilePathAnnotation {\n  end_index: number;\n\n  file_path: FilePathAnnotation.FilePath;\n\n  start_index: number;\n\n  /**\n   * The text in the message content that needs to be replaced.\n   */\n  text: string;\n\n  /**\n   * Always `file_path`.\n   */\n  type: 'file_path';\n}\n\nexport namespace FilePathAnnotation {\n  export interface FilePath {\n    /**\n     * The ID of the file that was generated.\n     */\n    file_id: string;\n  }\n}\n\n/**\n * A URL for the file that's generated when the assistant used the\n * `code_interpreter` tool to generate a file.\n */\nexport interface FilePathDeltaAnnotation {\n  /**\n   * The index of the annotation in the text content part.\n   */\n  index: number;\n\n  /**\n   * Always `file_path`.\n   */\n  type: 'file_path';\n\n  end_index?: number;\n\n  file_path?: FilePathDeltaAnnotation.FilePath;\n\n  start_index?: number;\n\n  /**\n   * The text in the message content that needs to be replaced.\n   */\n  text?: string;\n}\n\nexport namespace FilePathDeltaAnnotation {\n  export interface FilePath {\n    /**\n     * The ID of the file that was generated.\n     */\n    file_id?: string;\n  }\n}\n\nexport interface ImageFile {\n  /**\n   * The [File](https://platform.openai.com/docs/api-reference/files) ID of the image\n   * in the message content. Set `purpose=\"vision\"` when uploading the File if you\n   * need to later display the file content.\n   */\n  file_id: string;\n\n  /**\n   * Specifies the detail level of the image if specified by the user. `low` uses\n   * fewer tokens, you can opt in to high resolution using `high`.\n   */\n  detail?: 'auto' | 'low' | 'high';\n}\n\n/**\n * References an image [File](https://platform.openai.com/docs/api-reference/files)\n * in the content of a message.\n */\nexport interface ImageFileContentBlock {\n  image_file: ImageFile;\n\n  /**\n   * Always `image_file`.\n   */\n  type: 'image_file';\n}\n\nexport interface ImageFileDelta {\n  /**\n   * Specifies the detail level of the image if specified by the user. `low` uses\n   * fewer tokens, you can opt in to high resolution using `high`.\n   */\n  detail?: 'auto' | 'low' | 'high';\n\n  /**\n   * The [File](https://platform.openai.com/docs/api-reference/files) ID of the image\n   * in the message content. Set `purpose=\"vision\"` when uploading the File if you\n   * need to later display the file content.\n   */\n  file_id?: string;\n}\n\n/**\n * References an image [File](https://platform.openai.com/docs/api-reference/files)\n * in the content of a message.\n */\nexport interface ImageFileDeltaBlock {\n  /**\n   * The index of the content part in the message.\n   */\n  index: number;\n\n  /**\n   * Always `image_file`.\n   */\n  type: 'image_file';\n\n  image_file?: ImageFileDelta;\n}\n\nexport interface ImageURL {\n  /**\n   * The external URL of the image, must be a supported image types: jpeg, jpg, png,\n   * gif, webp.\n   */\n  url: string;\n\n  /**\n   * Specifies the detail level of the image. `low` uses fewer tokens, you can opt in\n   * to high resolution using `high`. Default value is `auto`\n   */\n  detail?: 'auto' | 'low' | 'high';\n}\n\n/**\n * References an image URL in the content of a message.\n */\nexport interface ImageURLContentBlock {\n  image_url: ImageURL;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'image_url';\n}\n\nexport interface ImageURLDelta {\n  /**\n   * Specifies the detail level of the image. `low` uses fewer tokens, you can opt in\n   * to high resolution using `high`.\n   */\n  detail?: 'auto' | 'low' | 'high';\n\n  /**\n   * The URL of the image, must be a supported image types: jpeg, jpg, png, gif,\n   * webp.\n   */\n  url?: string;\n}\n\n/**\n * References an image URL in the content of a message.\n */\nexport interface ImageURLDeltaBlock {\n  /**\n   * The index of the content part in the message.\n   */\n  index: number;\n\n  /**\n   * Always `image_url`.\n   */\n  type: 'image_url';\n\n  image_url?: ImageURLDelta;\n}\n\n/**\n * Represents a message within a\n * [thread](https://platform.openai.com/docs/api-reference/threads).\n */\nexport interface Message {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * If applicable, the ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) that\n   * authored this message.\n   */\n  assistant_id: string | null;\n\n  /**\n   * A list of files attached to the message, and the tools they were added to.\n   */\n  attachments: Array<Message.Attachment> | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the message was completed.\n   */\n  completed_at: number | null;\n\n  /**\n   * The content of the message in array of text and/or images.\n   */\n  content: Array<MessageContent>;\n\n  /**\n   * The Unix timestamp (in seconds) for when the message was created.\n   */\n  created_at: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the message was marked as incomplete.\n   */\n  incomplete_at: number | null;\n\n  /**\n   * On an incomplete message, details about why the message is incomplete.\n   */\n  incomplete_details: Message.IncompleteDetails | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The object type, which is always `thread.message`.\n   */\n  object: 'thread.message';\n\n  /**\n   * The entity that produced the message. One of `user` or `assistant`.\n   */\n  role: 'user' | 'assistant';\n\n  /**\n   * The ID of the [run](https://platform.openai.com/docs/api-reference/runs)\n   * associated with the creation of this message. Value is `null` when messages are\n   * created manually using the create message or create thread endpoints.\n   */\n  run_id: string | null;\n\n  /**\n   * The status of the message, which can be either `in_progress`, `incomplete`, or\n   * `completed`.\n   */\n  status: 'in_progress' | 'incomplete' | 'completed';\n\n  /**\n   * The [thread](https://platform.openai.com/docs/api-reference/threads) ID that\n   * this message belongs to.\n   */\n  thread_id: string;\n}\n\nexport namespace Message {\n  export interface Attachment {\n    /**\n     * The ID of the file to attach to the message.\n     */\n    file_id?: string;\n\n    /**\n     * The tools to add this file to.\n     */\n    tools?: Array<AssistantsAPI.CodeInterpreterTool | Attachment.AssistantToolsFileSearchTypeOnly>;\n  }\n\n  export namespace Attachment {\n    export interface AssistantToolsFileSearchTypeOnly {\n      /**\n       * The type of tool being defined: `file_search`\n       */\n      type: 'file_search';\n    }\n  }\n\n  /**\n   * On an incomplete message, details about why the message is incomplete.\n   */\n  export interface IncompleteDetails {\n    /**\n     * The reason the message is incomplete.\n     */\n    reason: 'content_filter' | 'max_tokens' | 'run_cancelled' | 'run_expired' | 'run_failed';\n  }\n}\n\n/**\n * References an image [File](https://platform.openai.com/docs/api-reference/files)\n * in the content of a message.\n */\nexport type MessageContent =\n  | ImageFileContentBlock\n  | ImageURLContentBlock\n  | TextContentBlock\n  | RefusalContentBlock;\n\n/**\n * References an image [File](https://platform.openai.com/docs/api-reference/files)\n * in the content of a message.\n */\nexport type MessageContentDelta =\n  | ImageFileDeltaBlock\n  | TextDeltaBlock\n  | RefusalDeltaBlock\n  | ImageURLDeltaBlock;\n\n/**\n * References an image [File](https://platform.openai.com/docs/api-reference/files)\n * in the content of a message.\n */\nexport type MessageContentPartParam = ImageFileContentBlock | ImageURLContentBlock | TextContentBlockParam;\n\nexport interface MessageDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'thread.message.deleted';\n}\n\n/**\n * The delta containing the fields that have changed on the Message.\n */\nexport interface MessageDelta {\n  /**\n   * The content of the message in array of text and/or images.\n   */\n  content?: Array<MessageContentDelta>;\n\n  /**\n   * The entity that produced the message. One of `user` or `assistant`.\n   */\n  role?: 'user' | 'assistant';\n}\n\n/**\n * Represents a message delta i.e. any changed fields on a message during\n * streaming.\n */\nexport interface MessageDeltaEvent {\n  /**\n   * The identifier of the message, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The delta containing the fields that have changed on the Message.\n   */\n  delta: MessageDelta;\n\n  /**\n   * The object type, which is always `thread.message.delta`.\n   */\n  object: 'thread.message.delta';\n}\n\n/**\n * The refusal content generated by the assistant.\n */\nexport interface RefusalContentBlock {\n  refusal: string;\n\n  /**\n   * Always `refusal`.\n   */\n  type: 'refusal';\n}\n\n/**\n * The refusal content that is part of a message.\n */\nexport interface RefusalDeltaBlock {\n  /**\n   * The index of the refusal part in the message.\n   */\n  index: number;\n\n  /**\n   * Always `refusal`.\n   */\n  type: 'refusal';\n\n  refusal?: string;\n}\n\nexport interface Text {\n  annotations: Array<Annotation>;\n\n  /**\n   * The data that makes up the text.\n   */\n  value: string;\n}\n\n/**\n * The text content that is part of a message.\n */\nexport interface TextContentBlock {\n  text: Text;\n\n  /**\n   * Always `text`.\n   */\n  type: 'text';\n}\n\n/**\n * The text content that is part of a message.\n */\nexport interface TextContentBlockParam {\n  /**\n   * Text content to be sent to the model\n   */\n  text: string;\n\n  /**\n   * Always `text`.\n   */\n  type: 'text';\n}\n\nexport interface TextDelta {\n  annotations?: Array<AnnotationDelta>;\n\n  /**\n   * The data that makes up the text.\n   */\n  value?: string;\n}\n\n/**\n * The text content that is part of a message.\n */\nexport interface TextDeltaBlock {\n  /**\n   * The index of the content part in the message.\n   */\n  index: number;\n\n  /**\n   * Always `text`.\n   */\n  type: 'text';\n\n  text?: TextDelta;\n}\n\nexport interface MessageCreateParams {\n  /**\n   * The text contents of the message.\n   */\n  content: string | Array<MessageContentPartParam>;\n\n  /**\n   * The role of the entity that is creating the message. Allowed values include:\n   *\n   * - `user`: Indicates the message is sent by an actual user and should be used in\n   *   most cases to represent user-generated messages.\n   * - `assistant`: Indicates the message is generated by the assistant. Use this\n   *   value to insert messages from the assistant into the conversation.\n   */\n  role: 'user' | 'assistant';\n\n  /**\n   * A list of files attached to the message, and the tools they should be added to.\n   */\n  attachments?: Array<MessageCreateParams.Attachment> | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n}\n\nexport namespace MessageCreateParams {\n  export interface Attachment {\n    /**\n     * The ID of the file to attach to the message.\n     */\n    file_id?: string;\n\n    /**\n     * The tools to add this file to.\n     */\n    tools?: Array<AssistantsAPI.CodeInterpreterTool | Attachment.FileSearch>;\n  }\n\n  export namespace Attachment {\n    export interface FileSearch {\n      /**\n       * The type of tool being defined: `file_search`\n       */\n      type: 'file_search';\n    }\n  }\n}\n\nexport interface MessageRetrieveParams {\n  /**\n   * The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)\n   * to which this message belongs.\n   */\n  thread_id: string;\n}\n\nexport interface MessageUpdateParams {\n  /**\n   * Path param: The ID of the thread to which this message belongs.\n   */\n  thread_id: string;\n\n  /**\n   * Body param: Set of 16 key-value pairs that can be attached to an object. This\n   * can be useful for storing additional information about the object in a\n   * structured format, and querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n}\n\nexport interface MessageListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * starting with obj_foo, your subsequent call can include before=obj_foo in order\n   * to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n\n  /**\n   * Filter messages by the run ID that generated them.\n   */\n  run_id?: string;\n}\n\nexport interface MessageDeleteParams {\n  /**\n   * The ID of the thread to which this message belongs.\n   */\n  thread_id: string;\n}\n\nexport declare namespace Messages {\n  export {\n    type Annotation as Annotation,\n    type AnnotationDelta as AnnotationDelta,\n    type FileCitationAnnotation as FileCitationAnnotation,\n    type FileCitationDeltaAnnotation as FileCitationDeltaAnnotation,\n    type FilePathAnnotation as FilePathAnnotation,\n    type FilePathDeltaAnnotation as FilePathDeltaAnnotation,\n    type ImageFile as ImageFile,\n    type ImageFileContentBlock as ImageFileContentBlock,\n    type ImageFileDelta as ImageFileDelta,\n    type ImageFileDeltaBlock as ImageFileDeltaBlock,\n    type ImageURL as ImageURL,\n    type ImageURLContentBlock as ImageURLContentBlock,\n    type ImageURLDelta as ImageURLDelta,\n    type ImageURLDeltaBlock as ImageURLDeltaBlock,\n    type Message as Message,\n    type MessageContent as MessageContent,\n    type MessageContentDelta as MessageContentDelta,\n    type MessageContentPartParam as MessageContentPartParam,\n    type MessageDeleted as MessageDeleted,\n    type MessageDelta as MessageDelta,\n    type MessageDeltaEvent as MessageDeltaEvent,\n    type RefusalContentBlock as RefusalContentBlock,\n    type RefusalDeltaBlock as RefusalDeltaBlock,\n    type Text as Text,\n    type TextContentBlock as TextContentBlock,\n    type TextContentBlockParam as TextContentBlockParam,\n    type TextDelta as TextDelta,\n    type TextDeltaBlock as TextDeltaBlock,\n    type MessagesPage as MessagesPage,\n    type MessageCreateParams as MessageCreateParams,\n    type MessageRetrieveParams as MessageRetrieveParams,\n    type MessageUpdateParams as MessageUpdateParams,\n    type MessageListParams as MessageListParams,\n    type MessageDeleteParams as MessageDeleteParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../../core/resource';\nimport * as StepsAPI from './steps';\nimport * as Shared from '../../../shared';\nimport { APIPromise } from '../../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../../core/pagination';\nimport { buildHeaders } from '../../../../internal/headers';\nimport { RequestOptions } from '../../../../internal/request-options';\nimport { path } from '../../../../internal/utils/path';\n\n/**\n * @deprecated The Assistants API is deprecated in favor of the Responses API\n */\nexport class Steps extends APIResource {\n  /**\n   * Retrieves a run step.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  retrieve(stepID: string, params: StepRetrieveParams, options?: RequestOptions): APIPromise<RunStep> {\n    const { thread_id, run_id, ...query } = params;\n    return this._client.get(path`/threads/${thread_id}/runs/${run_id}/steps/${stepID}`, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Returns a list of run steps belonging to a run.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  list(runID: string, params: StepListParams, options?: RequestOptions): PagePromise<RunStepsPage, RunStep> {\n    const { thread_id, ...query } = params;\n    return this._client.getAPIList(path`/threads/${thread_id}/runs/${runID}/steps`, CursorPage<RunStep>, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n}\n\nexport type RunStepsPage = CursorPage<RunStep>;\n\n/**\n * Text output from the Code Interpreter tool call as part of a run step.\n */\nexport interface CodeInterpreterLogs {\n  /**\n   * The index of the output in the outputs array.\n   */\n  index: number;\n\n  /**\n   * Always `logs`.\n   */\n  type: 'logs';\n\n  /**\n   * The text output from the Code Interpreter tool call.\n   */\n  logs?: string;\n}\n\nexport interface CodeInterpreterOutputImage {\n  /**\n   * The index of the output in the outputs array.\n   */\n  index: number;\n\n  /**\n   * Always `image`.\n   */\n  type: 'image';\n\n  image?: CodeInterpreterOutputImage.Image;\n}\n\nexport namespace CodeInterpreterOutputImage {\n  export interface Image {\n    /**\n     * The [file](https://platform.openai.com/docs/api-reference/files) ID of the\n     * image.\n     */\n    file_id?: string;\n  }\n}\n\n/**\n * Details of the Code Interpreter tool call the run step was involved in.\n */\nexport interface CodeInterpreterToolCall {\n  /**\n   * The ID of the tool call.\n   */\n  id: string;\n\n  /**\n   * The Code Interpreter tool call definition.\n   */\n  code_interpreter: CodeInterpreterToolCall.CodeInterpreter;\n\n  /**\n   * The type of tool call. This is always going to be `code_interpreter` for this\n   * type of tool call.\n   */\n  type: 'code_interpreter';\n}\n\nexport namespace CodeInterpreterToolCall {\n  /**\n   * The Code Interpreter tool call definition.\n   */\n  export interface CodeInterpreter {\n    /**\n     * The input to the Code Interpreter tool call.\n     */\n    input: string;\n\n    /**\n     * The outputs from the Code Interpreter tool call. Code Interpreter can output one\n     * or more items, including text (`logs`) or images (`image`). Each of these are\n     * represented by a different object type.\n     */\n    outputs: Array<CodeInterpreter.Logs | CodeInterpreter.Image>;\n  }\n\n  export namespace CodeInterpreter {\n    /**\n     * Text output from the Code Interpreter tool call as part of a run step.\n     */\n    export interface Logs {\n      /**\n       * The text output from the Code Interpreter tool call.\n       */\n      logs: string;\n\n      /**\n       * Always `logs`.\n       */\n      type: 'logs';\n    }\n\n    export interface Image {\n      image: Image.Image;\n\n      /**\n       * Always `image`.\n       */\n      type: 'image';\n    }\n\n    export namespace Image {\n      export interface Image {\n        /**\n         * The [file](https://platform.openai.com/docs/api-reference/files) ID of the\n         * image.\n         */\n        file_id: string;\n      }\n    }\n  }\n}\n\n/**\n * Details of the Code Interpreter tool call the run step was involved in.\n */\nexport interface CodeInterpreterToolCallDelta {\n  /**\n   * The index of the tool call in the tool calls array.\n   */\n  index: number;\n\n  /**\n   * The type of tool call. This is always going to be `code_interpreter` for this\n   * type of tool call.\n   */\n  type: 'code_interpreter';\n\n  /**\n   * The ID of the tool call.\n   */\n  id?: string;\n\n  /**\n   * The Code Interpreter tool call definition.\n   */\n  code_interpreter?: CodeInterpreterToolCallDelta.CodeInterpreter;\n}\n\nexport namespace CodeInterpreterToolCallDelta {\n  /**\n   * The Code Interpreter tool call definition.\n   */\n  export interface CodeInterpreter {\n    /**\n     * The input to the Code Interpreter tool call.\n     */\n    input?: string;\n\n    /**\n     * The outputs from the Code Interpreter tool call. Code Interpreter can output one\n     * or more items, including text (`logs`) or images (`image`). Each of these are\n     * represented by a different object type.\n     */\n    outputs?: Array<StepsAPI.CodeInterpreterLogs | StepsAPI.CodeInterpreterOutputImage>;\n  }\n}\n\nexport interface FileSearchToolCall {\n  /**\n   * The ID of the tool call object.\n   */\n  id: string;\n\n  /**\n   * For now, this is always going to be an empty object.\n   */\n  file_search: FileSearchToolCall.FileSearch;\n\n  /**\n   * The type of tool call. This is always going to be `file_search` for this type of\n   * tool call.\n   */\n  type: 'file_search';\n}\n\nexport namespace FileSearchToolCall {\n  /**\n   * For now, this is always going to be an empty object.\n   */\n  export interface FileSearch {\n    /**\n     * The ranking options for the file search.\n     */\n    ranking_options?: FileSearch.RankingOptions;\n\n    /**\n     * The results of the file search.\n     */\n    results?: Array<FileSearch.Result>;\n  }\n\n  export namespace FileSearch {\n    /**\n     * The ranking options for the file search.\n     */\n    export interface RankingOptions {\n      /**\n       * The ranker to use for the file search. If not specified will use the `auto`\n       * ranker.\n       */\n      ranker: 'auto' | 'default_2024_08_21';\n\n      /**\n       * The score threshold for the file search. All values must be a floating point\n       * number between 0 and 1.\n       */\n      score_threshold: number;\n    }\n\n    /**\n     * A result instance of the file search.\n     */\n    export interface Result {\n      /**\n       * The ID of the file that result was found in.\n       */\n      file_id: string;\n\n      /**\n       * The name of the file that result was found in.\n       */\n      file_name: string;\n\n      /**\n       * The score of the result. All values must be a floating point number between 0\n       * and 1.\n       */\n      score: number;\n\n      /**\n       * The content of the result that was found. The content is only included if\n       * requested via the include query parameter.\n       */\n      content?: Array<Result.Content>;\n    }\n\n    export namespace Result {\n      export interface Content {\n        /**\n         * The text content of the file.\n         */\n        text?: string;\n\n        /**\n         * The type of the content.\n         */\n        type?: 'text';\n      }\n    }\n  }\n}\n\nexport interface FileSearchToolCallDelta {\n  /**\n   * For now, this is always going to be an empty object.\n   */\n  file_search: unknown;\n\n  /**\n   * The index of the tool call in the tool calls array.\n   */\n  index: number;\n\n  /**\n   * The type of tool call. This is always going to be `file_search` for this type of\n   * tool call.\n   */\n  type: 'file_search';\n\n  /**\n   * The ID of the tool call object.\n   */\n  id?: string;\n}\n\nexport interface FunctionToolCall {\n  /**\n   * The ID of the tool call object.\n   */\n  id: string;\n\n  /**\n   * The definition of the function that was called.\n   */\n  function: FunctionToolCall.Function;\n\n  /**\n   * The type of tool call. This is always going to be `function` for this type of\n   * tool call.\n   */\n  type: 'function';\n}\n\nexport namespace FunctionToolCall {\n  /**\n   * The definition of the function that was called.\n   */\n  export interface Function {\n    /**\n     * The arguments passed to the function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function.\n     */\n    name: string;\n\n    /**\n     * The output of the function. This will be `null` if the outputs have not been\n     * [submitted](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)\n     * yet.\n     */\n    output: string | null;\n  }\n}\n\nexport interface FunctionToolCallDelta {\n  /**\n   * The index of the tool call in the tool calls array.\n   */\n  index: number;\n\n  /**\n   * The type of tool call. This is always going to be `function` for this type of\n   * tool call.\n   */\n  type: 'function';\n\n  /**\n   * The ID of the tool call object.\n   */\n  id?: string;\n\n  /**\n   * The definition of the function that was called.\n   */\n  function?: FunctionToolCallDelta.Function;\n}\n\nexport namespace FunctionToolCallDelta {\n  /**\n   * The definition of the function that was called.\n   */\n  export interface Function {\n    /**\n     * The arguments passed to the function.\n     */\n    arguments?: string;\n\n    /**\n     * The name of the function.\n     */\n    name?: string;\n\n    /**\n     * The output of the function. This will be `null` if the outputs have not been\n     * [submitted](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)\n     * yet.\n     */\n    output?: string | null;\n  }\n}\n\n/**\n * Details of the message creation by the run step.\n */\nexport interface MessageCreationStepDetails {\n  message_creation: MessageCreationStepDetails.MessageCreation;\n\n  /**\n   * Always `message_creation`.\n   */\n  type: 'message_creation';\n}\n\nexport namespace MessageCreationStepDetails {\n  export interface MessageCreation {\n    /**\n     * The ID of the message that was created by this run step.\n     */\n    message_id: string;\n  }\n}\n\n/**\n * Represents a step in execution of a run.\n */\nexport interface RunStep {\n  /**\n   * The identifier of the run step, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants)\n   * associated with the run step.\n   */\n  assistant_id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step was cancelled.\n   */\n  cancelled_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step completed.\n   */\n  completed_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step was created.\n   */\n  created_at: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step expired. A step is\n   * considered expired if the parent run is expired.\n   */\n  expired_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step failed.\n   */\n  failed_at: number | null;\n\n  /**\n   * The last error associated with this run step. Will be `null` if there are no\n   * errors.\n   */\n  last_error: RunStep.LastError | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The object type, which is always `thread.run.step`.\n   */\n  object: 'thread.run.step';\n\n  /**\n   * The ID of the [run](https://platform.openai.com/docs/api-reference/runs) that\n   * this run step is a part of.\n   */\n  run_id: string;\n\n  /**\n   * The status of the run step, which can be either `in_progress`, `cancelled`,\n   * `failed`, `completed`, or `expired`.\n   */\n  status: 'in_progress' | 'cancelled' | 'failed' | 'completed' | 'expired';\n\n  /**\n   * The details of the run step.\n   */\n  step_details: MessageCreationStepDetails | ToolCallsStepDetails;\n\n  /**\n   * The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)\n   * that was run.\n   */\n  thread_id: string;\n\n  /**\n   * The type of run step, which can be either `message_creation` or `tool_calls`.\n   */\n  type: 'message_creation' | 'tool_calls';\n\n  /**\n   * Usage statistics related to the run step. This value will be `null` while the\n   * run step's status is `in_progress`.\n   */\n  usage: RunStep.Usage | null;\n}\n\nexport namespace RunStep {\n  /**\n   * The last error associated with this run step. Will be `null` if there are no\n   * errors.\n   */\n  export interface LastError {\n    /**\n     * One of `server_error` or `rate_limit_exceeded`.\n     */\n    code: 'server_error' | 'rate_limit_exceeded';\n\n    /**\n     * A human-readable description of the error.\n     */\n    message: string;\n  }\n\n  /**\n   * Usage statistics related to the run step. This value will be `null` while the\n   * run step's status is `in_progress`.\n   */\n  export interface Usage {\n    /**\n     * Number of completion tokens used over the course of the run step.\n     */\n    completion_tokens: number;\n\n    /**\n     * Number of prompt tokens used over the course of the run step.\n     */\n    prompt_tokens: number;\n\n    /**\n     * Total number of tokens used (prompt + completion).\n     */\n    total_tokens: number;\n  }\n}\n\n/**\n * The delta containing the fields that have changed on the run step.\n */\nexport interface RunStepDelta {\n  /**\n   * The details of the run step.\n   */\n  step_details?: RunStepDeltaMessageDelta | ToolCallDeltaObject;\n}\n\n/**\n * Represents a run step delta i.e. any changed fields on a run step during\n * streaming.\n */\nexport interface RunStepDeltaEvent {\n  /**\n   * The identifier of the run step, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The delta containing the fields that have changed on the run step.\n   */\n  delta: RunStepDelta;\n\n  /**\n   * The object type, which is always `thread.run.step.delta`.\n   */\n  object: 'thread.run.step.delta';\n}\n\n/**\n * Details of the message creation by the run step.\n */\nexport interface RunStepDeltaMessageDelta {\n  /**\n   * Always `message_creation`.\n   */\n  type: 'message_creation';\n\n  message_creation?: RunStepDeltaMessageDelta.MessageCreation;\n}\n\nexport namespace RunStepDeltaMessageDelta {\n  export interface MessageCreation {\n    /**\n     * The ID of the message that was created by this run step.\n     */\n    message_id?: string;\n  }\n}\n\nexport type RunStepInclude = 'step_details.tool_calls[*].file_search.results[*].content';\n\n/**\n * Details of the Code Interpreter tool call the run step was involved in.\n */\nexport type ToolCall = CodeInterpreterToolCall | FileSearchToolCall | FunctionToolCall;\n\n/**\n * Details of the Code Interpreter tool call the run step was involved in.\n */\nexport type ToolCallDelta = CodeInterpreterToolCallDelta | FileSearchToolCallDelta | FunctionToolCallDelta;\n\n/**\n * Details of the tool call.\n */\nexport interface ToolCallDeltaObject {\n  /**\n   * Always `tool_calls`.\n   */\n  type: 'tool_calls';\n\n  /**\n   * An array of tool calls the run step was involved in. These can be associated\n   * with one of three types of tools: `code_interpreter`, `file_search`, or\n   * `function`.\n   */\n  tool_calls?: Array<ToolCallDelta>;\n}\n\n/**\n * Details of the tool call.\n */\nexport interface ToolCallsStepDetails {\n  /**\n   * An array of tool calls the run step was involved in. These can be associated\n   * with one of three types of tools: `code_interpreter`, `file_search`, or\n   * `function`.\n   */\n  tool_calls: Array<ToolCall>;\n\n  /**\n   * Always `tool_calls`.\n   */\n  type: 'tool_calls';\n}\n\nexport interface StepRetrieveParams {\n  /**\n   * Path param: The ID of the thread to which the run and run step belongs.\n   */\n  thread_id: string;\n\n  /**\n   * Path param: The ID of the run to which the run step belongs.\n   */\n  run_id: string;\n\n  /**\n   * Query param: A list of additional fields to include in the response. Currently\n   * the only supported value is\n   * `step_details.tool_calls[*].file_search.results[*].content` to fetch the file\n   * search result content.\n   *\n   * See the\n   * [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)\n   * for more information.\n   */\n  include?: Array<RunStepInclude>;\n}\n\nexport interface StepListParams extends CursorPageParams {\n  /**\n   * Path param: The ID of the thread the run and run steps belong to.\n   */\n  thread_id: string;\n\n  /**\n   * Query param: A cursor for use in pagination. `before` is an object ID that\n   * defines your place in the list. For instance, if you make a list request and\n   * receive 100 objects, starting with obj_foo, your subsequent call can include\n   * before=obj_foo in order to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Query param: A list of additional fields to include in the response. Currently\n   * the only supported value is\n   * `step_details.tool_calls[*].file_search.results[*].content` to fetch the file\n   * search result content.\n   *\n   * See the\n   * [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)\n   * for more information.\n   */\n  include?: Array<RunStepInclude>;\n\n  /**\n   * Query param: Sort order by the `created_at` timestamp of the objects. `asc` for\n   * ascending order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport declare namespace Steps {\n  export {\n    type CodeInterpreterLogs as CodeInterpreterLogs,\n    type CodeInterpreterOutputImage as CodeInterpreterOutputImage,\n    type CodeInterpreterToolCall as CodeInterpreterToolCall,\n    type CodeInterpreterToolCallDelta as CodeInterpreterToolCallDelta,\n    type FileSearchToolCall as FileSearchToolCall,\n    type FileSearchToolCallDelta as FileSearchToolCallDelta,\n    type FunctionToolCall as FunctionToolCall,\n    type FunctionToolCallDelta as FunctionToolCallDelta,\n    type MessageCreationStepDetails as MessageCreationStepDetails,\n    type RunStep as RunStep,\n    type RunStepDelta as RunStepDelta,\n    type RunStepDeltaEvent as RunStepDeltaEvent,\n    type RunStepDeltaMessageDelta as RunStepDeltaMessageDelta,\n    type RunStepInclude as RunStepInclude,\n    type ToolCall as ToolCall,\n    type ToolCallDelta as ToolCallDelta,\n    type ToolCallDeltaObject as ToolCallDeltaObject,\n    type ToolCallsStepDetails as ToolCallsStepDetails,\n    type RunStepsPage as RunStepsPage,\n    type StepRetrieveParams as StepRetrieveParams,\n    type StepListParams as StepListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { OpenAIError } from '../../core/error';\nimport { encodeUTF8 } from './bytes';\n\nexport const toBase64 = (data: string | Uint8Array | null | undefined): string => {\n  if (!data) return '';\n\n  if (typeof (globalThis as any).Buffer !== 'undefined') {\n    return (globalThis as any).Buffer.from(data).toString('base64');\n  }\n\n  if (typeof data === 'string') {\n    data = encodeUTF8(data);\n  }\n\n  if (typeof btoa !== 'undefined') {\n    return btoa(String.fromCharCode.apply(null, data as any));\n  }\n\n  throw new OpenAIError('Cannot generate base64 string; Expected `Buffer` or `btoa` to be defined');\n};\n\nexport const fromBase64 = (str: string): Uint8Array => {\n  if (typeof (globalThis as any).Buffer !== 'undefined') {\n    const buf = (globalThis as any).Buffer.from(str, 'base64');\n    return new Uint8Array(buf.buffer, buf.byteOffset, buf.byteLength);\n  }\n\n  if (typeof atob !== 'undefined') {\n    const bstr = atob(str);\n    const buf = new Uint8Array(bstr.length);\n    for (let i = 0; i < bstr.length; i++) {\n      buf[i] = bstr.charCodeAt(i);\n    }\n    return buf;\n  }\n\n  throw new OpenAIError('Cannot decode base64 string; Expected `Buffer` or `atob` to be defined');\n};\n\n/**\n * Converts a Base64 encoded string to a Float32Array.\n * @param base64Str - The Base64 encoded string.\n * @returns An Array of numbers interpreted as Float32 values.\n */\nexport const toFloat32Array = (base64Str: string): Array<number> => {\n  if (typeof Buffer !== 'undefined') {\n    // for Node.js environment\n    const buf = Buffer.from(base64Str, 'base64');\n    return Array.from(\n      new Float32Array(buf.buffer, buf.byteOffset, buf.length / Float32Array.BYTES_PER_ELEMENT),\n    );\n  } else {\n    // for legacy web platform APIs\n    const binaryStr = atob(base64Str);\n    const len = binaryStr.length;\n    const bytes = new Uint8Array(len);\n    for (let i = 0; i < len; i++) {\n      bytes[i] = binaryStr.charCodeAt(i);\n    }\n    return Array.from(new Float32Array(bytes.buffer));\n  }\n};\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n/**\n * Read an environment variable.\n *\n * Trims beginning and trailing whitespace.\n *\n * Will return undefined if the environment variable doesn't exist or cannot be accessed.\n */\nexport const readEnv = (env: string): string | undefined => {\n  if (typeof (globalThis as any).process !== 'undefined') {\n    return (globalThis as any).process.env?.[env]?.trim() ?? undefined;\n  }\n  if (typeof (globalThis as any).Deno !== 'undefined') {\n    return (globalThis as any).Deno.env?.get?.(env)?.trim();\n  }\n  return undefined;\n};\n", "import {\n  TextContentBlock,\n  ImageFileContentBlock,\n  Message,\n  MessageContentDelta,\n  Text,\n  ImageFile,\n  TextDelta,\n  MessageDelta,\n  MessageContent,\n} from '../resources/beta/threads/messages';\nimport { RequestOptions } from '../internal/request-options';\nimport {\n  Run,\n  RunCreateParamsBase,\n  RunCreateParamsStreaming,\n  Runs,\n  RunSubmitToolOutputsParamsBase,\n  RunSubmitToolOutputsParamsStreaming,\n} from '../resources/beta/threads/runs/runs';\nimport { type ReadableStream } from '../internal/shim-types';\nimport { Stream } from '../streaming';\nimport { APIUserAbortError, OpenAIError } from '../error';\nimport {\n  AssistantStreamEvent,\n  MessageStreamEvent,\n  RunStepStreamEvent,\n  RunStreamEvent,\n} from '../resources/beta/assistants';\nimport { RunStep, RunStepDelta, ToolCall, ToolCallDelta } from '../resources/beta/threads/runs/steps';\nimport { ThreadCreateAndRunParamsBase, Threads } from '../resources/beta/threads/threads';\nimport { BaseEvents, EventStream } from './EventStream';\nimport { isObj } from '../internal/utils';\n\nexport interface AssistantStreamEvents extends BaseEvents {\n  run: (run: Run) => void;\n\n  //New event structure\n  messageCreated: (message: Message) => void;\n  messageDelta: (message: MessageDelta, snapshot: Message) => void;\n  messageDone: (message: Message) => void;\n\n  runStepCreated: (runStep: RunStep) => void;\n  runStepDelta: (delta: RunStepDelta, snapshot: Runs.RunStep) => void;\n  runStepDone: (runStep: Runs.RunStep, snapshot: Runs.RunStep) => void;\n\n  toolCallCreated: (toolCall: ToolCall) => void;\n  toolCallDelta: (delta: ToolCallDelta, snapshot: ToolCall) => void;\n  toolCallDone: (toolCall: ToolCall) => void;\n\n  textCreated: (content: Text) => void;\n  textDelta: (delta: TextDelta, snapshot: Text) => void;\n  textDone: (content: Text, snapshot: Message) => void;\n\n  //No created or delta as this is not streamed\n  imageFileDone: (content: ImageFile, snapshot: Message) => void;\n\n  event: (event: AssistantStreamEvent) => void;\n}\n\nexport type ThreadCreateAndRunParamsBaseStream = Omit<ThreadCreateAndRunParamsBase, 'stream'> & {\n  stream?: true;\n};\n\nexport type RunCreateParamsBaseStream = Omit<RunCreateParamsBase, 'stream'> & {\n  stream?: true;\n};\n\nexport type RunSubmitToolOutputsParamsStream = Omit<RunSubmitToolOutputsParamsBase, 'stream'> & {\n  stream?: true;\n};\n\nexport class AssistantStream\n  extends EventStream<AssistantStreamEvents>\n  implements AsyncIterable<AssistantStreamEvent>\n{\n  //Track all events in a single list for reference\n  #events: AssistantStreamEvent[] = [];\n\n  //Used to accumulate deltas\n  //We are accumulating many types so the value here is not strict\n  #runStepSnapshots: { [id: string]: Runs.RunStep } = {};\n  #messageSnapshots: { [id: string]: Message } = {};\n  #messageSnapshot: Message | undefined;\n  #finalRun: Run | undefined;\n  #currentContentIndex: number | undefined;\n  #currentContent: MessageContent | undefined;\n  #currentToolCallIndex: number | undefined;\n  #currentToolCall: ToolCall | undefined;\n\n  //For current snapshot methods\n  #currentEvent: AssistantStreamEvent | undefined;\n  #currentRunSnapshot: Run | undefined;\n  #currentRunStepSnapshot: Runs.RunStep | undefined;\n\n  [Symbol.asyncIterator](): AsyncIterator<AssistantStreamEvent> {\n    const pushQueue: AssistantStreamEvent[] = [];\n    const readQueue: {\n      resolve: (chunk: AssistantStreamEvent | undefined) => void;\n      reject: (err: unknown) => void;\n    }[] = [];\n    let done = false;\n\n    //Catch all for passing along all events\n    this.on('event', (event) => {\n      const reader = readQueue.shift();\n      if (reader) {\n        reader.resolve(event);\n      } else {\n        pushQueue.push(event);\n      }\n    });\n\n    this.on('end', () => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.resolve(undefined);\n      }\n      readQueue.length = 0;\n    });\n\n    this.on('abort', (err) => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.reject(err);\n      }\n      readQueue.length = 0;\n    });\n\n    this.on('error', (err) => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.reject(err);\n      }\n      readQueue.length = 0;\n    });\n\n    return {\n      next: async (): Promise<IteratorResult<AssistantStreamEvent>> => {\n        if (!pushQueue.length) {\n          if (done) {\n            return { value: undefined, done: true };\n          }\n          return new Promise<AssistantStreamEvent | undefined>((resolve, reject) =>\n            readQueue.push({ resolve, reject }),\n          ).then((chunk) => (chunk ? { value: chunk, done: false } : { value: undefined, done: true }));\n        }\n        const chunk = pushQueue.shift()!;\n        return { value: chunk, done: false };\n      },\n      return: async () => {\n        this.abort();\n        return { value: undefined, done: true };\n      },\n    };\n  }\n\n  static fromReadableStream(stream: ReadableStream): AssistantStream {\n    const runner = new AssistantStream();\n    runner._run(() => runner._fromReadableStream(stream));\n    return runner;\n  }\n\n  protected async _fromReadableStream(\n    readableStream: ReadableStream,\n    options?: RequestOptions,\n  ): Promise<Run> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this._connected();\n    const stream = Stream.fromReadableStream<AssistantStreamEvent>(readableStream, this.controller);\n    for await (const event of stream) {\n      this.#addEvent(event);\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n    return this._addRun(this.#endRequest());\n  }\n\n  toReadableStream(): ReadableStream {\n    const stream = new Stream(this[Symbol.asyncIterator].bind(this), this.controller);\n    return stream.toReadableStream();\n  }\n\n  static createToolAssistantStream(\n    runId: string,\n    runs: Runs,\n    params: RunSubmitToolOutputsParamsStream,\n    options: RequestOptions | undefined,\n  ): AssistantStream {\n    const runner = new AssistantStream();\n    runner._run(() =>\n      runner._runToolAssistantStream(runId, runs, params, {\n        ...options,\n        headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' },\n      }),\n    );\n    return runner;\n  }\n\n  protected async _createToolAssistantStream(\n    run: Runs,\n    runId: string,\n    params: RunSubmitToolOutputsParamsStream,\n    options?: RequestOptions,\n  ): Promise<Run> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n\n    const body: RunSubmitToolOutputsParamsStreaming = { ...params, stream: true };\n    const stream = await run.submitToolOutputs(runId, body, {\n      ...options,\n      signal: this.controller.signal,\n    });\n\n    this._connected();\n\n    for await (const event of stream) {\n      this.#addEvent(event);\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n\n    return this._addRun(this.#endRequest());\n  }\n\n  static createThreadAssistantStream(\n    params: ThreadCreateAndRunParamsBaseStream,\n    thread: Threads,\n    options?: RequestOptions,\n  ): AssistantStream {\n    const runner = new AssistantStream();\n    runner._run(() =>\n      runner._threadAssistantStream(params, thread, {\n        ...options,\n        headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' },\n      }),\n    );\n    return runner;\n  }\n\n  static createAssistantStream(\n    threadId: string,\n    runs: Runs,\n    params: RunCreateParamsBaseStream,\n    options?: RequestOptions,\n  ): AssistantStream {\n    const runner = new AssistantStream();\n    runner._run(() =>\n      runner._runAssistantStream(threadId, runs, params, {\n        ...options,\n        headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' },\n      }),\n    );\n    return runner;\n  }\n\n  currentEvent(): AssistantStreamEvent | undefined {\n    return this.#currentEvent;\n  }\n\n  currentRun(): Run | undefined {\n    return this.#currentRunSnapshot;\n  }\n\n  currentMessageSnapshot(): Message | undefined {\n    return this.#messageSnapshot;\n  }\n\n  currentRunStepSnapshot(): Runs.RunStep | undefined {\n    return this.#currentRunStepSnapshot;\n  }\n\n  async finalRunSteps(): Promise<Runs.RunStep[]> {\n    await this.done();\n\n    return Object.values(this.#runStepSnapshots);\n  }\n\n  async finalMessages(): Promise<Message[]> {\n    await this.done();\n\n    return Object.values(this.#messageSnapshots);\n  }\n\n  async finalRun(): Promise<Run> {\n    await this.done();\n    if (!this.#finalRun) throw Error('Final run was not received.');\n\n    return this.#finalRun;\n  }\n\n  protected async _createThreadAssistantStream(\n    thread: Threads,\n    params: ThreadCreateAndRunParamsBase,\n    options?: RequestOptions,\n  ): Promise<Run> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n\n    const body: RunCreateParamsStreaming = { ...params, stream: true };\n    const stream = await thread.createAndRun(body, { ...options, signal: this.controller.signal });\n\n    this._connected();\n\n    for await (const event of stream) {\n      this.#addEvent(event);\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n\n    return this._addRun(this.#endRequest());\n  }\n\n  protected async _createAssistantStream(\n    run: Runs,\n    threadId: string,\n    params: RunCreateParamsBase,\n    options?: RequestOptions,\n  ): Promise<Run> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n\n    const body: RunCreateParamsStreaming = { ...params, stream: true };\n    const stream = await run.create(threadId, body, { ...options, signal: this.controller.signal });\n\n    this._connected();\n\n    for await (const event of stream) {\n      this.#addEvent(event);\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n\n    return this._addRun(this.#endRequest());\n  }\n\n  #addEvent(event: AssistantStreamEvent) {\n    if (this.ended) return;\n\n    this.#currentEvent = event;\n\n    this.#handleEvent(event);\n\n    switch (event.event) {\n      case 'thread.created':\n        //No action on this event.\n        break;\n\n      case 'thread.run.created':\n      case 'thread.run.queued':\n      case 'thread.run.in_progress':\n      case 'thread.run.requires_action':\n      case 'thread.run.completed':\n      case 'thread.run.incomplete':\n      case 'thread.run.failed':\n      case 'thread.run.cancelling':\n      case 'thread.run.cancelled':\n      case 'thread.run.expired':\n        this.#handleRun(event);\n        break;\n\n      case 'thread.run.step.created':\n      case 'thread.run.step.in_progress':\n      case 'thread.run.step.delta':\n      case 'thread.run.step.completed':\n      case 'thread.run.step.failed':\n      case 'thread.run.step.cancelled':\n      case 'thread.run.step.expired':\n        this.#handleRunStep(event);\n        break;\n\n      case 'thread.message.created':\n      case 'thread.message.in_progress':\n      case 'thread.message.delta':\n      case 'thread.message.completed':\n      case 'thread.message.incomplete':\n        this.#handleMessage(event);\n        break;\n\n      case 'error':\n        //This is included for completeness, but errors are processed in the SSE event processing so this should not occur\n        throw new Error(\n          'Encountered an error event in event processing - errors should be processed earlier',\n        );\n      default:\n        assertNever(event);\n    }\n  }\n\n  #endRequest(): Run {\n    if (this.ended) {\n      throw new OpenAIError(`stream has ended, this shouldn't happen`);\n    }\n\n    if (!this.#finalRun) throw Error('Final run has not been received');\n\n    return this.#finalRun;\n  }\n\n  #handleMessage(this: AssistantStream, event: MessageStreamEvent) {\n    const [accumulatedMessage, newContent] = this.#accumulateMessage(event, this.#messageSnapshot);\n    this.#messageSnapshot = accumulatedMessage;\n    this.#messageSnapshots[accumulatedMessage.id] = accumulatedMessage;\n\n    for (const content of newContent) {\n      const snapshotContent = accumulatedMessage.content[content.index];\n      if (snapshotContent?.type == 'text') {\n        this._emit('textCreated', snapshotContent.text);\n      }\n    }\n\n    switch (event.event) {\n      case 'thread.message.created':\n        this._emit('messageCreated', event.data);\n        break;\n\n      case 'thread.message.in_progress':\n        break;\n\n      case 'thread.message.delta':\n        this._emit('messageDelta', event.data.delta, accumulatedMessage);\n\n        if (event.data.delta.content) {\n          for (const content of event.data.delta.content) {\n            //If it is text delta, emit a text delta event\n            if (content.type == 'text' && content.text) {\n              let textDelta = content.text;\n              let snapshot = accumulatedMessage.content[content.index];\n              if (snapshot && snapshot.type == 'text') {\n                this._emit('textDelta', textDelta, snapshot.text);\n              } else {\n                throw Error('The snapshot associated with this text delta is not text or missing');\n              }\n            }\n\n            if (content.index != this.#currentContentIndex) {\n              //See if we have in progress content\n              if (this.#currentContent) {\n                switch (this.#currentContent.type) {\n                  case 'text':\n                    this._emit('textDone', this.#currentContent.text, this.#messageSnapshot);\n                    break;\n                  case 'image_file':\n                    this._emit('imageFileDone', this.#currentContent.image_file, this.#messageSnapshot);\n                    break;\n                }\n              }\n\n              this.#currentContentIndex = content.index;\n            }\n\n            this.#currentContent = accumulatedMessage.content[content.index];\n          }\n        }\n\n        break;\n\n      case 'thread.message.completed':\n      case 'thread.message.incomplete':\n        //We emit the latest content we were working on on completion (including incomplete)\n        if (this.#currentContentIndex !== undefined) {\n          const currentContent = event.data.content[this.#currentContentIndex];\n          if (currentContent) {\n            switch (currentContent.type) {\n              case 'image_file':\n                this._emit('imageFileDone', currentContent.image_file, this.#messageSnapshot);\n                break;\n              case 'text':\n                this._emit('textDone', currentContent.text, this.#messageSnapshot);\n                break;\n            }\n          }\n        }\n\n        if (this.#messageSnapshot) {\n          this._emit('messageDone', event.data);\n        }\n\n        this.#messageSnapshot = undefined;\n    }\n  }\n\n  #handleRunStep(this: AssistantStream, event: RunStepStreamEvent) {\n    const accumulatedRunStep = this.#accumulateRunStep(event);\n    this.#currentRunStepSnapshot = accumulatedRunStep;\n\n    switch (event.event) {\n      case 'thread.run.step.created':\n        this._emit('runStepCreated', event.data);\n        break;\n      case 'thread.run.step.delta':\n        const delta = event.data.delta;\n        if (\n          delta.step_details &&\n          delta.step_details.type == 'tool_calls' &&\n          delta.step_details.tool_calls &&\n          accumulatedRunStep.step_details.type == 'tool_calls'\n        ) {\n          for (const toolCall of delta.step_details.tool_calls) {\n            if (toolCall.index == this.#currentToolCallIndex) {\n              this._emit(\n                'toolCallDelta',\n                toolCall,\n                accumulatedRunStep.step_details.tool_calls[toolCall.index] as ToolCall,\n              );\n            } else {\n              if (this.#currentToolCall) {\n                this._emit('toolCallDone', this.#currentToolCall);\n              }\n\n              this.#currentToolCallIndex = toolCall.index;\n              this.#currentToolCall = accumulatedRunStep.step_details.tool_calls[toolCall.index];\n              if (this.#currentToolCall) this._emit('toolCallCreated', this.#currentToolCall);\n            }\n          }\n        }\n\n        this._emit('runStepDelta', event.data.delta, accumulatedRunStep);\n        break;\n      case 'thread.run.step.completed':\n      case 'thread.run.step.failed':\n      case 'thread.run.step.cancelled':\n      case 'thread.run.step.expired':\n        this.#currentRunStepSnapshot = undefined;\n        const details = event.data.step_details;\n        if (details.type == 'tool_calls') {\n          if (this.#currentToolCall) {\n            this._emit('toolCallDone', this.#currentToolCall as ToolCall);\n            this.#currentToolCall = undefined;\n          }\n        }\n        this._emit('runStepDone', event.data, accumulatedRunStep);\n        break;\n      case 'thread.run.step.in_progress':\n        break;\n    }\n  }\n\n  #handleEvent(this: AssistantStream, event: AssistantStreamEvent) {\n    this.#events.push(event);\n    this._emit('event', event);\n  }\n\n  #accumulateRunStep(event: RunStepStreamEvent): Runs.RunStep {\n    switch (event.event) {\n      case 'thread.run.step.created':\n        this.#runStepSnapshots[event.data.id] = event.data;\n        return event.data;\n\n      case 'thread.run.step.delta':\n        let snapshot = this.#runStepSnapshots[event.data.id] as Runs.RunStep;\n        if (!snapshot) {\n          throw Error('Received a RunStepDelta before creation of a snapshot');\n        }\n\n        let data = event.data;\n\n        if (data.delta) {\n          const accumulated = AssistantStream.accumulateDelta(snapshot, data.delta) as Runs.RunStep;\n          this.#runStepSnapshots[event.data.id] = accumulated;\n        }\n\n        return this.#runStepSnapshots[event.data.id] as Runs.RunStep;\n\n      case 'thread.run.step.completed':\n      case 'thread.run.step.failed':\n      case 'thread.run.step.cancelled':\n      case 'thread.run.step.expired':\n      case 'thread.run.step.in_progress':\n        this.#runStepSnapshots[event.data.id] = event.data;\n        break;\n    }\n\n    if (this.#runStepSnapshots[event.data.id]) return this.#runStepSnapshots[event.data.id] as Runs.RunStep;\n    throw new Error('No snapshot available');\n  }\n\n  #accumulateMessage(\n    event: AssistantStreamEvent,\n    snapshot: Message | undefined,\n  ): [Message, MessageContentDelta[]] {\n    let newContent: MessageContentDelta[] = [];\n\n    switch (event.event) {\n      case 'thread.message.created':\n        //On creation the snapshot is just the initial message\n        return [event.data, newContent];\n\n      case 'thread.message.delta':\n        if (!snapshot) {\n          throw Error(\n            'Received a delta with no existing snapshot (there should be one from message creation)',\n          );\n        }\n\n        let data = event.data;\n\n        //If this delta does not have content, nothing to process\n        if (data.delta.content) {\n          for (const contentElement of data.delta.content) {\n            if (contentElement.index in snapshot.content) {\n              let currentContent = snapshot.content[contentElement.index];\n              snapshot.content[contentElement.index] = this.#accumulateContent(\n                contentElement,\n                currentContent,\n              );\n            } else {\n              snapshot.content[contentElement.index] = contentElement as MessageContent;\n              // This is a new element\n              newContent.push(contentElement);\n            }\n          }\n        }\n\n        return [snapshot, newContent];\n\n      case 'thread.message.in_progress':\n      case 'thread.message.completed':\n      case 'thread.message.incomplete':\n        //No changes on other thread events\n        if (snapshot) {\n          return [snapshot, newContent];\n        } else {\n          throw Error('Received thread message event with no existing snapshot');\n        }\n    }\n    throw Error('Tried to accumulate a non-message event');\n  }\n\n  #accumulateContent(\n    contentElement: MessageContentDelta,\n    currentContent: MessageContent | undefined,\n  ): TextContentBlock | ImageFileContentBlock {\n    return AssistantStream.accumulateDelta(currentContent as unknown as Record<any, any>, contentElement) as\n      | TextContentBlock\n      | ImageFileContentBlock;\n  }\n\n  static accumulateDelta(acc: Record<string, any>, delta: Record<string, any>): Record<string, any> {\n    for (const [key, deltaValue] of Object.entries(delta)) {\n      if (!acc.hasOwnProperty(key)) {\n        acc[key] = deltaValue;\n        continue;\n      }\n\n      let accValue = acc[key];\n      if (accValue === null || accValue === undefined) {\n        acc[key] = deltaValue;\n        continue;\n      }\n\n      // We don't accumulate these special properties\n      if (key === 'index' || key === 'type') {\n        acc[key] = deltaValue;\n        continue;\n      }\n\n      // Type-specific accumulation logic\n      if (typeof accValue === 'string' && typeof deltaValue === 'string') {\n        accValue += deltaValue;\n      } else if (typeof accValue === 'number' && typeof deltaValue === 'number') {\n        accValue += deltaValue;\n      } else if (isObj(accValue) && isObj(deltaValue)) {\n        accValue = this.accumulateDelta(accValue as Record<string, any>, deltaValue as Record<string, any>);\n      } else if (Array.isArray(accValue) && Array.isArray(deltaValue)) {\n        if (accValue.every((x) => typeof x === 'string' || typeof x === 'number')) {\n          accValue.push(...deltaValue); // Use spread syntax for efficient addition\n          continue;\n        }\n\n        for (const deltaEntry of deltaValue) {\n          if (!isObj(deltaEntry)) {\n            throw new Error(`Expected array delta entry to be an object but got: ${deltaEntry}`);\n          }\n\n          const index = deltaEntry['index'];\n          if (index == null) {\n            console.error(deltaEntry);\n            throw new Error('Expected array delta entry to have an `index` property');\n          }\n\n          if (typeof index !== 'number') {\n            throw new Error(`Expected array delta entry \\`index\\` property to be a number but got ${index}`);\n          }\n\n          const accEntry = accValue[index];\n          if (accEntry == null) {\n            accValue.push(deltaEntry);\n          } else {\n            accValue[index] = this.accumulateDelta(accEntry, deltaEntry);\n          }\n        }\n        continue;\n      } else {\n        throw Error(`Unhandled record type: ${key}, deltaValue: ${deltaValue}, accValue: ${accValue}`);\n      }\n      acc[key] = accValue;\n    }\n\n    return acc;\n  }\n\n  #handleRun(this: AssistantStream, event: RunStreamEvent) {\n    this.#currentRunSnapshot = event.data;\n\n    switch (event.event) {\n      case 'thread.run.created':\n        break;\n      case 'thread.run.queued':\n        break;\n      case 'thread.run.in_progress':\n        break;\n      case 'thread.run.requires_action':\n      case 'thread.run.cancelled':\n      case 'thread.run.failed':\n      case 'thread.run.completed':\n      case 'thread.run.expired':\n      case 'thread.run.incomplete':\n        this.#finalRun = event.data;\n        if (this.#currentToolCall) {\n          this._emit('toolCallDone', this.#currentToolCall);\n          this.#currentToolCall = undefined;\n        }\n        break;\n      case 'thread.run.cancelling':\n        break;\n    }\n  }\n\n  protected _addRun(run: Run): Run {\n    return run;\n  }\n\n  protected async _threadAssistantStream(\n    params: ThreadCreateAndRunParamsBase,\n    thread: Threads,\n    options?: RequestOptions,\n  ): Promise<Run> {\n    return await this._createThreadAssistantStream(thread, params, options);\n  }\n\n  protected async _runAssistantStream(\n    threadId: string,\n    runs: Runs,\n    params: RunCreateParamsBase,\n    options?: RequestOptions,\n  ): Promise<Run> {\n    return await this._createAssistantStream(runs, threadId, params, options);\n  }\n\n  protected async _runToolAssistantStream(\n    runId: string,\n    runs: Runs,\n    params: RunSubmitToolOutputsParamsStream,\n    options?: RequestOptions,\n  ): Promise<Run> {\n    return await this._createToolAssistantStream(runs, runId, params, options);\n  }\n}\n\nfunction assertNever(_x: never) {}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../../core/resource';\nimport * as RunsAPI from './runs';\nimport * as Shared from '../../../shared';\nimport * as AssistantsAPI from '../../assistants';\nimport * as MessagesAPI from '../messages';\nimport * as ThreadsAPI from '../threads';\nimport * as StepsAPI from './steps';\nimport {\n  CodeInterpreterLogs,\n  CodeInterpreterOutputImage,\n  CodeInterpreterToolCall,\n  CodeInterpreterToolCallDelta,\n  FileSearchToolCall,\n  FileSearchToolCallDelta,\n  FunctionToolCall,\n  FunctionToolCallDelta,\n  MessageCreationStepDetails,\n  RunStep,\n  RunStepDelta,\n  RunStepDeltaEvent,\n  RunStepDeltaMessageDelta,\n  RunStepInclude,\n  RunStepsPage,\n  StepListParams,\n  StepRetrieveParams,\n  Steps,\n  ToolCall,\n  ToolCallDelta,\n  ToolCallDeltaObject,\n  ToolCallsStepDetails,\n} from './steps';\nimport { APIPromise } from '../../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../../core/pagination';\nimport { Stream } from '../../../../core/streaming';\nimport { buildHeaders } from '../../../../internal/headers';\nimport { RequestOptions } from '../../../../internal/request-options';\nimport { AssistantStream, RunCreateParamsBaseStream } from '../../../../lib/AssistantStream';\nimport { sleep } from '../../../../internal/utils/sleep';\nimport { RunSubmitToolOutputsParamsStream } from '../../../../lib/AssistantStream';\nimport { path } from '../../../../internal/utils/path';\n\n/**\n * @deprecated The Assistants API is deprecated in favor of the Responses API\n */\nexport class Runs extends APIResource {\n  steps: StepsAPI.Steps = new StepsAPI.Steps(this._client);\n\n  /**\n   * Create a run.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  create(threadID: string, params: RunCreateParamsNonStreaming, options?: RequestOptions): APIPromise<Run>;\n  create(\n    threadID: string,\n    params: RunCreateParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>>;\n  create(\n    threadID: string,\n    params: RunCreateParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<AssistantsAPI.AssistantStreamEvent> | Run>;\n  create(\n    threadID: string,\n    params: RunCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<Run> | APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>> {\n    const { include, ...body } = params;\n    return this._client.post(path`/threads/${threadID}/runs`, {\n      query: { include },\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n      stream: params.stream ?? false,\n      __synthesizeEventData: true,\n    }) as APIPromise<Run> | APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>>;\n  }\n\n  /**\n   * Retrieves a run.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  retrieve(runID: string, params: RunRetrieveParams, options?: RequestOptions): APIPromise<Run> {\n    const { thread_id } = params;\n    return this._client.get(path`/threads/${thread_id}/runs/${runID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Modifies a run.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  update(runID: string, params: RunUpdateParams, options?: RequestOptions): APIPromise<Run> {\n    const { thread_id, ...body } = params;\n    return this._client.post(path`/threads/${thread_id}/runs/${runID}`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Returns a list of runs belonging to a thread.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  list(\n    threadID: string,\n    query: RunListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<RunsPage, Run> {\n    return this._client.getAPIList(path`/threads/${threadID}/runs`, CursorPage<Run>, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Cancels a run that is `in_progress`.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  cancel(runID: string, params: RunCancelParams, options?: RequestOptions): APIPromise<Run> {\n    const { thread_id } = params;\n    return this._client.post(path`/threads/${thread_id}/runs/${runID}/cancel`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * A helper to create a run an poll for a terminal state. More information on Run\n   * lifecycles can be found here:\n   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n   */\n  async createAndPoll(\n    threadId: string,\n    body: RunCreateParamsNonStreaming,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<Run> {\n    const run = await this.create(threadId, body, options);\n    return await this.poll(run.id, { thread_id: threadId }, options);\n  }\n\n  /**\n   * Create a Run stream\n   *\n   * @deprecated use `stream` instead\n   */\n  createAndStream(\n    threadId: string,\n    body: RunCreateParamsBaseStream,\n    options?: RequestOptions,\n  ): AssistantStream {\n    return AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);\n  }\n\n  /**\n   * A helper to poll a run status until it reaches a terminal state. More\n   * information on Run lifecycles can be found here:\n   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n   */\n  async poll(\n    runId: string,\n    params: RunRetrieveParams,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<Run> {\n    const headers = buildHeaders([\n      options?.headers,\n      {\n        'X-Stainless-Poll-Helper': 'true',\n        'X-Stainless-Custom-Poll-Interval': options?.pollIntervalMs?.toString() ?? undefined,\n      },\n    ]);\n\n    while (true) {\n      const { data: run, response } = await this.retrieve(runId, params, {\n        ...options,\n        headers: { ...options?.headers, ...headers },\n      }).withResponse();\n\n      switch (run.status) {\n        //If we are in any sort of intermediate state we poll\n        case 'queued':\n        case 'in_progress':\n        case 'cancelling':\n          let sleepInterval = 5000;\n\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        //We return the run in any terminal state.\n        case 'requires_action':\n        case 'incomplete':\n        case 'cancelled':\n        case 'completed':\n        case 'failed':\n        case 'expired':\n          return run;\n      }\n    }\n  }\n\n  /**\n   * Create a Run stream\n   */\n  stream(threadId: string, body: RunCreateParamsBaseStream, options?: RequestOptions): AssistantStream {\n    return AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);\n  }\n\n  /**\n   * When a run has the `status: \"requires_action\"` and `required_action.type` is\n   * `submit_tool_outputs`, this endpoint can be used to submit the outputs from the\n   * tool calls once they're all completed. All outputs must be submitted in a single\n   * request.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  submitToolOutputs(\n    runID: string,\n    params: RunSubmitToolOutputsParamsNonStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Run>;\n  submitToolOutputs(\n    runID: string,\n    params: RunSubmitToolOutputsParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>>;\n  submitToolOutputs(\n    runID: string,\n    params: RunSubmitToolOutputsParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<AssistantsAPI.AssistantStreamEvent> | Run>;\n  submitToolOutputs(\n    runID: string,\n    params: RunSubmitToolOutputsParams,\n    options?: RequestOptions,\n  ): APIPromise<Run> | APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>> {\n    const { thread_id, ...body } = params;\n    return this._client.post(path`/threads/${thread_id}/runs/${runID}/submit_tool_outputs`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n      stream: params.stream ?? false,\n      __synthesizeEventData: true,\n    }) as APIPromise<Run> | APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>>;\n  }\n\n  /**\n   * A helper to submit a tool output to a run and poll for a terminal run state.\n   * More information on Run lifecycles can be found here:\n   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n   */\n  async submitToolOutputsAndPoll(\n    runId: string,\n    params: RunSubmitToolOutputsParamsNonStreaming,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<Run> {\n    const run = await this.submitToolOutputs(runId, params, options);\n    return await this.poll(run.id, params, options);\n  }\n\n  /**\n   * Submit the tool outputs from a previous run and stream the run to a terminal\n   * state. More information on Run lifecycles can be found here:\n   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n   */\n  submitToolOutputsStream(\n    runId: string,\n    params: RunSubmitToolOutputsParamsStream,\n    options?: RequestOptions,\n  ): AssistantStream {\n    return AssistantStream.createToolAssistantStream(runId, this._client.beta.threads.runs, params, options);\n  }\n}\n\nexport type RunsPage = CursorPage<Run>;\n\n/**\n * Tool call objects\n */\nexport interface RequiredActionFunctionToolCall {\n  /**\n   * The ID of the tool call. This ID must be referenced when you submit the tool\n   * outputs in using the\n   * [Submit tool outputs to run](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)\n   * endpoint.\n   */\n  id: string;\n\n  /**\n   * The function definition.\n   */\n  function: RequiredActionFunctionToolCall.Function;\n\n  /**\n   * The type of tool call the output is required for. For now, this is always\n   * `function`.\n   */\n  type: 'function';\n}\n\nexport namespace RequiredActionFunctionToolCall {\n  /**\n   * The function definition.\n   */\n  export interface Function {\n    /**\n     * The arguments that the model expects you to pass to the function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function.\n     */\n    name: string;\n  }\n}\n\n/**\n * Represents an execution run on a\n * [thread](https://platform.openai.com/docs/api-reference/threads).\n */\nexport interface Run {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * execution of this run.\n   */\n  assistant_id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was cancelled.\n   */\n  cancelled_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was completed.\n   */\n  completed_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was created.\n   */\n  created_at: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run will expire.\n   */\n  expires_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run failed.\n   */\n  failed_at: number | null;\n\n  /**\n   * Details on why the run is incomplete. Will be `null` if the run is not\n   * incomplete.\n   */\n  incomplete_details: Run.IncompleteDetails | null;\n\n  /**\n   * The instructions that the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * this run.\n   */\n  instructions: string;\n\n  /**\n   * The last error associated with this run. Will be `null` if there are no errors.\n   */\n  last_error: Run.LastError | null;\n\n  /**\n   * The maximum number of completion tokens specified to have been used over the\n   * course of the run.\n   */\n  max_completion_tokens: number | null;\n\n  /**\n   * The maximum number of prompt tokens specified to have been used over the course\n   * of the run.\n   */\n  max_prompt_tokens: number | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The model that the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * this run.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `thread.run`.\n   */\n  object: 'thread.run';\n\n  /**\n   * Whether to enable\n   * [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n   * during tool use.\n   */\n  parallel_tool_calls: boolean;\n\n  /**\n   * Details on the action required to continue the run. Will be `null` if no action\n   * is required.\n   */\n  required_action: Run.RequiredAction | null;\n\n  /**\n   * Specifies the format that the model must output. Compatible with\n   * [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),\n   * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n   * Outputs which ensures the model will match your supplied JSON schema. Learn more\n   * in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format: ThreadsAPI.AssistantResponseFormatOption | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was started.\n   */\n  started_at: number | null;\n\n  /**\n   * The status of the run, which can be either `queued`, `in_progress`,\n   * `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`,\n   * `incomplete`, or `expired`.\n   */\n  status: RunStatus;\n\n  /**\n   * The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)\n   * that was executed on as a part of this run.\n   */\n  thread_id: string;\n\n  /**\n   * Controls which (if any) tool is called by the model. `none` means the model will\n   * not call any tools and instead generates a message. `auto` is the default value\n   * and means the model can pick between generating a message or calling one or more\n   * tools. `required` means the model must call one or more tools before responding\n   * to the user. Specifying a particular tool like `{\"type\": \"file_search\"}` or\n   * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n   * call that tool.\n   */\n  tool_choice: ThreadsAPI.AssistantToolChoiceOption | null;\n\n  /**\n   * The list of tools that the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * this run.\n   */\n  tools: Array<AssistantsAPI.AssistantTool>;\n\n  /**\n   * Controls for how a thread will be truncated prior to the run. Use this to\n   * control the initial context window of the run.\n   */\n  truncation_strategy: Run.TruncationStrategy | null;\n\n  /**\n   * Usage statistics related to the run. This value will be `null` if the run is not\n   * in a terminal state (i.e. `in_progress`, `queued`, etc.).\n   */\n  usage: Run.Usage | null;\n\n  /**\n   * The sampling temperature used for this run. If not set, defaults to 1.\n   */\n  temperature?: number | null;\n\n  /**\n   * The nucleus sampling value used for this run. If not set, defaults to 1.\n   */\n  top_p?: number | null;\n}\n\nexport namespace Run {\n  /**\n   * Details on why the run is incomplete. Will be `null` if the run is not\n   * incomplete.\n   */\n  export interface IncompleteDetails {\n    /**\n     * The reason why the run is incomplete. This will point to which specific token\n     * limit was reached over the course of the run.\n     */\n    reason?: 'max_completion_tokens' | 'max_prompt_tokens';\n  }\n\n  /**\n   * The last error associated with this run. Will be `null` if there are no errors.\n   */\n  export interface LastError {\n    /**\n     * One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`.\n     */\n    code: 'server_error' | 'rate_limit_exceeded' | 'invalid_prompt';\n\n    /**\n     * A human-readable description of the error.\n     */\n    message: string;\n  }\n\n  /**\n   * Details on the action required to continue the run. Will be `null` if no action\n   * is required.\n   */\n  export interface RequiredAction {\n    /**\n     * Details on the tool outputs needed for this run to continue.\n     */\n    submit_tool_outputs: RequiredAction.SubmitToolOutputs;\n\n    /**\n     * For now, this is always `submit_tool_outputs`.\n     */\n    type: 'submit_tool_outputs';\n  }\n\n  export namespace RequiredAction {\n    /**\n     * Details on the tool outputs needed for this run to continue.\n     */\n    export interface SubmitToolOutputs {\n      /**\n       * A list of the relevant tool calls.\n       */\n      tool_calls: Array<RunsAPI.RequiredActionFunctionToolCall>;\n    }\n  }\n\n  /**\n   * Controls for how a thread will be truncated prior to the run. Use this to\n   * control the initial context window of the run.\n   */\n  export interface TruncationStrategy {\n    /**\n     * The truncation strategy to use for the thread. The default is `auto`. If set to\n     * `last_messages`, the thread will be truncated to the n most recent messages in\n     * the thread. When set to `auto`, messages in the middle of the thread will be\n     * dropped to fit the context length of the model, `max_prompt_tokens`.\n     */\n    type: 'auto' | 'last_messages';\n\n    /**\n     * The number of most recent messages from the thread when constructing the context\n     * for the run.\n     */\n    last_messages?: number | null;\n  }\n\n  /**\n   * Usage statistics related to the run. This value will be `null` if the run is not\n   * in a terminal state (i.e. `in_progress`, `queued`, etc.).\n   */\n  export interface Usage {\n    /**\n     * Number of completion tokens used over the course of the run.\n     */\n    completion_tokens: number;\n\n    /**\n     * Number of prompt tokens used over the course of the run.\n     */\n    prompt_tokens: number;\n\n    /**\n     * Total number of tokens used (prompt + completion).\n     */\n    total_tokens: number;\n  }\n}\n\n/**\n * The status of the run, which can be either `queued`, `in_progress`,\n * `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`,\n * `incomplete`, or `expired`.\n */\nexport type RunStatus =\n  | 'queued'\n  | 'in_progress'\n  | 'requires_action'\n  | 'cancelling'\n  | 'cancelled'\n  | 'failed'\n  | 'completed'\n  | 'incomplete'\n  | 'expired';\n\nexport type RunCreateParams = RunCreateParamsNonStreaming | RunCreateParamsStreaming;\n\nexport interface RunCreateParamsBase {\n  /**\n   * Body param: The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to\n   * execute this run.\n   */\n  assistant_id: string;\n\n  /**\n   * Query param: A list of additional fields to include in the response. Currently\n   * the only supported value is\n   * `step_details.tool_calls[*].file_search.results[*].content` to fetch the file\n   * search result content.\n   *\n   * See the\n   * [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)\n   * for more information.\n   */\n  include?: Array<StepsAPI.RunStepInclude>;\n\n  /**\n   * Body param: Appends additional instructions at the end of the instructions for\n   * the run. This is useful for modifying the behavior on a per-run basis without\n   * overriding other instructions.\n   */\n  additional_instructions?: string | null;\n\n  /**\n   * Body param: Adds additional messages to the thread before creating the run.\n   */\n  additional_messages?: Array<RunCreateParams.AdditionalMessage> | null;\n\n  /**\n   * Body param: Overrides the\n   * [instructions](https://platform.openai.com/docs/api-reference/assistants/createAssistant)\n   * of the assistant. This is useful for modifying the behavior on a per-run basis.\n   */\n  instructions?: string | null;\n\n  /**\n   * Body param: The maximum number of completion tokens that may be used over the\n   * course of the run. The run will make a best effort to use only the number of\n   * completion tokens specified, across multiple turns of the run. If the run\n   * exceeds the number of completion tokens specified, the run will end with status\n   * `incomplete`. See `incomplete_details` for more info.\n   */\n  max_completion_tokens?: number | null;\n\n  /**\n   * Body param: The maximum number of prompt tokens that may be used over the course\n   * of the run. The run will make a best effort to use only the number of prompt\n   * tokens specified, across multiple turns of the run. If the run exceeds the\n   * number of prompt tokens specified, the run will end with status `incomplete`.\n   * See `incomplete_details` for more info.\n   */\n  max_prompt_tokens?: number | null;\n\n  /**\n   * Body param: Set of 16 key-value pairs that can be attached to an object. This\n   * can be useful for storing additional information about the object in a\n   * structured format, and querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * Body param: The ID of the\n   * [Model](https://platform.openai.com/docs/api-reference/models) to be used to\n   * execute this run. If a value is provided here, it will override the model\n   * associated with the assistant. If not, the model associated with the assistant\n   * will be used.\n   */\n  model?: (string & {}) | Shared.ChatModel | null;\n\n  /**\n   * Body param: Whether to enable\n   * [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n   * during tool use.\n   */\n  parallel_tool_calls?: boolean;\n\n  /**\n   * Body param: Constrains effort on reasoning for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n   * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n   * Reducing reasoning effort can result in faster responses and fewer tokens used\n   * on reasoning in a response.\n   *\n   * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n   *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n   *   calls are supported for all reasoning values in gpt-5.1.\n   * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n   *   support `none`.\n   * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n   * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n   */\n  reasoning_effort?: Shared.ReasoningEffort | null;\n\n  /**\n   * Body param: Specifies the format that the model must output. Compatible with\n   * [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),\n   * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n   * Outputs which ensures the model will match your supplied JSON schema. Learn more\n   * in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: ThreadsAPI.AssistantResponseFormatOption | null;\n\n  /**\n   * Body param: If `true`, returns a stream of events that happen during the Run as\n   * server-sent events, terminating when the Run enters a terminal state with a\n   * `data: [DONE]` message.\n   */\n  stream?: boolean | null;\n\n  /**\n   * Body param: What sampling temperature to use, between 0 and 2. Higher values\n   * like 0.8 will make the output more random, while lower values like 0.2 will make\n   * it more focused and deterministic.\n   */\n  temperature?: number | null;\n\n  /**\n   * Body param: Controls which (if any) tool is called by the model. `none` means\n   * the model will not call any tools and instead generates a message. `auto` is the\n   * default value and means the model can pick between generating a message or\n   * calling one or more tools. `required` means the model must call one or more\n   * tools before responding to the user. Specifying a particular tool like\n   * `{\"type\": \"file_search\"}` or\n   * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n   * call that tool.\n   */\n  tool_choice?: ThreadsAPI.AssistantToolChoiceOption | null;\n\n  /**\n   * Body param: Override the tools the assistant can use for this run. This is\n   * useful for modifying the behavior on a per-run basis.\n   */\n  tools?: Array<AssistantsAPI.AssistantTool> | null;\n\n  /**\n   * Body param: An alternative to sampling with temperature, called nucleus\n   * sampling, where the model considers the results of the tokens with top_p\n   * probability mass. So 0.1 means only the tokens comprising the top 10%\n   * probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * Body param: Controls for how a thread will be truncated prior to the run. Use\n   * this to control the initial context window of the run.\n   */\n  truncation_strategy?: RunCreateParams.TruncationStrategy | null;\n}\n\nexport namespace RunCreateParams {\n  export interface AdditionalMessage {\n    /**\n     * The text contents of the message.\n     */\n    content: string | Array<MessagesAPI.MessageContentPartParam>;\n\n    /**\n     * The role of the entity that is creating the message. Allowed values include:\n     *\n     * - `user`: Indicates the message is sent by an actual user and should be used in\n     *   most cases to represent user-generated messages.\n     * - `assistant`: Indicates the message is generated by the assistant. Use this\n     *   value to insert messages from the assistant into the conversation.\n     */\n    role: 'user' | 'assistant';\n\n    /**\n     * A list of files attached to the message, and the tools they should be added to.\n     */\n    attachments?: Array<AdditionalMessage.Attachment> | null;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n  }\n\n  export namespace AdditionalMessage {\n    export interface Attachment {\n      /**\n       * The ID of the file to attach to the message.\n       */\n      file_id?: string;\n\n      /**\n       * The tools to add this file to.\n       */\n      tools?: Array<AssistantsAPI.CodeInterpreterTool | Attachment.FileSearch>;\n    }\n\n    export namespace Attachment {\n      export interface FileSearch {\n        /**\n         * The type of tool being defined: `file_search`\n         */\n        type: 'file_search';\n      }\n    }\n  }\n\n  /**\n   * Controls for how a thread will be truncated prior to the run. Use this to\n   * control the initial context window of the run.\n   */\n  export interface TruncationStrategy {\n    /**\n     * The truncation strategy to use for the thread. The default is `auto`. If set to\n     * `last_messages`, the thread will be truncated to the n most recent messages in\n     * the thread. When set to `auto`, messages in the middle of the thread will be\n     * dropped to fit the context length of the model, `max_prompt_tokens`.\n     */\n    type: 'auto' | 'last_messages';\n\n    /**\n     * The number of most recent messages from the thread when constructing the context\n     * for the run.\n     */\n    last_messages?: number | null;\n  }\n\n  export type RunCreateParamsNonStreaming = RunsAPI.RunCreateParamsNonStreaming;\n  export type RunCreateParamsStreaming = RunsAPI.RunCreateParamsStreaming;\n}\n\nexport interface RunCreateParamsNonStreaming extends RunCreateParamsBase {\n  /**\n   * Body param: If `true`, returns a stream of events that happen during the Run as\n   * server-sent events, terminating when the Run enters a terminal state with a\n   * `data: [DONE]` message.\n   */\n  stream?: false | null;\n}\n\nexport interface RunCreateParamsStreaming extends RunCreateParamsBase {\n  /**\n   * Body param: If `true`, returns a stream of events that happen during the Run as\n   * server-sent events, terminating when the Run enters a terminal state with a\n   * `data: [DONE]` message.\n   */\n  stream: true;\n}\n\nexport interface RunRetrieveParams {\n  /**\n   * The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)\n   * that was run.\n   */\n  thread_id: string;\n}\n\nexport interface RunUpdateParams {\n  /**\n   * Path param: The ID of the\n   * [thread](https://platform.openai.com/docs/api-reference/threads) that was run.\n   */\n  thread_id: string;\n\n  /**\n   * Body param: Set of 16 key-value pairs that can be attached to an object. This\n   * can be useful for storing additional information about the object in a\n   * structured format, and querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n}\n\nexport interface RunListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * starting with obj_foo, your subsequent call can include before=obj_foo in order\n   * to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface RunCancelParams {\n  /**\n   * The ID of the thread to which this run belongs.\n   */\n  thread_id: string;\n}\n\nexport type RunCreateAndPollParams = ThreadsAPI.ThreadCreateAndRunParamsNonStreaming;\n\nexport type RunCreateAndStreamParams = RunCreateParamsBaseStream;\n\nexport type RunStreamParams = RunCreateParamsBaseStream;\n\nexport type RunSubmitToolOutputsParams =\n  | RunSubmitToolOutputsParamsNonStreaming\n  | RunSubmitToolOutputsParamsStreaming;\n\nexport interface RunSubmitToolOutputsParamsBase {\n  /**\n   * Path param: The ID of the\n   * [thread](https://platform.openai.com/docs/api-reference/threads) to which this\n   * run belongs.\n   */\n  thread_id: string;\n\n  /**\n   * Body param: A list of tools for which the outputs are being submitted.\n   */\n  tool_outputs: Array<RunSubmitToolOutputsParams.ToolOutput>;\n\n  /**\n   * Body param: If `true`, returns a stream of events that happen during the Run as\n   * server-sent events, terminating when the Run enters a terminal state with a\n   * `data: [DONE]` message.\n   */\n  stream?: boolean | null;\n}\n\nexport namespace RunSubmitToolOutputsParams {\n  export interface ToolOutput {\n    /**\n     * The output of the tool call to be submitted to continue the run.\n     */\n    output?: string;\n\n    /**\n     * The ID of the tool call in the `required_action` object within the run object\n     * the output is being submitted for.\n     */\n    tool_call_id?: string;\n  }\n\n  export type RunSubmitToolOutputsParamsNonStreaming = RunsAPI.RunSubmitToolOutputsParamsNonStreaming;\n  export type RunSubmitToolOutputsParamsStreaming = RunsAPI.RunSubmitToolOutputsParamsStreaming;\n}\n\nexport interface RunSubmitToolOutputsParamsNonStreaming extends RunSubmitToolOutputsParamsBase {\n  /**\n   * Body param: If `true`, returns a stream of events that happen during the Run as\n   * server-sent events, terminating when the Run enters a terminal state with a\n   * `data: [DONE]` message.\n   */\n  stream?: false | null;\n}\n\nexport interface RunSubmitToolOutputsParamsStreaming extends RunSubmitToolOutputsParamsBase {\n  /**\n   * Body param: If `true`, returns a stream of events that happen during the Run as\n   * server-sent events, terminating when the Run enters a terminal state with a\n   * `data: [DONE]` message.\n   */\n  stream: true;\n}\n\nexport type RunSubmitToolOutputsAndPollParams = RunSubmitToolOutputsParamsNonStreaming;\nexport type RunSubmitToolOutputsStreamParams = RunSubmitToolOutputsParamsStream;\n\nRuns.Steps = Steps;\n\nexport declare namespace Runs {\n  export {\n    type RequiredActionFunctionToolCall as RequiredActionFunctionToolCall,\n    type Run as Run,\n    type RunStatus as RunStatus,\n    type RunsPage as RunsPage,\n    type RunCreateParams as RunCreateParams,\n    type RunCreateParamsNonStreaming as RunCreateParamsNonStreaming,\n    type RunCreateParamsStreaming as RunCreateParamsStreaming,\n    type RunRetrieveParams as RunRetrieveParams,\n    type RunUpdateParams as RunUpdateParams,\n    type RunListParams as RunListParams,\n    type RunCreateAndPollParams,\n    type RunCreateAndStreamParams,\n    type RunStreamParams,\n    type RunSubmitToolOutputsParams as RunSubmitToolOutputsParams,\n    type RunSubmitToolOutputsParamsNonStreaming as RunSubmitToolOutputsParamsNonStreaming,\n    type RunSubmitToolOutputsParamsStreaming as RunSubmitToolOutputsParamsStreaming,\n    type RunSubmitToolOutputsAndPollParams,\n    type RunSubmitToolOutputsStreamParams,\n  };\n\n  export {\n    Steps as Steps,\n    type CodeInterpreterLogs as CodeInterpreterLogs,\n    type CodeInterpreterOutputImage as CodeInterpreterOutputImage,\n    type CodeInterpreterToolCall as CodeInterpreterToolCall,\n    type CodeInterpreterToolCallDelta as CodeInterpreterToolCallDelta,\n    type FileSearchToolCall as FileSearchToolCall,\n    type FileSearchToolCallDelta as FileSearchToolCallDelta,\n    type FunctionToolCall as FunctionToolCall,\n    type FunctionToolCallDelta as FunctionToolCallDelta,\n    type MessageCreationStepDetails as MessageCreationStepDetails,\n    type RunStep as RunStep,\n    type RunStepDelta as RunStepDelta,\n    type RunStepDeltaEvent as RunStepDeltaEvent,\n    type RunStepDeltaMessageDelta as RunStepDeltaMessageDelta,\n    type RunStepInclude as RunStepInclude,\n    type ToolCall as ToolCall,\n    type ToolCallDelta as ToolCallDelta,\n    type ToolCallDeltaObject as ToolCallDeltaObject,\n    type ToolCallsStepDetails as ToolCallsStepDetails,\n    type RunStepsPage as RunStepsPage,\n    type StepRetrieveParams as StepRetrieveParams,\n    type StepListParams as StepListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as ThreadsAPI from './threads';\nimport * as Shared from '../../shared';\nimport * as AssistantsAPI from '../assistants';\nimport * as MessagesAPI from './messages';\nimport {\n  Annotation,\n  AnnotationDelta,\n  FileCitationAnnotation,\n  FileCitationDeltaAnnotation,\n  FilePathAnnotation,\n  FilePathDeltaAnnotation,\n  ImageFile,\n  ImageFileContentBlock,\n  ImageFileDelta,\n  ImageFileDeltaBlock,\n  ImageURL,\n  ImageURLContentBlock,\n  ImageURLDelta,\n  ImageURLDeltaBlock,\n  Message as MessagesAPIMessage,\n  MessageContent,\n  MessageContentDelta,\n  MessageContentPartParam,\n  MessageCreateParams,\n  MessageDeleteParams,\n  MessageDeleted,\n  MessageDelta,\n  MessageDeltaEvent,\n  MessageListParams,\n  MessageRetrieveParams,\n  MessageUpdateParams,\n  Messages,\n  MessagesPage,\n  RefusalContentBlock,\n  RefusalDeltaBlock,\n  Text,\n  TextContentBlock,\n  TextContentBlockParam,\n  TextDelta,\n  TextDeltaBlock,\n} from './messages';\nimport * as RunsAPI from './runs/runs';\nimport {\n  RequiredActionFunctionToolCall,\n  Run,\n  RunCreateAndPollParams,\n  RunCreateAndStreamParams,\n  RunCancelParams,\n  RunCreateParams,\n  RunCreateParamsNonStreaming,\n  RunCreateParamsStreaming,\n  RunListParams,\n  RunRetrieveParams,\n  RunStatus,\n  RunStreamParams,\n  RunSubmitToolOutputsAndPollParams,\n  RunSubmitToolOutputsParams,\n  RunSubmitToolOutputsParamsNonStreaming,\n  RunSubmitToolOutputsParamsStreaming,\n  RunSubmitToolOutputsStreamParams,\n  RunUpdateParams,\n  Runs,\n  RunsPage,\n} from './runs/runs';\nimport { APIPromise } from '../../../core/api-promise';\nimport { Stream } from '../../../core/streaming';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { AssistantStream, ThreadCreateAndRunParamsBaseStream } from '../../../lib/AssistantStream';\nimport { path } from '../../../internal/utils/path';\n\n/**\n * @deprecated The Assistants API is deprecated in favor of the Responses API\n */\nexport class Threads extends APIResource {\n  runs: RunsAPI.Runs = new RunsAPI.Runs(this._client);\n  messages: MessagesAPI.Messages = new MessagesAPI.Messages(this._client);\n\n  /**\n   * Create a thread.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  create(body: ThreadCreateParams | null | undefined = {}, options?: RequestOptions): APIPromise<Thread> {\n    return this._client.post('/threads', {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Retrieves a thread.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  retrieve(threadID: string, options?: RequestOptions): APIPromise<Thread> {\n    return this._client.get(path`/threads/${threadID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Modifies a thread.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  update(threadID: string, body: ThreadUpdateParams, options?: RequestOptions): APIPromise<Thread> {\n    return this._client.post(path`/threads/${threadID}`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Delete a thread.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  delete(threadID: string, options?: RequestOptions): APIPromise<ThreadDeleted> {\n    return this._client.delete(path`/threads/${threadID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Create a thread and run it in one request.\n   *\n   * @deprecated The Assistants API is deprecated in favor of the Responses API\n   */\n  createAndRun(body: ThreadCreateAndRunParamsNonStreaming, options?: RequestOptions): APIPromise<RunsAPI.Run>;\n  createAndRun(\n    body: ThreadCreateAndRunParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>>;\n  createAndRun(\n    body: ThreadCreateAndRunParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<AssistantsAPI.AssistantStreamEvent> | RunsAPI.Run>;\n  createAndRun(\n    body: ThreadCreateAndRunParams,\n    options?: RequestOptions,\n  ): APIPromise<RunsAPI.Run> | APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>> {\n    return this._client.post('/threads/runs', {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n      stream: body.stream ?? false,\n      __synthesizeEventData: true,\n    }) as APIPromise<RunsAPI.Run> | APIPromise<Stream<AssistantsAPI.AssistantStreamEvent>>;\n  }\n\n  /**\n   * A helper to create a thread, start a run and then poll for a terminal state.\n   * More information on Run lifecycles can be found here:\n   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n   */\n  async createAndRunPoll(\n    body: ThreadCreateAndRunParamsNonStreaming,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<Threads.Run> {\n    const run = await this.createAndRun(body, options);\n    return await this.runs.poll(run.id, { thread_id: run.thread_id }, options);\n  }\n\n  /**\n   * Create a thread and stream the run back\n   */\n  createAndRunStream(body: ThreadCreateAndRunParamsBaseStream, options?: RequestOptions): AssistantStream {\n    return AssistantStream.createThreadAssistantStream(body, this._client.beta.threads, options);\n  }\n}\n\n/**\n * Specifies the format that the model must output. Compatible with\n * [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n * [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),\n * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n *\n * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n * Outputs which ensures the model will match your supplied JSON schema. Learn more\n * in the\n * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n *\n * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\n * message the model generates is valid JSON.\n *\n * **Important:** when using JSON mode, you **must** also instruct the model to\n * produce JSON yourself via a system or user message. Without this, the model may\n * generate an unending stream of whitespace until the generation reaches the token\n * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n * the message content may be partially cut off if `finish_reason=\"length\"`, which\n * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n * max context length.\n */\nexport type AssistantResponseFormatOption =\n  | 'auto'\n  | Shared.ResponseFormatText\n  | Shared.ResponseFormatJSONObject\n  | Shared.ResponseFormatJSONSchema;\n\n/**\n * Specifies a tool the model should use. Use to force the model to call a specific\n * tool.\n */\nexport interface AssistantToolChoice {\n  /**\n   * The type of the tool. If type is `function`, the function name must be set\n   */\n  type: 'function' | 'code_interpreter' | 'file_search';\n\n  function?: AssistantToolChoiceFunction;\n}\n\nexport interface AssistantToolChoiceFunction {\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n}\n\n/**\n * Controls which (if any) tool is called by the model. `none` means the model will\n * not call any tools and instead generates a message. `auto` is the default value\n * and means the model can pick between generating a message or calling one or more\n * tools. `required` means the model must call one or more tools before responding\n * to the user. Specifying a particular tool like `{\"type\": \"file_search\"}` or\n * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n * call that tool.\n */\nexport type AssistantToolChoiceOption = 'none' | 'auto' | 'required' | AssistantToolChoice;\n\n/**\n * Represents a thread that contains\n * [messages](https://platform.openai.com/docs/api-reference/messages).\n */\nexport interface Thread {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the thread was created.\n   */\n  created_at: number;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The object type, which is always `thread`.\n   */\n  object: 'thread';\n\n  /**\n   * A set of resources that are made available to the assistant's tools in this\n   * thread. The resources are specific to the type of tool. For example, the\n   * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n   * tool requires a list of vector store IDs.\n   */\n  tool_resources: Thread.ToolResources | null;\n}\n\nexport namespace Thread {\n  /**\n   * A set of resources that are made available to the assistant's tools in this\n   * thread. The resources are specific to the type of tool. For example, the\n   * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n   * tool requires a list of vector store IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n       * available to the `code_interpreter` tool. There can be a maximum of 20 files\n       * associated with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * The\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this thread. There can be a maximum of 1 vector store attached to\n       * the thread.\n       */\n      vector_store_ids?: Array<string>;\n    }\n  }\n}\n\nexport interface ThreadDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'thread.deleted';\n}\n\nexport interface ThreadCreateParams {\n  /**\n   * A list of [messages](https://platform.openai.com/docs/api-reference/messages) to\n   * start the thread with.\n   */\n  messages?: Array<ThreadCreateParams.Message>;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * A set of resources that are made available to the assistant's tools in this\n   * thread. The resources are specific to the type of tool. For example, the\n   * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n   * tool requires a list of vector store IDs.\n   */\n  tool_resources?: ThreadCreateParams.ToolResources | null;\n}\n\nexport namespace ThreadCreateParams {\n  export interface Message {\n    /**\n     * The text contents of the message.\n     */\n    content: string | Array<MessagesAPI.MessageContentPartParam>;\n\n    /**\n     * The role of the entity that is creating the message. Allowed values include:\n     *\n     * - `user`: Indicates the message is sent by an actual user and should be used in\n     *   most cases to represent user-generated messages.\n     * - `assistant`: Indicates the message is generated by the assistant. Use this\n     *   value to insert messages from the assistant into the conversation.\n     */\n    role: 'user' | 'assistant';\n\n    /**\n     * A list of files attached to the message, and the tools they should be added to.\n     */\n    attachments?: Array<Message.Attachment> | null;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n  }\n\n  export namespace Message {\n    export interface Attachment {\n      /**\n       * The ID of the file to attach to the message.\n       */\n      file_id?: string;\n\n      /**\n       * The tools to add this file to.\n       */\n      tools?: Array<AssistantsAPI.CodeInterpreterTool | Attachment.FileSearch>;\n    }\n\n    export namespace Attachment {\n      export interface FileSearch {\n        /**\n         * The type of tool being defined: `file_search`\n         */\n        type: 'file_search';\n      }\n    }\n  }\n\n  /**\n   * A set of resources that are made available to the assistant's tools in this\n   * thread. The resources are specific to the type of tool. For example, the\n   * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n   * tool requires a list of vector store IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n       * available to the `code_interpreter` tool. There can be a maximum of 20 files\n       * associated with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * The\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this thread. There can be a maximum of 1 vector store attached to\n       * the thread.\n       */\n      vector_store_ids?: Array<string>;\n\n      /**\n       * A helper to create a\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * with file_ids and attach it to this thread. There can be a maximum of 1 vector\n       * store attached to the thread.\n       */\n      vector_stores?: Array<FileSearch.VectorStore>;\n    }\n\n    export namespace FileSearch {\n      export interface VectorStore {\n        /**\n         * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n         * strategy.\n         */\n        chunking_strategy?: VectorStore.Auto | VectorStore.Static;\n\n        /**\n         * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to\n         * add to the vector store. There can be a maximum of 10000 files in a vector\n         * store.\n         */\n        file_ids?: Array<string>;\n\n        /**\n         * Set of 16 key-value pairs that can be attached to an object. This can be useful\n         * for storing additional information about the object in a structured format, and\n         * querying for objects via API or the dashboard.\n         *\n         * Keys are strings with a maximum length of 64 characters. Values are strings with\n         * a maximum length of 512 characters.\n         */\n        metadata?: Shared.Metadata | null;\n      }\n\n      export namespace VectorStore {\n        /**\n         * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of\n         * `800` and `chunk_overlap_tokens` of `400`.\n         */\n        export interface Auto {\n          /**\n           * Always `auto`.\n           */\n          type: 'auto';\n        }\n\n        export interface Static {\n          static: Static.Static;\n\n          /**\n           * Always `static`.\n           */\n          type: 'static';\n        }\n\n        export namespace Static {\n          export interface Static {\n            /**\n             * The number of tokens that overlap between chunks. The default value is `400`.\n             *\n             * Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n             */\n            chunk_overlap_tokens: number;\n\n            /**\n             * The maximum number of tokens in each chunk. The default value is `800`. The\n             * minimum value is `100` and the maximum value is `4096`.\n             */\n            max_chunk_size_tokens: number;\n          }\n        }\n      }\n    }\n  }\n}\n\nexport interface ThreadUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * A set of resources that are made available to the assistant's tools in this\n   * thread. The resources are specific to the type of tool. For example, the\n   * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n   * tool requires a list of vector store IDs.\n   */\n  tool_resources?: ThreadUpdateParams.ToolResources | null;\n}\n\nexport namespace ThreadUpdateParams {\n  /**\n   * A set of resources that are made available to the assistant's tools in this\n   * thread. The resources are specific to the type of tool. For example, the\n   * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n   * tool requires a list of vector store IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n       * available to the `code_interpreter` tool. There can be a maximum of 20 files\n       * associated with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * The\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this thread. There can be a maximum of 1 vector store attached to\n       * the thread.\n       */\n      vector_store_ids?: Array<string>;\n    }\n  }\n}\n\nexport type ThreadCreateAndRunParams =\n  | ThreadCreateAndRunParamsNonStreaming\n  | ThreadCreateAndRunParamsStreaming;\n\nexport interface ThreadCreateAndRunParamsBase {\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to\n   * execute this run.\n   */\n  assistant_id: string;\n\n  /**\n   * Override the default system message of the assistant. This is useful for\n   * modifying the behavior on a per-run basis.\n   */\n  instructions?: string | null;\n\n  /**\n   * The maximum number of completion tokens that may be used over the course of the\n   * run. The run will make a best effort to use only the number of completion tokens\n   * specified, across multiple turns of the run. If the run exceeds the number of\n   * completion tokens specified, the run will end with status `incomplete`. See\n   * `incomplete_details` for more info.\n   */\n  max_completion_tokens?: number | null;\n\n  /**\n   * The maximum number of prompt tokens that may be used over the course of the run.\n   * The run will make a best effort to use only the number of prompt tokens\n   * specified, across multiple turns of the run. If the run exceeds the number of\n   * prompt tokens specified, the run will end with status `incomplete`. See\n   * `incomplete_details` for more info.\n   */\n  max_prompt_tokens?: number | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to\n   * be used to execute this run. If a value is provided here, it will override the\n   * model associated with the assistant. If not, the model associated with the\n   * assistant will be used.\n   */\n  model?: (string & {}) | Shared.ChatModel | null;\n\n  /**\n   * Whether to enable\n   * [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n   * during tool use.\n   */\n  parallel_tool_calls?: boolean;\n\n  /**\n   * Specifies the format that the model must output. Compatible with\n   * [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),\n   * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n   * Outputs which ensures the model will match your supplied JSON schema. Learn more\n   * in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: AssistantResponseFormatOption | null;\n\n  /**\n   * If `true`, returns a stream of events that happen during the Run as server-sent\n   * events, terminating when the Run enters a terminal state with a `data: [DONE]`\n   * message.\n   */\n  stream?: boolean | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   */\n  temperature?: number | null;\n\n  /**\n   * Options to create a new thread. If no thread is provided when running a request,\n   * an empty thread will be created.\n   */\n  thread?: ThreadCreateAndRunParams.Thread;\n\n  /**\n   * Controls which (if any) tool is called by the model. `none` means the model will\n   * not call any tools and instead generates a message. `auto` is the default value\n   * and means the model can pick between generating a message or calling one or more\n   * tools. `required` means the model must call one or more tools before responding\n   * to the user. Specifying a particular tool like `{\"type\": \"file_search\"}` or\n   * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n   * call that tool.\n   */\n  tool_choice?: AssistantToolChoiceOption | null;\n\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  tool_resources?: ThreadCreateAndRunParams.ToolResources | null;\n\n  /**\n   * Override the tools the assistant can use for this run. This is useful for\n   * modifying the behavior on a per-run basis.\n   */\n  tools?: Array<AssistantsAPI.AssistantTool> | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * Controls for how a thread will be truncated prior to the run. Use this to\n   * control the initial context window of the run.\n   */\n  truncation_strategy?: ThreadCreateAndRunParams.TruncationStrategy | null;\n}\n\nexport namespace ThreadCreateAndRunParams {\n  /**\n   * Options to create a new thread. If no thread is provided when running a request,\n   * an empty thread will be created.\n   */\n  export interface Thread {\n    /**\n     * A list of [messages](https://platform.openai.com/docs/api-reference/messages) to\n     * start the thread with.\n     */\n    messages?: Array<Thread.Message>;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n\n    /**\n     * A set of resources that are made available to the assistant's tools in this\n     * thread. The resources are specific to the type of tool. For example, the\n     * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n     * tool requires a list of vector store IDs.\n     */\n    tool_resources?: Thread.ToolResources | null;\n  }\n\n  export namespace Thread {\n    export interface Message {\n      /**\n       * The text contents of the message.\n       */\n      content: string | Array<MessagesAPI.MessageContentPartParam>;\n\n      /**\n       * The role of the entity that is creating the message. Allowed values include:\n       *\n       * - `user`: Indicates the message is sent by an actual user and should be used in\n       *   most cases to represent user-generated messages.\n       * - `assistant`: Indicates the message is generated by the assistant. Use this\n       *   value to insert messages from the assistant into the conversation.\n       */\n      role: 'user' | 'assistant';\n\n      /**\n       * A list of files attached to the message, and the tools they should be added to.\n       */\n      attachments?: Array<Message.Attachment> | null;\n\n      /**\n       * Set of 16 key-value pairs that can be attached to an object. This can be useful\n       * for storing additional information about the object in a structured format, and\n       * querying for objects via API or the dashboard.\n       *\n       * Keys are strings with a maximum length of 64 characters. Values are strings with\n       * a maximum length of 512 characters.\n       */\n      metadata?: Shared.Metadata | null;\n    }\n\n    export namespace Message {\n      export interface Attachment {\n        /**\n         * The ID of the file to attach to the message.\n         */\n        file_id?: string;\n\n        /**\n         * The tools to add this file to.\n         */\n        tools?: Array<AssistantsAPI.CodeInterpreterTool | Attachment.FileSearch>;\n      }\n\n      export namespace Attachment {\n        export interface FileSearch {\n          /**\n           * The type of tool being defined: `file_search`\n           */\n          type: 'file_search';\n        }\n      }\n    }\n\n    /**\n     * A set of resources that are made available to the assistant's tools in this\n     * thread. The resources are specific to the type of tool. For example, the\n     * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n     * tool requires a list of vector store IDs.\n     */\n    export interface ToolResources {\n      code_interpreter?: ToolResources.CodeInterpreter;\n\n      file_search?: ToolResources.FileSearch;\n    }\n\n    export namespace ToolResources {\n      export interface CodeInterpreter {\n        /**\n         * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n         * available to the `code_interpreter` tool. There can be a maximum of 20 files\n         * associated with the tool.\n         */\n        file_ids?: Array<string>;\n      }\n\n      export interface FileSearch {\n        /**\n         * The\n         * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n         * attached to this thread. There can be a maximum of 1 vector store attached to\n         * the thread.\n         */\n        vector_store_ids?: Array<string>;\n\n        /**\n         * A helper to create a\n         * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n         * with file_ids and attach it to this thread. There can be a maximum of 1 vector\n         * store attached to the thread.\n         */\n        vector_stores?: Array<FileSearch.VectorStore>;\n      }\n\n      export namespace FileSearch {\n        export interface VectorStore {\n          /**\n           * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n           * strategy.\n           */\n          chunking_strategy?: VectorStore.Auto | VectorStore.Static;\n\n          /**\n           * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to\n           * add to the vector store. There can be a maximum of 10000 files in a vector\n           * store.\n           */\n          file_ids?: Array<string>;\n\n          /**\n           * Set of 16 key-value pairs that can be attached to an object. This can be useful\n           * for storing additional information about the object in a structured format, and\n           * querying for objects via API or the dashboard.\n           *\n           * Keys are strings with a maximum length of 64 characters. Values are strings with\n           * a maximum length of 512 characters.\n           */\n          metadata?: Shared.Metadata | null;\n        }\n\n        export namespace VectorStore {\n          /**\n           * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of\n           * `800` and `chunk_overlap_tokens` of `400`.\n           */\n          export interface Auto {\n            /**\n             * Always `auto`.\n             */\n            type: 'auto';\n          }\n\n          export interface Static {\n            static: Static.Static;\n\n            /**\n             * Always `static`.\n             */\n            type: 'static';\n          }\n\n          export namespace Static {\n            export interface Static {\n              /**\n               * The number of tokens that overlap between chunks. The default value is `400`.\n               *\n               * Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n               */\n              chunk_overlap_tokens: number;\n\n              /**\n               * The maximum number of tokens in each chunk. The default value is `800`. The\n               * minimum value is `100` and the maximum value is `4096`.\n               */\n              max_chunk_size_tokens: number;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n       * available to the `code_interpreter` tool. There can be a maximum of 20 files\n       * associated with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * The ID of the\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this assistant. There can be a maximum of 1 vector store attached to\n       * the assistant.\n       */\n      vector_store_ids?: Array<string>;\n    }\n  }\n\n  /**\n   * Controls for how a thread will be truncated prior to the run. Use this to\n   * control the initial context window of the run.\n   */\n  export interface TruncationStrategy {\n    /**\n     * The truncation strategy to use for the thread. The default is `auto`. If set to\n     * `last_messages`, the thread will be truncated to the n most recent messages in\n     * the thread. When set to `auto`, messages in the middle of the thread will be\n     * dropped to fit the context length of the model, `max_prompt_tokens`.\n     */\n    type: 'auto' | 'last_messages';\n\n    /**\n     * The number of most recent messages from the thread when constructing the context\n     * for the run.\n     */\n    last_messages?: number | null;\n  }\n\n  export type ThreadCreateAndRunParamsNonStreaming = ThreadsAPI.ThreadCreateAndRunParamsNonStreaming;\n  export type ThreadCreateAndRunParamsStreaming = ThreadsAPI.ThreadCreateAndRunParamsStreaming;\n}\n\nexport interface ThreadCreateAndRunParamsNonStreaming extends ThreadCreateAndRunParamsBase {\n  /**\n   * If `true`, returns a stream of events that happen during the Run as server-sent\n   * events, terminating when the Run enters a terminal state with a `data: [DONE]`\n   * message.\n   */\n  stream?: false | null;\n}\n\nexport interface ThreadCreateAndRunParamsStreaming extends ThreadCreateAndRunParamsBase {\n  /**\n   * If `true`, returns a stream of events that happen during the Run as server-sent\n   * events, terminating when the Run enters a terminal state with a `data: [DONE]`\n   * message.\n   */\n  stream: true;\n}\n\nexport interface ThreadCreateAndRunPollParams {\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to\n   * execute this run.\n   */\n  assistant_id: string;\n\n  /**\n   * Override the default system message of the assistant. This is useful for\n   * modifying the behavior on a per-run basis.\n   */\n  instructions?: string | null;\n\n  /**\n   * The maximum number of completion tokens that may be used over the course of the\n   * run. The run will make a best effort to use only the number of completion tokens\n   * specified, across multiple turns of the run. If the run exceeds the number of\n   * completion tokens specified, the run will end with status `incomplete`. See\n   * `incomplete_details` for more info.\n   */\n  max_completion_tokens?: number | null;\n\n  /**\n   * The maximum number of prompt tokens that may be used over the course of the run.\n   * The run will make a best effort to use only the number of prompt tokens\n   * specified, across multiple turns of the run. If the run exceeds the number of\n   * prompt tokens specified, the run will end with status `incomplete`. See\n   * `incomplete_details` for more info.\n   */\n  max_prompt_tokens?: number | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n\n  /**\n   * The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to\n   * be used to execute this run. If a value is provided here, it will override the\n   * model associated with the assistant. If not, the model associated with the\n   * assistant will be used.\n   */\n  model?:\n    | (string & {})\n    | 'gpt-4o'\n    | 'gpt-4o-2024-05-13'\n    | 'gpt-4-turbo'\n    | 'gpt-4-turbo-2024-04-09'\n    | 'gpt-4-0125-preview'\n    | 'gpt-4-turbo-preview'\n    | 'gpt-4-1106-preview'\n    | 'gpt-4-vision-preview'\n    | 'gpt-4'\n    | 'gpt-4-0314'\n    | 'gpt-4-0613'\n    | 'gpt-4-32k'\n    | 'gpt-4-32k-0314'\n    | 'gpt-4-32k-0613'\n    | 'gpt-3.5-turbo'\n    | 'gpt-3.5-turbo-16k'\n    | 'gpt-3.5-turbo-0613'\n    | 'gpt-3.5-turbo-1106'\n    | 'gpt-3.5-turbo-0125'\n    | 'gpt-3.5-turbo-16k-0613'\n    | null;\n\n  /**\n   * Specifies the format that the model must output. Compatible with\n   * [GPT-4o](https://platform.openai.com/docs/models/gpt-4o),\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4),\n   * and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: AssistantResponseFormatOption | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   */\n  temperature?: number | null;\n\n  /**\n   * If no thread is provided, an empty thread will be created.\n   */\n  thread?: ThreadCreateAndRunPollParams.Thread;\n\n  /**\n   * Controls which (if any) tool is called by the model. `none` means the model will\n   * not call any tools and instead generates a message. `auto` is the default value\n   * and means the model can pick between generating a message or calling one or more\n   * tools. `required` means the model must call one or more tools before responding\n   * to the user. Specifying a particular tool like `{\"type\": \"file_search\"}` or\n   * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n   * call that tool.\n   */\n  tool_choice?: AssistantToolChoiceOption | null;\n\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  tool_resources?: ThreadCreateAndRunPollParams.ToolResources | null;\n\n  /**\n   * Override the tools the assistant can use for this run. This is useful for\n   * modifying the behavior on a per-run basis.\n   */\n  tools?: Array<\n    AssistantsAPI.CodeInterpreterTool | AssistantsAPI.FileSearchTool | AssistantsAPI.FunctionTool\n  > | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * Controls for how a thread will be truncated prior to the run. Use this to\n   * control the intial context window of the run.\n   */\n  truncation_strategy?: ThreadCreateAndRunPollParams.TruncationStrategy | null;\n}\n\nexport namespace ThreadCreateAndRunPollParams {\n  /**\n   * If no thread is provided, an empty thread will be created.\n   */\n  export interface Thread {\n    /**\n     * A list of [messages](https://platform.openai.com/docs/api-reference/messages) to\n     * start the thread with.\n     */\n    messages?: Array<Thread.Message>;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format. Keys\n     * can be a maximum of 64 characters long and values can be a maxium of 512\n     * characters long.\n     */\n    metadata?: unknown | null;\n\n    /**\n     * A set of resources that are made available to the assistant's tools in this\n     * thread. The resources are specific to the type of tool. For example, the\n     * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n     * tool requires a list of vector store IDs.\n     */\n    tool_resources?: Thread.ToolResources | null;\n  }\n\n  export namespace Thread {\n    export interface Message {\n      /**\n       * The text contents of the message.\n       */\n      content: string | Array<MessagesAPI.MessageContentPartParam>;\n\n      /**\n       * The role of the entity that is creating the message. Allowed values include:\n       *\n       * - `user`: Indicates the message is sent by an actual user and should be used in\n       *   most cases to represent user-generated messages.\n       * - `assistant`: Indicates the message is generated by the assistant. Use this\n       *   value to insert messages from the assistant into the conversation.\n       */\n      role: 'user' | 'assistant';\n\n      /**\n       * A list of files attached to the message, and the tools they should be added to.\n       */\n      attachments?: Array<Message.Attachment> | null;\n\n      /**\n       * Set of 16 key-value pairs that can be attached to an object. This can be useful\n       * for storing additional information about the object in a structured format. Keys\n       * can be a maximum of 64 characters long and values can be a maxium of 512\n       * characters long.\n       */\n      metadata?: unknown | null;\n    }\n\n    export namespace Message {\n      export interface Attachment {\n        /**\n         * The ID of the file to attach to the message.\n         */\n        file_id?: string;\n\n        /**\n         * The tools to add this file to.\n         */\n        tools?: Array<AssistantsAPI.CodeInterpreterTool | AssistantsAPI.FileSearchTool>;\n      }\n    }\n\n    /**\n     * A set of resources that are made available to the assistant's tools in this\n     * thread. The resources are specific to the type of tool. For example, the\n     * `code_interpreter` tool requires a list of file IDs, while the `file_search`\n     * tool requires a list of vector store IDs.\n     */\n    export interface ToolResources {\n      code_interpreter?: ToolResources.CodeInterpreter;\n\n      file_search?: ToolResources.FileSearch;\n    }\n\n    export namespace ToolResources {\n      export interface CodeInterpreter {\n        /**\n         * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n         * available to the `code_interpreter` tool. There can be a maximum of 20 files\n         * associated with the tool.\n         */\n        file_ids?: Array<string>;\n      }\n\n      export interface FileSearch {\n        /**\n         * The\n         * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n         * attached to this thread. There can be a maximum of 1 vector store attached to\n         * the thread.\n         */\n        vector_store_ids?: Array<string>;\n\n        /**\n         * A helper to create a\n         * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n         * with file_ids and attach it to this thread. There can be a maximum of 1 vector\n         * store attached to the thread.\n         */\n        vector_stores?: Array<FileSearch.VectorStore>;\n      }\n\n      export namespace FileSearch {\n        export interface VectorStore {\n          /**\n           * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to\n           * add to the vector store. There can be a maximum of 10000 files in a vector\n           * store.\n           */\n          file_ids?: Array<string>;\n\n          /**\n           * Set of 16 key-value pairs that can be attached to a vector store. This can be\n           * useful for storing additional information about the vector store in a structured\n           * format. Keys can be a maximum of 64 characters long and values can be a maxium\n           * of 512 characters long.\n           */\n          metadata?: unknown;\n        }\n      }\n    }\n  }\n\n  /**\n   * A set of resources that are used by the assistant's tools. The resources are\n   * specific to the type of tool. For example, the `code_interpreter` tool requires\n   * a list of file IDs, while the `file_search` tool requires a list of vector store\n   * IDs.\n   */\n  export interface ToolResources {\n    code_interpreter?: ToolResources.CodeInterpreter;\n\n    file_search?: ToolResources.FileSearch;\n  }\n\n  export namespace ToolResources {\n    export interface CodeInterpreter {\n      /**\n       * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made\n       * available to the `code_interpreter` tool. There can be a maximum of 20 files\n       * associated with the tool.\n       */\n      file_ids?: Array<string>;\n    }\n\n    export interface FileSearch {\n      /**\n       * The ID of the\n       * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n       * attached to this assistant. There can be a maximum of 1 vector store attached to\n       * the assistant.\n       */\n      vector_store_ids?: Array<string>;\n    }\n  }\n\n  /**\n   * Controls for how a thread will be truncated prior to the run. Use this to\n   * control the intial context window of the run.\n   */\n  export interface TruncationStrategy {\n    /**\n     * The truncation strategy to use for the thread. The default is `auto`. If set to\n     * `last_messages`, the thread will be truncated to the n most recent messages in\n     * the thread. When set to `auto`, messages in the middle of the thread will be\n     * dropped to fit the context length of the model, `max_prompt_tokens`.\n     */\n    type: 'auto' | 'last_messages';\n\n    /**\n     * The number of most recent messages from the thread when constructing the context\n     * for the run.\n     */\n    last_messages?: number | null;\n  }\n}\n\nexport type ThreadCreateAndRunStreamParams = ThreadCreateAndRunParamsBaseStream;\n\nThreads.Runs = Runs;\nThreads.Messages = Messages;\n\nexport declare namespace Threads {\n  export {\n    type AssistantResponseFormatOption as AssistantResponseFormatOption,\n    type AssistantToolChoice as AssistantToolChoice,\n    type AssistantToolChoiceFunction as AssistantToolChoiceFunction,\n    type AssistantToolChoiceOption as AssistantToolChoiceOption,\n    type Thread as Thread,\n    type ThreadDeleted as ThreadDeleted,\n    type ThreadCreateParams as ThreadCreateParams,\n    type ThreadUpdateParams as ThreadUpdateParams,\n    type ThreadCreateAndRunParams as ThreadCreateAndRunParams,\n    type ThreadCreateAndRunParamsNonStreaming as ThreadCreateAndRunParamsNonStreaming,\n    type ThreadCreateAndRunParamsStreaming as ThreadCreateAndRunParamsStreaming,\n    type ThreadCreateAndRunPollParams,\n    type ThreadCreateAndRunStreamParams,\n  };\n\n  export {\n    Runs as Runs,\n    type RequiredActionFunctionToolCall as RequiredActionFunctionToolCall,\n    type Run as Run,\n    type RunStatus as RunStatus,\n    type RunsPage as RunsPage,\n    type RunCreateParams as RunCreateParams,\n    type RunCreateParamsNonStreaming as RunCreateParamsNonStreaming,\n    type RunCreateParamsStreaming as RunCreateParamsStreaming,\n    type RunRetrieveParams as RunRetrieveParams,\n    type RunUpdateParams as RunUpdateParams,\n    type RunListParams as RunListParams,\n    type RunCancelParams as RunCancelParams,\n    type RunCreateAndPollParams,\n    type RunCreateAndStreamParams,\n    type RunStreamParams,\n    type RunSubmitToolOutputsParams as RunSubmitToolOutputsParams,\n    type RunSubmitToolOutputsParamsNonStreaming as RunSubmitToolOutputsParamsNonStreaming,\n    type RunSubmitToolOutputsParamsStreaming as RunSubmitToolOutputsParamsStreaming,\n    type RunSubmitToolOutputsAndPollParams,\n    type RunSubmitToolOutputsStreamParams,\n  };\n\n  export {\n    Messages as Messages,\n    type Annotation as Annotation,\n    type AnnotationDelta as AnnotationDelta,\n    type FileCitationAnnotation as FileCitationAnnotation,\n    type FileCitationDeltaAnnotation as FileCitationDeltaAnnotation,\n    type FilePathAnnotation as FilePathAnnotation,\n    type FilePathDeltaAnnotation as FilePathDeltaAnnotation,\n    type ImageFile as ImageFile,\n    type ImageFileContentBlock as ImageFileContentBlock,\n    type ImageFileDelta as ImageFileDelta,\n    type ImageFileDeltaBlock as ImageFileDeltaBlock,\n    type ImageURL as ImageURL,\n    type ImageURLContentBlock as ImageURLContentBlock,\n    type ImageURLDelta as ImageURLDelta,\n    type ImageURLDeltaBlock as ImageURLDeltaBlock,\n    type MessagesAPIMessage as Message,\n    type MessageContent as MessageContent,\n    type MessageContentDelta as MessageContentDelta,\n    type MessageContentPartParam as MessageContentPartParam,\n    type MessageDeleted as MessageDeleted,\n    type MessageDelta as MessageDelta,\n    type MessageDeltaEvent as MessageDeltaEvent,\n    type RefusalContentBlock as RefusalContentBlock,\n    type RefusalDeltaBlock as RefusalDeltaBlock,\n    type Text as Text,\n    type TextContentBlock as TextContentBlock,\n    type TextContentBlockParam as TextContentBlockParam,\n    type TextDelta as TextDelta,\n    type TextDeltaBlock as TextDeltaBlock,\n    type MessagesPage as MessagesPage,\n    type MessageCreateParams as MessageCreateParams,\n    type MessageRetrieveParams as MessageRetrieveParams,\n    type MessageUpdateParams as MessageUpdateParams,\n    type MessageListParams as MessageListParams,\n    type MessageDeleteParams as MessageDeleteParams,\n  };\n\n  export { AssistantStream };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as AssistantsAPI from './assistants';\nimport {\n  Assistant,\n  AssistantCreateParams,\n  AssistantDeleted,\n  AssistantListParams,\n  AssistantStreamEvent,\n  AssistantTool,\n  AssistantUpdateParams,\n  Assistants,\n  AssistantsPage,\n  CodeInterpreterTool,\n  FileSearchTool,\n  FunctionTool,\n  MessageStreamEvent,\n  RunStepStreamEvent,\n  RunStreamEvent,\n  ThreadStreamEvent,\n} from './assistants';\nimport * as RealtimeAPI from './realtime/realtime';\nimport {\n  ConversationCreatedEvent,\n  ConversationItem,\n  ConversationItemContent,\n  ConversationItemCreateEvent,\n  ConversationItemCreatedEvent,\n  ConversationItemDeleteEvent,\n  ConversationItemDeletedEvent,\n  ConversationItemInputAudioTranscriptionCompletedEvent,\n  ConversationItemInputAudioTranscriptionDeltaEvent,\n  ConversationItemInputAudioTranscriptionFailedEvent,\n  ConversationItemRetrieveEvent,\n  ConversationItemTruncateEvent,\n  ConversationItemTruncatedEvent,\n  ConversationItemWithReference,\n  ErrorEvent,\n  InputAudioBufferAppendEvent,\n  InputAudioBufferClearEvent,\n  InputAudioBufferClearedEvent,\n  InputAudioBufferCommitEvent,\n  InputAudioBufferCommittedEvent,\n  InputAudioBufferSpeechStartedEvent,\n  InputAudioBufferSpeechStoppedEvent,\n  RateLimitsUpdatedEvent,\n  Realtime,\n  RealtimeClientEvent,\n  RealtimeResponse,\n  RealtimeResponseStatus,\n  RealtimeResponseUsage,\n  RealtimeServerEvent,\n  ResponseAudioDeltaEvent,\n  ResponseAudioDoneEvent,\n  ResponseAudioTranscriptDeltaEvent,\n  ResponseAudioTranscriptDoneEvent,\n  ResponseCancelEvent,\n  ResponseContentPartAddedEvent,\n  ResponseContentPartDoneEvent,\n  ResponseCreateEvent,\n  ResponseCreatedEvent,\n  ResponseDoneEvent,\n  ResponseFunctionCallArgumentsDeltaEvent,\n  ResponseFunctionCallArgumentsDoneEvent,\n  ResponseOutputItemAddedEvent,\n  ResponseOutputItemDoneEvent,\n  ResponseTextDeltaEvent,\n  ResponseTextDoneEvent,\n  SessionCreatedEvent,\n  SessionUpdateEvent,\n  SessionUpdatedEvent,\n  TranscriptionSessionUpdate,\n  TranscriptionSessionUpdatedEvent,\n} from './realtime/realtime';\nimport * as ChatKitAPI from './chatkit/chatkit';\nimport { ChatKit, ChatKitWorkflow } from './chatkit/chatkit';\nimport * as ThreadsAPI from './threads/threads';\nimport {\n  AssistantResponseFormatOption,\n  AssistantToolChoice,\n  AssistantToolChoiceFunction,\n  AssistantToolChoiceOption,\n  Thread,\n  ThreadCreateAndRunParams,\n  ThreadCreateAndRunParamsNonStreaming,\n  ThreadCreateAndRunParamsStreaming,\n  ThreadCreateAndRunPollParams,\n  ThreadCreateAndRunStreamParams,\n  ThreadCreateParams,\n  ThreadDeleted,\n  ThreadUpdateParams,\n  Threads,\n} from './threads/threads';\n\nexport class Beta extends APIResource {\n  realtime: RealtimeAPI.Realtime = new RealtimeAPI.Realtime(this._client);\n  chatkit: ChatKitAPI.ChatKit = new ChatKitAPI.ChatKit(this._client);\n  assistants: AssistantsAPI.Assistants = new AssistantsAPI.Assistants(this._client);\n  threads: ThreadsAPI.Threads = new ThreadsAPI.Threads(this._client);\n}\n\nBeta.Realtime = Realtime;\nBeta.ChatKit = ChatKit;\nBeta.Assistants = Assistants;\nBeta.Threads = Threads;\n\nexport declare namespace Beta {\n  export {\n    Realtime as Realtime,\n    type ConversationCreatedEvent as ConversationCreatedEvent,\n    type ConversationItem as ConversationItem,\n    type ConversationItemContent as ConversationItemContent,\n    type ConversationItemCreateEvent as ConversationItemCreateEvent,\n    type ConversationItemCreatedEvent as ConversationItemCreatedEvent,\n    type ConversationItemDeleteEvent as ConversationItemDeleteEvent,\n    type ConversationItemDeletedEvent as ConversationItemDeletedEvent,\n    type ConversationItemInputAudioTranscriptionCompletedEvent as ConversationItemInputAudioTranscriptionCompletedEvent,\n    type ConversationItemInputAudioTranscriptionDeltaEvent as ConversationItemInputAudioTranscriptionDeltaEvent,\n    type ConversationItemInputAudioTranscriptionFailedEvent as ConversationItemInputAudioTranscriptionFailedEvent,\n    type ConversationItemRetrieveEvent as ConversationItemRetrieveEvent,\n    type ConversationItemTruncateEvent as ConversationItemTruncateEvent,\n    type ConversationItemTruncatedEvent as ConversationItemTruncatedEvent,\n    type ConversationItemWithReference as ConversationItemWithReference,\n    type ErrorEvent as ErrorEvent,\n    type InputAudioBufferAppendEvent as InputAudioBufferAppendEvent,\n    type InputAudioBufferClearEvent as InputAudioBufferClearEvent,\n    type InputAudioBufferClearedEvent as InputAudioBufferClearedEvent,\n    type InputAudioBufferCommitEvent as InputAudioBufferCommitEvent,\n    type InputAudioBufferCommittedEvent as InputAudioBufferCommittedEvent,\n    type InputAudioBufferSpeechStartedEvent as InputAudioBufferSpeechStartedEvent,\n    type InputAudioBufferSpeechStoppedEvent as InputAudioBufferSpeechStoppedEvent,\n    type RateLimitsUpdatedEvent as RateLimitsUpdatedEvent,\n    type RealtimeClientEvent as RealtimeClientEvent,\n    type RealtimeResponse as RealtimeResponse,\n    type RealtimeResponseStatus as RealtimeResponseStatus,\n    type RealtimeResponseUsage as RealtimeResponseUsage,\n    type RealtimeServerEvent as RealtimeServerEvent,\n    type ResponseAudioDeltaEvent as ResponseAudioDeltaEvent,\n    type ResponseAudioDoneEvent as ResponseAudioDoneEvent,\n    type ResponseAudioTranscriptDeltaEvent as ResponseAudioTranscriptDeltaEvent,\n    type ResponseAudioTranscriptDoneEvent as ResponseAudioTranscriptDoneEvent,\n    type ResponseCancelEvent as ResponseCancelEvent,\n    type ResponseContentPartAddedEvent as ResponseContentPartAddedEvent,\n    type ResponseContentPartDoneEvent as ResponseContentPartDoneEvent,\n    type ResponseCreateEvent as ResponseCreateEvent,\n    type ResponseCreatedEvent as ResponseCreatedEvent,\n    type ResponseDoneEvent as ResponseDoneEvent,\n    type ResponseFunctionCallArgumentsDeltaEvent as ResponseFunctionCallArgumentsDeltaEvent,\n    type ResponseFunctionCallArgumentsDoneEvent as ResponseFunctionCallArgumentsDoneEvent,\n    type ResponseOutputItemAddedEvent as ResponseOutputItemAddedEvent,\n    type ResponseOutputItemDoneEvent as ResponseOutputItemDoneEvent,\n    type ResponseTextDeltaEvent as ResponseTextDeltaEvent,\n    type ResponseTextDoneEvent as ResponseTextDoneEvent,\n    type SessionCreatedEvent as SessionCreatedEvent,\n    type SessionUpdateEvent as SessionUpdateEvent,\n    type SessionUpdatedEvent as SessionUpdatedEvent,\n    type TranscriptionSessionUpdate as TranscriptionSessionUpdate,\n    type TranscriptionSessionUpdatedEvent as TranscriptionSessionUpdatedEvent,\n    ChatKit as ChatKit,\n    type ChatKitWorkflow as ChatKitWorkflow,\n  };\n\n  export {\n    Assistants as Assistants,\n    type Assistant as Assistant,\n    type AssistantDeleted as AssistantDeleted,\n    type AssistantStreamEvent as AssistantStreamEvent,\n    type AssistantTool as AssistantTool,\n    type CodeInterpreterTool as CodeInterpreterTool,\n    type FileSearchTool as FileSearchTool,\n    type FunctionTool as FunctionTool,\n    type MessageStreamEvent as MessageStreamEvent,\n    type RunStepStreamEvent as RunStepStreamEvent,\n    type RunStreamEvent as RunStreamEvent,\n    type ThreadStreamEvent as ThreadStreamEvent,\n    type AssistantsPage as AssistantsPage,\n    type AssistantCreateParams as AssistantCreateParams,\n    type AssistantUpdateParams as AssistantUpdateParams,\n    type AssistantListParams as AssistantListParams,\n  };\n\n  export {\n    Threads as Threads,\n    type AssistantResponseFormatOption as AssistantResponseFormatOption,\n    type AssistantToolChoice as AssistantToolChoice,\n    type AssistantToolChoiceFunction as AssistantToolChoiceFunction,\n    type AssistantToolChoiceOption as AssistantToolChoiceOption,\n    type Thread as Thread,\n    type ThreadDeleted as ThreadDeleted,\n    type ThreadCreateParams as ThreadCreateParams,\n    type ThreadUpdateParams as ThreadUpdateParams,\n    type ThreadCreateAndRunParams as ThreadCreateAndRunParams,\n    type ThreadCreateAndRunParamsNonStreaming as ThreadCreateAndRunParamsNonStreaming,\n    type ThreadCreateAndRunParamsStreaming as ThreadCreateAndRunParamsStreaming,\n    type ThreadCreateAndRunPollParams,\n    type ThreadCreateAndRunStreamParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport * as CompletionsAPI from './completions';\nimport * as CompletionsCompletionsAPI from './chat/completions/completions';\nimport { APIPromise } from '../core/api-promise';\nimport { Stream } from '../core/streaming';\nimport { RequestOptions } from '../internal/request-options';\n\nexport class Completions extends APIResource {\n  /**\n   * Creates a completion for the provided prompt and parameters.\n   *\n   * Returns a completion object, or a sequence of completion objects if the request\n   * is streamed.\n   *\n   * @example\n   * ```ts\n   * const completion = await client.completions.create({\n   *   model: 'string',\n   *   prompt: 'This is a test.',\n   * });\n   * ```\n   */\n  create(body: CompletionCreateParamsNonStreaming, options?: RequestOptions): APIPromise<Completion>;\n  create(body: CompletionCreateParamsStreaming, options?: RequestOptions): APIPromise<Stream<Completion>>;\n  create(\n    body: CompletionCreateParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<Completion> | Completion>;\n  create(\n    body: CompletionCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<Completion> | APIPromise<Stream<Completion>> {\n    return this._client.post('/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<Completion>\n      | APIPromise<Stream<Completion>>;\n  }\n}\n\n/**\n * Represents a completion response from the API. Note: both the streamed and\n * non-streamed response objects share the same shape (unlike the chat endpoint).\n */\nexport interface Completion {\n  /**\n   * A unique identifier for the completion.\n   */\n  id: string;\n\n  /**\n   * The list of completion choices the model generated for the input prompt.\n   */\n  choices: Array<CompletionChoice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"text_completion\"\n   */\n  object: 'text_completion';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   *\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionUsage;\n}\n\nexport interface CompletionChoice {\n  /**\n   * The reason the model stopped generating tokens. This will be `stop` if the model\n   * hit a natural stop point or a provided stop sequence, `length` if the maximum\n   * number of tokens specified in the request was reached, or `content_filter` if\n   * content was omitted due to a flag from our content filters.\n   */\n  finish_reason: 'stop' | 'length' | 'content_filter';\n\n  index: number;\n\n  logprobs: CompletionChoice.Logprobs | null;\n\n  text: string;\n}\n\nexport namespace CompletionChoice {\n  export interface Logprobs {\n    text_offset?: Array<number>;\n\n    token_logprobs?: Array<number>;\n\n    tokens?: Array<string>;\n\n    top_logprobs?: Array<{ [key: string]: number }>;\n  }\n}\n\n/**\n * Usage statistics for the completion request.\n */\nexport interface CompletionUsage {\n  /**\n   * Number of tokens in the generated completion.\n   */\n  completion_tokens: number;\n\n  /**\n   * Number of tokens in the prompt.\n   */\n  prompt_tokens: number;\n\n  /**\n   * Total number of tokens used in the request (prompt + completion).\n   */\n  total_tokens: number;\n\n  /**\n   * Breakdown of tokens used in a completion.\n   */\n  completion_tokens_details?: CompletionUsage.CompletionTokensDetails;\n\n  /**\n   * Breakdown of tokens used in the prompt.\n   */\n  prompt_tokens_details?: CompletionUsage.PromptTokensDetails;\n}\n\nexport namespace CompletionUsage {\n  /**\n   * Breakdown of tokens used in a completion.\n   */\n  export interface CompletionTokensDetails {\n    /**\n     * When using Predicted Outputs, the number of tokens in the prediction that\n     * appeared in the completion.\n     */\n    accepted_prediction_tokens?: number;\n\n    /**\n     * Audio input tokens generated by the model.\n     */\n    audio_tokens?: number;\n\n    /**\n     * Tokens generated by the model for reasoning.\n     */\n    reasoning_tokens?: number;\n\n    /**\n     * When using Predicted Outputs, the number of tokens in the prediction that did\n     * not appear in the completion. However, like reasoning tokens, these tokens are\n     * still counted in the total completion tokens for purposes of billing, output,\n     * and context window limits.\n     */\n    rejected_prediction_tokens?: number;\n  }\n\n  /**\n   * Breakdown of tokens used in the prompt.\n   */\n  export interface PromptTokensDetails {\n    /**\n     * Audio input tokens present in the prompt.\n     */\n    audio_tokens?: number;\n\n    /**\n     * Cached tokens present in the prompt.\n     */\n    cached_tokens?: number;\n  }\n}\n\nexport type CompletionCreateParams = CompletionCreateParamsNonStreaming | CompletionCreateParamsStreaming;\n\nexport interface CompletionCreateParamsBase {\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models) for descriptions of\n   * them.\n   */\n  model: (string & {}) | 'gpt-3.5-turbo-instruct' | 'davinci-002' | 'babbage-002';\n\n  /**\n   * The prompt(s) to generate completions for, encoded as a string, array of\n   * strings, array of tokens, or array of token arrays.\n   *\n   * Note that <|endoftext|> is the document separator that the model sees during\n   * training, so if a prompt is not specified the model will generate as if from the\n   * beginning of a new document.\n   */\n  prompt: string | Array<string> | Array<number> | Array<Array<number>> | null;\n\n  /**\n   * Generates `best_of` completions server-side and returns the \"best\" (the one with\n   * the highest log probability per token). Results cannot be streamed.\n   *\n   * When used with `n`, `best_of` controls the number of candidate completions and\n   * `n` specifies how many to return  `best_of` must be greater than `n`.\n   *\n   * **Note:** Because this parameter generates many completions, it can quickly\n   * consume your token quota. Use carefully and ensure that you have reasonable\n   * settings for `max_tokens` and `stop`.\n   */\n  best_of?: number | null;\n\n  /**\n   * Echo back the prompt in addition to the completion\n   */\n  echo?: boolean | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in the GPT\n   * tokenizer) to an associated bias value from -100 to 100. You can use this\n   * [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.\n   * Mathematically, the bias is added to the logits generated by the model prior to\n   * sampling. The exact effect will vary per model, but values between -1 and 1\n   * should decrease or increase likelihood of selection; values like -100 or 100\n   * should result in a ban or exclusive selection of the relevant token.\n   *\n   * As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token\n   * from being generated.\n   */\n  logit_bias?: { [key: string]: number } | null;\n\n  /**\n   * Include the log probabilities on the `logprobs` most likely output tokens, as\n   * well the chosen tokens. For example, if `logprobs` is 5, the API will return a\n   * list of the 5 most likely tokens. The API will always return the `logprob` of\n   * the sampled token, so there may be up to `logprobs+1` elements in the response.\n   *\n   * The maximum value for `logprobs` is 5.\n   */\n  logprobs?: number | null;\n\n  /**\n   * The maximum number of [tokens](/tokenizer) that can be generated in the\n   * completion.\n   *\n   * The token count of your prompt plus `max_tokens` cannot exceed the model's\n   * context length.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n   * for counting tokens.\n   */\n  max_tokens?: number | null;\n\n  /**\n   * How many completions to generate for each prompt.\n   *\n   * **Note:** Because this parameter generates many completions, it can quickly\n   * consume your token quota. Use carefully and ensure that you have reasonable\n   * settings for `max_tokens` and `stop`.\n   */\n  n?: number | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * If specified, our system will make a best effort to sample deterministically,\n   * such that repeated requests with the same `seed` and parameters should return\n   * the same result.\n   *\n   * Determinism is not guaranteed, and you should refer to the `system_fingerprint`\n   * response parameter to monitor changes in the backend.\n   */\n  seed?: number | null;\n\n  /**\n   * Not supported with latest reasoning models `o3` and `o4-mini`.\n   *\n   * Up to 4 sequences where the API will stop generating further tokens. The\n   * returned text will not contain the stop sequence.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: boolean | null;\n\n  /**\n   * Options for streaming response. Only set this when you set `stream: true`.\n   */\n  stream_options?: CompletionsCompletionsAPI.ChatCompletionStreamOptions | null;\n\n  /**\n   * The suffix that comes after a completion of inserted text.\n   *\n   * This parameter is only supported for `gpt-3.5-turbo-instruct`.\n   */\n  suffix?: string | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace CompletionCreateParams {\n  export type CompletionCreateParamsNonStreaming = CompletionsAPI.CompletionCreateParamsNonStreaming;\n  export type CompletionCreateParamsStreaming = CompletionsAPI.CompletionCreateParamsStreaming;\n}\n\nexport interface CompletionCreateParamsNonStreaming extends CompletionCreateParamsBase {\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: false | null;\n}\n\nexport interface CompletionCreateParamsStreaming extends CompletionCreateParamsBase {\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream: true;\n}\n\nexport declare namespace Completions {\n  export {\n    type Completion as Completion,\n    type CompletionChoice as CompletionChoice,\n    type CompletionUsage as CompletionUsage,\n    type CompletionCreateParams as CompletionCreateParams,\n    type CompletionCreateParamsNonStreaming as CompletionCreateParamsNonStreaming,\n    type CompletionCreateParamsStreaming as CompletionCreateParamsStreaming,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport { APIPromise } from '../../../core/api-promise';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Content extends APIResource {\n  /**\n   * Retrieve Container File Content\n   */\n  retrieve(fileID: string, params: ContentRetrieveParams, options?: RequestOptions): APIPromise<Response> {\n    const { container_id } = params;\n    return this._client.get(path`/containers/${container_id}/files/${fileID}/content`, {\n      ...options,\n      headers: buildHeaders([{ Accept: 'application/binary' }, options?.headers]),\n      __binaryResponse: true,\n    });\n  }\n}\n\nexport interface ContentRetrieveParams {\n  container_id: string;\n}\n\nexport declare namespace Content {\n  export { type ContentRetrieveParams as ContentRetrieveParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as ContentAPI from './content';\nimport { Content, ContentRetrieveParams } from './content';\nimport { APIPromise } from '../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { type Uploadable } from '../../../core/uploads';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { maybeMultipartFormRequestOptions } from '../../../internal/uploads';\nimport { path } from '../../../internal/utils/path';\n\nexport class Files extends APIResource {\n  content: ContentAPI.Content = new ContentAPI.Content(this._client);\n\n  /**\n   * Create a Container File\n   *\n   * You can send either a multipart/form-data request with the raw file content, or\n   * a JSON request with a file ID.\n   */\n  create(\n    containerID: string,\n    body: FileCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<FileCreateResponse> {\n    return this._client.post(\n      path`/containers/${containerID}/files`,\n      maybeMultipartFormRequestOptions({ body, ...options }, this._client),\n    );\n  }\n\n  /**\n   * Retrieve Container File\n   */\n  retrieve(\n    fileID: string,\n    params: FileRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<FileRetrieveResponse> {\n    const { container_id } = params;\n    return this._client.get(path`/containers/${container_id}/files/${fileID}`, options);\n  }\n\n  /**\n   * List Container files\n   */\n  list(\n    containerID: string,\n    query: FileListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<FileListResponsesPage, FileListResponse> {\n    return this._client.getAPIList(path`/containers/${containerID}/files`, CursorPage<FileListResponse>, {\n      query,\n      ...options,\n    });\n  }\n\n  /**\n   * Delete Container File\n   */\n  delete(fileID: string, params: FileDeleteParams, options?: RequestOptions): APIPromise<void> {\n    const { container_id } = params;\n    return this._client.delete(path`/containers/${container_id}/files/${fileID}`, {\n      ...options,\n      headers: buildHeaders([{ Accept: '*/*' }, options?.headers]),\n    });\n  }\n}\n\nexport type FileListResponsesPage = CursorPage<FileListResponse>;\n\nexport interface FileCreateResponse {\n  /**\n   * Unique identifier for the file.\n   */\n  id: string;\n\n  /**\n   * Size of the file in bytes.\n   */\n  bytes: number;\n\n  /**\n   * The container this file belongs to.\n   */\n  container_id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the file was created.\n   */\n  created_at: number;\n\n  /**\n   * The type of this object (`container.file`).\n   */\n  object: 'container.file';\n\n  /**\n   * Path of the file in the container.\n   */\n  path: string;\n\n  /**\n   * Source of the file (e.g., `user`, `assistant`).\n   */\n  source: string;\n}\n\nexport interface FileRetrieveResponse {\n  /**\n   * Unique identifier for the file.\n   */\n  id: string;\n\n  /**\n   * Size of the file in bytes.\n   */\n  bytes: number;\n\n  /**\n   * The container this file belongs to.\n   */\n  container_id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the file was created.\n   */\n  created_at: number;\n\n  /**\n   * The type of this object (`container.file`).\n   */\n  object: 'container.file';\n\n  /**\n   * Path of the file in the container.\n   */\n  path: string;\n\n  /**\n   * Source of the file (e.g., `user`, `assistant`).\n   */\n  source: string;\n}\n\nexport interface FileListResponse {\n  /**\n   * Unique identifier for the file.\n   */\n  id: string;\n\n  /**\n   * Size of the file in bytes.\n   */\n  bytes: number;\n\n  /**\n   * The container this file belongs to.\n   */\n  container_id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the file was created.\n   */\n  created_at: number;\n\n  /**\n   * The type of this object (`container.file`).\n   */\n  object: 'container.file';\n\n  /**\n   * Path of the file in the container.\n   */\n  path: string;\n\n  /**\n   * Source of the file (e.g., `user`, `assistant`).\n   */\n  source: string;\n}\n\nexport interface FileCreateParams {\n  /**\n   * The File object (not file name) to be uploaded.\n   */\n  file?: Uploadable;\n\n  /**\n   * Name of the file to create.\n   */\n  file_id?: string;\n}\n\nexport interface FileRetrieveParams {\n  container_id: string;\n}\n\nexport interface FileListParams extends CursorPageParams {\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface FileDeleteParams {\n  container_id: string;\n}\n\nFiles.Content = Content;\n\nexport declare namespace Files {\n  export {\n    type FileCreateResponse as FileCreateResponse,\n    type FileRetrieveResponse as FileRetrieveResponse,\n    type FileListResponse as FileListResponse,\n    type FileListResponsesPage as FileListResponsesPage,\n    type FileCreateParams as FileCreateParams,\n    type FileRetrieveParams as FileRetrieveParams,\n    type FileListParams as FileListParams,\n    type FileDeleteParams as FileDeleteParams,\n  };\n\n  export { Content as Content, type ContentRetrieveParams as ContentRetrieveParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as ResponsesAPI from '../responses/responses';\nimport * as FilesAPI from './files/files';\nimport {\n  FileCreateParams,\n  FileCreateResponse,\n  FileDeleteParams,\n  FileListParams,\n  FileListResponse,\n  FileListResponsesPage,\n  FileRetrieveParams,\n  FileRetrieveResponse,\n  Files,\n} from './files/files';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class Containers extends APIResource {\n  files: FilesAPI.Files = new FilesAPI.Files(this._client);\n\n  /**\n   * Create Container\n   */\n  create(body: ContainerCreateParams, options?: RequestOptions): APIPromise<ContainerCreateResponse> {\n    return this._client.post('/containers', { body, ...options });\n  }\n\n  /**\n   * Retrieve Container\n   */\n  retrieve(containerID: string, options?: RequestOptions): APIPromise<ContainerRetrieveResponse> {\n    return this._client.get(path`/containers/${containerID}`, options);\n  }\n\n  /**\n   * List Containers\n   */\n  list(\n    query: ContainerListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<ContainerListResponsesPage, ContainerListResponse> {\n    return this._client.getAPIList('/containers', CursorPage<ContainerListResponse>, { query, ...options });\n  }\n\n  /**\n   * Delete Container\n   */\n  delete(containerID: string, options?: RequestOptions): APIPromise<void> {\n    return this._client.delete(path`/containers/${containerID}`, {\n      ...options,\n      headers: buildHeaders([{ Accept: '*/*' }, options?.headers]),\n    });\n  }\n}\n\nexport type ContainerListResponsesPage = CursorPage<ContainerListResponse>;\n\nexport interface ContainerCreateResponse {\n  /**\n   * Unique identifier for the container.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the container was created.\n   */\n  created_at: number;\n\n  /**\n   * Name of the container.\n   */\n  name: string;\n\n  /**\n   * The type of this object.\n   */\n  object: string;\n\n  /**\n   * Status of the container (e.g., active, deleted).\n   */\n  status: string;\n\n  /**\n   * The container will expire after this time period. The anchor is the reference\n   * point for the expiration. The minutes is the number of minutes after the anchor\n   * before the container expires.\n   */\n  expires_after?: ContainerCreateResponse.ExpiresAfter;\n\n  /**\n   * Unix timestamp (in seconds) when the container was last active.\n   */\n  last_active_at?: number;\n\n  /**\n   * The memory limit configured for the container.\n   */\n  memory_limit?: '1g' | '4g' | '16g' | '64g';\n\n  /**\n   * Network access policy for the container.\n   */\n  network_policy?: ContainerCreateResponse.NetworkPolicy;\n}\n\nexport namespace ContainerCreateResponse {\n  /**\n   * The container will expire after this time period. The anchor is the reference\n   * point for the expiration. The minutes is the number of minutes after the anchor\n   * before the container expires.\n   */\n  export interface ExpiresAfter {\n    /**\n     * The reference point for the expiration.\n     */\n    anchor?: 'last_active_at';\n\n    /**\n     * The number of minutes after the anchor before the container expires.\n     */\n    minutes?: number;\n  }\n\n  /**\n   * Network access policy for the container.\n   */\n  export interface NetworkPolicy {\n    /**\n     * The network policy mode.\n     */\n    type: 'allowlist' | 'disabled';\n\n    /**\n     * Allowed outbound domains when `type` is `allowlist`.\n     */\n    allowed_domains?: Array<string>;\n  }\n}\n\nexport interface ContainerRetrieveResponse {\n  /**\n   * Unique identifier for the container.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the container was created.\n   */\n  created_at: number;\n\n  /**\n   * Name of the container.\n   */\n  name: string;\n\n  /**\n   * The type of this object.\n   */\n  object: string;\n\n  /**\n   * Status of the container (e.g., active, deleted).\n   */\n  status: string;\n\n  /**\n   * The container will expire after this time period. The anchor is the reference\n   * point for the expiration. The minutes is the number of minutes after the anchor\n   * before the container expires.\n   */\n  expires_after?: ContainerRetrieveResponse.ExpiresAfter;\n\n  /**\n   * Unix timestamp (in seconds) when the container was last active.\n   */\n  last_active_at?: number;\n\n  /**\n   * The memory limit configured for the container.\n   */\n  memory_limit?: '1g' | '4g' | '16g' | '64g';\n\n  /**\n   * Network access policy for the container.\n   */\n  network_policy?: ContainerRetrieveResponse.NetworkPolicy;\n}\n\nexport namespace ContainerRetrieveResponse {\n  /**\n   * The container will expire after this time period. The anchor is the reference\n   * point for the expiration. The minutes is the number of minutes after the anchor\n   * before the container expires.\n   */\n  export interface ExpiresAfter {\n    /**\n     * The reference point for the expiration.\n     */\n    anchor?: 'last_active_at';\n\n    /**\n     * The number of minutes after the anchor before the container expires.\n     */\n    minutes?: number;\n  }\n\n  /**\n   * Network access policy for the container.\n   */\n  export interface NetworkPolicy {\n    /**\n     * The network policy mode.\n     */\n    type: 'allowlist' | 'disabled';\n\n    /**\n     * Allowed outbound domains when `type` is `allowlist`.\n     */\n    allowed_domains?: Array<string>;\n  }\n}\n\nexport interface ContainerListResponse {\n  /**\n   * Unique identifier for the container.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the container was created.\n   */\n  created_at: number;\n\n  /**\n   * Name of the container.\n   */\n  name: string;\n\n  /**\n   * The type of this object.\n   */\n  object: string;\n\n  /**\n   * Status of the container (e.g., active, deleted).\n   */\n  status: string;\n\n  /**\n   * The container will expire after this time period. The anchor is the reference\n   * point for the expiration. The minutes is the number of minutes after the anchor\n   * before the container expires.\n   */\n  expires_after?: ContainerListResponse.ExpiresAfter;\n\n  /**\n   * Unix timestamp (in seconds) when the container was last active.\n   */\n  last_active_at?: number;\n\n  /**\n   * The memory limit configured for the container.\n   */\n  memory_limit?: '1g' | '4g' | '16g' | '64g';\n\n  /**\n   * Network access policy for the container.\n   */\n  network_policy?: ContainerListResponse.NetworkPolicy;\n}\n\nexport namespace ContainerListResponse {\n  /**\n   * The container will expire after this time period. The anchor is the reference\n   * point for the expiration. The minutes is the number of minutes after the anchor\n   * before the container expires.\n   */\n  export interface ExpiresAfter {\n    /**\n     * The reference point for the expiration.\n     */\n    anchor?: 'last_active_at';\n\n    /**\n     * The number of minutes after the anchor before the container expires.\n     */\n    minutes?: number;\n  }\n\n  /**\n   * Network access policy for the container.\n   */\n  export interface NetworkPolicy {\n    /**\n     * The network policy mode.\n     */\n    type: 'allowlist' | 'disabled';\n\n    /**\n     * Allowed outbound domains when `type` is `allowlist`.\n     */\n    allowed_domains?: Array<string>;\n  }\n}\n\nexport interface ContainerCreateParams {\n  /**\n   * Name of the container to create.\n   */\n  name: string;\n\n  /**\n   * Container expiration time in seconds relative to the 'anchor' time.\n   */\n  expires_after?: ContainerCreateParams.ExpiresAfter;\n\n  /**\n   * IDs of files to copy to the container.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * Optional memory limit for the container. Defaults to \"1g\".\n   */\n  memory_limit?: '1g' | '4g' | '16g' | '64g';\n\n  /**\n   * Network access policy for the container.\n   */\n  network_policy?: ResponsesAPI.ContainerNetworkPolicyDisabled | ResponsesAPI.ContainerNetworkPolicyAllowlist;\n\n  /**\n   * An optional list of skills referenced by id or inline data.\n   */\n  skills?: Array<ResponsesAPI.SkillReference | ResponsesAPI.InlineSkill>;\n}\n\nexport namespace ContainerCreateParams {\n  /**\n   * Container expiration time in seconds relative to the 'anchor' time.\n   */\n  export interface ExpiresAfter {\n    /**\n     * Time anchor for the expiration time. Currently only 'last_active_at' is\n     * supported.\n     */\n    anchor: 'last_active_at';\n\n    minutes: number;\n  }\n}\n\nexport interface ContainerListParams extends CursorPageParams {\n  /**\n   * Filter results by container name.\n   */\n  name?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nContainers.Files = Files;\n\nexport declare namespace Containers {\n  export {\n    type ContainerCreateResponse as ContainerCreateResponse,\n    type ContainerRetrieveResponse as ContainerRetrieveResponse,\n    type ContainerListResponse as ContainerListResponse,\n    type ContainerListResponsesPage as ContainerListResponsesPage,\n    type ContainerCreateParams as ContainerCreateParams,\n    type ContainerListParams as ContainerListParams,\n  };\n\n  export {\n    Files as Files,\n    type FileCreateResponse as FileCreateResponse,\n    type FileRetrieveResponse as FileRetrieveResponse,\n    type FileListResponse as FileListResponse,\n    type FileListResponsesPage as FileListResponsesPage,\n    type FileCreateParams as FileCreateParams,\n    type FileRetrieveParams as FileRetrieveParams,\n    type FileListParams as FileListParams,\n    type FileDeleteParams as FileDeleteParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as ConversationsAPI from './conversations';\nimport * as ResponsesAPI from '../responses/responses';\nimport { APIPromise } from '../../core/api-promise';\nimport {\n  ConversationCursorPage,\n  type ConversationCursorPageParams,\n  PagePromise,\n} from '../../core/pagination';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class Items extends APIResource {\n  /**\n   * Create items in a conversation with the given ID.\n   */\n  create(\n    conversationID: string,\n    params: ItemCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<ConversationItemList> {\n    const { include, ...body } = params;\n    return this._client.post(path`/conversations/${conversationID}/items`, {\n      query: { include },\n      body,\n      ...options,\n    });\n  }\n\n  /**\n   * Get a single item from a conversation with the given IDs.\n   */\n  retrieve(\n    itemID: string,\n    params: ItemRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<ConversationItem> {\n    const { conversation_id, ...query } = params;\n    return this._client.get(path`/conversations/${conversation_id}/items/${itemID}`, { query, ...options });\n  }\n\n  /**\n   * List all items for a conversation with the given ID.\n   */\n  list(\n    conversationID: string,\n    query: ItemListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<ConversationItemsPage, ConversationItem> {\n    return this._client.getAPIList(\n      path`/conversations/${conversationID}/items`,\n      ConversationCursorPage<ConversationItem>,\n      { query, ...options },\n    );\n  }\n\n  /**\n   * Delete an item from a conversation with the given IDs.\n   */\n  delete(\n    itemID: string,\n    params: ItemDeleteParams,\n    options?: RequestOptions,\n  ): APIPromise<ConversationsAPI.Conversation> {\n    const { conversation_id } = params;\n    return this._client.delete(path`/conversations/${conversation_id}/items/${itemID}`, options);\n  }\n}\n\nexport type ConversationItemsPage = ConversationCursorPage<ConversationItem>;\n\n/**\n * A single item within a conversation. The set of possible types are the same as\n * the `output` type of a\n * [Response object](https://platform.openai.com/docs/api-reference/responses/object#responses/object-output).\n */\nexport type ConversationItem =\n  | ConversationsAPI.Message\n  | ResponsesAPI.ResponseFunctionToolCallItem\n  | ResponsesAPI.ResponseFunctionToolCallOutputItem\n  | ResponsesAPI.ResponseFileSearchToolCall\n  | ResponsesAPI.ResponseFunctionWebSearch\n  | ConversationItem.ImageGenerationCall\n  | ResponsesAPI.ResponseComputerToolCall\n  | ResponsesAPI.ResponseComputerToolCallOutputItem\n  | ResponsesAPI.ResponseReasoningItem\n  | ResponsesAPI.ResponseCodeInterpreterToolCall\n  | ConversationItem.LocalShellCall\n  | ConversationItem.LocalShellCallOutput\n  | ResponsesAPI.ResponseFunctionShellToolCall\n  | ResponsesAPI.ResponseFunctionShellToolCallOutput\n  | ResponsesAPI.ResponseApplyPatchToolCall\n  | ResponsesAPI.ResponseApplyPatchToolCallOutput\n  | ConversationItem.McpListTools\n  | ConversationItem.McpApprovalRequest\n  | ConversationItem.McpApprovalResponse\n  | ConversationItem.McpCall\n  | ResponsesAPI.ResponseCustomToolCall\n  | ResponsesAPI.ResponseCustomToolCallOutput;\n\nexport namespace ConversationItem {\n  /**\n   * An image generation request made by the model.\n   */\n  export interface ImageGenerationCall {\n    /**\n     * The unique ID of the image generation call.\n     */\n    id: string;\n\n    /**\n     * The generated image encoded in base64.\n     */\n    result: string | null;\n\n    /**\n     * The status of the image generation call.\n     */\n    status: 'in_progress' | 'completed' | 'generating' | 'failed';\n\n    /**\n     * The type of the image generation call. Always `image_generation_call`.\n     */\n    type: 'image_generation_call';\n  }\n\n  /**\n   * A tool call to run a command on the local shell.\n   */\n  export interface LocalShellCall {\n    /**\n     * The unique ID of the local shell call.\n     */\n    id: string;\n\n    /**\n     * Execute a shell command on the server.\n     */\n    action: LocalShellCall.Action;\n\n    /**\n     * The unique ID of the local shell tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * The status of the local shell call.\n     */\n    status: 'in_progress' | 'completed' | 'incomplete';\n\n    /**\n     * The type of the local shell call. Always `local_shell_call`.\n     */\n    type: 'local_shell_call';\n  }\n\n  export namespace LocalShellCall {\n    /**\n     * Execute a shell command on the server.\n     */\n    export interface Action {\n      /**\n       * The command to run.\n       */\n      command: Array<string>;\n\n      /**\n       * Environment variables to set for the command.\n       */\n      env: { [key: string]: string };\n\n      /**\n       * The type of the local shell action. Always `exec`.\n       */\n      type: 'exec';\n\n      /**\n       * Optional timeout in milliseconds for the command.\n       */\n      timeout_ms?: number | null;\n\n      /**\n       * Optional user to run the command as.\n       */\n      user?: string | null;\n\n      /**\n       * Optional working directory to run the command in.\n       */\n      working_directory?: string | null;\n    }\n  }\n\n  /**\n   * The output of a local shell tool call.\n   */\n  export interface LocalShellCallOutput {\n    /**\n     * The unique ID of the local shell tool call generated by the model.\n     */\n    id: string;\n\n    /**\n     * A JSON string of the output of the local shell tool call.\n     */\n    output: string;\n\n    /**\n     * The type of the local shell tool call output. Always `local_shell_call_output`.\n     */\n    type: 'local_shell_call_output';\n\n    /**\n     * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | null;\n  }\n\n  /**\n   * A list of tools available on an MCP server.\n   */\n  export interface McpListTools {\n    /**\n     * The unique ID of the list.\n     */\n    id: string;\n\n    /**\n     * The label of the MCP server.\n     */\n    server_label: string;\n\n    /**\n     * The tools available on the server.\n     */\n    tools: Array<McpListTools.Tool>;\n\n    /**\n     * The type of the item. Always `mcp_list_tools`.\n     */\n    type: 'mcp_list_tools';\n\n    /**\n     * Error message if the server could not list tools.\n     */\n    error?: string | null;\n  }\n\n  export namespace McpListTools {\n    /**\n     * A tool available on an MCP server.\n     */\n    export interface Tool {\n      /**\n       * The JSON schema describing the tool's input.\n       */\n      input_schema: unknown;\n\n      /**\n       * The name of the tool.\n       */\n      name: string;\n\n      /**\n       * Additional annotations about the tool.\n       */\n      annotations?: unknown | null;\n\n      /**\n       * The description of the tool.\n       */\n      description?: string | null;\n    }\n  }\n\n  /**\n   * A request for human approval of a tool invocation.\n   */\n  export interface McpApprovalRequest {\n    /**\n     * The unique ID of the approval request.\n     */\n    id: string;\n\n    /**\n     * A JSON string of arguments for the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool to run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server making the request.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_approval_request`.\n     */\n    type: 'mcp_approval_request';\n  }\n\n  /**\n   * A response to an MCP approval request.\n   */\n  export interface McpApprovalResponse {\n    /**\n     * The unique ID of the approval response\n     */\n    id: string;\n\n    /**\n     * The ID of the approval request being answered.\n     */\n    approval_request_id: string;\n\n    /**\n     * Whether the request was approved.\n     */\n    approve: boolean;\n\n    /**\n     * The type of the item. Always `mcp_approval_response`.\n     */\n    type: 'mcp_approval_response';\n\n    /**\n     * Optional reason for the decision.\n     */\n    reason?: string | null;\n  }\n\n  /**\n   * An invocation of a tool on an MCP server.\n   */\n  export interface McpCall {\n    /**\n     * The unique ID of the tool call.\n     */\n    id: string;\n\n    /**\n     * A JSON string of the arguments passed to the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool that was run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server running the tool.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_call`.\n     */\n    type: 'mcp_call';\n\n    /**\n     * Unique identifier for the MCP tool call approval request. Include this value in\n     * a subsequent `mcp_approval_response` input to approve or reject the\n     * corresponding tool call.\n     */\n    approval_request_id?: string | null;\n\n    /**\n     * The error from the tool call, if any.\n     */\n    error?: string | null;\n\n    /**\n     * The output from the tool call.\n     */\n    output?: string | null;\n\n    /**\n     * The status of the tool call. One of `in_progress`, `completed`, `incomplete`,\n     * `calling`, or `failed`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | 'calling' | 'failed';\n  }\n}\n\n/**\n * A list of Conversation items.\n */\nexport interface ConversationItemList {\n  /**\n   * A list of conversation items.\n   */\n  data: Array<ConversationItem>;\n\n  /**\n   * The ID of the first item in the list.\n   */\n  first_id: string;\n\n  /**\n   * Whether there are more items available.\n   */\n  has_more: boolean;\n\n  /**\n   * The ID of the last item in the list.\n   */\n  last_id: string;\n\n  /**\n   * The type of object returned, must be `list`.\n   */\n  object: 'list';\n}\n\nexport interface ItemCreateParams {\n  /**\n   * Body param: The items to add to the conversation. You may add up to 20 items at\n   * a time.\n   */\n  items: Array<ResponsesAPI.ResponseInputItem>;\n\n  /**\n   * Query param: Additional fields to include in the response. See the `include`\n   * parameter for\n   * [listing Conversation items above](https://platform.openai.com/docs/api-reference/conversations/list-items#conversations_list_items-include)\n   * for more information.\n   */\n  include?: Array<ResponsesAPI.ResponseIncludable>;\n}\n\nexport interface ItemRetrieveParams {\n  /**\n   * Path param: The ID of the conversation that contains the item.\n   */\n  conversation_id: string;\n\n  /**\n   * Query param: Additional fields to include in the response. See the `include`\n   * parameter for\n   * [listing Conversation items above](https://platform.openai.com/docs/api-reference/conversations/list-items#conversations_list_items-include)\n   * for more information.\n   */\n  include?: Array<ResponsesAPI.ResponseIncludable>;\n}\n\nexport interface ItemListParams extends ConversationCursorPageParams {\n  /**\n   * Specify additional output data to include in the model response. Currently\n   * supported values are:\n   *\n   * - `web_search_call.action.sources`: Include the sources of the web search tool\n   *   call.\n   * - `code_interpreter_call.outputs`: Includes the outputs of python code execution\n   *   in code interpreter tool call items.\n   * - `computer_call_output.output.image_url`: Include image urls from the computer\n   *   call output.\n   * - `file_search_call.results`: Include the search results of the file search tool\n   *   call.\n   * - `message.input_image.image_url`: Include image urls from the input message.\n   * - `message.output_text.logprobs`: Include logprobs with assistant messages.\n   * - `reasoning.encrypted_content`: Includes an encrypted version of reasoning\n   *   tokens in reasoning item outputs. This enables reasoning items to be used in\n   *   multi-turn conversations when using the Responses API statelessly (like when\n   *   the `store` parameter is set to `false`, or when an organization is enrolled\n   *   in the zero data retention program).\n   */\n  include?: Array<ResponsesAPI.ResponseIncludable>;\n\n  /**\n   * The order to return the input items in. Default is `desc`.\n   *\n   * - `asc`: Return the input items in ascending order.\n   * - `desc`: Return the input items in descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface ItemDeleteParams {\n  /**\n   * The ID of the conversation that contains the item.\n   */\n  conversation_id: string;\n}\n\nexport declare namespace Items {\n  export {\n    type ConversationItem as ConversationItem,\n    type ConversationItemList as ConversationItemList,\n    type ConversationItemsPage as ConversationItemsPage,\n    type ItemCreateParams as ItemCreateParams,\n    type ItemRetrieveParams as ItemRetrieveParams,\n    type ItemListParams as ItemListParams,\n    type ItemDeleteParams as ItemDeleteParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as Shared from '../shared';\nimport * as ItemsAPI from './items';\nimport {\n  ConversationItem,\n  ConversationItemList,\n  ConversationItemsPage,\n  ItemCreateParams,\n  ItemDeleteParams,\n  ItemListParams,\n  ItemRetrieveParams,\n  Items,\n} from './items';\nimport * as ResponsesAPI from '../responses/responses';\nimport { APIPromise } from '../../core/api-promise';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class Conversations extends APIResource {\n  items: ItemsAPI.Items = new ItemsAPI.Items(this._client);\n\n  /**\n   * Create a conversation.\n   */\n  create(\n    body: ConversationCreateParams | null | undefined = {},\n    options?: RequestOptions,\n  ): APIPromise<Conversation> {\n    return this._client.post('/conversations', { body, ...options });\n  }\n\n  /**\n   * Get a conversation\n   */\n  retrieve(conversationID: string, options?: RequestOptions): APIPromise<Conversation> {\n    return this._client.get(path`/conversations/${conversationID}`, options);\n  }\n\n  /**\n   * Update a conversation\n   */\n  update(\n    conversationID: string,\n    body: ConversationUpdateParams,\n    options?: RequestOptions,\n  ): APIPromise<Conversation> {\n    return this._client.post(path`/conversations/${conversationID}`, { body, ...options });\n  }\n\n  /**\n   * Delete a conversation. Items in the conversation will not be deleted.\n   */\n  delete(conversationID: string, options?: RequestOptions): APIPromise<ConversationDeletedResource> {\n    return this._client.delete(path`/conversations/${conversationID}`, options);\n  }\n}\n\n/**\n * A screenshot of a computer.\n */\nexport interface ComputerScreenshotContent {\n  /**\n   * The identifier of an uploaded file that contains the screenshot.\n   */\n  file_id: string | null;\n\n  /**\n   * The URL of the screenshot image.\n   */\n  image_url: string | null;\n\n  /**\n   * Specifies the event type. For a computer screenshot, this property is always set\n   * to `computer_screenshot`.\n   */\n  type: 'computer_screenshot';\n}\n\nexport interface Conversation {\n  /**\n   * The unique ID of the conversation.\n   */\n  id: string;\n\n  /**\n   * The time at which the conversation was created, measured in seconds since the\n   * Unix epoch.\n   */\n  created_at: number;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\n   * length of 64 characters. Values are strings with a maximum length of 512\n   * characters.\n   */\n  metadata: unknown;\n\n  /**\n   * The object type, which is always `conversation`.\n   */\n  object: 'conversation';\n}\n\nexport interface ConversationDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'conversation.deleted';\n}\n\nexport interface ConversationDeletedResource {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'conversation.deleted';\n}\n\n/**\n * A message to or from the model.\n */\nexport interface Message {\n  /**\n   * The unique ID of the message.\n   */\n  id: string;\n\n  /**\n   * The content of the message\n   */\n  content: Array<\n    | ResponsesAPI.ResponseInputText\n    | ResponsesAPI.ResponseOutputText\n    | TextContent\n    | SummaryTextContent\n    | Message.ReasoningText\n    | ResponsesAPI.ResponseOutputRefusal\n    | ResponsesAPI.ResponseInputImage\n    | ComputerScreenshotContent\n    | ResponsesAPI.ResponseInputFile\n  >;\n\n  /**\n   * The role of the message. One of `unknown`, `user`, `assistant`, `system`,\n   * `critic`, `discriminator`, `developer`, or `tool`.\n   */\n  role: 'unknown' | 'user' | 'assistant' | 'system' | 'critic' | 'discriminator' | 'developer' | 'tool';\n\n  /**\n   * The status of item. One of `in_progress`, `completed`, or `incomplete`.\n   * Populated when items are returned via API.\n   */\n  status: 'in_progress' | 'completed' | 'incomplete';\n\n  /**\n   * The type of the message. Always set to `message`.\n   */\n  type: 'message';\n}\n\nexport namespace Message {\n  /**\n   * Reasoning text from the model.\n   */\n  export interface ReasoningText {\n    /**\n     * The reasoning text from the model.\n     */\n    text: string;\n\n    /**\n     * The type of the reasoning text. Always `reasoning_text`.\n     */\n    type: 'reasoning_text';\n  }\n}\n\n/**\n * A summary text from the model.\n */\nexport interface SummaryTextContent {\n  /**\n   * A summary of the reasoning output from the model so far.\n   */\n  text: string;\n\n  /**\n   * The type of the object. Always `summary_text`.\n   */\n  type: 'summary_text';\n}\n\n/**\n * A text content.\n */\nexport interface TextContent {\n  text: string;\n\n  type: 'text';\n}\n\nexport type InputTextContent = ResponsesAPI.ResponseInputText;\n\nexport type OutputTextContent = ResponsesAPI.ResponseOutputText;\n\nexport type RefusalContent = ResponsesAPI.ResponseOutputRefusal;\n\nexport type InputImageContent = ResponsesAPI.ResponseInputImage;\n\nexport type InputFileContent = ResponsesAPI.ResponseInputFile;\n\nexport interface ConversationCreateParams {\n  /**\n   * Initial items to include in the conversation context. You may add up to 20 items\n   * at a time.\n   */\n  items?: Array<ResponsesAPI.ResponseInputItem> | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n}\n\nexport interface ConversationUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n}\n\nConversations.Items = Items;\n\nexport declare namespace Conversations {\n  export {\n    type ComputerScreenshotContent as ComputerScreenshotContent,\n    type Conversation as Conversation,\n    type ConversationDeleted as ConversationDeleted,\n    type ConversationDeletedResource as ConversationDeletedResource,\n    type Message as Message,\n    type SummaryTextContent as SummaryTextContent,\n    type TextContent as TextContent,\n    type InputTextContent as InputTextContent,\n    type OutputTextContent as OutputTextContent,\n    type RefusalContent as RefusalContent,\n    type InputImageContent as InputImageContent,\n    type InputFileContent as InputFileContent,\n    type ConversationCreateParams as ConversationCreateParams,\n    type ConversationUpdateParams as ConversationUpdateParams,\n  };\n\n  export {\n    Items as Items,\n    type ConversationItem as ConversationItem,\n    type ConversationItemList as ConversationItemList,\n    type ConversationItemsPage as ConversationItemsPage,\n    type ItemCreateParams as ItemCreateParams,\n    type ItemRetrieveParams as ItemRetrieveParams,\n    type ItemListParams as ItemListParams,\n    type ItemDeleteParams as ItemDeleteParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport { APIPromise } from '../core/api-promise';\nimport { RequestOptions } from '../internal/request-options';\nimport { loggerFor, toFloat32Array } from '../internal/utils';\n\nexport class Embeddings extends APIResource {\n  /**\n   * Creates an embedding vector representing the input text.\n   *\n   * @example\n   * ```ts\n   * const createEmbeddingResponse =\n   *   await client.embeddings.create({\n   *     input: 'The quick brown fox jumped over the lazy dog',\n   *     model: 'text-embedding-3-small',\n   *   });\n   * ```\n   */\n  create(body: EmbeddingCreateParams, options?: RequestOptions): APIPromise<CreateEmbeddingResponse> {\n    const hasUserProvidedEncodingFormat = !!body.encoding_format;\n    // No encoding_format specified, defaulting to base64 for performance reasons\n    // See https://github.com/openai/openai-node/pull/1312\n    let encoding_format: EmbeddingCreateParams['encoding_format'] =\n      hasUserProvidedEncodingFormat ? body.encoding_format : 'base64';\n\n    if (hasUserProvidedEncodingFormat) {\n      loggerFor(this._client).debug('embeddings/user defined encoding_format:', body.encoding_format);\n    }\n\n    const response: APIPromise<CreateEmbeddingResponse> = this._client.post('/embeddings', {\n      body: {\n        ...body,\n        encoding_format: encoding_format as EmbeddingCreateParams['encoding_format'],\n      },\n      ...options,\n    });\n\n    // if the user specified an encoding_format, return the response as-is\n    if (hasUserProvidedEncodingFormat) {\n      return response;\n    }\n\n    // in this stage, we are sure the user did not specify an encoding_format\n    // and we defaulted to base64 for performance reasons\n    // we are sure then that the response is base64 encoded, let's decode it\n    // the returned result will be a float32 array since this is OpenAI API's default encoding\n    loggerFor(this._client).debug('embeddings/decoding base64 embeddings from base64');\n\n    return (response as APIPromise<CreateEmbeddingResponse>)._thenUnwrap((response) => {\n      if (response && response.data) {\n        response.data.forEach((embeddingBase64Obj) => {\n          const embeddingBase64Str = embeddingBase64Obj.embedding as unknown as string;\n          embeddingBase64Obj.embedding = toFloat32Array(embeddingBase64Str);\n        });\n      }\n\n      return response;\n    });\n  }\n}\n\nexport interface CreateEmbeddingResponse {\n  /**\n   * The list of embeddings generated by the model.\n   */\n  data: Array<Embedding>;\n\n  /**\n   * The name of the model used to generate the embedding.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"list\".\n   */\n  object: 'list';\n\n  /**\n   * The usage information for the request.\n   */\n  usage: CreateEmbeddingResponse.Usage;\n}\n\nexport namespace CreateEmbeddingResponse {\n  /**\n   * The usage information for the request.\n   */\n  export interface Usage {\n    /**\n     * The number of tokens used by the prompt.\n     */\n    prompt_tokens: number;\n\n    /**\n     * The total number of tokens used by the request.\n     */\n    total_tokens: number;\n  }\n}\n\n/**\n * Represents an embedding vector returned by embedding endpoint.\n */\nexport interface Embedding {\n  /**\n   * The embedding vector, which is a list of floats. The length of vector depends on\n   * the model as listed in the\n   * [embedding guide](https://platform.openai.com/docs/guides/embeddings).\n   */\n  embedding: Array<number>;\n\n  /**\n   * The index of the embedding in the list of embeddings.\n   */\n  index: number;\n\n  /**\n   * The object type, which is always \"embedding\".\n   */\n  object: 'embedding';\n}\n\nexport type EmbeddingModel = 'text-embedding-ada-002' | 'text-embedding-3-small' | 'text-embedding-3-large';\n\nexport interface EmbeddingCreateParams {\n  /**\n   * Input text to embed, encoded as a string or array of tokens. To embed multiple\n   * inputs in a single request, pass an array of strings or array of token arrays.\n   * The input must not exceed the max input tokens for the model (8192 tokens for\n   * all embedding models), cannot be an empty string, and any array must be 2048\n   * dimensions or less.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n   * for counting tokens. In addition to the per-input token limit, all embedding\n   * models enforce a maximum of 300,000 tokens summed across all inputs in a single\n   * request.\n   */\n  input: string | Array<string> | Array<number> | Array<Array<number>>;\n\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models) for descriptions of\n   * them.\n   */\n  model: (string & {}) | EmbeddingModel;\n\n  /**\n   * The number of dimensions the resulting output embeddings should have. Only\n   * supported in `text-embedding-3` and later models.\n   */\n  dimensions?: number;\n\n  /**\n   * The format to return the embeddings in. Can be either `float` or\n   * [`base64`](https://pypi.org/project/pybase64/).\n   */\n  encoding_format?: 'float' | 'base64';\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\n   */\n  user?: string;\n}\n\nexport declare namespace Embeddings {\n  export {\n    type CreateEmbeddingResponse as CreateEmbeddingResponse,\n    type Embedding as Embedding,\n    type EmbeddingModel as EmbeddingModel,\n    type EmbeddingCreateParams as EmbeddingCreateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as RunsAPI from './runs';\nimport { APIPromise } from '../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class OutputItems extends APIResource {\n  /**\n   * Get an evaluation run output item by ID.\n   */\n  retrieve(\n    outputItemID: string,\n    params: OutputItemRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<OutputItemRetrieveResponse> {\n    const { eval_id, run_id } = params;\n    return this._client.get(path`/evals/${eval_id}/runs/${run_id}/output_items/${outputItemID}`, options);\n  }\n\n  /**\n   * Get a list of output items for an evaluation run.\n   */\n  list(\n    runID: string,\n    params: OutputItemListParams,\n    options?: RequestOptions,\n  ): PagePromise<OutputItemListResponsesPage, OutputItemListResponse> {\n    const { eval_id, ...query } = params;\n    return this._client.getAPIList(\n      path`/evals/${eval_id}/runs/${runID}/output_items`,\n      CursorPage<OutputItemListResponse>,\n      { query, ...options },\n    );\n  }\n}\n\nexport type OutputItemListResponsesPage = CursorPage<OutputItemListResponse>;\n\n/**\n * A schema representing an evaluation run output item.\n */\nexport interface OutputItemRetrieveResponse {\n  /**\n   * Unique identifier for the evaluation run output item.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the evaluation run was created.\n   */\n  created_at: number;\n\n  /**\n   * Details of the input data source item.\n   */\n  datasource_item: { [key: string]: unknown };\n\n  /**\n   * The identifier for the data source item.\n   */\n  datasource_item_id: number;\n\n  /**\n   * The identifier of the evaluation group.\n   */\n  eval_id: string;\n\n  /**\n   * The type of the object. Always \"eval.run.output_item\".\n   */\n  object: 'eval.run.output_item';\n\n  /**\n   * A list of grader results for this output item.\n   */\n  results: Array<OutputItemRetrieveResponse.Result>;\n\n  /**\n   * The identifier of the evaluation run associated with this output item.\n   */\n  run_id: string;\n\n  /**\n   * A sample containing the input and output of the evaluation run.\n   */\n  sample: OutputItemRetrieveResponse.Sample;\n\n  /**\n   * The status of the evaluation run.\n   */\n  status: string;\n}\n\nexport namespace OutputItemRetrieveResponse {\n  /**\n   * A single grader result for an evaluation run output item.\n   */\n  export interface Result {\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * Whether the grader considered the output a pass.\n     */\n    passed: boolean;\n\n    /**\n     * The numeric score produced by the grader.\n     */\n    score: number;\n\n    /**\n     * Optional sample or intermediate data produced by the grader.\n     */\n    sample?: { [key: string]: unknown } | null;\n\n    /**\n     * The grader type (for example, \"string-check-grader\").\n     */\n    type?: string;\n\n    [k: string]: unknown;\n  }\n\n  /**\n   * A sample containing the input and output of the evaluation run.\n   */\n  export interface Sample {\n    /**\n     * An object representing an error response from the Eval API.\n     */\n    error: RunsAPI.EvalAPIError;\n\n    /**\n     * The reason why the sample generation was finished.\n     */\n    finish_reason: string;\n\n    /**\n     * An array of input messages.\n     */\n    input: Array<Sample.Input>;\n\n    /**\n     * The maximum number of tokens allowed for completion.\n     */\n    max_completion_tokens: number;\n\n    /**\n     * The model used for generating the sample.\n     */\n    model: string;\n\n    /**\n     * An array of output messages.\n     */\n    output: Array<Sample.Output>;\n\n    /**\n     * The seed used for generating the sample.\n     */\n    seed: number;\n\n    /**\n     * The sampling temperature used.\n     */\n    temperature: number;\n\n    /**\n     * The top_p value used for sampling.\n     */\n    top_p: number;\n\n    /**\n     * Token usage details for the sample.\n     */\n    usage: Sample.Usage;\n  }\n\n  export namespace Sample {\n    /**\n     * An input message.\n     */\n    export interface Input {\n      /**\n       * The content of the message.\n       */\n      content: string;\n\n      /**\n       * The role of the message sender (e.g., system, user, developer).\n       */\n      role: string;\n    }\n\n    export interface Output {\n      /**\n       * The content of the message.\n       */\n      content?: string;\n\n      /**\n       * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n       */\n      role?: string;\n    }\n\n    /**\n     * Token usage details for the sample.\n     */\n    export interface Usage {\n      /**\n       * The number of tokens retrieved from cache.\n       */\n      cached_tokens: number;\n\n      /**\n       * The number of completion tokens generated.\n       */\n      completion_tokens: number;\n\n      /**\n       * The number of prompt tokens used.\n       */\n      prompt_tokens: number;\n\n      /**\n       * The total number of tokens used.\n       */\n      total_tokens: number;\n    }\n  }\n}\n\n/**\n * A schema representing an evaluation run output item.\n */\nexport interface OutputItemListResponse {\n  /**\n   * Unique identifier for the evaluation run output item.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the evaluation run was created.\n   */\n  created_at: number;\n\n  /**\n   * Details of the input data source item.\n   */\n  datasource_item: { [key: string]: unknown };\n\n  /**\n   * The identifier for the data source item.\n   */\n  datasource_item_id: number;\n\n  /**\n   * The identifier of the evaluation group.\n   */\n  eval_id: string;\n\n  /**\n   * The type of the object. Always \"eval.run.output_item\".\n   */\n  object: 'eval.run.output_item';\n\n  /**\n   * A list of grader results for this output item.\n   */\n  results: Array<OutputItemListResponse.Result>;\n\n  /**\n   * The identifier of the evaluation run associated with this output item.\n   */\n  run_id: string;\n\n  /**\n   * A sample containing the input and output of the evaluation run.\n   */\n  sample: OutputItemListResponse.Sample;\n\n  /**\n   * The status of the evaluation run.\n   */\n  status: string;\n}\n\nexport namespace OutputItemListResponse {\n  /**\n   * A single grader result for an evaluation run output item.\n   */\n  export interface Result {\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * Whether the grader considered the output a pass.\n     */\n    passed: boolean;\n\n    /**\n     * The numeric score produced by the grader.\n     */\n    score: number;\n\n    /**\n     * Optional sample or intermediate data produced by the grader.\n     */\n    sample?: { [key: string]: unknown } | null;\n\n    /**\n     * The grader type (for example, \"string-check-grader\").\n     */\n    type?: string;\n\n    [k: string]: unknown;\n  }\n\n  /**\n   * A sample containing the input and output of the evaluation run.\n   */\n  export interface Sample {\n    /**\n     * An object representing an error response from the Eval API.\n     */\n    error: RunsAPI.EvalAPIError;\n\n    /**\n     * The reason why the sample generation was finished.\n     */\n    finish_reason: string;\n\n    /**\n     * An array of input messages.\n     */\n    input: Array<Sample.Input>;\n\n    /**\n     * The maximum number of tokens allowed for completion.\n     */\n    max_completion_tokens: number;\n\n    /**\n     * The model used for generating the sample.\n     */\n    model: string;\n\n    /**\n     * An array of output messages.\n     */\n    output: Array<Sample.Output>;\n\n    /**\n     * The seed used for generating the sample.\n     */\n    seed: number;\n\n    /**\n     * The sampling temperature used.\n     */\n    temperature: number;\n\n    /**\n     * The top_p value used for sampling.\n     */\n    top_p: number;\n\n    /**\n     * Token usage details for the sample.\n     */\n    usage: Sample.Usage;\n  }\n\n  export namespace Sample {\n    /**\n     * An input message.\n     */\n    export interface Input {\n      /**\n       * The content of the message.\n       */\n      content: string;\n\n      /**\n       * The role of the message sender (e.g., system, user, developer).\n       */\n      role: string;\n    }\n\n    export interface Output {\n      /**\n       * The content of the message.\n       */\n      content?: string;\n\n      /**\n       * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n       */\n      role?: string;\n    }\n\n    /**\n     * Token usage details for the sample.\n     */\n    export interface Usage {\n      /**\n       * The number of tokens retrieved from cache.\n       */\n      cached_tokens: number;\n\n      /**\n       * The number of completion tokens generated.\n       */\n      completion_tokens: number;\n\n      /**\n       * The number of prompt tokens used.\n       */\n      prompt_tokens: number;\n\n      /**\n       * The total number of tokens used.\n       */\n      total_tokens: number;\n    }\n  }\n}\n\nexport interface OutputItemRetrieveParams {\n  /**\n   * The ID of the evaluation to retrieve runs for.\n   */\n  eval_id: string;\n\n  /**\n   * The ID of the run to retrieve.\n   */\n  run_id: string;\n}\n\nexport interface OutputItemListParams extends CursorPageParams {\n  /**\n   * Path param: The ID of the evaluation to retrieve runs for.\n   */\n  eval_id: string;\n\n  /**\n   * Query param: Sort order for output items by timestamp. Use `asc` for ascending\n   * order or `desc` for descending order. Defaults to `asc`.\n   */\n  order?: 'asc' | 'desc';\n\n  /**\n   * Query param: Filter output items by status. Use `failed` to filter by failed\n   * output items or `pass` to filter by passed output items.\n   */\n  status?: 'fail' | 'pass';\n}\n\nexport declare namespace OutputItems {\n  export {\n    type OutputItemRetrieveResponse as OutputItemRetrieveResponse,\n    type OutputItemListResponse as OutputItemListResponse,\n    type OutputItemListResponsesPage as OutputItemListResponsesPage,\n    type OutputItemRetrieveParams as OutputItemRetrieveParams,\n    type OutputItemListParams as OutputItemListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as Shared from '../../shared';\nimport * as GraderModelsAPI from '../../graders/grader-models';\nimport * as ResponsesAPI from '../../responses/responses';\nimport * as CompletionsAPI from '../../chat/completions/completions';\nimport * as OutputItemsAPI from './output-items';\nimport {\n  OutputItemListParams,\n  OutputItemListResponse,\n  OutputItemListResponsesPage,\n  OutputItemRetrieveParams,\n  OutputItemRetrieveResponse,\n  OutputItems,\n} from './output-items';\nimport { APIPromise } from '../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Runs extends APIResource {\n  outputItems: OutputItemsAPI.OutputItems = new OutputItemsAPI.OutputItems(this._client);\n\n  /**\n   * Kicks off a new run for a given evaluation, specifying the data source, and what\n   * model configuration to use to test. The datasource will be validated against the\n   * schema specified in the config of the evaluation.\n   */\n  create(evalID: string, body: RunCreateParams, options?: RequestOptions): APIPromise<RunCreateResponse> {\n    return this._client.post(path`/evals/${evalID}/runs`, { body, ...options });\n  }\n\n  /**\n   * Get an evaluation run by ID.\n   */\n  retrieve(\n    runID: string,\n    params: RunRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<RunRetrieveResponse> {\n    const { eval_id } = params;\n    return this._client.get(path`/evals/${eval_id}/runs/${runID}`, options);\n  }\n\n  /**\n   * Get a list of runs for an evaluation.\n   */\n  list(\n    evalID: string,\n    query: RunListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<RunListResponsesPage, RunListResponse> {\n    return this._client.getAPIList(path`/evals/${evalID}/runs`, CursorPage<RunListResponse>, {\n      query,\n      ...options,\n    });\n  }\n\n  /**\n   * Delete an eval run.\n   */\n  delete(runID: string, params: RunDeleteParams, options?: RequestOptions): APIPromise<RunDeleteResponse> {\n    const { eval_id } = params;\n    return this._client.delete(path`/evals/${eval_id}/runs/${runID}`, options);\n  }\n\n  /**\n   * Cancel an ongoing evaluation run.\n   */\n  cancel(runID: string, params: RunCancelParams, options?: RequestOptions): APIPromise<RunCancelResponse> {\n    const { eval_id } = params;\n    return this._client.post(path`/evals/${eval_id}/runs/${runID}`, options);\n  }\n}\n\nexport type RunListResponsesPage = CursorPage<RunListResponse>;\n\n/**\n * A CompletionsRunDataSource object describing a model sampling configuration.\n */\nexport interface CreateEvalCompletionsRunDataSource {\n  /**\n   * Determines what populates the `item` namespace in this run's data source.\n   */\n  source:\n    | CreateEvalCompletionsRunDataSource.FileContent\n    | CreateEvalCompletionsRunDataSource.FileID\n    | CreateEvalCompletionsRunDataSource.StoredCompletions;\n\n  /**\n   * The type of run data source. Always `completions`.\n   */\n  type: 'completions';\n\n  /**\n   * Used when sampling from a model. Dictates the structure of the messages passed\n   * into the model. Can either be a reference to a prebuilt trajectory (ie,\n   * `item.input_trajectory`), or a template with variable references to the `item`\n   * namespace.\n   */\n  input_messages?:\n    | CreateEvalCompletionsRunDataSource.Template\n    | CreateEvalCompletionsRunDataSource.ItemReference;\n\n  /**\n   * The name of the model to use for generating completions (e.g. \"o3-mini\").\n   */\n  model?: string;\n\n  sampling_params?: CreateEvalCompletionsRunDataSource.SamplingParams;\n}\n\nexport namespace CreateEvalCompletionsRunDataSource {\n  export interface FileContent {\n    /**\n     * The content of the jsonl file.\n     */\n    content: Array<FileContent.Content>;\n\n    /**\n     * The type of jsonl source. Always `file_content`.\n     */\n    type: 'file_content';\n  }\n\n  export namespace FileContent {\n    export interface Content {\n      item: { [key: string]: unknown };\n\n      sample?: { [key: string]: unknown };\n    }\n  }\n\n  export interface FileID {\n    /**\n     * The identifier of the file.\n     */\n    id: string;\n\n    /**\n     * The type of jsonl source. Always `file_id`.\n     */\n    type: 'file_id';\n  }\n\n  /**\n   * A StoredCompletionsRunDataSource configuration describing a set of filters\n   */\n  export interface StoredCompletions {\n    /**\n     * The type of source. Always `stored_completions`.\n     */\n    type: 'stored_completions';\n\n    /**\n     * An optional Unix timestamp to filter items created after this time.\n     */\n    created_after?: number | null;\n\n    /**\n     * An optional Unix timestamp to filter items created before this time.\n     */\n    created_before?: number | null;\n\n    /**\n     * An optional maximum number of items to return.\n     */\n    limit?: number | null;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n\n    /**\n     * An optional model to filter by (e.g., 'gpt-4o').\n     */\n    model?: string | null;\n  }\n\n  export interface Template {\n    /**\n     * A list of chat messages forming the prompt or context. May include variable\n     * references to the `item` namespace, ie {{item.name}}.\n     */\n    template: Array<ResponsesAPI.EasyInputMessage | Template.EvalItem>;\n\n    /**\n     * The type of input messages. Always `template`.\n     */\n    type: 'template';\n  }\n\n  export namespace Template {\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface EvalItem {\n      /**\n       * Inputs to the model - can contain template strings. Supports text, output text,\n       * input images, and input audio, either as a single item or an array of items.\n       */\n      content:\n        | string\n        | ResponsesAPI.ResponseInputText\n        | EvalItem.OutputText\n        | EvalItem.InputImage\n        | ResponsesAPI.ResponseInputAudio\n        | GraderModelsAPI.GraderInputs;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace EvalItem {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n\n      /**\n       * An image input block used within EvalItem content arrays.\n       */\n      export interface InputImage {\n        /**\n         * The URL of the image input.\n         */\n        image_url: string;\n\n        /**\n         * The type of the image input. Always `input_image`.\n         */\n        type: 'input_image';\n\n        /**\n         * The detail level of the image to be sent to the model. One of `high`, `low`, or\n         * `auto`. Defaults to `auto`.\n         */\n        detail?: string;\n      }\n    }\n  }\n\n  export interface ItemReference {\n    /**\n     * A reference to a variable in the `item` namespace. Ie, \"item.input_trajectory\"\n     */\n    item_reference: string;\n\n    /**\n     * The type of input messages. Always `item_reference`.\n     */\n    type: 'item_reference';\n  }\n\n  export interface SamplingParams {\n    /**\n     * The maximum number of tokens in the generated output.\n     */\n    max_completion_tokens?: number;\n\n    /**\n     * Constrains effort on reasoning for\n     * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n     * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n     * Reducing reasoning effort can result in faster responses and fewer tokens used\n     * on reasoning in a response.\n     *\n     * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n     *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n     *   calls are supported for all reasoning values in gpt-5.1.\n     * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n     *   support `none`.\n     * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n     * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n     */\n    reasoning_effort?: Shared.ReasoningEffort | null;\n\n    /**\n     * An object specifying the format that the model must output.\n     *\n     * Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\n     * Outputs which ensures the model will match your supplied JSON schema. Learn more\n     * in the\n     * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n     *\n     * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n     * ensures the message the model generates is valid JSON. Using `json_schema` is\n     * preferred for models that support it.\n     */\n    response_format?:\n      | Shared.ResponseFormatText\n      | Shared.ResponseFormatJSONSchema\n      | Shared.ResponseFormatJSONObject;\n\n    /**\n     * A seed value to initialize the randomness, during sampling.\n     */\n    seed?: number;\n\n    /**\n     * A higher temperature increases randomness in the outputs.\n     */\n    temperature?: number;\n\n    /**\n     * A list of tools the model may call. Currently, only functions are supported as a\n     * tool. Use this to provide a list of functions the model may generate JSON inputs\n     * for. A max of 128 functions are supported.\n     */\n    tools?: Array<CompletionsAPI.ChatCompletionFunctionTool>;\n\n    /**\n     * An alternative to temperature for nucleus sampling; 1.0 includes all tokens.\n     */\n    top_p?: number;\n  }\n}\n\n/**\n * A JsonlRunDataSource object with that specifies a JSONL file that matches the\n * eval\n */\nexport interface CreateEvalJSONLRunDataSource {\n  /**\n   * Determines what populates the `item` namespace in the data source.\n   */\n  source: CreateEvalJSONLRunDataSource.FileContent | CreateEvalJSONLRunDataSource.FileID;\n\n  /**\n   * The type of data source. Always `jsonl`.\n   */\n  type: 'jsonl';\n}\n\nexport namespace CreateEvalJSONLRunDataSource {\n  export interface FileContent {\n    /**\n     * The content of the jsonl file.\n     */\n    content: Array<FileContent.Content>;\n\n    /**\n     * The type of jsonl source. Always `file_content`.\n     */\n    type: 'file_content';\n  }\n\n  export namespace FileContent {\n    export interface Content {\n      item: { [key: string]: unknown };\n\n      sample?: { [key: string]: unknown };\n    }\n  }\n\n  export interface FileID {\n    /**\n     * The identifier of the file.\n     */\n    id: string;\n\n    /**\n     * The type of jsonl source. Always `file_id`.\n     */\n    type: 'file_id';\n  }\n}\n\n/**\n * An object representing an error response from the Eval API.\n */\nexport interface EvalAPIError {\n  /**\n   * The error code.\n   */\n  code: string;\n\n  /**\n   * The error message.\n   */\n  message: string;\n}\n\n/**\n * A schema representing an evaluation run.\n */\nexport interface RunCreateResponse {\n  /**\n   * Unique identifier for the evaluation run.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the evaluation run was created.\n   */\n  created_at: number;\n\n  /**\n   * Information about the run's data source.\n   */\n  data_source:\n    | CreateEvalJSONLRunDataSource\n    | CreateEvalCompletionsRunDataSource\n    | RunCreateResponse.Responses;\n\n  /**\n   * An object representing an error response from the Eval API.\n   */\n  error: EvalAPIError;\n\n  /**\n   * The identifier of the associated evaluation.\n   */\n  eval_id: string;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The model that is evaluated, if applicable.\n   */\n  model: string;\n\n  /**\n   * The name of the evaluation run.\n   */\n  name: string;\n\n  /**\n   * The type of the object. Always \"eval.run\".\n   */\n  object: 'eval.run';\n\n  /**\n   * Usage statistics for each model during the evaluation run.\n   */\n  per_model_usage: Array<RunCreateResponse.PerModelUsage>;\n\n  /**\n   * Results per testing criteria applied during the evaluation run.\n   */\n  per_testing_criteria_results: Array<RunCreateResponse.PerTestingCriteriaResult>;\n\n  /**\n   * The URL to the rendered evaluation run report on the UI dashboard.\n   */\n  report_url: string;\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  result_counts: RunCreateResponse.ResultCounts;\n\n  /**\n   * The status of the evaluation run.\n   */\n  status: string;\n}\n\nexport namespace RunCreateResponse {\n  /**\n   * A ResponsesRunDataSource object describing a model sampling configuration.\n   */\n  export interface Responses {\n    /**\n     * Determines what populates the `item` namespace in this run's data source.\n     */\n    source: Responses.FileContent | Responses.FileID | Responses.Responses;\n\n    /**\n     * The type of run data source. Always `responses`.\n     */\n    type: 'responses';\n\n    /**\n     * Used when sampling from a model. Dictates the structure of the messages passed\n     * into the model. Can either be a reference to a prebuilt trajectory (ie,\n     * `item.input_trajectory`), or a template with variable references to the `item`\n     * namespace.\n     */\n    input_messages?: Responses.Template | Responses.ItemReference;\n\n    /**\n     * The name of the model to use for generating completions (e.g. \"o3-mini\").\n     */\n    model?: string;\n\n    sampling_params?: Responses.SamplingParams;\n  }\n\n  export namespace Responses {\n    export interface FileContent {\n      /**\n       * The content of the jsonl file.\n       */\n      content: Array<FileContent.Content>;\n\n      /**\n       * The type of jsonl source. Always `file_content`.\n       */\n      type: 'file_content';\n    }\n\n    export namespace FileContent {\n      export interface Content {\n        item: { [key: string]: unknown };\n\n        sample?: { [key: string]: unknown };\n      }\n    }\n\n    export interface FileID {\n      /**\n       * The identifier of the file.\n       */\n      id: string;\n\n      /**\n       * The type of jsonl source. Always `file_id`.\n       */\n      type: 'file_id';\n    }\n\n    /**\n     * A EvalResponsesSource object describing a run data source configuration.\n     */\n    export interface Responses {\n      /**\n       * The type of run data source. Always `responses`.\n       */\n      type: 'responses';\n\n      /**\n       * Only include items created after this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_after?: number | null;\n\n      /**\n       * Only include items created before this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_before?: number | null;\n\n      /**\n       * Optional string to search the 'instructions' field. This is a query parameter\n       * used to select responses.\n       */\n      instructions_search?: string | null;\n\n      /**\n       * Metadata filter for the responses. This is a query parameter used to select\n       * responses.\n       */\n      metadata?: unknown | null;\n\n      /**\n       * The name of the model to find responses for. This is a query parameter used to\n       * select responses.\n       */\n      model?: string | null;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * Sampling temperature. This is a query parameter used to select responses.\n       */\n      temperature?: number | null;\n\n      /**\n       * List of tool names. This is a query parameter used to select responses.\n       */\n      tools?: Array<string> | null;\n\n      /**\n       * Nucleus sampling parameter. This is a query parameter used to select responses.\n       */\n      top_p?: number | null;\n\n      /**\n       * List of user identifiers. This is a query parameter used to select responses.\n       */\n      users?: Array<string> | null;\n    }\n\n    export interface Template {\n      /**\n       * A list of chat messages forming the prompt or context. May include variable\n       * references to the `item` namespace, ie {{item.name}}.\n       */\n      template: Array<Template.ChatMessage | Template.EvalItem>;\n\n      /**\n       * The type of input messages. Always `template`.\n       */\n      type: 'template';\n    }\n\n    export namespace Template {\n      export interface ChatMessage {\n        /**\n         * The content of the message.\n         */\n        content: string;\n\n        /**\n         * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n         */\n        role: string;\n      }\n\n      /**\n       * A message input to the model with a role indicating instruction following\n       * hierarchy. Instructions given with the `developer` or `system` role take\n       * precedence over instructions given with the `user` role. Messages with the\n       * `assistant` role are presumed to have been generated by the model in previous\n       * interactions.\n       */\n      export interface EvalItem {\n        /**\n         * Inputs to the model - can contain template strings. Supports text, output text,\n         * input images, and input audio, either as a single item or an array of items.\n         */\n        content:\n          | string\n          | ResponsesAPI.ResponseInputText\n          | EvalItem.OutputText\n          | EvalItem.InputImage\n          | ResponsesAPI.ResponseInputAudio\n          | GraderModelsAPI.GraderInputs;\n\n        /**\n         * The role of the message input. One of `user`, `assistant`, `system`, or\n         * `developer`.\n         */\n        role: 'user' | 'assistant' | 'system' | 'developer';\n\n        /**\n         * The type of the message input. Always `message`.\n         */\n        type?: 'message';\n      }\n\n      export namespace EvalItem {\n        /**\n         * A text output from the model.\n         */\n        export interface OutputText {\n          /**\n           * The text output from the model.\n           */\n          text: string;\n\n          /**\n           * The type of the output text. Always `output_text`.\n           */\n          type: 'output_text';\n        }\n\n        /**\n         * An image input block used within EvalItem content arrays.\n         */\n        export interface InputImage {\n          /**\n           * The URL of the image input.\n           */\n          image_url: string;\n\n          /**\n           * The type of the image input. Always `input_image`.\n           */\n          type: 'input_image';\n\n          /**\n           * The detail level of the image to be sent to the model. One of `high`, `low`, or\n           * `auto`. Defaults to `auto`.\n           */\n          detail?: string;\n        }\n      }\n    }\n\n    export interface ItemReference {\n      /**\n       * A reference to a variable in the `item` namespace. Ie, \"item.name\"\n       */\n      item_reference: string;\n\n      /**\n       * The type of input messages. Always `item_reference`.\n       */\n      type: 'item_reference';\n    }\n\n    export interface SamplingParams {\n      /**\n       * The maximum number of tokens in the generated output.\n       */\n      max_completion_tokens?: number;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * A seed value to initialize the randomness, during sampling.\n       */\n      seed?: number;\n\n      /**\n       * A higher temperature increases randomness in the outputs.\n       */\n      temperature?: number;\n\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      text?: SamplingParams.Text;\n\n      /**\n       * An array of tools the model may call while generating a response. You can\n       * specify which tool to use by setting the `tool_choice` parameter.\n       *\n       * The two categories of tools you can provide the model are:\n       *\n       * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n       *   capabilities, like\n       *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n       *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n       *   Learn more about\n       *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n       * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n       *   the model to call your own code. Learn more about\n       *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n       */\n      tools?: Array<ResponsesAPI.Tool>;\n\n      /**\n       * An alternative to temperature for nucleus sampling; 1.0 includes all tokens.\n       */\n      top_p?: number;\n    }\n\n    export namespace SamplingParams {\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      export interface Text {\n        /**\n         * An object specifying the format that the model must output.\n         *\n         * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n         * ensures the model will match your supplied JSON schema. Learn more in the\n         * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n         *\n         * The default format is `{ \"type\": \"text\" }` with no additional options.\n         *\n         * **Not recommended for gpt-4o and newer models:**\n         *\n         * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n         * ensures the message the model generates is valid JSON. Using `json_schema` is\n         * preferred for models that support it.\n         */\n        format?: ResponsesAPI.ResponseFormatTextConfig;\n      }\n    }\n  }\n\n  export interface PerModelUsage {\n    /**\n     * The number of tokens retrieved from cache.\n     */\n    cached_tokens: number;\n\n    /**\n     * The number of completion tokens generated.\n     */\n    completion_tokens: number;\n\n    /**\n     * The number of invocations.\n     */\n    invocation_count: number;\n\n    /**\n     * The name of the model.\n     */\n    model_name: string;\n\n    /**\n     * The number of prompt tokens used.\n     */\n    prompt_tokens: number;\n\n    /**\n     * The total number of tokens used.\n     */\n    total_tokens: number;\n  }\n\n  export interface PerTestingCriteriaResult {\n    /**\n     * Number of tests failed for this criteria.\n     */\n    failed: number;\n\n    /**\n     * Number of tests passed for this criteria.\n     */\n    passed: number;\n\n    /**\n     * A description of the testing criteria.\n     */\n    testing_criteria: string;\n  }\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  export interface ResultCounts {\n    /**\n     * Number of output items that resulted in an error.\n     */\n    errored: number;\n\n    /**\n     * Number of output items that failed to pass the evaluation.\n     */\n    failed: number;\n\n    /**\n     * Number of output items that passed the evaluation.\n     */\n    passed: number;\n\n    /**\n     * Total number of executed output items.\n     */\n    total: number;\n  }\n}\n\n/**\n * A schema representing an evaluation run.\n */\nexport interface RunRetrieveResponse {\n  /**\n   * Unique identifier for the evaluation run.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the evaluation run was created.\n   */\n  created_at: number;\n\n  /**\n   * Information about the run's data source.\n   */\n  data_source:\n    | CreateEvalJSONLRunDataSource\n    | CreateEvalCompletionsRunDataSource\n    | RunRetrieveResponse.Responses;\n\n  /**\n   * An object representing an error response from the Eval API.\n   */\n  error: EvalAPIError;\n\n  /**\n   * The identifier of the associated evaluation.\n   */\n  eval_id: string;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The model that is evaluated, if applicable.\n   */\n  model: string;\n\n  /**\n   * The name of the evaluation run.\n   */\n  name: string;\n\n  /**\n   * The type of the object. Always \"eval.run\".\n   */\n  object: 'eval.run';\n\n  /**\n   * Usage statistics for each model during the evaluation run.\n   */\n  per_model_usage: Array<RunRetrieveResponse.PerModelUsage>;\n\n  /**\n   * Results per testing criteria applied during the evaluation run.\n   */\n  per_testing_criteria_results: Array<RunRetrieveResponse.PerTestingCriteriaResult>;\n\n  /**\n   * The URL to the rendered evaluation run report on the UI dashboard.\n   */\n  report_url: string;\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  result_counts: RunRetrieveResponse.ResultCounts;\n\n  /**\n   * The status of the evaluation run.\n   */\n  status: string;\n}\n\nexport namespace RunRetrieveResponse {\n  /**\n   * A ResponsesRunDataSource object describing a model sampling configuration.\n   */\n  export interface Responses {\n    /**\n     * Determines what populates the `item` namespace in this run's data source.\n     */\n    source: Responses.FileContent | Responses.FileID | Responses.Responses;\n\n    /**\n     * The type of run data source. Always `responses`.\n     */\n    type: 'responses';\n\n    /**\n     * Used when sampling from a model. Dictates the structure of the messages passed\n     * into the model. Can either be a reference to a prebuilt trajectory (ie,\n     * `item.input_trajectory`), or a template with variable references to the `item`\n     * namespace.\n     */\n    input_messages?: Responses.Template | Responses.ItemReference;\n\n    /**\n     * The name of the model to use for generating completions (e.g. \"o3-mini\").\n     */\n    model?: string;\n\n    sampling_params?: Responses.SamplingParams;\n  }\n\n  export namespace Responses {\n    export interface FileContent {\n      /**\n       * The content of the jsonl file.\n       */\n      content: Array<FileContent.Content>;\n\n      /**\n       * The type of jsonl source. Always `file_content`.\n       */\n      type: 'file_content';\n    }\n\n    export namespace FileContent {\n      export interface Content {\n        item: { [key: string]: unknown };\n\n        sample?: { [key: string]: unknown };\n      }\n    }\n\n    export interface FileID {\n      /**\n       * The identifier of the file.\n       */\n      id: string;\n\n      /**\n       * The type of jsonl source. Always `file_id`.\n       */\n      type: 'file_id';\n    }\n\n    /**\n     * A EvalResponsesSource object describing a run data source configuration.\n     */\n    export interface Responses {\n      /**\n       * The type of run data source. Always `responses`.\n       */\n      type: 'responses';\n\n      /**\n       * Only include items created after this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_after?: number | null;\n\n      /**\n       * Only include items created before this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_before?: number | null;\n\n      /**\n       * Optional string to search the 'instructions' field. This is a query parameter\n       * used to select responses.\n       */\n      instructions_search?: string | null;\n\n      /**\n       * Metadata filter for the responses. This is a query parameter used to select\n       * responses.\n       */\n      metadata?: unknown | null;\n\n      /**\n       * The name of the model to find responses for. This is a query parameter used to\n       * select responses.\n       */\n      model?: string | null;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * Sampling temperature. This is a query parameter used to select responses.\n       */\n      temperature?: number | null;\n\n      /**\n       * List of tool names. This is a query parameter used to select responses.\n       */\n      tools?: Array<string> | null;\n\n      /**\n       * Nucleus sampling parameter. This is a query parameter used to select responses.\n       */\n      top_p?: number | null;\n\n      /**\n       * List of user identifiers. This is a query parameter used to select responses.\n       */\n      users?: Array<string> | null;\n    }\n\n    export interface Template {\n      /**\n       * A list of chat messages forming the prompt or context. May include variable\n       * references to the `item` namespace, ie {{item.name}}.\n       */\n      template: Array<Template.ChatMessage | Template.EvalItem>;\n\n      /**\n       * The type of input messages. Always `template`.\n       */\n      type: 'template';\n    }\n\n    export namespace Template {\n      export interface ChatMessage {\n        /**\n         * The content of the message.\n         */\n        content: string;\n\n        /**\n         * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n         */\n        role: string;\n      }\n\n      /**\n       * A message input to the model with a role indicating instruction following\n       * hierarchy. Instructions given with the `developer` or `system` role take\n       * precedence over instructions given with the `user` role. Messages with the\n       * `assistant` role are presumed to have been generated by the model in previous\n       * interactions.\n       */\n      export interface EvalItem {\n        /**\n         * Inputs to the model - can contain template strings. Supports text, output text,\n         * input images, and input audio, either as a single item or an array of items.\n         */\n        content:\n          | string\n          | ResponsesAPI.ResponseInputText\n          | EvalItem.OutputText\n          | EvalItem.InputImage\n          | ResponsesAPI.ResponseInputAudio\n          | GraderModelsAPI.GraderInputs;\n\n        /**\n         * The role of the message input. One of `user`, `assistant`, `system`, or\n         * `developer`.\n         */\n        role: 'user' | 'assistant' | 'system' | 'developer';\n\n        /**\n         * The type of the message input. Always `message`.\n         */\n        type?: 'message';\n      }\n\n      export namespace EvalItem {\n        /**\n         * A text output from the model.\n         */\n        export interface OutputText {\n          /**\n           * The text output from the model.\n           */\n          text: string;\n\n          /**\n           * The type of the output text. Always `output_text`.\n           */\n          type: 'output_text';\n        }\n\n        /**\n         * An image input block used within EvalItem content arrays.\n         */\n        export interface InputImage {\n          /**\n           * The URL of the image input.\n           */\n          image_url: string;\n\n          /**\n           * The type of the image input. Always `input_image`.\n           */\n          type: 'input_image';\n\n          /**\n           * The detail level of the image to be sent to the model. One of `high`, `low`, or\n           * `auto`. Defaults to `auto`.\n           */\n          detail?: string;\n        }\n      }\n    }\n\n    export interface ItemReference {\n      /**\n       * A reference to a variable in the `item` namespace. Ie, \"item.name\"\n       */\n      item_reference: string;\n\n      /**\n       * The type of input messages. Always `item_reference`.\n       */\n      type: 'item_reference';\n    }\n\n    export interface SamplingParams {\n      /**\n       * The maximum number of tokens in the generated output.\n       */\n      max_completion_tokens?: number;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * A seed value to initialize the randomness, during sampling.\n       */\n      seed?: number;\n\n      /**\n       * A higher temperature increases randomness in the outputs.\n       */\n      temperature?: number;\n\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      text?: SamplingParams.Text;\n\n      /**\n       * An array of tools the model may call while generating a response. You can\n       * specify which tool to use by setting the `tool_choice` parameter.\n       *\n       * The two categories of tools you can provide the model are:\n       *\n       * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n       *   capabilities, like\n       *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n       *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n       *   Learn more about\n       *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n       * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n       *   the model to call your own code. Learn more about\n       *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n       */\n      tools?: Array<ResponsesAPI.Tool>;\n\n      /**\n       * An alternative to temperature for nucleus sampling; 1.0 includes all tokens.\n       */\n      top_p?: number;\n    }\n\n    export namespace SamplingParams {\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      export interface Text {\n        /**\n         * An object specifying the format that the model must output.\n         *\n         * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n         * ensures the model will match your supplied JSON schema. Learn more in the\n         * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n         *\n         * The default format is `{ \"type\": \"text\" }` with no additional options.\n         *\n         * **Not recommended for gpt-4o and newer models:**\n         *\n         * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n         * ensures the message the model generates is valid JSON. Using `json_schema` is\n         * preferred for models that support it.\n         */\n        format?: ResponsesAPI.ResponseFormatTextConfig;\n      }\n    }\n  }\n\n  export interface PerModelUsage {\n    /**\n     * The number of tokens retrieved from cache.\n     */\n    cached_tokens: number;\n\n    /**\n     * The number of completion tokens generated.\n     */\n    completion_tokens: number;\n\n    /**\n     * The number of invocations.\n     */\n    invocation_count: number;\n\n    /**\n     * The name of the model.\n     */\n    model_name: string;\n\n    /**\n     * The number of prompt tokens used.\n     */\n    prompt_tokens: number;\n\n    /**\n     * The total number of tokens used.\n     */\n    total_tokens: number;\n  }\n\n  export interface PerTestingCriteriaResult {\n    /**\n     * Number of tests failed for this criteria.\n     */\n    failed: number;\n\n    /**\n     * Number of tests passed for this criteria.\n     */\n    passed: number;\n\n    /**\n     * A description of the testing criteria.\n     */\n    testing_criteria: string;\n  }\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  export interface ResultCounts {\n    /**\n     * Number of output items that resulted in an error.\n     */\n    errored: number;\n\n    /**\n     * Number of output items that failed to pass the evaluation.\n     */\n    failed: number;\n\n    /**\n     * Number of output items that passed the evaluation.\n     */\n    passed: number;\n\n    /**\n     * Total number of executed output items.\n     */\n    total: number;\n  }\n}\n\n/**\n * A schema representing an evaluation run.\n */\nexport interface RunListResponse {\n  /**\n   * Unique identifier for the evaluation run.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the evaluation run was created.\n   */\n  created_at: number;\n\n  /**\n   * Information about the run's data source.\n   */\n  data_source: CreateEvalJSONLRunDataSource | CreateEvalCompletionsRunDataSource | RunListResponse.Responses;\n\n  /**\n   * An object representing an error response from the Eval API.\n   */\n  error: EvalAPIError;\n\n  /**\n   * The identifier of the associated evaluation.\n   */\n  eval_id: string;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The model that is evaluated, if applicable.\n   */\n  model: string;\n\n  /**\n   * The name of the evaluation run.\n   */\n  name: string;\n\n  /**\n   * The type of the object. Always \"eval.run\".\n   */\n  object: 'eval.run';\n\n  /**\n   * Usage statistics for each model during the evaluation run.\n   */\n  per_model_usage: Array<RunListResponse.PerModelUsage>;\n\n  /**\n   * Results per testing criteria applied during the evaluation run.\n   */\n  per_testing_criteria_results: Array<RunListResponse.PerTestingCriteriaResult>;\n\n  /**\n   * The URL to the rendered evaluation run report on the UI dashboard.\n   */\n  report_url: string;\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  result_counts: RunListResponse.ResultCounts;\n\n  /**\n   * The status of the evaluation run.\n   */\n  status: string;\n}\n\nexport namespace RunListResponse {\n  /**\n   * A ResponsesRunDataSource object describing a model sampling configuration.\n   */\n  export interface Responses {\n    /**\n     * Determines what populates the `item` namespace in this run's data source.\n     */\n    source: Responses.FileContent | Responses.FileID | Responses.Responses;\n\n    /**\n     * The type of run data source. Always `responses`.\n     */\n    type: 'responses';\n\n    /**\n     * Used when sampling from a model. Dictates the structure of the messages passed\n     * into the model. Can either be a reference to a prebuilt trajectory (ie,\n     * `item.input_trajectory`), or a template with variable references to the `item`\n     * namespace.\n     */\n    input_messages?: Responses.Template | Responses.ItemReference;\n\n    /**\n     * The name of the model to use for generating completions (e.g. \"o3-mini\").\n     */\n    model?: string;\n\n    sampling_params?: Responses.SamplingParams;\n  }\n\n  export namespace Responses {\n    export interface FileContent {\n      /**\n       * The content of the jsonl file.\n       */\n      content: Array<FileContent.Content>;\n\n      /**\n       * The type of jsonl source. Always `file_content`.\n       */\n      type: 'file_content';\n    }\n\n    export namespace FileContent {\n      export interface Content {\n        item: { [key: string]: unknown };\n\n        sample?: { [key: string]: unknown };\n      }\n    }\n\n    export interface FileID {\n      /**\n       * The identifier of the file.\n       */\n      id: string;\n\n      /**\n       * The type of jsonl source. Always `file_id`.\n       */\n      type: 'file_id';\n    }\n\n    /**\n     * A EvalResponsesSource object describing a run data source configuration.\n     */\n    export interface Responses {\n      /**\n       * The type of run data source. Always `responses`.\n       */\n      type: 'responses';\n\n      /**\n       * Only include items created after this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_after?: number | null;\n\n      /**\n       * Only include items created before this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_before?: number | null;\n\n      /**\n       * Optional string to search the 'instructions' field. This is a query parameter\n       * used to select responses.\n       */\n      instructions_search?: string | null;\n\n      /**\n       * Metadata filter for the responses. This is a query parameter used to select\n       * responses.\n       */\n      metadata?: unknown | null;\n\n      /**\n       * The name of the model to find responses for. This is a query parameter used to\n       * select responses.\n       */\n      model?: string | null;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * Sampling temperature. This is a query parameter used to select responses.\n       */\n      temperature?: number | null;\n\n      /**\n       * List of tool names. This is a query parameter used to select responses.\n       */\n      tools?: Array<string> | null;\n\n      /**\n       * Nucleus sampling parameter. This is a query parameter used to select responses.\n       */\n      top_p?: number | null;\n\n      /**\n       * List of user identifiers. This is a query parameter used to select responses.\n       */\n      users?: Array<string> | null;\n    }\n\n    export interface Template {\n      /**\n       * A list of chat messages forming the prompt or context. May include variable\n       * references to the `item` namespace, ie {{item.name}}.\n       */\n      template: Array<Template.ChatMessage | Template.EvalItem>;\n\n      /**\n       * The type of input messages. Always `template`.\n       */\n      type: 'template';\n    }\n\n    export namespace Template {\n      export interface ChatMessage {\n        /**\n         * The content of the message.\n         */\n        content: string;\n\n        /**\n         * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n         */\n        role: string;\n      }\n\n      /**\n       * A message input to the model with a role indicating instruction following\n       * hierarchy. Instructions given with the `developer` or `system` role take\n       * precedence over instructions given with the `user` role. Messages with the\n       * `assistant` role are presumed to have been generated by the model in previous\n       * interactions.\n       */\n      export interface EvalItem {\n        /**\n         * Inputs to the model - can contain template strings. Supports text, output text,\n         * input images, and input audio, either as a single item or an array of items.\n         */\n        content:\n          | string\n          | ResponsesAPI.ResponseInputText\n          | EvalItem.OutputText\n          | EvalItem.InputImage\n          | ResponsesAPI.ResponseInputAudio\n          | GraderModelsAPI.GraderInputs;\n\n        /**\n         * The role of the message input. One of `user`, `assistant`, `system`, or\n         * `developer`.\n         */\n        role: 'user' | 'assistant' | 'system' | 'developer';\n\n        /**\n         * The type of the message input. Always `message`.\n         */\n        type?: 'message';\n      }\n\n      export namespace EvalItem {\n        /**\n         * A text output from the model.\n         */\n        export interface OutputText {\n          /**\n           * The text output from the model.\n           */\n          text: string;\n\n          /**\n           * The type of the output text. Always `output_text`.\n           */\n          type: 'output_text';\n        }\n\n        /**\n         * An image input block used within EvalItem content arrays.\n         */\n        export interface InputImage {\n          /**\n           * The URL of the image input.\n           */\n          image_url: string;\n\n          /**\n           * The type of the image input. Always `input_image`.\n           */\n          type: 'input_image';\n\n          /**\n           * The detail level of the image to be sent to the model. One of `high`, `low`, or\n           * `auto`. Defaults to `auto`.\n           */\n          detail?: string;\n        }\n      }\n    }\n\n    export interface ItemReference {\n      /**\n       * A reference to a variable in the `item` namespace. Ie, \"item.name\"\n       */\n      item_reference: string;\n\n      /**\n       * The type of input messages. Always `item_reference`.\n       */\n      type: 'item_reference';\n    }\n\n    export interface SamplingParams {\n      /**\n       * The maximum number of tokens in the generated output.\n       */\n      max_completion_tokens?: number;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * A seed value to initialize the randomness, during sampling.\n       */\n      seed?: number;\n\n      /**\n       * A higher temperature increases randomness in the outputs.\n       */\n      temperature?: number;\n\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      text?: SamplingParams.Text;\n\n      /**\n       * An array of tools the model may call while generating a response. You can\n       * specify which tool to use by setting the `tool_choice` parameter.\n       *\n       * The two categories of tools you can provide the model are:\n       *\n       * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n       *   capabilities, like\n       *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n       *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n       *   Learn more about\n       *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n       * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n       *   the model to call your own code. Learn more about\n       *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n       */\n      tools?: Array<ResponsesAPI.Tool>;\n\n      /**\n       * An alternative to temperature for nucleus sampling; 1.0 includes all tokens.\n       */\n      top_p?: number;\n    }\n\n    export namespace SamplingParams {\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      export interface Text {\n        /**\n         * An object specifying the format that the model must output.\n         *\n         * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n         * ensures the model will match your supplied JSON schema. Learn more in the\n         * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n         *\n         * The default format is `{ \"type\": \"text\" }` with no additional options.\n         *\n         * **Not recommended for gpt-4o and newer models:**\n         *\n         * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n         * ensures the message the model generates is valid JSON. Using `json_schema` is\n         * preferred for models that support it.\n         */\n        format?: ResponsesAPI.ResponseFormatTextConfig;\n      }\n    }\n  }\n\n  export interface PerModelUsage {\n    /**\n     * The number of tokens retrieved from cache.\n     */\n    cached_tokens: number;\n\n    /**\n     * The number of completion tokens generated.\n     */\n    completion_tokens: number;\n\n    /**\n     * The number of invocations.\n     */\n    invocation_count: number;\n\n    /**\n     * The name of the model.\n     */\n    model_name: string;\n\n    /**\n     * The number of prompt tokens used.\n     */\n    prompt_tokens: number;\n\n    /**\n     * The total number of tokens used.\n     */\n    total_tokens: number;\n  }\n\n  export interface PerTestingCriteriaResult {\n    /**\n     * Number of tests failed for this criteria.\n     */\n    failed: number;\n\n    /**\n     * Number of tests passed for this criteria.\n     */\n    passed: number;\n\n    /**\n     * A description of the testing criteria.\n     */\n    testing_criteria: string;\n  }\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  export interface ResultCounts {\n    /**\n     * Number of output items that resulted in an error.\n     */\n    errored: number;\n\n    /**\n     * Number of output items that failed to pass the evaluation.\n     */\n    failed: number;\n\n    /**\n     * Number of output items that passed the evaluation.\n     */\n    passed: number;\n\n    /**\n     * Total number of executed output items.\n     */\n    total: number;\n  }\n}\n\nexport interface RunDeleteResponse {\n  deleted?: boolean;\n\n  object?: string;\n\n  run_id?: string;\n}\n\n/**\n * A schema representing an evaluation run.\n */\nexport interface RunCancelResponse {\n  /**\n   * Unique identifier for the evaluation run.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the evaluation run was created.\n   */\n  created_at: number;\n\n  /**\n   * Information about the run's data source.\n   */\n  data_source:\n    | CreateEvalJSONLRunDataSource\n    | CreateEvalCompletionsRunDataSource\n    | RunCancelResponse.Responses;\n\n  /**\n   * An object representing an error response from the Eval API.\n   */\n  error: EvalAPIError;\n\n  /**\n   * The identifier of the associated evaluation.\n   */\n  eval_id: string;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The model that is evaluated, if applicable.\n   */\n  model: string;\n\n  /**\n   * The name of the evaluation run.\n   */\n  name: string;\n\n  /**\n   * The type of the object. Always \"eval.run\".\n   */\n  object: 'eval.run';\n\n  /**\n   * Usage statistics for each model during the evaluation run.\n   */\n  per_model_usage: Array<RunCancelResponse.PerModelUsage>;\n\n  /**\n   * Results per testing criteria applied during the evaluation run.\n   */\n  per_testing_criteria_results: Array<RunCancelResponse.PerTestingCriteriaResult>;\n\n  /**\n   * The URL to the rendered evaluation run report on the UI dashboard.\n   */\n  report_url: string;\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  result_counts: RunCancelResponse.ResultCounts;\n\n  /**\n   * The status of the evaluation run.\n   */\n  status: string;\n}\n\nexport namespace RunCancelResponse {\n  /**\n   * A ResponsesRunDataSource object describing a model sampling configuration.\n   */\n  export interface Responses {\n    /**\n     * Determines what populates the `item` namespace in this run's data source.\n     */\n    source: Responses.FileContent | Responses.FileID | Responses.Responses;\n\n    /**\n     * The type of run data source. Always `responses`.\n     */\n    type: 'responses';\n\n    /**\n     * Used when sampling from a model. Dictates the structure of the messages passed\n     * into the model. Can either be a reference to a prebuilt trajectory (ie,\n     * `item.input_trajectory`), or a template with variable references to the `item`\n     * namespace.\n     */\n    input_messages?: Responses.Template | Responses.ItemReference;\n\n    /**\n     * The name of the model to use for generating completions (e.g. \"o3-mini\").\n     */\n    model?: string;\n\n    sampling_params?: Responses.SamplingParams;\n  }\n\n  export namespace Responses {\n    export interface FileContent {\n      /**\n       * The content of the jsonl file.\n       */\n      content: Array<FileContent.Content>;\n\n      /**\n       * The type of jsonl source. Always `file_content`.\n       */\n      type: 'file_content';\n    }\n\n    export namespace FileContent {\n      export interface Content {\n        item: { [key: string]: unknown };\n\n        sample?: { [key: string]: unknown };\n      }\n    }\n\n    export interface FileID {\n      /**\n       * The identifier of the file.\n       */\n      id: string;\n\n      /**\n       * The type of jsonl source. Always `file_id`.\n       */\n      type: 'file_id';\n    }\n\n    /**\n     * A EvalResponsesSource object describing a run data source configuration.\n     */\n    export interface Responses {\n      /**\n       * The type of run data source. Always `responses`.\n       */\n      type: 'responses';\n\n      /**\n       * Only include items created after this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_after?: number | null;\n\n      /**\n       * Only include items created before this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_before?: number | null;\n\n      /**\n       * Optional string to search the 'instructions' field. This is a query parameter\n       * used to select responses.\n       */\n      instructions_search?: string | null;\n\n      /**\n       * Metadata filter for the responses. This is a query parameter used to select\n       * responses.\n       */\n      metadata?: unknown | null;\n\n      /**\n       * The name of the model to find responses for. This is a query parameter used to\n       * select responses.\n       */\n      model?: string | null;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * Sampling temperature. This is a query parameter used to select responses.\n       */\n      temperature?: number | null;\n\n      /**\n       * List of tool names. This is a query parameter used to select responses.\n       */\n      tools?: Array<string> | null;\n\n      /**\n       * Nucleus sampling parameter. This is a query parameter used to select responses.\n       */\n      top_p?: number | null;\n\n      /**\n       * List of user identifiers. This is a query parameter used to select responses.\n       */\n      users?: Array<string> | null;\n    }\n\n    export interface Template {\n      /**\n       * A list of chat messages forming the prompt or context. May include variable\n       * references to the `item` namespace, ie {{item.name}}.\n       */\n      template: Array<Template.ChatMessage | Template.EvalItem>;\n\n      /**\n       * The type of input messages. Always `template`.\n       */\n      type: 'template';\n    }\n\n    export namespace Template {\n      export interface ChatMessage {\n        /**\n         * The content of the message.\n         */\n        content: string;\n\n        /**\n         * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n         */\n        role: string;\n      }\n\n      /**\n       * A message input to the model with a role indicating instruction following\n       * hierarchy. Instructions given with the `developer` or `system` role take\n       * precedence over instructions given with the `user` role. Messages with the\n       * `assistant` role are presumed to have been generated by the model in previous\n       * interactions.\n       */\n      export interface EvalItem {\n        /**\n         * Inputs to the model - can contain template strings. Supports text, output text,\n         * input images, and input audio, either as a single item or an array of items.\n         */\n        content:\n          | string\n          | ResponsesAPI.ResponseInputText\n          | EvalItem.OutputText\n          | EvalItem.InputImage\n          | ResponsesAPI.ResponseInputAudio\n          | GraderModelsAPI.GraderInputs;\n\n        /**\n         * The role of the message input. One of `user`, `assistant`, `system`, or\n         * `developer`.\n         */\n        role: 'user' | 'assistant' | 'system' | 'developer';\n\n        /**\n         * The type of the message input. Always `message`.\n         */\n        type?: 'message';\n      }\n\n      export namespace EvalItem {\n        /**\n         * A text output from the model.\n         */\n        export interface OutputText {\n          /**\n           * The text output from the model.\n           */\n          text: string;\n\n          /**\n           * The type of the output text. Always `output_text`.\n           */\n          type: 'output_text';\n        }\n\n        /**\n         * An image input block used within EvalItem content arrays.\n         */\n        export interface InputImage {\n          /**\n           * The URL of the image input.\n           */\n          image_url: string;\n\n          /**\n           * The type of the image input. Always `input_image`.\n           */\n          type: 'input_image';\n\n          /**\n           * The detail level of the image to be sent to the model. One of `high`, `low`, or\n           * `auto`. Defaults to `auto`.\n           */\n          detail?: string;\n        }\n      }\n    }\n\n    export interface ItemReference {\n      /**\n       * A reference to a variable in the `item` namespace. Ie, \"item.name\"\n       */\n      item_reference: string;\n\n      /**\n       * The type of input messages. Always `item_reference`.\n       */\n      type: 'item_reference';\n    }\n\n    export interface SamplingParams {\n      /**\n       * The maximum number of tokens in the generated output.\n       */\n      max_completion_tokens?: number;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * A seed value to initialize the randomness, during sampling.\n       */\n      seed?: number;\n\n      /**\n       * A higher temperature increases randomness in the outputs.\n       */\n      temperature?: number;\n\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      text?: SamplingParams.Text;\n\n      /**\n       * An array of tools the model may call while generating a response. You can\n       * specify which tool to use by setting the `tool_choice` parameter.\n       *\n       * The two categories of tools you can provide the model are:\n       *\n       * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n       *   capabilities, like\n       *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n       *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n       *   Learn more about\n       *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n       * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n       *   the model to call your own code. Learn more about\n       *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n       */\n      tools?: Array<ResponsesAPI.Tool>;\n\n      /**\n       * An alternative to temperature for nucleus sampling; 1.0 includes all tokens.\n       */\n      top_p?: number;\n    }\n\n    export namespace SamplingParams {\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      export interface Text {\n        /**\n         * An object specifying the format that the model must output.\n         *\n         * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n         * ensures the model will match your supplied JSON schema. Learn more in the\n         * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n         *\n         * The default format is `{ \"type\": \"text\" }` with no additional options.\n         *\n         * **Not recommended for gpt-4o and newer models:**\n         *\n         * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n         * ensures the message the model generates is valid JSON. Using `json_schema` is\n         * preferred for models that support it.\n         */\n        format?: ResponsesAPI.ResponseFormatTextConfig;\n      }\n    }\n  }\n\n  export interface PerModelUsage {\n    /**\n     * The number of tokens retrieved from cache.\n     */\n    cached_tokens: number;\n\n    /**\n     * The number of completion tokens generated.\n     */\n    completion_tokens: number;\n\n    /**\n     * The number of invocations.\n     */\n    invocation_count: number;\n\n    /**\n     * The name of the model.\n     */\n    model_name: string;\n\n    /**\n     * The number of prompt tokens used.\n     */\n    prompt_tokens: number;\n\n    /**\n     * The total number of tokens used.\n     */\n    total_tokens: number;\n  }\n\n  export interface PerTestingCriteriaResult {\n    /**\n     * Number of tests failed for this criteria.\n     */\n    failed: number;\n\n    /**\n     * Number of tests passed for this criteria.\n     */\n    passed: number;\n\n    /**\n     * A description of the testing criteria.\n     */\n    testing_criteria: string;\n  }\n\n  /**\n   * Counters summarizing the outcomes of the evaluation run.\n   */\n  export interface ResultCounts {\n    /**\n     * Number of output items that resulted in an error.\n     */\n    errored: number;\n\n    /**\n     * Number of output items that failed to pass the evaluation.\n     */\n    failed: number;\n\n    /**\n     * Number of output items that passed the evaluation.\n     */\n    passed: number;\n\n    /**\n     * Total number of executed output items.\n     */\n    total: number;\n  }\n}\n\nexport interface RunCreateParams {\n  /**\n   * Details about the run's data source.\n   */\n  data_source:\n    | CreateEvalJSONLRunDataSource\n    | CreateEvalCompletionsRunDataSource\n    | RunCreateParams.CreateEvalResponsesRunDataSource;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The name of the run.\n   */\n  name?: string;\n}\n\nexport namespace RunCreateParams {\n  /**\n   * A ResponsesRunDataSource object describing a model sampling configuration.\n   */\n  export interface CreateEvalResponsesRunDataSource {\n    /**\n     * Determines what populates the `item` namespace in this run's data source.\n     */\n    source:\n      | CreateEvalResponsesRunDataSource.FileContent\n      | CreateEvalResponsesRunDataSource.FileID\n      | CreateEvalResponsesRunDataSource.Responses;\n\n    /**\n     * The type of run data source. Always `responses`.\n     */\n    type: 'responses';\n\n    /**\n     * Used when sampling from a model. Dictates the structure of the messages passed\n     * into the model. Can either be a reference to a prebuilt trajectory (ie,\n     * `item.input_trajectory`), or a template with variable references to the `item`\n     * namespace.\n     */\n    input_messages?:\n      | CreateEvalResponsesRunDataSource.Template\n      | CreateEvalResponsesRunDataSource.ItemReference;\n\n    /**\n     * The name of the model to use for generating completions (e.g. \"o3-mini\").\n     */\n    model?: string;\n\n    sampling_params?: CreateEvalResponsesRunDataSource.SamplingParams;\n  }\n\n  export namespace CreateEvalResponsesRunDataSource {\n    export interface FileContent {\n      /**\n       * The content of the jsonl file.\n       */\n      content: Array<FileContent.Content>;\n\n      /**\n       * The type of jsonl source. Always `file_content`.\n       */\n      type: 'file_content';\n    }\n\n    export namespace FileContent {\n      export interface Content {\n        item: { [key: string]: unknown };\n\n        sample?: { [key: string]: unknown };\n      }\n    }\n\n    export interface FileID {\n      /**\n       * The identifier of the file.\n       */\n      id: string;\n\n      /**\n       * The type of jsonl source. Always `file_id`.\n       */\n      type: 'file_id';\n    }\n\n    /**\n     * A EvalResponsesSource object describing a run data source configuration.\n     */\n    export interface Responses {\n      /**\n       * The type of run data source. Always `responses`.\n       */\n      type: 'responses';\n\n      /**\n       * Only include items created after this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_after?: number | null;\n\n      /**\n       * Only include items created before this timestamp (inclusive). This is a query\n       * parameter used to select responses.\n       */\n      created_before?: number | null;\n\n      /**\n       * Optional string to search the 'instructions' field. This is a query parameter\n       * used to select responses.\n       */\n      instructions_search?: string | null;\n\n      /**\n       * Metadata filter for the responses. This is a query parameter used to select\n       * responses.\n       */\n      metadata?: unknown | null;\n\n      /**\n       * The name of the model to find responses for. This is a query parameter used to\n       * select responses.\n       */\n      model?: string | null;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * Sampling temperature. This is a query parameter used to select responses.\n       */\n      temperature?: number | null;\n\n      /**\n       * List of tool names. This is a query parameter used to select responses.\n       */\n      tools?: Array<string> | null;\n\n      /**\n       * Nucleus sampling parameter. This is a query parameter used to select responses.\n       */\n      top_p?: number | null;\n\n      /**\n       * List of user identifiers. This is a query parameter used to select responses.\n       */\n      users?: Array<string> | null;\n    }\n\n    export interface Template {\n      /**\n       * A list of chat messages forming the prompt or context. May include variable\n       * references to the `item` namespace, ie {{item.name}}.\n       */\n      template: Array<Template.ChatMessage | Template.EvalItem>;\n\n      /**\n       * The type of input messages. Always `template`.\n       */\n      type: 'template';\n    }\n\n    export namespace Template {\n      export interface ChatMessage {\n        /**\n         * The content of the message.\n         */\n        content: string;\n\n        /**\n         * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n         */\n        role: string;\n      }\n\n      /**\n       * A message input to the model with a role indicating instruction following\n       * hierarchy. Instructions given with the `developer` or `system` role take\n       * precedence over instructions given with the `user` role. Messages with the\n       * `assistant` role are presumed to have been generated by the model in previous\n       * interactions.\n       */\n      export interface EvalItem {\n        /**\n         * Inputs to the model - can contain template strings. Supports text, output text,\n         * input images, and input audio, either as a single item or an array of items.\n         */\n        content:\n          | string\n          | ResponsesAPI.ResponseInputText\n          | EvalItem.OutputText\n          | EvalItem.InputImage\n          | ResponsesAPI.ResponseInputAudio\n          | GraderModelsAPI.GraderInputs;\n\n        /**\n         * The role of the message input. One of `user`, `assistant`, `system`, or\n         * `developer`.\n         */\n        role: 'user' | 'assistant' | 'system' | 'developer';\n\n        /**\n         * The type of the message input. Always `message`.\n         */\n        type?: 'message';\n      }\n\n      export namespace EvalItem {\n        /**\n         * A text output from the model.\n         */\n        export interface OutputText {\n          /**\n           * The text output from the model.\n           */\n          text: string;\n\n          /**\n           * The type of the output text. Always `output_text`.\n           */\n          type: 'output_text';\n        }\n\n        /**\n         * An image input block used within EvalItem content arrays.\n         */\n        export interface InputImage {\n          /**\n           * The URL of the image input.\n           */\n          image_url: string;\n\n          /**\n           * The type of the image input. Always `input_image`.\n           */\n          type: 'input_image';\n\n          /**\n           * The detail level of the image to be sent to the model. One of `high`, `low`, or\n           * `auto`. Defaults to `auto`.\n           */\n          detail?: string;\n        }\n      }\n    }\n\n    export interface ItemReference {\n      /**\n       * A reference to a variable in the `item` namespace. Ie, \"item.name\"\n       */\n      item_reference: string;\n\n      /**\n       * The type of input messages. Always `item_reference`.\n       */\n      type: 'item_reference';\n    }\n\n    export interface SamplingParams {\n      /**\n       * The maximum number of tokens in the generated output.\n       */\n      max_completion_tokens?: number;\n\n      /**\n       * Constrains effort on reasoning for\n       * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n       * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n       * Reducing reasoning effort can result in faster responses and fewer tokens used\n       * on reasoning in a response.\n       *\n       * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n       *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n       *   calls are supported for all reasoning values in gpt-5.1.\n       * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n       *   support `none`.\n       * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n       * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n       */\n      reasoning_effort?: Shared.ReasoningEffort | null;\n\n      /**\n       * A seed value to initialize the randomness, during sampling.\n       */\n      seed?: number;\n\n      /**\n       * A higher temperature increases randomness in the outputs.\n       */\n      temperature?: number;\n\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      text?: SamplingParams.Text;\n\n      /**\n       * An array of tools the model may call while generating a response. You can\n       * specify which tool to use by setting the `tool_choice` parameter.\n       *\n       * The two categories of tools you can provide the model are:\n       *\n       * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n       *   capabilities, like\n       *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n       *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n       *   Learn more about\n       *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n       * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n       *   the model to call your own code. Learn more about\n       *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n       */\n      tools?: Array<ResponsesAPI.Tool>;\n\n      /**\n       * An alternative to temperature for nucleus sampling; 1.0 includes all tokens.\n       */\n      top_p?: number;\n    }\n\n    export namespace SamplingParams {\n      /**\n       * Configuration options for a text response from the model. Can be plain text or\n       * structured JSON data. Learn more:\n       *\n       * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n       * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n       */\n      export interface Text {\n        /**\n         * An object specifying the format that the model must output.\n         *\n         * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n         * ensures the model will match your supplied JSON schema. Learn more in the\n         * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n         *\n         * The default format is `{ \"type\": \"text\" }` with no additional options.\n         *\n         * **Not recommended for gpt-4o and newer models:**\n         *\n         * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n         * ensures the message the model generates is valid JSON. Using `json_schema` is\n         * preferred for models that support it.\n         */\n        format?: ResponsesAPI.ResponseFormatTextConfig;\n      }\n    }\n  }\n}\n\nexport interface RunRetrieveParams {\n  /**\n   * The ID of the evaluation to retrieve runs for.\n   */\n  eval_id: string;\n}\n\nexport interface RunListParams extends CursorPageParams {\n  /**\n   * Sort order for runs by timestamp. Use `asc` for ascending order or `desc` for\n   * descending order. Defaults to `asc`.\n   */\n  order?: 'asc' | 'desc';\n\n  /**\n   * Filter runs by status. One of `queued` | `in_progress` | `failed` | `completed`\n   * | `canceled`.\n   */\n  status?: 'queued' | 'in_progress' | 'completed' | 'canceled' | 'failed';\n}\n\nexport interface RunDeleteParams {\n  /**\n   * The ID of the evaluation to delete the run from.\n   */\n  eval_id: string;\n}\n\nexport interface RunCancelParams {\n  /**\n   * The ID of the evaluation whose run you want to cancel.\n   */\n  eval_id: string;\n}\n\nRuns.OutputItems = OutputItems;\n\nexport declare namespace Runs {\n  export {\n    type CreateEvalCompletionsRunDataSource as CreateEvalCompletionsRunDataSource,\n    type CreateEvalJSONLRunDataSource as CreateEvalJSONLRunDataSource,\n    type EvalAPIError as EvalAPIError,\n    type RunCreateResponse as RunCreateResponse,\n    type RunRetrieveResponse as RunRetrieveResponse,\n    type RunListResponse as RunListResponse,\n    type RunDeleteResponse as RunDeleteResponse,\n    type RunCancelResponse as RunCancelResponse,\n    type RunListResponsesPage as RunListResponsesPage,\n    type RunCreateParams as RunCreateParams,\n    type RunRetrieveParams as RunRetrieveParams,\n    type RunListParams as RunListParams,\n    type RunDeleteParams as RunDeleteParams,\n    type RunCancelParams as RunCancelParams,\n  };\n\n  export {\n    OutputItems as OutputItems,\n    type OutputItemRetrieveResponse as OutputItemRetrieveResponse,\n    type OutputItemListResponse as OutputItemListResponse,\n    type OutputItemListResponsesPage as OutputItemListResponsesPage,\n    type OutputItemRetrieveParams as OutputItemRetrieveParams,\n    type OutputItemListParams as OutputItemListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as Shared from '../shared';\nimport * as GraderModelsAPI from '../graders/grader-models';\nimport * as ResponsesAPI from '../responses/responses';\nimport * as RunsAPI from './runs/runs';\nimport {\n  CreateEvalCompletionsRunDataSource,\n  CreateEvalJSONLRunDataSource,\n  EvalAPIError,\n  RunCancelParams,\n  RunCancelResponse,\n  RunCreateParams,\n  RunCreateResponse,\n  RunDeleteParams,\n  RunDeleteResponse,\n  RunListParams,\n  RunListResponse,\n  RunListResponsesPage,\n  RunRetrieveParams,\n  RunRetrieveResponse,\n  Runs,\n} from './runs/runs';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class Evals extends APIResource {\n  runs: RunsAPI.Runs = new RunsAPI.Runs(this._client);\n\n  /**\n   * Create the structure of an evaluation that can be used to test a model's\n   * performance. An evaluation is a set of testing criteria and the config for a\n   * data source, which dictates the schema of the data used in the evaluation. After\n   * creating an evaluation, you can run it on different models and model parameters.\n   * We support several types of graders and datasources. For more information, see\n   * the [Evals guide](https://platform.openai.com/docs/guides/evals).\n   */\n  create(body: EvalCreateParams, options?: RequestOptions): APIPromise<EvalCreateResponse> {\n    return this._client.post('/evals', { body, ...options });\n  }\n\n  /**\n   * Get an evaluation by ID.\n   */\n  retrieve(evalID: string, options?: RequestOptions): APIPromise<EvalRetrieveResponse> {\n    return this._client.get(path`/evals/${evalID}`, options);\n  }\n\n  /**\n   * Update certain properties of an evaluation.\n   */\n  update(evalID: string, body: EvalUpdateParams, options?: RequestOptions): APIPromise<EvalUpdateResponse> {\n    return this._client.post(path`/evals/${evalID}`, { body, ...options });\n  }\n\n  /**\n   * List evaluations for a project.\n   */\n  list(\n    query: EvalListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<EvalListResponsesPage, EvalListResponse> {\n    return this._client.getAPIList('/evals', CursorPage<EvalListResponse>, { query, ...options });\n  }\n\n  /**\n   * Delete an evaluation.\n   */\n  delete(evalID: string, options?: RequestOptions): APIPromise<EvalDeleteResponse> {\n    return this._client.delete(path`/evals/${evalID}`, options);\n  }\n}\n\nexport type EvalListResponsesPage = CursorPage<EvalListResponse>;\n\n/**\n * A CustomDataSourceConfig which specifies the schema of your `item` and\n * optionally `sample` namespaces. The response schema defines the shape of the\n * data that will be:\n *\n * - Used to define your testing criteria and\n * - What data is required when creating a run\n */\nexport interface EvalCustomDataSourceConfig {\n  /**\n   * The json schema for the run data source items. Learn how to build JSON schemas\n   * [here](https://json-schema.org/).\n   */\n  schema: { [key: string]: unknown };\n\n  /**\n   * The type of data source. Always `custom`.\n   */\n  type: 'custom';\n}\n\n/**\n * @deprecated Deprecated in favor of LogsDataSourceConfig.\n */\nexport interface EvalStoredCompletionsDataSourceConfig {\n  /**\n   * The json schema for the run data source items. Learn how to build JSON schemas\n   * [here](https://json-schema.org/).\n   */\n  schema: { [key: string]: unknown };\n\n  /**\n   * The type of data source. Always `stored_completions`.\n   */\n  type: 'stored_completions';\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o4-mini is better at my usecase than gpt-4o\n */\nexport interface EvalCreateResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config:\n    | EvalCustomDataSourceConfig\n    | EvalCreateResponse.Logs\n    | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | GraderModelsAPI.LabelModelGrader\n    | GraderModelsAPI.StringCheckGrader\n    | EvalCreateResponse.EvalGraderTextSimilarity\n    | EvalCreateResponse.EvalGraderPython\n    | EvalCreateResponse.EvalGraderScoreModel\n  >;\n}\n\nexport namespace EvalCreateResponse {\n  /**\n   * A LogsDataSourceConfig which specifies the metadata property of your logs query.\n   * This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The\n   * schema returned by this data source config is used to defined what variables are\n   * available in your evals. `item` and `sample` are both defined when using this\n   * data source config.\n   */\n  export interface Logs {\n    /**\n     * The json schema for the run data source items. Learn how to build JSON schemas\n     * [here](https://json-schema.org/).\n     */\n    schema: { [key: string]: unknown };\n\n    /**\n     * The type of data source. Always `logs`.\n     */\n    type: 'logs';\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n  }\n\n  /**\n   * A TextSimilarityGrader object which grades text based on similarity metrics.\n   */\n  export interface EvalGraderTextSimilarity extends GraderModelsAPI.TextSimilarityGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold: number;\n  }\n\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface EvalGraderPython extends GraderModelsAPI.PythonGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface EvalGraderScoreModel extends GraderModelsAPI.ScoreModelGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o4-mini is better at my usecase than gpt-4o\n */\nexport interface EvalRetrieveResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config:\n    | EvalCustomDataSourceConfig\n    | EvalRetrieveResponse.Logs\n    | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | GraderModelsAPI.LabelModelGrader\n    | GraderModelsAPI.StringCheckGrader\n    | EvalRetrieveResponse.EvalGraderTextSimilarity\n    | EvalRetrieveResponse.EvalGraderPython\n    | EvalRetrieveResponse.EvalGraderScoreModel\n  >;\n}\n\nexport namespace EvalRetrieveResponse {\n  /**\n   * A LogsDataSourceConfig which specifies the metadata property of your logs query.\n   * This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The\n   * schema returned by this data source config is used to defined what variables are\n   * available in your evals. `item` and `sample` are both defined when using this\n   * data source config.\n   */\n  export interface Logs {\n    /**\n     * The json schema for the run data source items. Learn how to build JSON schemas\n     * [here](https://json-schema.org/).\n     */\n    schema: { [key: string]: unknown };\n\n    /**\n     * The type of data source. Always `logs`.\n     */\n    type: 'logs';\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n  }\n\n  /**\n   * A TextSimilarityGrader object which grades text based on similarity metrics.\n   */\n  export interface EvalGraderTextSimilarity extends GraderModelsAPI.TextSimilarityGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold: number;\n  }\n\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface EvalGraderPython extends GraderModelsAPI.PythonGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface EvalGraderScoreModel extends GraderModelsAPI.ScoreModelGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o4-mini is better at my usecase than gpt-4o\n */\nexport interface EvalUpdateResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config:\n    | EvalCustomDataSourceConfig\n    | EvalUpdateResponse.Logs\n    | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | GraderModelsAPI.LabelModelGrader\n    | GraderModelsAPI.StringCheckGrader\n    | EvalUpdateResponse.EvalGraderTextSimilarity\n    | EvalUpdateResponse.EvalGraderPython\n    | EvalUpdateResponse.EvalGraderScoreModel\n  >;\n}\n\nexport namespace EvalUpdateResponse {\n  /**\n   * A LogsDataSourceConfig which specifies the metadata property of your logs query.\n   * This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The\n   * schema returned by this data source config is used to defined what variables are\n   * available in your evals. `item` and `sample` are both defined when using this\n   * data source config.\n   */\n  export interface Logs {\n    /**\n     * The json schema for the run data source items. Learn how to build JSON schemas\n     * [here](https://json-schema.org/).\n     */\n    schema: { [key: string]: unknown };\n\n    /**\n     * The type of data source. Always `logs`.\n     */\n    type: 'logs';\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n  }\n\n  /**\n   * A TextSimilarityGrader object which grades text based on similarity metrics.\n   */\n  export interface EvalGraderTextSimilarity extends GraderModelsAPI.TextSimilarityGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold: number;\n  }\n\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface EvalGraderPython extends GraderModelsAPI.PythonGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface EvalGraderScoreModel extends GraderModelsAPI.ScoreModelGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o4-mini is better at my usecase than gpt-4o\n */\nexport interface EvalListResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config:\n    | EvalCustomDataSourceConfig\n    | EvalListResponse.Logs\n    | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | GraderModelsAPI.LabelModelGrader\n    | GraderModelsAPI.StringCheckGrader\n    | EvalListResponse.EvalGraderTextSimilarity\n    | EvalListResponse.EvalGraderPython\n    | EvalListResponse.EvalGraderScoreModel\n  >;\n}\n\nexport namespace EvalListResponse {\n  /**\n   * A LogsDataSourceConfig which specifies the metadata property of your logs query.\n   * This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The\n   * schema returned by this data source config is used to defined what variables are\n   * available in your evals. `item` and `sample` are both defined when using this\n   * data source config.\n   */\n  export interface Logs {\n    /**\n     * The json schema for the run data source items. Learn how to build JSON schemas\n     * [here](https://json-schema.org/).\n     */\n    schema: { [key: string]: unknown };\n\n    /**\n     * The type of data source. Always `logs`.\n     */\n    type: 'logs';\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard.\n     *\n     * Keys are strings with a maximum length of 64 characters. Values are strings with\n     * a maximum length of 512 characters.\n     */\n    metadata?: Shared.Metadata | null;\n  }\n\n  /**\n   * A TextSimilarityGrader object which grades text based on similarity metrics.\n   */\n  export interface EvalGraderTextSimilarity extends GraderModelsAPI.TextSimilarityGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold: number;\n  }\n\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface EvalGraderPython extends GraderModelsAPI.PythonGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface EvalGraderScoreModel extends GraderModelsAPI.ScoreModelGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n}\n\nexport interface EvalDeleteResponse {\n  deleted: boolean;\n\n  eval_id: string;\n\n  object: string;\n}\n\nexport interface EvalCreateParams {\n  /**\n   * The configuration for the data source used for the evaluation runs. Dictates the\n   * schema of the data used in the evaluation.\n   */\n  data_source_config: EvalCreateParams.Custom | EvalCreateParams.Logs | EvalCreateParams.StoredCompletions;\n\n  /**\n   * A list of graders for all eval runs in this group. Graders can reference\n   * variables in the data source using double curly braces notation, like\n   * `{{item.variable_name}}`. To reference the model's output, use the `sample`\n   * namespace (ie, `{{sample.output_text}}`).\n   */\n  testing_criteria: Array<\n    | EvalCreateParams.LabelModel\n    | GraderModelsAPI.StringCheckGrader\n    | EvalCreateParams.TextSimilarity\n    | EvalCreateParams.Python\n    | EvalCreateParams.ScoreModel\n  >;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name?: string;\n}\n\nexport namespace EvalCreateParams {\n  /**\n   * A CustomDataSourceConfig object that defines the schema for the data source used\n   * for the evaluation runs. This schema is used to define the shape of the data\n   * that will be:\n   *\n   * - Used to define your testing criteria and\n   * - What data is required when creating a run\n   */\n  export interface Custom {\n    /**\n     * The json schema for each row in the data source.\n     */\n    item_schema: { [key: string]: unknown };\n\n    /**\n     * The type of data source. Always `custom`.\n     */\n    type: 'custom';\n\n    /**\n     * Whether the eval should expect you to populate the sample namespace (ie, by\n     * generating responses off of your data source)\n     */\n    include_sample_schema?: boolean;\n  }\n\n  /**\n   * A data source config which specifies the metadata property of your logs query.\n   * This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc.\n   */\n  export interface Logs {\n    /**\n     * The type of data source. Always `logs`.\n     */\n    type: 'logs';\n\n    /**\n     * Metadata filters for the logs data source.\n     */\n    metadata?: { [key: string]: unknown };\n  }\n\n  /**\n   * @deprecated Deprecated in favor of LogsDataSourceConfig.\n   */\n  export interface StoredCompletions {\n    /**\n     * The type of data source. Always `stored_completions`.\n     */\n    type: 'stored_completions';\n\n    /**\n     * Metadata filters for the stored completions data source.\n     */\n    metadata?: { [key: string]: unknown };\n  }\n\n  /**\n   * A LabelModelGrader object which uses a model to assign labels to each item in\n   * the evaluation.\n   */\n  export interface LabelModel {\n    /**\n     * A list of chat messages forming the prompt or context. May include variable\n     * references to the `item` namespace, ie {{item.name}}.\n     */\n    input: Array<LabelModel.SimpleInputMessage | LabelModel.EvalItem>;\n\n    /**\n     * The labels to classify to each item in the evaluation.\n     */\n    labels: Array<string>;\n\n    /**\n     * The model to use for the evaluation. Must support structured outputs.\n     */\n    model: string;\n\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The labels that indicate a passing result. Must be a subset of labels.\n     */\n    passing_labels: Array<string>;\n\n    /**\n     * The object type, which is always `label_model`.\n     */\n    type: 'label_model';\n  }\n\n  export namespace LabelModel {\n    export interface SimpleInputMessage {\n      /**\n       * The content of the message.\n       */\n      content: string;\n\n      /**\n       * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n       */\n      role: string;\n    }\n\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface EvalItem {\n      /**\n       * Inputs to the model - can contain template strings. Supports text, output text,\n       * input images, and input audio, either as a single item or an array of items.\n       */\n      content:\n        | string\n        | ResponsesAPI.ResponseInputText\n        | EvalItem.OutputText\n        | EvalItem.InputImage\n        | ResponsesAPI.ResponseInputAudio\n        | GraderModelsAPI.GraderInputs;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace EvalItem {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n\n      /**\n       * An image input block used within EvalItem content arrays.\n       */\n      export interface InputImage {\n        /**\n         * The URL of the image input.\n         */\n        image_url: string;\n\n        /**\n         * The type of the image input. Always `input_image`.\n         */\n        type: 'input_image';\n\n        /**\n         * The detail level of the image to be sent to the model. One of `high`, `low`, or\n         * `auto`. Defaults to `auto`.\n         */\n        detail?: string;\n      }\n    }\n  }\n\n  /**\n   * A TextSimilarityGrader object which grades text based on similarity metrics.\n   */\n  export interface TextSimilarity extends GraderModelsAPI.TextSimilarityGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold: number;\n  }\n\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface Python extends GraderModelsAPI.PythonGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface ScoreModel extends GraderModelsAPI.ScoreModelGrader {\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n}\n\nexport interface EvalUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * Rename the evaluation.\n   */\n  name?: string;\n}\n\nexport interface EvalListParams extends CursorPageParams {\n  /**\n   * Sort order for evals by timestamp. Use `asc` for ascending order or `desc` for\n   * descending order.\n   */\n  order?: 'asc' | 'desc';\n\n  /**\n   * Evals can be ordered by creation time or last updated time. Use `created_at` for\n   * creation time or `updated_at` for last updated time.\n   */\n  order_by?: 'created_at' | 'updated_at';\n}\n\nEvals.Runs = Runs;\n\nexport declare namespace Evals {\n  export {\n    type EvalCustomDataSourceConfig as EvalCustomDataSourceConfig,\n    type EvalStoredCompletionsDataSourceConfig as EvalStoredCompletionsDataSourceConfig,\n    type EvalCreateResponse as EvalCreateResponse,\n    type EvalRetrieveResponse as EvalRetrieveResponse,\n    type EvalUpdateResponse as EvalUpdateResponse,\n    type EvalListResponse as EvalListResponse,\n    type EvalDeleteResponse as EvalDeleteResponse,\n    type EvalListResponsesPage as EvalListResponsesPage,\n    type EvalCreateParams as EvalCreateParams,\n    type EvalUpdateParams as EvalUpdateParams,\n    type EvalListParams as EvalListParams,\n  };\n\n  export {\n    Runs as Runs,\n    type CreateEvalCompletionsRunDataSource as CreateEvalCompletionsRunDataSource,\n    type CreateEvalJSONLRunDataSource as CreateEvalJSONLRunDataSource,\n    type EvalAPIError as EvalAPIError,\n    type RunCreateResponse as RunCreateResponse,\n    type RunRetrieveResponse as RunRetrieveResponse,\n    type RunListResponse as RunListResponse,\n    type RunDeleteResponse as RunDeleteResponse,\n    type RunCancelResponse as RunCancelResponse,\n    type RunListResponsesPage as RunListResponsesPage,\n    type RunCreateParams as RunCreateParams,\n    type RunRetrieveParams as RunRetrieveParams,\n    type RunListParams as RunListParams,\n    type RunDeleteParams as RunDeleteParams,\n    type RunCancelParams as RunCancelParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport { APIPromise } from '../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../core/pagination';\nimport { type Uploadable } from '../core/uploads';\nimport { buildHeaders } from '../internal/headers';\nimport { RequestOptions } from '../internal/request-options';\nimport { sleep } from '../internal/utils/sleep';\nimport { APIConnectionTimeoutError } from '../error';\nimport { multipartFormRequestOptions } from '../internal/uploads';\nimport { path } from '../internal/utils/path';\n\nexport class Files extends APIResource {\n  /**\n   * Upload a file that can be used across various endpoints. Individual files can be\n   * up to 512 MB, and each project can store up to 2.5 TB of files in total. There\n   * is no organization-wide storage limit.\n   *\n   * - The Assistants API supports files up to 2 million tokens and of specific file\n   *   types. See the\n   *   [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools)\n   *   for details.\n   * - The Fine-tuning API only supports `.jsonl` files. The input also has certain\n   *   required formats for fine-tuning\n   *   [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input)\n   *   or\n   *   [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)\n   *   models.\n   * - The Batch API only supports `.jsonl` files up to 200 MB in size. The input\n   *   also has a specific required\n   *   [format](https://platform.openai.com/docs/api-reference/batch/request-input).\n   *\n   * Please [contact us](https://help.openai.com/) if you need to increase these\n   * storage limits.\n   */\n  create(body: FileCreateParams, options?: RequestOptions): APIPromise<FileObject> {\n    return this._client.post('/files', multipartFormRequestOptions({ body, ...options }, this._client));\n  }\n\n  /**\n   * Returns information about a specific file.\n   */\n  retrieve(fileID: string, options?: RequestOptions): APIPromise<FileObject> {\n    return this._client.get(path`/files/${fileID}`, options);\n  }\n\n  /**\n   * Returns a list of files.\n   */\n  list(\n    query: FileListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<FileObjectsPage, FileObject> {\n    return this._client.getAPIList('/files', CursorPage<FileObject>, { query, ...options });\n  }\n\n  /**\n   * Delete a file and remove it from all vector stores.\n   */\n  delete(fileID: string, options?: RequestOptions): APIPromise<FileDeleted> {\n    return this._client.delete(path`/files/${fileID}`, options);\n  }\n\n  /**\n   * Returns the contents of the specified file.\n   */\n  content(fileID: string, options?: RequestOptions): APIPromise<Response> {\n    return this._client.get(path`/files/${fileID}/content`, {\n      ...options,\n      headers: buildHeaders([{ Accept: 'application/binary' }, options?.headers]),\n      __binaryResponse: true,\n    });\n  }\n\n  /**\n   * Waits for the given file to be processed, default timeout is 30 mins.\n   */\n  async waitForProcessing(\n    id: string,\n    { pollInterval = 5000, maxWait = 30 * 60 * 1000 }: { pollInterval?: number; maxWait?: number } = {},\n  ): Promise<FileObject> {\n    const TERMINAL_STATES = new Set(['processed', 'error', 'deleted']);\n\n    const start = Date.now();\n    let file = await this.retrieve(id);\n\n    while (!file.status || !TERMINAL_STATES.has(file.status)) {\n      await sleep(pollInterval);\n\n      file = await this.retrieve(id);\n      if (Date.now() - start > maxWait) {\n        throw new APIConnectionTimeoutError({\n          message: `Giving up on waiting for file ${id} to finish processing after ${maxWait} milliseconds.`,\n        });\n      }\n    }\n\n    return file;\n  }\n}\n\nexport type FileObjectsPage = CursorPage<FileObject>;\n\nexport type FileContent = string;\n\nexport interface FileDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'file';\n}\n\n/**\n * The `File` object represents a document that has been uploaded to OpenAI.\n */\nexport interface FileObject {\n  /**\n   * The file identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The size of the file, in bytes.\n   */\n  bytes: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the file was created.\n   */\n  created_at: number;\n\n  /**\n   * The name of the file.\n   */\n  filename: string;\n\n  /**\n   * The object type, which is always `file`.\n   */\n  object: 'file';\n\n  /**\n   * The intended purpose of the file. Supported values are `assistants`,\n   * `assistants_output`, `batch`, `batch_output`, `fine-tune`, `fine-tune-results`,\n   * `vision`, and `user_data`.\n   */\n  purpose:\n    | 'assistants'\n    | 'assistants_output'\n    | 'batch'\n    | 'batch_output'\n    | 'fine-tune'\n    | 'fine-tune-results'\n    | 'vision'\n    | 'user_data';\n\n  /**\n   * @deprecated Deprecated. The current status of the file, which can be either\n   * `uploaded`, `processed`, or `error`.\n   */\n  status: 'uploaded' | 'processed' | 'error';\n\n  /**\n   * The Unix timestamp (in seconds) for when the file will expire.\n   */\n  expires_at?: number;\n\n  /**\n   * @deprecated Deprecated. For details on why a fine-tuning training file failed\n   * validation, see the `error` field on `fine_tuning.job`.\n   */\n  status_details?: string;\n}\n\n/**\n * The intended purpose of the uploaded file. One of:\n *\n * - `assistants`: Used in the Assistants API\n * - `batch`: Used in the Batch API\n * - `fine-tune`: Used for fine-tuning\n * - `vision`: Images used for vision fine-tuning\n * - `user_data`: Flexible file type for any purpose\n * - `evals`: Used for eval data sets\n */\nexport type FilePurpose = 'assistants' | 'batch' | 'fine-tune' | 'vision' | 'user_data' | 'evals';\n\nexport interface FileCreateParams {\n  /**\n   * The File object (not file name) to be uploaded.\n   */\n  file: Uploadable;\n\n  /**\n   * The intended purpose of the uploaded file. One of:\n   *\n   * - `assistants`: Used in the Assistants API\n   * - `batch`: Used in the Batch API\n   * - `fine-tune`: Used for fine-tuning\n   * - `vision`: Images used for vision fine-tuning\n   * - `user_data`: Flexible file type for any purpose\n   * - `evals`: Used for eval data sets\n   */\n  purpose: FilePurpose;\n\n  /**\n   * The expiration policy for a file. By default, files with `purpose=batch` expire\n   * after 30 days and all other files are persisted until they are manually deleted.\n   */\n  expires_after?: FileCreateParams.ExpiresAfter;\n}\n\nexport namespace FileCreateParams {\n  /**\n   * The expiration policy for a file. By default, files with `purpose=batch` expire\n   * after 30 days and all other files are persisted until they are manually deleted.\n   */\n  export interface ExpiresAfter {\n    /**\n     * Anchor timestamp after which the expiration policy applies. Supported anchors:\n     * `created_at`.\n     */\n    anchor: 'created_at';\n\n    /**\n     * The number of seconds after the anchor time that the file will expire. Must be\n     * between 3600 (1 hour) and 2592000 (30 days).\n     */\n    seconds: number;\n  }\n}\n\nexport interface FileListParams extends CursorPageParams {\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n\n  /**\n   * Only return files with the given purpose.\n   */\n  purpose?: string;\n}\n\nexport declare namespace Files {\n  export {\n    type FileContent as FileContent,\n    type FileDeleted as FileDeleted,\n    type FileObject as FileObject,\n    type FilePurpose as FilePurpose,\n    type FileObjectsPage as FileObjectsPage,\n    type FileCreateParams as FileCreateParams,\n    type FileListParams as FileListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as GraderModelsAPI from '../graders/grader-models';\n\nexport class Methods extends APIResource {}\n\n/**\n * The hyperparameters used for the DPO fine-tuning job.\n */\nexport interface DpoHyperparameters {\n  /**\n   * Number of examples in each batch. A larger batch size means that model\n   * parameters are updated less frequently, but with lower variance.\n   */\n  batch_size?: 'auto' | number;\n\n  /**\n   * The beta value for the DPO method. A higher beta value will increase the weight\n   * of the penalty between the policy and reference model.\n   */\n  beta?: 'auto' | number;\n\n  /**\n   * Scaling factor for the learning rate. A smaller learning rate may be useful to\n   * avoid overfitting.\n   */\n  learning_rate_multiplier?: 'auto' | number;\n\n  /**\n   * The number of epochs to train the model for. An epoch refers to one full cycle\n   * through the training dataset.\n   */\n  n_epochs?: 'auto' | number;\n}\n\n/**\n * Configuration for the DPO fine-tuning method.\n */\nexport interface DpoMethod {\n  /**\n   * The hyperparameters used for the DPO fine-tuning job.\n   */\n  hyperparameters?: DpoHyperparameters;\n}\n\n/**\n * The hyperparameters used for the reinforcement fine-tuning job.\n */\nexport interface ReinforcementHyperparameters {\n  /**\n   * Number of examples in each batch. A larger batch size means that model\n   * parameters are updated less frequently, but with lower variance.\n   */\n  batch_size?: 'auto' | number;\n\n  /**\n   * Multiplier on amount of compute used for exploring search space during training.\n   */\n  compute_multiplier?: 'auto' | number;\n\n  /**\n   * The number of training steps between evaluation runs.\n   */\n  eval_interval?: 'auto' | number;\n\n  /**\n   * Number of evaluation samples to generate per training step.\n   */\n  eval_samples?: 'auto' | number;\n\n  /**\n   * Scaling factor for the learning rate. A smaller learning rate may be useful to\n   * avoid overfitting.\n   */\n  learning_rate_multiplier?: 'auto' | number;\n\n  /**\n   * The number of epochs to train the model for. An epoch refers to one full cycle\n   * through the training dataset.\n   */\n  n_epochs?: 'auto' | number;\n\n  /**\n   * Level of reasoning effort.\n   */\n  reasoning_effort?: 'default' | 'low' | 'medium' | 'high';\n}\n\n/**\n * Configuration for the reinforcement fine-tuning method.\n */\nexport interface ReinforcementMethod {\n  /**\n   * The grader used for the fine-tuning job.\n   */\n  grader:\n    | GraderModelsAPI.StringCheckGrader\n    | GraderModelsAPI.TextSimilarityGrader\n    | GraderModelsAPI.PythonGrader\n    | GraderModelsAPI.ScoreModelGrader\n    | GraderModelsAPI.MultiGrader;\n\n  /**\n   * The hyperparameters used for the reinforcement fine-tuning job.\n   */\n  hyperparameters?: ReinforcementHyperparameters;\n}\n\n/**\n * The hyperparameters used for the fine-tuning job.\n */\nexport interface SupervisedHyperparameters {\n  /**\n   * Number of examples in each batch. A larger batch size means that model\n   * parameters are updated less frequently, but with lower variance.\n   */\n  batch_size?: 'auto' | number;\n\n  /**\n   * Scaling factor for the learning rate. A smaller learning rate may be useful to\n   * avoid overfitting.\n   */\n  learning_rate_multiplier?: 'auto' | number;\n\n  /**\n   * The number of epochs to train the model for. An epoch refers to one full cycle\n   * through the training dataset.\n   */\n  n_epochs?: 'auto' | number;\n}\n\n/**\n * Configuration for the supervised fine-tuning method.\n */\nexport interface SupervisedMethod {\n  /**\n   * The hyperparameters used for the fine-tuning job.\n   */\n  hyperparameters?: SupervisedHyperparameters;\n}\n\nexport declare namespace Methods {\n  export {\n    type DpoHyperparameters as DpoHyperparameters,\n    type DpoMethod as DpoMethod,\n    type ReinforcementHyperparameters as ReinforcementHyperparameters,\n    type ReinforcementMethod as ReinforcementMethod,\n    type SupervisedHyperparameters as SupervisedHyperparameters,\n    type SupervisedMethod as SupervisedMethod,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as GraderModelsAPI from '../../graders/grader-models';\nimport { APIPromise } from '../../../core/api-promise';\nimport { RequestOptions } from '../../../internal/request-options';\n\nexport class Graders extends APIResource {\n  /**\n   * Run a grader.\n   *\n   * @example\n   * ```ts\n   * const response = await client.fineTuning.alpha.graders.run({\n   *   grader: {\n   *     input: 'input',\n   *     name: 'name',\n   *     operation: 'eq',\n   *     reference: 'reference',\n   *     type: 'string_check',\n   *   },\n   *   model_sample: 'model_sample',\n   * });\n   * ```\n   */\n  run(body: GraderRunParams, options?: RequestOptions): APIPromise<GraderRunResponse> {\n    return this._client.post('/fine_tuning/alpha/graders/run', { body, ...options });\n  }\n\n  /**\n   * Validate a grader.\n   *\n   * @example\n   * ```ts\n   * const response =\n   *   await client.fineTuning.alpha.graders.validate({\n   *     grader: {\n   *       input: 'input',\n   *       name: 'name',\n   *       operation: 'eq',\n   *       reference: 'reference',\n   *       type: 'string_check',\n   *     },\n   *   });\n   * ```\n   */\n  validate(body: GraderValidateParams, options?: RequestOptions): APIPromise<GraderValidateResponse> {\n    return this._client.post('/fine_tuning/alpha/graders/validate', { body, ...options });\n  }\n}\n\nexport interface GraderRunResponse {\n  metadata: GraderRunResponse.Metadata;\n\n  model_grader_token_usage_per_model: { [key: string]: unknown };\n\n  reward: number;\n\n  sub_rewards: { [key: string]: unknown };\n}\n\nexport namespace GraderRunResponse {\n  export interface Metadata {\n    errors: Metadata.Errors;\n\n    execution_time: number;\n\n    name: string;\n\n    sampled_model_name: string | null;\n\n    scores: { [key: string]: unknown };\n\n    token_usage: number | null;\n\n    type: string;\n  }\n\n  export namespace Metadata {\n    export interface Errors {\n      formula_parse_error: boolean;\n\n      invalid_variable_error: boolean;\n\n      model_grader_parse_error: boolean;\n\n      model_grader_refusal_error: boolean;\n\n      model_grader_server_error: boolean;\n\n      model_grader_server_error_details: string | null;\n\n      other_error: boolean;\n\n      python_grader_runtime_error: boolean;\n\n      python_grader_runtime_error_details: string | null;\n\n      python_grader_server_error: boolean;\n\n      python_grader_server_error_type: string | null;\n\n      sample_parse_error: boolean;\n\n      truncated_observation_error: boolean;\n\n      unresponsive_reward_error: boolean;\n    }\n  }\n}\n\nexport interface GraderValidateResponse {\n  /**\n   * The grader used for the fine-tuning job.\n   */\n  grader?:\n    | GraderModelsAPI.StringCheckGrader\n    | GraderModelsAPI.TextSimilarityGrader\n    | GraderModelsAPI.PythonGrader\n    | GraderModelsAPI.ScoreModelGrader\n    | GraderModelsAPI.MultiGrader;\n}\n\nexport interface GraderRunParams {\n  /**\n   * The grader used for the fine-tuning job.\n   */\n  grader:\n    | GraderModelsAPI.StringCheckGrader\n    | GraderModelsAPI.TextSimilarityGrader\n    | GraderModelsAPI.PythonGrader\n    | GraderModelsAPI.ScoreModelGrader\n    | GraderModelsAPI.MultiGrader;\n\n  /**\n   * The model sample to be evaluated. This value will be used to populate the\n   * `sample` namespace. See\n   * [the guide](https://platform.openai.com/docs/guides/graders) for more details.\n   * The `output_json` variable will be populated if the model sample is a valid JSON\n   * string.\n   */\n  model_sample: string;\n\n  /**\n   * The dataset item provided to the grader. This will be used to populate the\n   * `item` namespace. See\n   * [the guide](https://platform.openai.com/docs/guides/graders) for more details.\n   */\n  item?: unknown;\n}\n\nexport interface GraderValidateParams {\n  /**\n   * The grader used for the fine-tuning job.\n   */\n  grader:\n    | GraderModelsAPI.StringCheckGrader\n    | GraderModelsAPI.TextSimilarityGrader\n    | GraderModelsAPI.PythonGrader\n    | GraderModelsAPI.ScoreModelGrader\n    | GraderModelsAPI.MultiGrader;\n}\n\nexport declare namespace Graders {\n  export {\n    type GraderRunResponse as GraderRunResponse,\n    type GraderValidateResponse as GraderValidateResponse,\n    type GraderRunParams as GraderRunParams,\n    type GraderValidateParams as GraderValidateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as GradersAPI from './graders';\nimport {\n  GraderRunParams,\n  GraderRunResponse,\n  GraderValidateParams,\n  GraderValidateResponse,\n  Graders,\n} from './graders';\n\nexport class Alpha extends APIResource {\n  graders: GradersAPI.Graders = new GradersAPI.Graders(this._client);\n}\n\nAlpha.Graders = Graders;\n\nexport declare namespace Alpha {\n  export {\n    Graders as Graders,\n    type GraderRunResponse as GraderRunResponse,\n    type GraderValidateResponse as GraderValidateResponse,\n    type GraderRunParams as GraderRunParams,\n    type GraderValidateParams as GraderValidateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport { APIPromise } from '../../../core/api-promise';\nimport { Page, PagePromise } from '../../../core/pagination';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Permissions extends APIResource {\n  /**\n   * **NOTE:** Calling this endpoint requires an [admin API key](../admin-api-keys).\n   *\n   * This enables organization owners to share fine-tuned models with other projects\n   * in their organization.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const permissionCreateResponse of client.fineTuning.checkpoints.permissions.create(\n   *   'ft:gpt-4o-mini-2024-07-18:org:weather:B7R9VjQd',\n   *   { project_ids: ['string'] },\n   * )) {\n   *   // ...\n   * }\n   * ```\n   */\n  create(\n    fineTunedModelCheckpoint: string,\n    body: PermissionCreateParams,\n    options?: RequestOptions,\n  ): PagePromise<PermissionCreateResponsesPage, PermissionCreateResponse> {\n    return this._client.getAPIList(\n      path`/fine_tuning/checkpoints/${fineTunedModelCheckpoint}/permissions`,\n      Page<PermissionCreateResponse>,\n      { body, method: 'post', ...options },\n    );\n  }\n\n  /**\n   * **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).\n   *\n   * Organization owners can use this endpoint to view all permissions for a\n   * fine-tuned model checkpoint.\n   *\n   * @example\n   * ```ts\n   * const permission =\n   *   await client.fineTuning.checkpoints.permissions.retrieve(\n   *     'ft-AF1WoRqd3aJAHsqc9NY7iL8F',\n   *   );\n   * ```\n   */\n  retrieve(\n    fineTunedModelCheckpoint: string,\n    query: PermissionRetrieveParams | null | undefined = {},\n    options?: RequestOptions,\n  ): APIPromise<PermissionRetrieveResponse> {\n    return this._client.get(path`/fine_tuning/checkpoints/${fineTunedModelCheckpoint}/permissions`, {\n      query,\n      ...options,\n    });\n  }\n\n  /**\n   * **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).\n   *\n   * Organization owners can use this endpoint to delete a permission for a\n   * fine-tuned model checkpoint.\n   *\n   * @example\n   * ```ts\n   * const permission =\n   *   await client.fineTuning.checkpoints.permissions.delete(\n   *     'cp_zc4Q7MP6XxulcVzj4MZdwsAB',\n   *     {\n   *       fine_tuned_model_checkpoint:\n   *         'ft:gpt-4o-mini-2024-07-18:org:weather:B7R9VjQd',\n   *     },\n   *   );\n   * ```\n   */\n  delete(\n    permissionID: string,\n    params: PermissionDeleteParams,\n    options?: RequestOptions,\n  ): APIPromise<PermissionDeleteResponse> {\n    const { fine_tuned_model_checkpoint } = params;\n    return this._client.delete(\n      path`/fine_tuning/checkpoints/${fine_tuned_model_checkpoint}/permissions/${permissionID}`,\n      options,\n    );\n  }\n}\n\n// Note: no pagination actually occurs yet, this is for forwards-compatibility.\nexport type PermissionCreateResponsesPage = Page<PermissionCreateResponse>;\n\n/**\n * The `checkpoint.permission` object represents a permission for a fine-tuned\n * model checkpoint.\n */\nexport interface PermissionCreateResponse {\n  /**\n   * The permission identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the permission was created.\n   */\n  created_at: number;\n\n  /**\n   * The object type, which is always \"checkpoint.permission\".\n   */\n  object: 'checkpoint.permission';\n\n  /**\n   * The project identifier that the permission is for.\n   */\n  project_id: string;\n}\n\nexport interface PermissionRetrieveResponse {\n  data: Array<PermissionRetrieveResponse.Data>;\n\n  has_more: boolean;\n\n  object: 'list';\n\n  first_id?: string | null;\n\n  last_id?: string | null;\n}\n\nexport namespace PermissionRetrieveResponse {\n  /**\n   * The `checkpoint.permission` object represents a permission for a fine-tuned\n   * model checkpoint.\n   */\n  export interface Data {\n    /**\n     * The permission identifier, which can be referenced in the API endpoints.\n     */\n    id: string;\n\n    /**\n     * The Unix timestamp (in seconds) for when the permission was created.\n     */\n    created_at: number;\n\n    /**\n     * The object type, which is always \"checkpoint.permission\".\n     */\n    object: 'checkpoint.permission';\n\n    /**\n     * The project identifier that the permission is for.\n     */\n    project_id: string;\n  }\n}\n\nexport interface PermissionDeleteResponse {\n  /**\n   * The ID of the fine-tuned model checkpoint permission that was deleted.\n   */\n  id: string;\n\n  /**\n   * Whether the fine-tuned model checkpoint permission was successfully deleted.\n   */\n  deleted: boolean;\n\n  /**\n   * The object type, which is always \"checkpoint.permission\".\n   */\n  object: 'checkpoint.permission';\n}\n\nexport interface PermissionCreateParams {\n  /**\n   * The project identifiers to grant access to.\n   */\n  project_ids: Array<string>;\n}\n\nexport interface PermissionRetrieveParams {\n  /**\n   * Identifier for the last permission ID from the previous pagination request.\n   */\n  after?: string;\n\n  /**\n   * Number of permissions to retrieve.\n   */\n  limit?: number;\n\n  /**\n   * The order in which to retrieve permissions.\n   */\n  order?: 'ascending' | 'descending';\n\n  /**\n   * The ID of the project to get permissions for.\n   */\n  project_id?: string;\n}\n\nexport interface PermissionDeleteParams {\n  /**\n   * The ID of the fine-tuned model checkpoint to delete a permission for.\n   */\n  fine_tuned_model_checkpoint: string;\n}\n\nexport declare namespace Permissions {\n  export {\n    type PermissionCreateResponse as PermissionCreateResponse,\n    type PermissionRetrieveResponse as PermissionRetrieveResponse,\n    type PermissionDeleteResponse as PermissionDeleteResponse,\n    type PermissionCreateResponsesPage as PermissionCreateResponsesPage,\n    type PermissionCreateParams as PermissionCreateParams,\n    type PermissionRetrieveParams as PermissionRetrieveParams,\n    type PermissionDeleteParams as PermissionDeleteParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as PermissionsAPI from './permissions';\nimport {\n  PermissionCreateParams,\n  PermissionCreateResponse,\n  PermissionCreateResponsesPage,\n  PermissionDeleteParams,\n  PermissionDeleteResponse,\n  PermissionRetrieveParams,\n  PermissionRetrieveResponse,\n  Permissions,\n} from './permissions';\n\nexport class Checkpoints extends APIResource {\n  permissions: PermissionsAPI.Permissions = new PermissionsAPI.Permissions(this._client);\n}\n\nCheckpoints.Permissions = Permissions;\n\nexport declare namespace Checkpoints {\n  export {\n    Permissions as Permissions,\n    type PermissionCreateResponse as PermissionCreateResponse,\n    type PermissionRetrieveResponse as PermissionRetrieveResponse,\n    type PermissionDeleteResponse as PermissionDeleteResponse,\n    type PermissionCreateResponsesPage as PermissionCreateResponsesPage,\n    type PermissionCreateParams as PermissionCreateParams,\n    type PermissionRetrieveParams as PermissionRetrieveParams,\n    type PermissionDeleteParams as PermissionDeleteParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Checkpoints extends APIResource {\n  /**\n   * List checkpoints for a fine-tuning job.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const fineTuningJobCheckpoint of client.fineTuning.jobs.checkpoints.list(\n   *   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',\n   * )) {\n   *   // ...\n   * }\n   * ```\n   */\n  list(\n    fineTuningJobID: string,\n    query: CheckpointListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<FineTuningJobCheckpointsPage, FineTuningJobCheckpoint> {\n    return this._client.getAPIList(\n      path`/fine_tuning/jobs/${fineTuningJobID}/checkpoints`,\n      CursorPage<FineTuningJobCheckpoint>,\n      { query, ...options },\n    );\n  }\n}\n\nexport type FineTuningJobCheckpointsPage = CursorPage<FineTuningJobCheckpoint>;\n\n/**\n * The `fine_tuning.job.checkpoint` object represents a model checkpoint for a\n * fine-tuning job that is ready to use.\n */\nexport interface FineTuningJobCheckpoint {\n  /**\n   * The checkpoint identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the checkpoint was created.\n   */\n  created_at: number;\n\n  /**\n   * The name of the fine-tuned checkpoint model that is created.\n   */\n  fine_tuned_model_checkpoint: string;\n\n  /**\n   * The name of the fine-tuning job that this checkpoint was created from.\n   */\n  fine_tuning_job_id: string;\n\n  /**\n   * Metrics at the step number during the fine-tuning job.\n   */\n  metrics: FineTuningJobCheckpoint.Metrics;\n\n  /**\n   * The object type, which is always \"fine_tuning.job.checkpoint\".\n   */\n  object: 'fine_tuning.job.checkpoint';\n\n  /**\n   * The step number that the checkpoint was created at.\n   */\n  step_number: number;\n}\n\nexport namespace FineTuningJobCheckpoint {\n  /**\n   * Metrics at the step number during the fine-tuning job.\n   */\n  export interface Metrics {\n    full_valid_loss?: number;\n\n    full_valid_mean_token_accuracy?: number;\n\n    step?: number;\n\n    train_loss?: number;\n\n    train_mean_token_accuracy?: number;\n\n    valid_loss?: number;\n\n    valid_mean_token_accuracy?: number;\n  }\n}\n\nexport interface CheckpointListParams extends CursorPageParams {}\n\nexport declare namespace Checkpoints {\n  export {\n    type FineTuningJobCheckpoint as FineTuningJobCheckpoint,\n    type FineTuningJobCheckpointsPage as FineTuningJobCheckpointsPage,\n    type CheckpointListParams as CheckpointListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as Shared from '../../shared';\nimport * as MethodsAPI from '../methods';\nimport * as CheckpointsAPI from './checkpoints';\nimport {\n  CheckpointListParams,\n  Checkpoints,\n  FineTuningJobCheckpoint,\n  FineTuningJobCheckpointsPage,\n} from './checkpoints';\nimport { APIPromise } from '../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Jobs extends APIResource {\n  checkpoints: CheckpointsAPI.Checkpoints = new CheckpointsAPI.Checkpoints(this._client);\n\n  /**\n   * Creates a fine-tuning job which begins the process of creating a new model from\n   * a given dataset.\n   *\n   * Response includes details of the enqueued job including job status and the name\n   * of the fine-tuned models once complete.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)\n   *\n   * @example\n   * ```ts\n   * const fineTuningJob = await client.fineTuning.jobs.create({\n   *   model: 'gpt-4o-mini',\n   *   training_file: 'file-abc123',\n   * });\n   * ```\n   */\n  create(body: JobCreateParams, options?: RequestOptions): APIPromise<FineTuningJob> {\n    return this._client.post('/fine_tuning/jobs', { body, ...options });\n  }\n\n  /**\n   * Get info about a fine-tuning job.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)\n   *\n   * @example\n   * ```ts\n   * const fineTuningJob = await client.fineTuning.jobs.retrieve(\n   *   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',\n   * );\n   * ```\n   */\n  retrieve(fineTuningJobID: string, options?: RequestOptions): APIPromise<FineTuningJob> {\n    return this._client.get(path`/fine_tuning/jobs/${fineTuningJobID}`, options);\n  }\n\n  /**\n   * List your organization's fine-tuning jobs\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const fineTuningJob of client.fineTuning.jobs.list()) {\n   *   // ...\n   * }\n   * ```\n   */\n  list(\n    query: JobListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<FineTuningJobsPage, FineTuningJob> {\n    return this._client.getAPIList('/fine_tuning/jobs', CursorPage<FineTuningJob>, { query, ...options });\n  }\n\n  /**\n   * Immediately cancel a fine-tune job.\n   *\n   * @example\n   * ```ts\n   * const fineTuningJob = await client.fineTuning.jobs.cancel(\n   *   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',\n   * );\n   * ```\n   */\n  cancel(fineTuningJobID: string, options?: RequestOptions): APIPromise<FineTuningJob> {\n    return this._client.post(path`/fine_tuning/jobs/${fineTuningJobID}/cancel`, options);\n  }\n\n  /**\n   * Get status updates for a fine-tuning job.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const fineTuningJobEvent of client.fineTuning.jobs.listEvents(\n   *   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',\n   * )) {\n   *   // ...\n   * }\n   * ```\n   */\n  listEvents(\n    fineTuningJobID: string,\n    query: JobListEventsParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<FineTuningJobEventsPage, FineTuningJobEvent> {\n    return this._client.getAPIList(\n      path`/fine_tuning/jobs/${fineTuningJobID}/events`,\n      CursorPage<FineTuningJobEvent>,\n      { query, ...options },\n    );\n  }\n\n  /**\n   * Pause a fine-tune job.\n   *\n   * @example\n   * ```ts\n   * const fineTuningJob = await client.fineTuning.jobs.pause(\n   *   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',\n   * );\n   * ```\n   */\n  pause(fineTuningJobID: string, options?: RequestOptions): APIPromise<FineTuningJob> {\n    return this._client.post(path`/fine_tuning/jobs/${fineTuningJobID}/pause`, options);\n  }\n\n  /**\n   * Resume a fine-tune job.\n   *\n   * @example\n   * ```ts\n   * const fineTuningJob = await client.fineTuning.jobs.resume(\n   *   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',\n   * );\n   * ```\n   */\n  resume(fineTuningJobID: string, options?: RequestOptions): APIPromise<FineTuningJob> {\n    return this._client.post(path`/fine_tuning/jobs/${fineTuningJobID}/resume`, options);\n  }\n}\n\nexport type FineTuningJobsPage = CursorPage<FineTuningJob>;\n\nexport type FineTuningJobEventsPage = CursorPage<FineTuningJobEvent>;\n\n/**\n * The `fine_tuning.job` object represents a fine-tuning job that has been created\n * through the API.\n */\nexport interface FineTuningJob {\n  /**\n   * The object identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was created.\n   */\n  created_at: number;\n\n  /**\n   * For fine-tuning jobs that have `failed`, this will contain more information on\n   * the cause of the failure.\n   */\n  error: FineTuningJob.Error | null;\n\n  /**\n   * The name of the fine-tuned model that is being created. The value will be null\n   * if the fine-tuning job is still running.\n   */\n  fine_tuned_model: string | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was finished. The\n   * value will be null if the fine-tuning job is still running.\n   */\n  finished_at: number | null;\n\n  /**\n   * The hyperparameters used for the fine-tuning job. This value will only be\n   * returned when running `supervised` jobs.\n   */\n  hyperparameters: FineTuningJob.Hyperparameters;\n\n  /**\n   * The base model that is being fine-tuned.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"fine_tuning.job\".\n   */\n  object: 'fine_tuning.job';\n\n  /**\n   * The organization that owns the fine-tuning job.\n   */\n  organization_id: string;\n\n  /**\n   * The compiled results file ID(s) for the fine-tuning job. You can retrieve the\n   * results with the\n   * [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).\n   */\n  result_files: Array<string>;\n\n  /**\n   * The seed used for the fine-tuning job.\n   */\n  seed: number;\n\n  /**\n   * The current status of the fine-tuning job, which can be either\n   * `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.\n   */\n  status: 'validating_files' | 'queued' | 'running' | 'succeeded' | 'failed' | 'cancelled';\n\n  /**\n   * The total number of billable tokens processed by this fine-tuning job. The value\n   * will be null if the fine-tuning job is still running.\n   */\n  trained_tokens: number | null;\n\n  /**\n   * The file ID used for training. You can retrieve the training data with the\n   * [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).\n   */\n  training_file: string;\n\n  /**\n   * The file ID used for validation. You can retrieve the validation results with\n   * the\n   * [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).\n   */\n  validation_file: string | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job is estimated to\n   * finish. The value will be null if the fine-tuning job is not running.\n   */\n  estimated_finish?: number | null;\n\n  /**\n   * A list of integrations to enable for this fine-tuning job.\n   */\n  integrations?: Array<FineTuningJobWandbIntegrationObject> | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The method used for fine-tuning.\n   */\n  method?: FineTuningJob.Method;\n}\n\nexport namespace FineTuningJob {\n  /**\n   * For fine-tuning jobs that have `failed`, this will contain more information on\n   * the cause of the failure.\n   */\n  export interface Error {\n    /**\n     * A machine-readable error code.\n     */\n    code: string;\n\n    /**\n     * A human-readable error message.\n     */\n    message: string;\n\n    /**\n     * The parameter that was invalid, usually `training_file` or `validation_file`.\n     * This field will be null if the failure was not parameter-specific.\n     */\n    param: string | null;\n  }\n\n  /**\n   * The hyperparameters used for the fine-tuning job. This value will only be\n   * returned when running `supervised` jobs.\n   */\n  export interface Hyperparameters {\n    /**\n     * Number of examples in each batch. A larger batch size means that model\n     * parameters are updated less frequently, but with lower variance.\n     */\n    batch_size?: 'auto' | number | null;\n\n    /**\n     * Scaling factor for the learning rate. A smaller learning rate may be useful to\n     * avoid overfitting.\n     */\n    learning_rate_multiplier?: 'auto' | number;\n\n    /**\n     * The number of epochs to train the model for. An epoch refers to one full cycle\n     * through the training dataset.\n     */\n    n_epochs?: 'auto' | number;\n  }\n\n  /**\n   * The method used for fine-tuning.\n   */\n  export interface Method {\n    /**\n     * The type of method. Is either `supervised`, `dpo`, or `reinforcement`.\n     */\n    type: 'supervised' | 'dpo' | 'reinforcement';\n\n    /**\n     * Configuration for the DPO fine-tuning method.\n     */\n    dpo?: MethodsAPI.DpoMethod;\n\n    /**\n     * Configuration for the reinforcement fine-tuning method.\n     */\n    reinforcement?: MethodsAPI.ReinforcementMethod;\n\n    /**\n     * Configuration for the supervised fine-tuning method.\n     */\n    supervised?: MethodsAPI.SupervisedMethod;\n  }\n}\n\n/**\n * Fine-tuning job event object\n */\nexport interface FineTuningJobEvent {\n  /**\n   * The object identifier.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was created.\n   */\n  created_at: number;\n\n  /**\n   * The log level of the event.\n   */\n  level: 'info' | 'warn' | 'error';\n\n  /**\n   * The message of the event.\n   */\n  message: string;\n\n  /**\n   * The object type, which is always \"fine_tuning.job.event\".\n   */\n  object: 'fine_tuning.job.event';\n\n  /**\n   * The data associated with the event.\n   */\n  data?: unknown;\n\n  /**\n   * The type of event.\n   */\n  type?: 'message' | 'metrics';\n}\n\n/**\n * The settings for your integration with Weights and Biases. This payload\n * specifies the project that metrics will be sent to. Optionally, you can set an\n * explicit display name for your run, add tags to your run, and set a default\n * entity (team, username, etc) to be associated with your run.\n */\nexport interface FineTuningJobWandbIntegration {\n  /**\n   * The name of the project that the new run will be created under.\n   */\n  project: string;\n\n  /**\n   * The entity to use for the run. This allows you to set the team or username of\n   * the WandB user that you would like associated with the run. If not set, the\n   * default entity for the registered WandB API key is used.\n   */\n  entity?: string | null;\n\n  /**\n   * A display name to set for the run. If not set, we will use the Job ID as the\n   * name.\n   */\n  name?: string | null;\n\n  /**\n   * A list of tags to be attached to the newly created run. These tags are passed\n   * through directly to WandB. Some default tags are generated by OpenAI:\n   * \"openai/finetune\", \"openai/{base-model}\", \"openai/{ftjob-abcdef}\".\n   */\n  tags?: Array<string>;\n}\n\nexport interface FineTuningJobWandbIntegrationObject {\n  /**\n   * The type of the integration being enabled for the fine-tuning job\n   */\n  type: 'wandb';\n\n  /**\n   * The settings for your integration with Weights and Biases. This payload\n   * specifies the project that metrics will be sent to. Optionally, you can set an\n   * explicit display name for your run, add tags to your run, and set a default\n   * entity (team, username, etc) to be associated with your run.\n   */\n  wandb: FineTuningJobWandbIntegration;\n}\n\nexport type FineTuningJobIntegration = FineTuningJobWandbIntegrationObject;\n\nexport interface JobCreateParams {\n  /**\n   * The name of the model to fine-tune. You can select one of the\n   * [supported models](https://platform.openai.com/docs/guides/fine-tuning#which-models-can-be-fine-tuned).\n   */\n  model: (string & {}) | 'babbage-002' | 'davinci-002' | 'gpt-3.5-turbo' | 'gpt-4o-mini';\n\n  /**\n   * The ID of an uploaded file that contains training data.\n   *\n   * See [upload file](https://platform.openai.com/docs/api-reference/files/create)\n   * for how to upload a file.\n   *\n   * Your dataset must be formatted as a JSONL file. Additionally, you must upload\n   * your file with the purpose `fine-tune`.\n   *\n   * The contents of the file should differ depending on if the model uses the\n   * [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input),\n   * [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)\n   * format, or if the fine-tuning method uses the\n   * [preference](https://platform.openai.com/docs/api-reference/fine-tuning/preference-input)\n   * format.\n   *\n   * See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)\n   * for more details.\n   */\n  training_file: string;\n\n  /**\n   * @deprecated The hyperparameters used for the fine-tuning job. This value is now\n   * deprecated in favor of `method`, and should be passed in under the `method`\n   * parameter.\n   */\n  hyperparameters?: JobCreateParams.Hyperparameters;\n\n  /**\n   * A list of integrations to enable for your fine-tuning job.\n   */\n  integrations?: Array<JobCreateParams.Integration> | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The method used for fine-tuning.\n   */\n  method?: JobCreateParams.Method;\n\n  /**\n   * The seed controls the reproducibility of the job. Passing in the same seed and\n   * job parameters should produce the same results, but may differ in rare cases. If\n   * a seed is not specified, one will be generated for you.\n   */\n  seed?: number | null;\n\n  /**\n   * A string of up to 64 characters that will be added to your fine-tuned model\n   * name.\n   *\n   * For example, a `suffix` of \"custom-model-name\" would produce a model name like\n   * `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`.\n   */\n  suffix?: string | null;\n\n  /**\n   * The ID of an uploaded file that contains validation data.\n   *\n   * If you provide this file, the data is used to generate validation metrics\n   * periodically during fine-tuning. These metrics can be viewed in the fine-tuning\n   * results file. The same data should not be present in both train and validation\n   * files.\n   *\n   * Your dataset must be formatted as a JSONL file. You must upload your file with\n   * the purpose `fine-tune`.\n   *\n   * See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)\n   * for more details.\n   */\n  validation_file?: string | null;\n}\n\nexport namespace JobCreateParams {\n  /**\n   * @deprecated The hyperparameters used for the fine-tuning job. This value is now\n   * deprecated in favor of `method`, and should be passed in under the `method`\n   * parameter.\n   */\n  export interface Hyperparameters {\n    /**\n     * Number of examples in each batch. A larger batch size means that model\n     * parameters are updated less frequently, but with lower variance.\n     */\n    batch_size?: 'auto' | number;\n\n    /**\n     * Scaling factor for the learning rate. A smaller learning rate may be useful to\n     * avoid overfitting.\n     */\n    learning_rate_multiplier?: 'auto' | number;\n\n    /**\n     * The number of epochs to train the model for. An epoch refers to one full cycle\n     * through the training dataset.\n     */\n    n_epochs?: 'auto' | number;\n  }\n\n  export interface Integration {\n    /**\n     * The type of integration to enable. Currently, only \"wandb\" (Weights and Biases)\n     * is supported.\n     */\n    type: 'wandb';\n\n    /**\n     * The settings for your integration with Weights and Biases. This payload\n     * specifies the project that metrics will be sent to. Optionally, you can set an\n     * explicit display name for your run, add tags to your run, and set a default\n     * entity (team, username, etc) to be associated with your run.\n     */\n    wandb: Integration.Wandb;\n  }\n\n  export namespace Integration {\n    /**\n     * The settings for your integration with Weights and Biases. This payload\n     * specifies the project that metrics will be sent to. Optionally, you can set an\n     * explicit display name for your run, add tags to your run, and set a default\n     * entity (team, username, etc) to be associated with your run.\n     */\n    export interface Wandb {\n      /**\n       * The name of the project that the new run will be created under.\n       */\n      project: string;\n\n      /**\n       * The entity to use for the run. This allows you to set the team or username of\n       * the WandB user that you would like associated with the run. If not set, the\n       * default entity for the registered WandB API key is used.\n       */\n      entity?: string | null;\n\n      /**\n       * A display name to set for the run. If not set, we will use the Job ID as the\n       * name.\n       */\n      name?: string | null;\n\n      /**\n       * A list of tags to be attached to the newly created run. These tags are passed\n       * through directly to WandB. Some default tags are generated by OpenAI:\n       * \"openai/finetune\", \"openai/{base-model}\", \"openai/{ftjob-abcdef}\".\n       */\n      tags?: Array<string>;\n    }\n  }\n\n  /**\n   * The method used for fine-tuning.\n   */\n  export interface Method {\n    /**\n     * The type of method. Is either `supervised`, `dpo`, or `reinforcement`.\n     */\n    type: 'supervised' | 'dpo' | 'reinforcement';\n\n    /**\n     * Configuration for the DPO fine-tuning method.\n     */\n    dpo?: MethodsAPI.DpoMethod;\n\n    /**\n     * Configuration for the reinforcement fine-tuning method.\n     */\n    reinforcement?: MethodsAPI.ReinforcementMethod;\n\n    /**\n     * Configuration for the supervised fine-tuning method.\n     */\n    supervised?: MethodsAPI.SupervisedMethod;\n  }\n}\n\nexport interface JobListParams extends CursorPageParams {\n  /**\n   * Optional metadata filter. To filter, use the syntax `metadata[k]=v`.\n   * Alternatively, set `metadata=null` to indicate no metadata.\n   */\n  metadata?: { [key: string]: string } | null;\n}\n\nexport interface JobListEventsParams extends CursorPageParams {}\n\nJobs.Checkpoints = Checkpoints;\n\nexport declare namespace Jobs {\n  export {\n    type FineTuningJob as FineTuningJob,\n    type FineTuningJobEvent as FineTuningJobEvent,\n    type FineTuningJobWandbIntegration as FineTuningJobWandbIntegration,\n    type FineTuningJobWandbIntegrationObject as FineTuningJobWandbIntegrationObject,\n    type FineTuningJobIntegration as FineTuningJobIntegration,\n    type FineTuningJobsPage as FineTuningJobsPage,\n    type FineTuningJobEventsPage as FineTuningJobEventsPage,\n    type JobCreateParams as JobCreateParams,\n    type JobListParams as JobListParams,\n    type JobListEventsParams as JobListEventsParams,\n  };\n\n  export {\n    Checkpoints as Checkpoints,\n    type FineTuningJobCheckpoint as FineTuningJobCheckpoint,\n    type FineTuningJobCheckpointsPage as FineTuningJobCheckpointsPage,\n    type CheckpointListParams as CheckpointListParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as MethodsAPI from './methods';\nimport {\n  DpoHyperparameters,\n  DpoMethod,\n  Methods,\n  ReinforcementHyperparameters,\n  ReinforcementMethod,\n  SupervisedHyperparameters,\n  SupervisedMethod,\n} from './methods';\nimport * as AlphaAPI from './alpha/alpha';\nimport { Alpha } from './alpha/alpha';\nimport * as CheckpointsAPI from './checkpoints/checkpoints';\nimport { Checkpoints } from './checkpoints/checkpoints';\nimport * as JobsAPI from './jobs/jobs';\nimport {\n  FineTuningJob,\n  FineTuningJobEvent,\n  FineTuningJobEventsPage,\n  FineTuningJobIntegration,\n  FineTuningJobWandbIntegration,\n  FineTuningJobWandbIntegrationObject,\n  FineTuningJobsPage,\n  JobCreateParams,\n  JobListEventsParams,\n  JobListParams,\n  Jobs,\n} from './jobs/jobs';\n\nexport class FineTuning extends APIResource {\n  methods: MethodsAPI.Methods = new MethodsAPI.Methods(this._client);\n  jobs: JobsAPI.Jobs = new JobsAPI.Jobs(this._client);\n  checkpoints: CheckpointsAPI.Checkpoints = new CheckpointsAPI.Checkpoints(this._client);\n  alpha: AlphaAPI.Alpha = new AlphaAPI.Alpha(this._client);\n}\n\nFineTuning.Methods = Methods;\nFineTuning.Jobs = Jobs;\nFineTuning.Checkpoints = Checkpoints;\nFineTuning.Alpha = Alpha;\n\nexport declare namespace FineTuning {\n  export {\n    Methods as Methods,\n    type DpoHyperparameters as DpoHyperparameters,\n    type DpoMethod as DpoMethod,\n    type ReinforcementHyperparameters as ReinforcementHyperparameters,\n    type ReinforcementMethod as ReinforcementMethod,\n    type SupervisedHyperparameters as SupervisedHyperparameters,\n    type SupervisedMethod as SupervisedMethod,\n  };\n\n  export {\n    Jobs as Jobs,\n    type FineTuningJob as FineTuningJob,\n    type FineTuningJobEvent as FineTuningJobEvent,\n    type FineTuningJobWandbIntegration as FineTuningJobWandbIntegration,\n    type FineTuningJobWandbIntegrationObject as FineTuningJobWandbIntegrationObject,\n    type FineTuningJobIntegration as FineTuningJobIntegration,\n    type FineTuningJobsPage as FineTuningJobsPage,\n    type FineTuningJobEventsPage as FineTuningJobEventsPage,\n    type JobCreateParams as JobCreateParams,\n    type JobListParams as JobListParams,\n    type JobListEventsParams as JobListEventsParams,\n  };\n\n  export { Checkpoints as Checkpoints };\n\n  export { Alpha as Alpha };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as GraderModelsAPI from './grader-models';\nimport * as Shared from '../shared';\nimport * as ResponsesAPI from '../responses/responses';\n\nexport class GraderModels extends APIResource {}\n\n/**\n * A list of inputs, each of which may be either an input text, output text, input\n * image, or input audio object.\n */\nexport type GraderInputs = Array<\n  | string\n  | ResponsesAPI.ResponseInputText\n  | GraderInputs.OutputText\n  | GraderInputs.InputImage\n  | ResponsesAPI.ResponseInputAudio\n>;\n\nexport namespace GraderInputs {\n  /**\n   * A text output from the model.\n   */\n  export interface OutputText {\n    /**\n     * The text output from the model.\n     */\n    text: string;\n\n    /**\n     * The type of the output text. Always `output_text`.\n     */\n    type: 'output_text';\n  }\n\n  /**\n   * An image input block used within EvalItem content arrays.\n   */\n  export interface InputImage {\n    /**\n     * The URL of the image input.\n     */\n    image_url: string;\n\n    /**\n     * The type of the image input. Always `input_image`.\n     */\n    type: 'input_image';\n\n    /**\n     * The detail level of the image to be sent to the model. One of `high`, `low`, or\n     * `auto`. Defaults to `auto`.\n     */\n    detail?: string;\n  }\n}\n\n/**\n * A LabelModelGrader object which uses a model to assign labels to each item in\n * the evaluation.\n */\nexport interface LabelModelGrader {\n  input: Array<LabelModelGrader.Input>;\n\n  /**\n   * The labels to assign to each item in the evaluation.\n   */\n  labels: Array<string>;\n\n  /**\n   * The model to use for the evaluation. Must support structured outputs.\n   */\n  model: string;\n\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The labels that indicate a passing result. Must be a subset of labels.\n   */\n  passing_labels: Array<string>;\n\n  /**\n   * The object type, which is always `label_model`.\n   */\n  type: 'label_model';\n}\n\nexport namespace LabelModelGrader {\n  /**\n   * A message input to the model with a role indicating instruction following\n   * hierarchy. Instructions given with the `developer` or `system` role take\n   * precedence over instructions given with the `user` role. Messages with the\n   * `assistant` role are presumed to have been generated by the model in previous\n   * interactions.\n   */\n  export interface Input {\n    /**\n     * Inputs to the model - can contain template strings. Supports text, output text,\n     * input images, and input audio, either as a single item or an array of items.\n     */\n    content:\n      | string\n      | ResponsesAPI.ResponseInputText\n      | Input.OutputText\n      | Input.InputImage\n      | ResponsesAPI.ResponseInputAudio\n      | GraderModelsAPI.GraderInputs;\n\n    /**\n     * The role of the message input. One of `user`, `assistant`, `system`, or\n     * `developer`.\n     */\n    role: 'user' | 'assistant' | 'system' | 'developer';\n\n    /**\n     * The type of the message input. Always `message`.\n     */\n    type?: 'message';\n  }\n\n  export namespace Input {\n    /**\n     * A text output from the model.\n     */\n    export interface OutputText {\n      /**\n       * The text output from the model.\n       */\n      text: string;\n\n      /**\n       * The type of the output text. Always `output_text`.\n       */\n      type: 'output_text';\n    }\n\n    /**\n     * An image input block used within EvalItem content arrays.\n     */\n    export interface InputImage {\n      /**\n       * The URL of the image input.\n       */\n      image_url: string;\n\n      /**\n       * The type of the image input. Always `input_image`.\n       */\n      type: 'input_image';\n\n      /**\n       * The detail level of the image to be sent to the model. One of `high`, `low`, or\n       * `auto`. Defaults to `auto`.\n       */\n      detail?: string;\n    }\n  }\n}\n\n/**\n * A MultiGrader object combines the output of multiple graders to produce a single\n * score.\n */\nexport interface MultiGrader {\n  /**\n   * A formula to calculate the output based on grader results.\n   */\n  calculate_output: string;\n\n  /**\n   * A StringCheckGrader object that performs a string comparison between input and\n   * reference using a specified operation.\n   */\n  graders: StringCheckGrader | TextSimilarityGrader | PythonGrader | ScoreModelGrader | LabelModelGrader;\n\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The object type, which is always `multi`.\n   */\n  type: 'multi';\n}\n\n/**\n * A PythonGrader object that runs a python script on the input.\n */\nexport interface PythonGrader {\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The source code of the python script.\n   */\n  source: string;\n\n  /**\n   * The object type, which is always `python`.\n   */\n  type: 'python';\n\n  /**\n   * The image tag to use for the python script.\n   */\n  image_tag?: string;\n}\n\n/**\n * A ScoreModelGrader object that uses a model to assign a score to the input.\n */\nexport interface ScoreModelGrader {\n  /**\n   * The input messages evaluated by the grader. Supports text, output text, input\n   * image, and input audio content blocks, and may include template strings.\n   */\n  input: Array<ScoreModelGrader.Input>;\n\n  /**\n   * The model to use for the evaluation.\n   */\n  model: string;\n\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The object type, which is always `score_model`.\n   */\n  type: 'score_model';\n\n  /**\n   * The range of the score. Defaults to `[0, 1]`.\n   */\n  range?: Array<number>;\n\n  /**\n   * The sampling parameters for the model.\n   */\n  sampling_params?: ScoreModelGrader.SamplingParams;\n}\n\nexport namespace ScoreModelGrader {\n  /**\n   * A message input to the model with a role indicating instruction following\n   * hierarchy. Instructions given with the `developer` or `system` role take\n   * precedence over instructions given with the `user` role. Messages with the\n   * `assistant` role are presumed to have been generated by the model in previous\n   * interactions.\n   */\n  export interface Input {\n    /**\n     * Inputs to the model - can contain template strings. Supports text, output text,\n     * input images, and input audio, either as a single item or an array of items.\n     */\n    content:\n      | string\n      | ResponsesAPI.ResponseInputText\n      | Input.OutputText\n      | Input.InputImage\n      | ResponsesAPI.ResponseInputAudio\n      | GraderModelsAPI.GraderInputs;\n\n    /**\n     * The role of the message input. One of `user`, `assistant`, `system`, or\n     * `developer`.\n     */\n    role: 'user' | 'assistant' | 'system' | 'developer';\n\n    /**\n     * The type of the message input. Always `message`.\n     */\n    type?: 'message';\n  }\n\n  export namespace Input {\n    /**\n     * A text output from the model.\n     */\n    export interface OutputText {\n      /**\n       * The text output from the model.\n       */\n      text: string;\n\n      /**\n       * The type of the output text. Always `output_text`.\n       */\n      type: 'output_text';\n    }\n\n    /**\n     * An image input block used within EvalItem content arrays.\n     */\n    export interface InputImage {\n      /**\n       * The URL of the image input.\n       */\n      image_url: string;\n\n      /**\n       * The type of the image input. Always `input_image`.\n       */\n      type: 'input_image';\n\n      /**\n       * The detail level of the image to be sent to the model. One of `high`, `low`, or\n       * `auto`. Defaults to `auto`.\n       */\n      detail?: string;\n    }\n  }\n\n  /**\n   * The sampling parameters for the model.\n   */\n  export interface SamplingParams {\n    /**\n     * The maximum number of tokens the grader model may generate in its response.\n     */\n    max_completions_tokens?: number | null;\n\n    /**\n     * Constrains effort on reasoning for\n     * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n     * supported values are `none`, `minimal`, `low`, `medium`, `high`, and `xhigh`.\n     * Reducing reasoning effort can result in faster responses and fewer tokens used\n     * on reasoning in a response.\n     *\n     * - `gpt-5.1` defaults to `none`, which does not perform reasoning. The supported\n     *   reasoning values for `gpt-5.1` are `none`, `low`, `medium`, and `high`. Tool\n     *   calls are supported for all reasoning values in gpt-5.1.\n     * - All models before `gpt-5.1` default to `medium` reasoning effort, and do not\n     *   support `none`.\n     * - The `gpt-5-pro` model defaults to (and only supports) `high` reasoning effort.\n     * - `xhigh` is supported for all models after `gpt-5.1-codex-max`.\n     */\n    reasoning_effort?: Shared.ReasoningEffort | null;\n\n    /**\n     * A seed value to initialize the randomness, during sampling.\n     */\n    seed?: number | null;\n\n    /**\n     * A higher temperature increases randomness in the outputs.\n     */\n    temperature?: number | null;\n\n    /**\n     * An alternative to temperature for nucleus sampling; 1.0 includes all tokens.\n     */\n    top_p?: number | null;\n  }\n}\n\n/**\n * A StringCheckGrader object that performs a string comparison between input and\n * reference using a specified operation.\n */\nexport interface StringCheckGrader {\n  /**\n   * The input text. This may include template strings.\n   */\n  input: string;\n\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`.\n   */\n  operation: 'eq' | 'ne' | 'like' | 'ilike';\n\n  /**\n   * The reference text. This may include template strings.\n   */\n  reference: string;\n\n  /**\n   * The object type, which is always `string_check`.\n   */\n  type: 'string_check';\n}\n\n/**\n * A TextSimilarityGrader object which grades text based on similarity metrics.\n */\nexport interface TextSimilarityGrader {\n  /**\n   * The evaluation metric to use. One of `cosine`, `fuzzy_match`, `bleu`, `gleu`,\n   * `meteor`, `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`, or `rouge_l`.\n   */\n  evaluation_metric:\n    | 'cosine'\n    | 'fuzzy_match'\n    | 'bleu'\n    | 'gleu'\n    | 'meteor'\n    | 'rouge_1'\n    | 'rouge_2'\n    | 'rouge_3'\n    | 'rouge_4'\n    | 'rouge_5'\n    | 'rouge_l';\n\n  /**\n   * The text being graded.\n   */\n  input: string;\n\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The text being graded against.\n   */\n  reference: string;\n\n  /**\n   * The type of grader.\n   */\n  type: 'text_similarity';\n}\n\nexport declare namespace GraderModels {\n  export {\n    type GraderInputs as GraderInputs,\n    type LabelModelGrader as LabelModelGrader,\n    type MultiGrader as MultiGrader,\n    type PythonGrader as PythonGrader,\n    type ScoreModelGrader as ScoreModelGrader,\n    type StringCheckGrader as StringCheckGrader,\n    type TextSimilarityGrader as TextSimilarityGrader,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as GraderModelsAPI from './grader-models';\nimport {\n  GraderInputs,\n  GraderModels,\n  LabelModelGrader,\n  MultiGrader,\n  PythonGrader,\n  ScoreModelGrader,\n  StringCheckGrader,\n  TextSimilarityGrader,\n} from './grader-models';\n\nexport class Graders extends APIResource {\n  graderModels: GraderModelsAPI.GraderModels = new GraderModelsAPI.GraderModels(this._client);\n}\n\nGraders.GraderModels = GraderModels;\n\nexport declare namespace Graders {\n  export {\n    GraderModels as GraderModels,\n    type GraderInputs as GraderInputs,\n    type LabelModelGrader as LabelModelGrader,\n    type MultiGrader as MultiGrader,\n    type PythonGrader as PythonGrader,\n    type ScoreModelGrader as ScoreModelGrader,\n    type StringCheckGrader as StringCheckGrader,\n    type TextSimilarityGrader as TextSimilarityGrader,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport * as ImagesAPI from './images';\nimport { APIPromise } from '../core/api-promise';\nimport { Stream } from '../core/streaming';\nimport { type Uploadable } from '../core/uploads';\nimport { RequestOptions } from '../internal/request-options';\nimport { multipartFormRequestOptions } from '../internal/uploads';\n\nexport class Images extends APIResource {\n  /**\n   * Creates a variation of a given image. This endpoint only supports `dall-e-2`.\n   *\n   * @example\n   * ```ts\n   * const imagesResponse = await client.images.createVariation({\n   *   image: fs.createReadStream('otter.png'),\n   * });\n   * ```\n   */\n  createVariation(body: ImageCreateVariationParams, options?: RequestOptions): APIPromise<ImagesResponse> {\n    return this._client.post(\n      '/images/variations',\n      multipartFormRequestOptions({ body, ...options }, this._client),\n    );\n  }\n\n  /**\n   * Creates an edited or extended image given one or more source images and a\n   * prompt. This endpoint supports GPT Image models (`gpt-image-1.5`, `gpt-image-1`,\n   * `gpt-image-1-mini`, and `chatgpt-image-latest`) and `dall-e-2`.\n   *\n   * @example\n   * ```ts\n   * const imagesResponse = await client.images.edit({\n   *   image: fs.createReadStream('path/to/file'),\n   *   prompt: 'A cute baby sea otter wearing a beret',\n   * });\n   * ```\n   */\n  edit(body: ImageEditParamsNonStreaming, options?: RequestOptions): APIPromise<ImagesResponse>;\n  edit(body: ImageEditParamsStreaming, options?: RequestOptions): APIPromise<Stream<ImageEditStreamEvent>>;\n  edit(\n    body: ImageEditParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ImageEditStreamEvent> | ImagesResponse>;\n  edit(\n    body: ImageEditParams,\n    options?: RequestOptions,\n  ): APIPromise<ImagesResponse> | APIPromise<Stream<ImageEditStreamEvent>> {\n    return this._client.post(\n      '/images/edits',\n      multipartFormRequestOptions({ body, ...options, stream: body.stream ?? false }, this._client),\n    ) as APIPromise<ImagesResponse> | APIPromise<Stream<ImageEditStreamEvent>>;\n  }\n\n  /**\n   * Creates an image given a prompt.\n   * [Learn more](https://platform.openai.com/docs/guides/images).\n   *\n   * @example\n   * ```ts\n   * const imagesResponse = await client.images.generate({\n   *   prompt: 'A cute baby sea otter',\n   * });\n   * ```\n   */\n  generate(body: ImageGenerateParamsNonStreaming, options?: RequestOptions): APIPromise<ImagesResponse>;\n  generate(\n    body: ImageGenerateParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ImageGenStreamEvent>>;\n  generate(\n    body: ImageGenerateParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ImageGenStreamEvent> | ImagesResponse>;\n  generate(\n    body: ImageGenerateParams,\n    options?: RequestOptions,\n  ): APIPromise<ImagesResponse> | APIPromise<Stream<ImageGenStreamEvent>> {\n    return this._client.post('/images/generations', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<ImagesResponse>\n      | APIPromise<Stream<ImageGenStreamEvent>>;\n  }\n}\n\n/**\n * Represents the content or the URL of an image generated by the OpenAI API.\n */\nexport interface Image {\n  /**\n   * The base64-encoded JSON of the generated image. Returned by default for the GPT\n   * image models, and only present if `response_format` is set to `b64_json` for\n   * `dall-e-2` and `dall-e-3`.\n   */\n  b64_json?: string;\n\n  /**\n   * For `dall-e-3` only, the revised prompt that was used to generate the image.\n   */\n  revised_prompt?: string;\n\n  /**\n   * When using `dall-e-2` or `dall-e-3`, the URL of the generated image if\n   * `response_format` is set to `url` (default value). Unsupported for the GPT image\n   * models.\n   */\n  url?: string;\n}\n\n/**\n * Emitted when image editing has completed and the final image is available.\n */\nexport interface ImageEditCompletedEvent {\n  /**\n   * Base64-encoded final edited image data, suitable for rendering as an image.\n   */\n  b64_json: string;\n\n  /**\n   * The background setting for the edited image.\n   */\n  background: 'transparent' | 'opaque' | 'auto';\n\n  /**\n   * The Unix timestamp when the event was created.\n   */\n  created_at: number;\n\n  /**\n   * The output format for the edited image.\n   */\n  output_format: 'png' | 'webp' | 'jpeg';\n\n  /**\n   * The quality setting for the edited image.\n   */\n  quality: 'low' | 'medium' | 'high' | 'auto';\n\n  /**\n   * The size of the edited image.\n   */\n  size: '1024x1024' | '1024x1536' | '1536x1024' | 'auto';\n\n  /**\n   * The type of the event. Always `image_edit.completed`.\n   */\n  type: 'image_edit.completed';\n\n  /**\n   * For the GPT image models only, the token usage information for the image\n   * generation.\n   */\n  usage: ImageEditCompletedEvent.Usage;\n}\n\nexport namespace ImageEditCompletedEvent {\n  /**\n   * For the GPT image models only, the token usage information for the image\n   * generation.\n   */\n  export interface Usage {\n    /**\n     * The number of tokens (images and text) in the input prompt.\n     */\n    input_tokens: number;\n\n    /**\n     * The input tokens detailed information for the image generation.\n     */\n    input_tokens_details: Usage.InputTokensDetails;\n\n    /**\n     * The number of image tokens in the output image.\n     */\n    output_tokens: number;\n\n    /**\n     * The total number of tokens (images and text) used for the image generation.\n     */\n    total_tokens: number;\n  }\n\n  export namespace Usage {\n    /**\n     * The input tokens detailed information for the image generation.\n     */\n    export interface InputTokensDetails {\n      /**\n       * The number of image tokens in the input prompt.\n       */\n      image_tokens: number;\n\n      /**\n       * The number of text tokens in the input prompt.\n       */\n      text_tokens: number;\n    }\n  }\n}\n\n/**\n * Emitted when a partial image is available during image editing streaming.\n */\nexport interface ImageEditPartialImageEvent {\n  /**\n   * Base64-encoded partial image data, suitable for rendering as an image.\n   */\n  b64_json: string;\n\n  /**\n   * The background setting for the requested edited image.\n   */\n  background: 'transparent' | 'opaque' | 'auto';\n\n  /**\n   * The Unix timestamp when the event was created.\n   */\n  created_at: number;\n\n  /**\n   * The output format for the requested edited image.\n   */\n  output_format: 'png' | 'webp' | 'jpeg';\n\n  /**\n   * 0-based index for the partial image (streaming).\n   */\n  partial_image_index: number;\n\n  /**\n   * The quality setting for the requested edited image.\n   */\n  quality: 'low' | 'medium' | 'high' | 'auto';\n\n  /**\n   * The size of the requested edited image.\n   */\n  size: '1024x1024' | '1024x1536' | '1536x1024' | 'auto';\n\n  /**\n   * The type of the event. Always `image_edit.partial_image`.\n   */\n  type: 'image_edit.partial_image';\n}\n\n/**\n * Emitted when a partial image is available during image editing streaming.\n */\nexport type ImageEditStreamEvent = ImageEditPartialImageEvent | ImageEditCompletedEvent;\n\n/**\n * Emitted when image generation has completed and the final image is available.\n */\nexport interface ImageGenCompletedEvent {\n  /**\n   * Base64-encoded image data, suitable for rendering as an image.\n   */\n  b64_json: string;\n\n  /**\n   * The background setting for the generated image.\n   */\n  background: 'transparent' | 'opaque' | 'auto';\n\n  /**\n   * The Unix timestamp when the event was created.\n   */\n  created_at: number;\n\n  /**\n   * The output format for the generated image.\n   */\n  output_format: 'png' | 'webp' | 'jpeg';\n\n  /**\n   * The quality setting for the generated image.\n   */\n  quality: 'low' | 'medium' | 'high' | 'auto';\n\n  /**\n   * The size of the generated image.\n   */\n  size: '1024x1024' | '1024x1536' | '1536x1024' | 'auto';\n\n  /**\n   * The type of the event. Always `image_generation.completed`.\n   */\n  type: 'image_generation.completed';\n\n  /**\n   * For the GPT image models only, the token usage information for the image\n   * generation.\n   */\n  usage: ImageGenCompletedEvent.Usage;\n}\n\nexport namespace ImageGenCompletedEvent {\n  /**\n   * For the GPT image models only, the token usage information for the image\n   * generation.\n   */\n  export interface Usage {\n    /**\n     * The number of tokens (images and text) in the input prompt.\n     */\n    input_tokens: number;\n\n    /**\n     * The input tokens detailed information for the image generation.\n     */\n    input_tokens_details: Usage.InputTokensDetails;\n\n    /**\n     * The number of image tokens in the output image.\n     */\n    output_tokens: number;\n\n    /**\n     * The total number of tokens (images and text) used for the image generation.\n     */\n    total_tokens: number;\n  }\n\n  export namespace Usage {\n    /**\n     * The input tokens detailed information for the image generation.\n     */\n    export interface InputTokensDetails {\n      /**\n       * The number of image tokens in the input prompt.\n       */\n      image_tokens: number;\n\n      /**\n       * The number of text tokens in the input prompt.\n       */\n      text_tokens: number;\n    }\n  }\n}\n\n/**\n * Emitted when a partial image is available during image generation streaming.\n */\nexport interface ImageGenPartialImageEvent {\n  /**\n   * Base64-encoded partial image data, suitable for rendering as an image.\n   */\n  b64_json: string;\n\n  /**\n   * The background setting for the requested image.\n   */\n  background: 'transparent' | 'opaque' | 'auto';\n\n  /**\n   * The Unix timestamp when the event was created.\n   */\n  created_at: number;\n\n  /**\n   * The output format for the requested image.\n   */\n  output_format: 'png' | 'webp' | 'jpeg';\n\n  /**\n   * 0-based index for the partial image (streaming).\n   */\n  partial_image_index: number;\n\n  /**\n   * The quality setting for the requested image.\n   */\n  quality: 'low' | 'medium' | 'high' | 'auto';\n\n  /**\n   * The size of the requested image.\n   */\n  size: '1024x1024' | '1024x1536' | '1536x1024' | 'auto';\n\n  /**\n   * The type of the event. Always `image_generation.partial_image`.\n   */\n  type: 'image_generation.partial_image';\n}\n\n/**\n * Emitted when a partial image is available during image generation streaming.\n */\nexport type ImageGenStreamEvent = ImageGenPartialImageEvent | ImageGenCompletedEvent;\n\nexport type ImageModel = 'gpt-image-1.5' | 'dall-e-2' | 'dall-e-3' | 'gpt-image-1' | 'gpt-image-1-mini';\n\n/**\n * The response from the image generation endpoint.\n */\nexport interface ImagesResponse {\n  /**\n   * The Unix timestamp (in seconds) of when the image was created.\n   */\n  created: number;\n\n  /**\n   * The background parameter used for the image generation. Either `transparent` or\n   * `opaque`.\n   */\n  background?: 'transparent' | 'opaque';\n\n  /**\n   * The list of generated images.\n   */\n  data?: Array<Image>;\n\n  /**\n   * The output format of the image generation. Either `png`, `webp`, or `jpeg`.\n   */\n  output_format?: 'png' | 'webp' | 'jpeg';\n\n  /**\n   * The quality of the image generated. Either `low`, `medium`, or `high`.\n   */\n  quality?: 'low' | 'medium' | 'high';\n\n  /**\n   * The size of the image generated. Either `1024x1024`, `1024x1536`, or\n   * `1536x1024`.\n   */\n  size?: '1024x1024' | '1024x1536' | '1536x1024';\n\n  /**\n   * For `gpt-image-1` only, the token usage information for the image generation.\n   */\n  usage?: ImagesResponse.Usage;\n}\n\nexport namespace ImagesResponse {\n  /**\n   * For `gpt-image-1` only, the token usage information for the image generation.\n   */\n  export interface Usage {\n    /**\n     * The number of tokens (images and text) in the input prompt.\n     */\n    input_tokens: number;\n\n    /**\n     * The input tokens detailed information for the image generation.\n     */\n    input_tokens_details: Usage.InputTokensDetails;\n\n    /**\n     * The number of output tokens generated by the model.\n     */\n    output_tokens: number;\n\n    /**\n     * The total number of tokens (images and text) used for the image generation.\n     */\n    total_tokens: number;\n\n    /**\n     * The output token details for the image generation.\n     */\n    output_tokens_details?: Usage.OutputTokensDetails;\n  }\n\n  export namespace Usage {\n    /**\n     * The input tokens detailed information for the image generation.\n     */\n    export interface InputTokensDetails {\n      /**\n       * The number of image tokens in the input prompt.\n       */\n      image_tokens: number;\n\n      /**\n       * The number of text tokens in the input prompt.\n       */\n      text_tokens: number;\n    }\n\n    /**\n     * The output token details for the image generation.\n     */\n    export interface OutputTokensDetails {\n      /**\n       * The number of image output tokens generated by the model.\n       */\n      image_tokens: number;\n\n      /**\n       * The number of text output tokens generated by the model.\n       */\n      text_tokens: number;\n    }\n  }\n}\n\nexport interface ImageCreateVariationParams {\n  /**\n   * The image to use as the basis for the variation(s). Must be a valid PNG file,\n   * less than 4MB, and square.\n   */\n  image: Uploadable;\n\n  /**\n   * The model to use for image generation. Only `dall-e-2` is supported at this\n   * time.\n   */\n  model?: (string & {}) | ImageModel | null;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10.\n   */\n  n?: number | null;\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`. URLs are only valid for 60 minutes after the image has been\n   * generated.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\n   * `1024x1024`.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\n   */\n  user?: string;\n}\n\nexport type ImageEditParams = ImageEditParamsNonStreaming | ImageEditParamsStreaming;\n\nexport interface ImageEditParamsBase {\n  /**\n   * The image(s) to edit. Must be a supported image file or an array of images.\n   *\n   * For the GPT image models (`gpt-image-1`, `gpt-image-1-mini`, and\n   * `gpt-image-1.5`), each image should be a `png`, `webp`, or `jpg` file less than\n   * 50MB. You can provide up to 16 images. `chatgpt-image-latest` follows the same\n   * input constraints as GPT image models.\n   *\n   * For `dall-e-2`, you can only provide one image, and it should be a square `png`\n   * file less than 4MB.\n   */\n  image: Uploadable | Array<Uploadable>;\n\n  /**\n   * A text description of the desired image(s). The maximum length is 1000\n   * characters for `dall-e-2`, and 32000 characters for the GPT image models.\n   */\n  prompt: string;\n\n  /**\n   * Allows to set transparency for the background of the generated image(s). This\n   * parameter is only supported for the GPT image models. Must be one of\n   * `transparent`, `opaque` or `auto` (default value). When `auto` is used, the\n   * model will automatically determine the best background for the image.\n   *\n   * If `transparent`, the output format needs to support transparency, so it should\n   * be set to either `png` (default value) or `webp`.\n   */\n  background?: 'transparent' | 'opaque' | 'auto' | null;\n\n  /**\n   * Control how much effort the model will exert to match the style and features,\n   * especially facial features, of input images. This parameter is only supported\n   * for `gpt-image-1` and `gpt-image-1.5` and later models, unsupported for\n   * `gpt-image-1-mini`. Supports `high` and `low`. Defaults to `low`.\n   */\n  input_fidelity?: 'high' | 'low' | null;\n\n  /**\n   * An additional image whose fully transparent areas (e.g. where alpha is zero)\n   * indicate where `image` should be edited. If there are multiple images provided,\n   * the mask will be applied on the first image. Must be a valid PNG file, less than\n   * 4MB, and have the same dimensions as `image`.\n   */\n  mask?: Uploadable;\n\n  /**\n   * The model to use for image generation. Defaults to `gpt-image-1.5`.\n   */\n  model?: (string & {}) | ImageModel | null;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10.\n   */\n  n?: number | null;\n\n  /**\n   * The compression level (0-100%) for the generated images. This parameter is only\n   * supported for the GPT image models with the `webp` or `jpeg` output formats, and\n   * defaults to 100.\n   */\n  output_compression?: number | null;\n\n  /**\n   * The format in which the generated images are returned. This parameter is only\n   * supported for the GPT image models. Must be one of `png`, `jpeg`, or `webp`. The\n   * default value is `png`.\n   */\n  output_format?: 'png' | 'jpeg' | 'webp' | null;\n\n  /**\n   * The number of partial images to generate. This parameter is used for streaming\n   * responses that return partial images. Value must be between 0 and 3. When set to\n   * 0, the response will be a single image sent in one streaming event.\n   *\n   * Note that the final image may be sent before the full number of partial images\n   * are generated if the full image is generated more quickly.\n   */\n  partial_images?: number | null;\n\n  /**\n   * The quality of the image that will be generated for GPT image models. Defaults\n   * to `auto`.\n   */\n  quality?: 'standard' | 'low' | 'medium' | 'high' | 'auto' | null;\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`. URLs are only valid for 60 minutes after the image has been\n   * generated. This parameter is only supported for `dall-e-2` (default is `url` for\n   * `dall-e-2`), as GPT image models always return base64-encoded images.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `1024x1024`, `1536x1024`\n   * (landscape), `1024x1536` (portrait), or `auto` (default value) for the GPT image\n   * models, and one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | '1536x1024' | '1024x1536' | 'auto' | null;\n\n  /**\n   * Edit the image in streaming mode. Defaults to `false`. See the\n   * [Image generation guide](https://platform.openai.com/docs/guides/image-generation)\n   * for more information.\n   */\n  stream?: boolean | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace ImageEditParams {\n  export type ImageEditParamsNonStreaming = ImagesAPI.ImageEditParamsNonStreaming;\n  export type ImageEditParamsStreaming = ImagesAPI.ImageEditParamsStreaming;\n}\n\nexport interface ImageEditParamsNonStreaming extends ImageEditParamsBase {\n  /**\n   * Edit the image in streaming mode. Defaults to `false`. See the\n   * [Image generation guide](https://platform.openai.com/docs/guides/image-generation)\n   * for more information.\n   */\n  stream?: false | null;\n}\n\nexport interface ImageEditParamsStreaming extends ImageEditParamsBase {\n  /**\n   * Edit the image in streaming mode. Defaults to `false`. See the\n   * [Image generation guide](https://platform.openai.com/docs/guides/image-generation)\n   * for more information.\n   */\n  stream: true;\n}\n\nexport type ImageGenerateParams = ImageGenerateParamsNonStreaming | ImageGenerateParamsStreaming;\n\nexport interface ImageGenerateParamsBase {\n  /**\n   * A text description of the desired image(s). The maximum length is 32000\n   * characters for the GPT image models, 1000 characters for `dall-e-2` and 4000\n   * characters for `dall-e-3`.\n   */\n  prompt: string;\n\n  /**\n   * Allows to set transparency for the background of the generated image(s). This\n   * parameter is only supported for the GPT image models. Must be one of\n   * `transparent`, `opaque` or `auto` (default value). When `auto` is used, the\n   * model will automatically determine the best background for the image.\n   *\n   * If `transparent`, the output format needs to support transparency, so it should\n   * be set to either `png` (default value) or `webp`.\n   */\n  background?: 'transparent' | 'opaque' | 'auto' | null;\n\n  /**\n   * The model to use for image generation. One of `dall-e-2`, `dall-e-3`, or a GPT\n   * image model (`gpt-image-1`, `gpt-image-1-mini`, `gpt-image-1.5`). Defaults to\n   * `dall-e-2` unless a parameter specific to the GPT image models is used.\n   */\n  model?: (string & {}) | ImageModel | null;\n\n  /**\n   * Control the content-moderation level for images generated by the GPT image\n   * models. Must be either `low` for less restrictive filtering or `auto` (default\n   * value).\n   */\n  moderation?: 'low' | 'auto' | null;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only\n   * `n=1` is supported.\n   */\n  n?: number | null;\n\n  /**\n   * The compression level (0-100%) for the generated images. This parameter is only\n   * supported for the GPT image models with the `webp` or `jpeg` output formats, and\n   * defaults to 100.\n   */\n  output_compression?: number | null;\n\n  /**\n   * The format in which the generated images are returned. This parameter is only\n   * supported for the GPT image models. Must be one of `png`, `jpeg`, or `webp`.\n   */\n  output_format?: 'png' | 'jpeg' | 'webp' | null;\n\n  /**\n   * The number of partial images to generate. This parameter is used for streaming\n   * responses that return partial images. Value must be between 0 and 3. When set to\n   * 0, the response will be a single image sent in one streaming event.\n   *\n   * Note that the final image may be sent before the full number of partial images\n   * are generated if the full image is generated more quickly.\n   */\n  partial_images?: number | null;\n\n  /**\n   * The quality of the image that will be generated.\n   *\n   * - `auto` (default value) will automatically select the best quality for the\n   *   given model.\n   * - `high`, `medium` and `low` are supported for the GPT image models.\n   * - `hd` and `standard` are supported for `dall-e-3`.\n   * - `standard` is the only option for `dall-e-2`.\n   */\n  quality?: 'standard' | 'hd' | 'low' | 'medium' | 'high' | 'auto' | null;\n\n  /**\n   * The format in which generated images with `dall-e-2` and `dall-e-3` are\n   * returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes\n   * after the image has been generated. This parameter isn't supported for the GPT\n   * image models, which always return base64-encoded images.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `1024x1024`, `1536x1024`\n   * (landscape), `1024x1536` (portrait), or `auto` (default value) for the GPT image\n   * models, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and one of\n   * `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.\n   */\n  size?:\n    | 'auto'\n    | '1024x1024'\n    | '1536x1024'\n    | '1024x1536'\n    | '256x256'\n    | '512x512'\n    | '1792x1024'\n    | '1024x1792'\n    | null;\n\n  /**\n   * Generate the image in streaming mode. Defaults to `false`. See the\n   * [Image generation guide](https://platform.openai.com/docs/guides/image-generation)\n   * for more information. This parameter is only supported for the GPT image models.\n   */\n  stream?: boolean | null;\n\n  /**\n   * The style of the generated images. This parameter is only supported for\n   * `dall-e-3`. Must be one of `vivid` or `natural`. Vivid causes the model to lean\n   * towards generating hyper-real and dramatic images. Natural causes the model to\n   * produce more natural, less hyper-real looking images.\n   */\n  style?: 'vivid' | 'natural' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace ImageGenerateParams {\n  export type ImageGenerateParamsNonStreaming = ImagesAPI.ImageGenerateParamsNonStreaming;\n  export type ImageGenerateParamsStreaming = ImagesAPI.ImageGenerateParamsStreaming;\n}\n\nexport interface ImageGenerateParamsNonStreaming extends ImageGenerateParamsBase {\n  /**\n   * Generate the image in streaming mode. Defaults to `false`. See the\n   * [Image generation guide](https://platform.openai.com/docs/guides/image-generation)\n   * for more information. This parameter is only supported for the GPT image models.\n   */\n  stream?: false | null;\n}\n\nexport interface ImageGenerateParamsStreaming extends ImageGenerateParamsBase {\n  /**\n   * Generate the image in streaming mode. Defaults to `false`. See the\n   * [Image generation guide](https://platform.openai.com/docs/guides/image-generation)\n   * for more information. This parameter is only supported for the GPT image models.\n   */\n  stream: true;\n}\n\nexport declare namespace Images {\n  export {\n    type Image as Image,\n    type ImageEditCompletedEvent as ImageEditCompletedEvent,\n    type ImageEditPartialImageEvent as ImageEditPartialImageEvent,\n    type ImageEditStreamEvent as ImageEditStreamEvent,\n    type ImageGenCompletedEvent as ImageGenCompletedEvent,\n    type ImageGenPartialImageEvent as ImageGenPartialImageEvent,\n    type ImageGenStreamEvent as ImageGenStreamEvent,\n    type ImageModel as ImageModel,\n    type ImagesResponse as ImagesResponse,\n    type ImageCreateVariationParams as ImageCreateVariationParams,\n    type ImageEditParams as ImageEditParams,\n    type ImageEditParamsNonStreaming as ImageEditParamsNonStreaming,\n    type ImageEditParamsStreaming as ImageEditParamsStreaming,\n    type ImageGenerateParams as ImageGenerateParams,\n    type ImageGenerateParamsNonStreaming as ImageGenerateParamsNonStreaming,\n    type ImageGenerateParamsStreaming as ImageGenerateParamsStreaming,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport { APIPromise } from '../core/api-promise';\nimport { Page, PagePromise } from '../core/pagination';\nimport { RequestOptions } from '../internal/request-options';\nimport { path } from '../internal/utils/path';\n\nexport class Models extends APIResource {\n  /**\n   * Retrieves a model instance, providing basic information about the model such as\n   * the owner and permissioning.\n   */\n  retrieve(model: string, options?: RequestOptions): APIPromise<Model> {\n    return this._client.get(path`/models/${model}`, options);\n  }\n\n  /**\n   * Lists the currently available models, and provides basic information about each\n   * one such as the owner and availability.\n   */\n  list(options?: RequestOptions): PagePromise<ModelsPage, Model> {\n    return this._client.getAPIList('/models', Page<Model>, options);\n  }\n\n  /**\n   * Delete a fine-tuned model. You must have the Owner role in your organization to\n   * delete a model.\n   */\n  delete(model: string, options?: RequestOptions): APIPromise<ModelDeleted> {\n    return this._client.delete(path`/models/${model}`, options);\n  }\n}\n\n// Note: no pagination actually occurs yet, this is for forwards-compatibility.\nexport type ModelsPage = Page<Model>;\n\n/**\n * Describes an OpenAI model offering that can be used with the API.\n */\nexport interface Model {\n  /**\n   * The model identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) when the model was created.\n   */\n  created: number;\n\n  /**\n   * The object type, which is always \"model\".\n   */\n  object: 'model';\n\n  /**\n   * The organization that owns the model.\n   */\n  owned_by: string;\n}\n\nexport interface ModelDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: string;\n}\n\nexport declare namespace Models {\n  export { type Model as Model, type ModelDeleted as ModelDeleted, type ModelsPage as ModelsPage };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport { APIPromise } from '../core/api-promise';\nimport { RequestOptions } from '../internal/request-options';\n\nexport class Moderations extends APIResource {\n  /**\n   * Classifies if text and/or image inputs are potentially harmful. Learn more in\n   * the [moderation guide](https://platform.openai.com/docs/guides/moderation).\n   */\n  create(body: ModerationCreateParams, options?: RequestOptions): APIPromise<ModerationCreateResponse> {\n    return this._client.post('/moderations', { body, ...options });\n  }\n}\n\nexport interface Moderation {\n  /**\n   * A list of the categories, and whether they are flagged or not.\n   */\n  categories: Moderation.Categories;\n\n  /**\n   * A list of the categories along with the input type(s) that the score applies to.\n   */\n  category_applied_input_types: Moderation.CategoryAppliedInputTypes;\n\n  /**\n   * A list of the categories along with their scores as predicted by model.\n   */\n  category_scores: Moderation.CategoryScores;\n\n  /**\n   * Whether any of the below categories are flagged.\n   */\n  flagged: boolean;\n}\n\nexport namespace Moderation {\n  /**\n   * A list of the categories, and whether they are flagged or not.\n   */\n  export interface Categories {\n    /**\n     * Content that expresses, incites, or promotes harassing language towards any\n     * target.\n     */\n    harassment: boolean;\n\n    /**\n     * Harassment content that also includes violence or serious harm towards any\n     * target.\n     */\n    'harassment/threatening': boolean;\n\n    /**\n     * Content that expresses, incites, or promotes hate based on race, gender,\n     * ethnicity, religion, nationality, sexual orientation, disability status, or\n     * caste. Hateful content aimed at non-protected groups (e.g., chess players) is\n     * harassment.\n     */\n    hate: boolean;\n\n    /**\n     * Hateful content that also includes violence or serious harm towards the targeted\n     * group based on race, gender, ethnicity, religion, nationality, sexual\n     * orientation, disability status, or caste.\n     */\n    'hate/threatening': boolean;\n\n    /**\n     * Content that includes instructions or advice that facilitate the planning or\n     * execution of wrongdoing, or that gives advice or instruction on how to commit\n     * illicit acts. For example, \"how to shoplift\" would fit this category.\n     */\n    illicit: boolean | null;\n\n    /**\n     * Content that includes instructions or advice that facilitate the planning or\n     * execution of wrongdoing that also includes violence, or that gives advice or\n     * instruction on the procurement of any weapon.\n     */\n    'illicit/violent': boolean | null;\n\n    /**\n     * Content that promotes, encourages, or depicts acts of self-harm, such as\n     * suicide, cutting, and eating disorders.\n     */\n    'self-harm': boolean;\n\n    /**\n     * Content that encourages performing acts of self-harm, such as suicide, cutting,\n     * and eating disorders, or that gives instructions or advice on how to commit such\n     * acts.\n     */\n    'self-harm/instructions': boolean;\n\n    /**\n     * Content where the speaker expresses that they are engaging or intend to engage\n     * in acts of self-harm, such as suicide, cutting, and eating disorders.\n     */\n    'self-harm/intent': boolean;\n\n    /**\n     * Content meant to arouse sexual excitement, such as the description of sexual\n     * activity, or that promotes sexual services (excluding sex education and\n     * wellness).\n     */\n    sexual: boolean;\n\n    /**\n     * Sexual content that includes an individual who is under 18 years old.\n     */\n    'sexual/minors': boolean;\n\n    /**\n     * Content that depicts death, violence, or physical injury.\n     */\n    violence: boolean;\n\n    /**\n     * Content that depicts death, violence, or physical injury in graphic detail.\n     */\n    'violence/graphic': boolean;\n  }\n\n  /**\n   * A list of the categories along with the input type(s) that the score applies to.\n   */\n  export interface CategoryAppliedInputTypes {\n    /**\n     * The applied input type(s) for the category 'harassment'.\n     */\n    harassment: Array<'text'>;\n\n    /**\n     * The applied input type(s) for the category 'harassment/threatening'.\n     */\n    'harassment/threatening': Array<'text'>;\n\n    /**\n     * The applied input type(s) for the category 'hate'.\n     */\n    hate: Array<'text'>;\n\n    /**\n     * The applied input type(s) for the category 'hate/threatening'.\n     */\n    'hate/threatening': Array<'text'>;\n\n    /**\n     * The applied input type(s) for the category 'illicit'.\n     */\n    illicit: Array<'text'>;\n\n    /**\n     * The applied input type(s) for the category 'illicit/violent'.\n     */\n    'illicit/violent': Array<'text'>;\n\n    /**\n     * The applied input type(s) for the category 'self-harm'.\n     */\n    'self-harm': Array<'text' | 'image'>;\n\n    /**\n     * The applied input type(s) for the category 'self-harm/instructions'.\n     */\n    'self-harm/instructions': Array<'text' | 'image'>;\n\n    /**\n     * The applied input type(s) for the category 'self-harm/intent'.\n     */\n    'self-harm/intent': Array<'text' | 'image'>;\n\n    /**\n     * The applied input type(s) for the category 'sexual'.\n     */\n    sexual: Array<'text' | 'image'>;\n\n    /**\n     * The applied input type(s) for the category 'sexual/minors'.\n     */\n    'sexual/minors': Array<'text'>;\n\n    /**\n     * The applied input type(s) for the category 'violence'.\n     */\n    violence: Array<'text' | 'image'>;\n\n    /**\n     * The applied input type(s) for the category 'violence/graphic'.\n     */\n    'violence/graphic': Array<'text' | 'image'>;\n  }\n\n  /**\n   * A list of the categories along with their scores as predicted by model.\n   */\n  export interface CategoryScores {\n    /**\n     * The score for the category 'harassment'.\n     */\n    harassment: number;\n\n    /**\n     * The score for the category 'harassment/threatening'.\n     */\n    'harassment/threatening': number;\n\n    /**\n     * The score for the category 'hate'.\n     */\n    hate: number;\n\n    /**\n     * The score for the category 'hate/threatening'.\n     */\n    'hate/threatening': number;\n\n    /**\n     * The score for the category 'illicit'.\n     */\n    illicit: number;\n\n    /**\n     * The score for the category 'illicit/violent'.\n     */\n    'illicit/violent': number;\n\n    /**\n     * The score for the category 'self-harm'.\n     */\n    'self-harm': number;\n\n    /**\n     * The score for the category 'self-harm/instructions'.\n     */\n    'self-harm/instructions': number;\n\n    /**\n     * The score for the category 'self-harm/intent'.\n     */\n    'self-harm/intent': number;\n\n    /**\n     * The score for the category 'sexual'.\n     */\n    sexual: number;\n\n    /**\n     * The score for the category 'sexual/minors'.\n     */\n    'sexual/minors': number;\n\n    /**\n     * The score for the category 'violence'.\n     */\n    violence: number;\n\n    /**\n     * The score for the category 'violence/graphic'.\n     */\n    'violence/graphic': number;\n  }\n}\n\n/**\n * An object describing an image to classify.\n */\nexport interface ModerationImageURLInput {\n  /**\n   * Contains either an image URL or a data URL for a base64 encoded image.\n   */\n  image_url: ModerationImageURLInput.ImageURL;\n\n  /**\n   * Always `image_url`.\n   */\n  type: 'image_url';\n}\n\nexport namespace ModerationImageURLInput {\n  /**\n   * Contains either an image URL or a data URL for a base64 encoded image.\n   */\n  export interface ImageURL {\n    /**\n     * Either a URL of the image or the base64 encoded image data.\n     */\n    url: string;\n  }\n}\n\nexport type ModerationModel =\n  | 'omni-moderation-latest'\n  | 'omni-moderation-2024-09-26'\n  | 'text-moderation-latest'\n  | 'text-moderation-stable';\n\n/**\n * An object describing an image to classify.\n */\nexport type ModerationMultiModalInput = ModerationImageURLInput | ModerationTextInput;\n\n/**\n * An object describing text to classify.\n */\nexport interface ModerationTextInput {\n  /**\n   * A string of text to classify.\n   */\n  text: string;\n\n  /**\n   * Always `text`.\n   */\n  type: 'text';\n}\n\n/**\n * Represents if a given text input is potentially harmful.\n */\nexport interface ModerationCreateResponse {\n  /**\n   * The unique identifier for the moderation request.\n   */\n  id: string;\n\n  /**\n   * The model used to generate the moderation results.\n   */\n  model: string;\n\n  /**\n   * A list of moderation objects.\n   */\n  results: Array<Moderation>;\n}\n\nexport interface ModerationCreateParams {\n  /**\n   * Input (or inputs) to classify. Can be a single string, an array of strings, or\n   * an array of multi-modal input objects similar to other models.\n   */\n  input: string | Array<string> | Array<ModerationMultiModalInput>;\n\n  /**\n   * The content moderation model you would like to use. Learn more in\n   * [the moderation guide](https://platform.openai.com/docs/guides/moderation), and\n   * learn about available models\n   * [here](https://platform.openai.com/docs/models#moderation).\n   */\n  model?: (string & {}) | ModerationModel;\n}\n\nexport declare namespace Moderations {\n  export {\n    type Moderation as Moderation,\n    type ModerationImageURLInput as ModerationImageURLInput,\n    type ModerationModel as ModerationModel,\n    type ModerationMultiModalInput as ModerationMultiModalInput,\n    type ModerationTextInput as ModerationTextInput,\n    type ModerationCreateResponse as ModerationCreateResponse,\n    type ModerationCreateParams as ModerationCreateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as RealtimeAPI from './realtime';\nimport * as ResponsesAPI from '../responses/responses';\nimport { APIPromise } from '../../core/api-promise';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class Calls extends APIResource {\n  /**\n   * Accept an incoming SIP call and configure the realtime session that will handle\n   * it.\n   *\n   * @example\n   * ```ts\n   * await client.realtime.calls.accept('call_id', {\n   *   type: 'realtime',\n   * });\n   * ```\n   */\n  accept(callID: string, body: CallAcceptParams, options?: RequestOptions): APIPromise<void> {\n    return this._client.post(path`/realtime/calls/${callID}/accept`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ Accept: '*/*' }, options?.headers]),\n    });\n  }\n\n  /**\n   * End an active Realtime API call, whether it was initiated over SIP or WebRTC.\n   *\n   * @example\n   * ```ts\n   * await client.realtime.calls.hangup('call_id');\n   * ```\n   */\n  hangup(callID: string, options?: RequestOptions): APIPromise<void> {\n    return this._client.post(path`/realtime/calls/${callID}/hangup`, {\n      ...options,\n      headers: buildHeaders([{ Accept: '*/*' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Transfer an active SIP call to a new destination using the SIP REFER verb.\n   *\n   * @example\n   * ```ts\n   * await client.realtime.calls.refer('call_id', {\n   *   target_uri: 'tel:+14155550123',\n   * });\n   * ```\n   */\n  refer(callID: string, body: CallReferParams, options?: RequestOptions): APIPromise<void> {\n    return this._client.post(path`/realtime/calls/${callID}/refer`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ Accept: '*/*' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Decline an incoming SIP call by returning a SIP status code to the caller.\n   *\n   * @example\n   * ```ts\n   * await client.realtime.calls.reject('call_id');\n   * ```\n   */\n  reject(\n    callID: string,\n    body: CallRejectParams | null | undefined = {},\n    options?: RequestOptions,\n  ): APIPromise<void> {\n    return this._client.post(path`/realtime/calls/${callID}/reject`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ Accept: '*/*' }, options?.headers]),\n    });\n  }\n}\n\nexport interface CallAcceptParams {\n  /**\n   * The type of session to create. Always `realtime` for the Realtime API.\n   */\n  type: 'realtime';\n\n  /**\n   * Configuration for input and output audio.\n   */\n  audio?: RealtimeAPI.RealtimeAudioConfig;\n\n  /**\n   * Additional fields to include in server outputs.\n   *\n   * `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n   * transcription.\n   */\n  include?: Array<'item.input_audio_transcription.logprobs'>;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior.\n   *\n   * Note that the server sets default instructions which will be used if this field\n   * is not set and are visible in the `session.created` event at the start of the\n   * session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_output_tokens?: number | 'inf';\n\n  /**\n   * The Realtime model used for this session.\n   */\n  model?:\n    | (string & {})\n    | 'gpt-realtime'\n    | 'gpt-realtime-1.5'\n    | 'gpt-realtime-2025-08-28'\n    | 'gpt-4o-realtime-preview'\n    | 'gpt-4o-realtime-preview-2024-10-01'\n    | 'gpt-4o-realtime-preview-2024-12-17'\n    | 'gpt-4o-realtime-preview-2025-06-03'\n    | 'gpt-4o-mini-realtime-preview'\n    | 'gpt-4o-mini-realtime-preview-2024-12-17'\n    | 'gpt-realtime-mini'\n    | 'gpt-realtime-mini-2025-10-06'\n    | 'gpt-realtime-mini-2025-12-15'\n    | 'gpt-audio-1.5'\n    | 'gpt-audio-mini'\n    | 'gpt-audio-mini-2025-10-06'\n    | 'gpt-audio-mini-2025-12-15';\n\n  /**\n   * The set of modalities the model can respond with. It defaults to `[\"audio\"]`,\n   * indicating that the model will respond with audio plus a transcript. `[\"text\"]`\n   * can be used to make the model respond with text only. It is not possible to\n   * request both `text` and `audio` at the same time.\n   */\n  output_modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsesAPI.ResponsePrompt | null;\n\n  /**\n   * How the model chooses tools. Provide one of the string modes or force a specific\n   * function/MCP tool.\n   */\n  tool_choice?: RealtimeAPI.RealtimeToolChoiceConfig;\n\n  /**\n   * Tools available to the model.\n   */\n  tools?: RealtimeAPI.RealtimeToolsConfig;\n\n  /**\n   * Realtime API can write session traces to the\n   * [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\n   * tracing is enabled for a session, the configuration cannot be modified.\n   *\n   * `auto` will create a trace for the session with default values for the workflow\n   * name, group id, and metadata.\n   */\n  tracing?: RealtimeAPI.RealtimeTracingConfig | null;\n\n  /**\n   * When the number of tokens in a conversation exceeds the model's input token\n   * limit, the conversation be truncated, meaning messages (starting from the\n   * oldest) will not be included in the model's context. A 32k context model with\n   * 4,096 max output tokens can only include 28,224 tokens in the context before\n   * truncation occurs.\n   *\n   * Clients can configure truncation behavior to truncate with a lower max token\n   * limit, which is an effective way to control token usage and cost.\n   *\n   * Truncation will reduce the number of cached tokens on the next turn (busting the\n   * cache), since messages are dropped from the beginning of the context. However,\n   * clients can also configure truncation to retain messages up to a fraction of the\n   * maximum context size, which will reduce the need for future truncations and thus\n   * improve the cache rate.\n   *\n   * Truncation can be disabled entirely, which means the server will never truncate\n   * but would instead return an error if the conversation exceeds the model's input\n   * token limit.\n   */\n  truncation?: RealtimeAPI.RealtimeTruncation;\n}\n\nexport interface CallReferParams {\n  /**\n   * URI that should appear in the SIP Refer-To header. Supports values like\n   * `tel:+14155550123` or `sip:agent@example.com`.\n   */\n  target_uri: string;\n}\n\nexport interface CallRejectParams {\n  /**\n   * SIP response code to send back to the caller. Defaults to `603` (Decline) when\n   * omitted.\n   */\n  status_code?: number;\n}\n\nexport declare namespace Calls {\n  export {\n    type CallAcceptParams as CallAcceptParams,\n    type CallReferParams as CallReferParams,\n    type CallRejectParams as CallRejectParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as ClientSecretsAPI from './client-secrets';\nimport * as RealtimeAPI from './realtime';\nimport * as ResponsesAPI from '../responses/responses';\nimport { APIPromise } from '../../core/api-promise';\nimport { RequestOptions } from '../../internal/request-options';\n\nexport class ClientSecrets extends APIResource {\n  /**\n   * Create a Realtime client secret with an associated session configuration.\n   *\n   * Client secrets are short-lived tokens that can be passed to a client app, such\n   * as a web frontend or mobile client, which grants access to the Realtime API\n   * without leaking your main API key. You can configure a custom TTL for each\n   * client secret.\n   *\n   * You can also attach session configuration options to the client secret, which\n   * will be applied to any sessions created using that client secret, but these can\n   * also be overridden by the client connection.\n   *\n   * [Learn more about authentication with client secrets over WebRTC](https://platform.openai.com/docs/guides/realtime-webrtc).\n   *\n   * Returns the created client secret and the effective session object. The client\n   * secret is a string that looks like `ek_1234`.\n   *\n   * @example\n   * ```ts\n   * const clientSecret =\n   *   await client.realtime.clientSecrets.create();\n   * ```\n   */\n  create(body: ClientSecretCreateParams, options?: RequestOptions): APIPromise<ClientSecretCreateResponse> {\n    return this._client.post('/realtime/client_secrets', { body, ...options });\n  }\n}\n\n/**\n * Ephemeral key returned by the API.\n */\nexport interface RealtimeSessionClientSecret {\n  /**\n   * Timestamp for when the token expires. Currently, all tokens expire after one\n   * minute.\n   */\n  expires_at: number;\n\n  /**\n   * Ephemeral key usable in client environments to authenticate connections to the\n   * Realtime API. Use this in client-side environments rather than a standard API\n   * token, which should only be used server-side.\n   */\n  value: string;\n}\n\n/**\n * A new Realtime session configuration, with an ephemeral key. Default TTL for\n * keys is one minute.\n */\nexport interface RealtimeSessionCreateResponse {\n  /**\n   * Ephemeral key returned by the API.\n   */\n  client_secret: RealtimeSessionClientSecret;\n\n  /**\n   * The type of session to create. Always `realtime` for the Realtime API.\n   */\n  type: 'realtime';\n\n  /**\n   * Configuration for input and output audio.\n   */\n  audio?: RealtimeSessionCreateResponse.Audio;\n\n  /**\n   * Additional fields to include in server outputs.\n   *\n   * `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n   * transcription.\n   */\n  include?: Array<'item.input_audio_transcription.logprobs'>;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior.\n   *\n   * Note that the server sets default instructions which will be used if this field\n   * is not set and are visible in the `session.created` event at the start of the\n   * session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_output_tokens?: number | 'inf';\n\n  /**\n   * The Realtime model used for this session.\n   */\n  model?:\n    | (string & {})\n    | 'gpt-realtime'\n    | 'gpt-realtime-1.5'\n    | 'gpt-realtime-2025-08-28'\n    | 'gpt-4o-realtime-preview'\n    | 'gpt-4o-realtime-preview-2024-10-01'\n    | 'gpt-4o-realtime-preview-2024-12-17'\n    | 'gpt-4o-realtime-preview-2025-06-03'\n    | 'gpt-4o-mini-realtime-preview'\n    | 'gpt-4o-mini-realtime-preview-2024-12-17'\n    | 'gpt-realtime-mini'\n    | 'gpt-realtime-mini-2025-10-06'\n    | 'gpt-realtime-mini-2025-12-15'\n    | 'gpt-audio-1.5'\n    | 'gpt-audio-mini'\n    | 'gpt-audio-mini-2025-10-06'\n    | 'gpt-audio-mini-2025-12-15';\n\n  /**\n   * The set of modalities the model can respond with. It defaults to `[\"audio\"]`,\n   * indicating that the model will respond with audio plus a transcript. `[\"text\"]`\n   * can be used to make the model respond with text only. It is not possible to\n   * request both `text` and `audio` at the same time.\n   */\n  output_modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsesAPI.ResponsePrompt | null;\n\n  /**\n   * How the model chooses tools. Provide one of the string modes or force a specific\n   * function/MCP tool.\n   */\n  tool_choice?: ResponsesAPI.ToolChoiceOptions | ResponsesAPI.ToolChoiceFunction | ResponsesAPI.ToolChoiceMcp;\n\n  /**\n   * Tools available to the model.\n   */\n  tools?: Array<RealtimeAPI.RealtimeFunctionTool | RealtimeSessionCreateResponse.McpTool>;\n\n  /**\n   * Realtime API can write session traces to the\n   * [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\n   * tracing is enabled for a session, the configuration cannot be modified.\n   *\n   * `auto` will create a trace for the session with default values for the workflow\n   * name, group id, and metadata.\n   */\n  tracing?: 'auto' | RealtimeSessionCreateResponse.TracingConfiguration | null;\n\n  /**\n   * When the number of tokens in a conversation exceeds the model's input token\n   * limit, the conversation be truncated, meaning messages (starting from the\n   * oldest) will not be included in the model's context. A 32k context model with\n   * 4,096 max output tokens can only include 28,224 tokens in the context before\n   * truncation occurs.\n   *\n   * Clients can configure truncation behavior to truncate with a lower max token\n   * limit, which is an effective way to control token usage and cost.\n   *\n   * Truncation will reduce the number of cached tokens on the next turn (busting the\n   * cache), since messages are dropped from the beginning of the context. However,\n   * clients can also configure truncation to retain messages up to a fraction of the\n   * maximum context size, which will reduce the need for future truncations and thus\n   * improve the cache rate.\n   *\n   * Truncation can be disabled entirely, which means the server will never truncate\n   * but would instead return an error if the conversation exceeds the model's input\n   * token limit.\n   */\n  truncation?: RealtimeAPI.RealtimeTruncation;\n}\n\nexport namespace RealtimeSessionCreateResponse {\n  /**\n   * Configuration for input and output audio.\n   */\n  export interface Audio {\n    input?: Audio.Input;\n\n    output?: Audio.Output;\n  }\n\n  export namespace Audio {\n    export interface Input {\n      /**\n       * The format of the input audio.\n       */\n      format?: RealtimeAPI.RealtimeAudioFormats;\n\n      /**\n       * Configuration for input audio noise reduction. This can be set to `null` to turn\n       * off. Noise reduction filters audio added to the input audio buffer before it is\n       * sent to VAD and the model. Filtering the audio can improve VAD and turn\n       * detection accuracy (reducing false positives) and model performance by improving\n       * perception of the input audio.\n       */\n      noise_reduction?: Input.NoiseReduction;\n\n      /**\n       * Configuration for input audio transcription, defaults to off and can be set to\n       * `null` to turn off once on. Input audio transcription is not native to the\n       * model, since the model consumes audio directly. Transcription runs\n       * asynchronously through\n       * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n       * and should be treated as guidance of input audio content rather than precisely\n       * what the model heard. The client can optionally set the language and prompt for\n       * transcription, these offer additional guidance to the transcription service.\n       */\n      transcription?: RealtimeAPI.AudioTranscription;\n\n      /**\n       * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n       * set to `null` to turn off, in which case the client must manually trigger model\n       * response.\n       *\n       * Server VAD means that the model will detect the start and end of speech based on\n       * audio volume and respond at the end of user speech.\n       *\n       * Semantic VAD is more advanced and uses a turn detection model (in conjunction\n       * with VAD) to semantically estimate whether the user has finished speaking, then\n       * dynamically sets a timeout based on this probability. For example, if user audio\n       * trails off with \"uhhm\", the model will score a low probability of turn end and\n       * wait longer for the user to continue speaking. This can be useful for more\n       * natural conversations, but may have a higher latency.\n       */\n      turn_detection?: Input.ServerVad | Input.SemanticVad | null;\n    }\n\n    export namespace Input {\n      /**\n       * Configuration for input audio noise reduction. This can be set to `null` to turn\n       * off. Noise reduction filters audio added to the input audio buffer before it is\n       * sent to VAD and the model. Filtering the audio can improve VAD and turn\n       * detection accuracy (reducing false positives) and model performance by improving\n       * perception of the input audio.\n       */\n      export interface NoiseReduction {\n        /**\n         * Type of noise reduction. `near_field` is for close-talking microphones such as\n         * headphones, `far_field` is for far-field microphones such as laptop or\n         * conference room microphones.\n         */\n        type?: RealtimeAPI.NoiseReductionType;\n      }\n\n      /**\n       * Server-side voice activity detection (VAD) which flips on when user speech is\n       * detected and off after a period of silence.\n       */\n      export interface ServerVad {\n        /**\n         * Type of turn detection, `server_vad` to turn on simple Server VAD.\n         */\n        type: 'server_vad';\n\n        /**\n         * Whether or not to automatically generate a response when a VAD stop event\n         * occurs. If `interrupt_response` is set to `false` this may fail to create a\n         * response if the model is already responding.\n         *\n         * If both `create_response` and `interrupt_response` are set to `false`, the model\n         * will never respond automatically but VAD events will still be emitted.\n         */\n        create_response?: boolean;\n\n        /**\n         * Optional timeout after which a model response will be triggered automatically.\n         * This is useful for situations in which a long pause from the user is unexpected,\n         * such as a phone call. The model will effectively prompt the user to continue the\n         * conversation based on the current context.\n         *\n         * The timeout value will be applied after the last model response's audio has\n         * finished playing, i.e. it's set to the `response.done` time plus audio playback\n         * duration.\n         *\n         * An `input_audio_buffer.timeout_triggered` event (plus events associated with the\n         * Response) will be emitted when the timeout is reached. Idle timeout is currently\n         * only supported for `server_vad` mode.\n         */\n        idle_timeout_ms?: number | null;\n\n        /**\n         * Whether or not to automatically interrupt (cancel) any ongoing response with\n         * output to the default conversation (i.e. `conversation` of `auto`) when a VAD\n         * start event occurs. If `true` then the response will be cancelled, otherwise it\n         * will continue until complete.\n         *\n         * If both `create_response` and `interrupt_response` are set to `false`, the model\n         * will never respond automatically but VAD events will still be emitted.\n         */\n        interrupt_response?: boolean;\n\n        /**\n         * Used only for `server_vad` mode. Amount of audio to include before the VAD\n         * detected speech (in milliseconds). Defaults to 300ms.\n         */\n        prefix_padding_ms?: number;\n\n        /**\n         * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n         * milliseconds). Defaults to 500ms. With shorter values the model will respond\n         * more quickly, but may jump in on short pauses from the user.\n         */\n        silence_duration_ms?: number;\n\n        /**\n         * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n         * defaults to 0.5. A higher threshold will require louder audio to activate the\n         * model, and thus might perform better in noisy environments.\n         */\n        threshold?: number;\n      }\n\n      /**\n       * Server-side semantic turn detection which uses a model to determine when the\n       * user has finished speaking.\n       */\n      export interface SemanticVad {\n        /**\n         * Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n         */\n        type: 'semantic_vad';\n\n        /**\n         * Whether or not to automatically generate a response when a VAD stop event\n         * occurs.\n         */\n        create_response?: boolean;\n\n        /**\n         * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n         * will wait longer for the user to continue speaking, `high` will respond more\n         * quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`,\n         * and `high` have max timeouts of 8s, 4s, and 2s respectively.\n         */\n        eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n        /**\n         * Whether or not to automatically interrupt any ongoing response with output to\n         * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n         * occurs.\n         */\n        interrupt_response?: boolean;\n      }\n    }\n\n    export interface Output {\n      /**\n       * The format of the output audio.\n       */\n      format?: RealtimeAPI.RealtimeAudioFormats;\n\n      /**\n       * The speed of the model's spoken response as a multiple of the original speed.\n       * 1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed.\n       * This value can only be changed in between model turns, not while a response is\n       * in progress.\n       *\n       * This parameter is a post-processing adjustment to the audio after it is\n       * generated, it's also possible to prompt the model to speak faster or slower.\n       */\n      speed?: number;\n\n      /**\n       * The voice the model uses to respond. Voice cannot be changed during the session\n       * once the model has responded with audio at least once. Current voice options are\n       * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`,\n       * and `cedar`. We recommend `marin` and `cedar` for best quality.\n       */\n      voice?:\n        | (string & {})\n        | 'alloy'\n        | 'ash'\n        | 'ballad'\n        | 'coral'\n        | 'echo'\n        | 'sage'\n        | 'shimmer'\n        | 'verse'\n        | 'marin'\n        | 'cedar';\n    }\n  }\n\n  /**\n   * Give the model access to additional tools via remote Model Context Protocol\n   * (MCP) servers.\n   * [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n   */\n  export interface McpTool {\n    /**\n     * A label for this MCP server, used to identify it in tool calls.\n     */\n    server_label: string;\n\n    /**\n     * The type of the MCP tool. Always `mcp`.\n     */\n    type: 'mcp';\n\n    /**\n     * List of allowed tool names or a filter object.\n     */\n    allowed_tools?: Array<string> | McpTool.McpToolFilter | null;\n\n    /**\n     * An OAuth access token that can be used with a remote MCP server, either with a\n     * custom MCP server URL or a service connector. Your application must handle the\n     * OAuth authorization flow and provide the token here.\n     */\n    authorization?: string;\n\n    /**\n     * Identifier for service connectors, like those available in ChatGPT. One of\n     * `server_url` or `connector_id` must be provided. Learn more about service\n     * connectors\n     * [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n     *\n     * Currently supported `connector_id` values are:\n     *\n     * - Dropbox: `connector_dropbox`\n     * - Gmail: `connector_gmail`\n     * - Google Calendar: `connector_googlecalendar`\n     * - Google Drive: `connector_googledrive`\n     * - Microsoft Teams: `connector_microsoftteams`\n     * - Outlook Calendar: `connector_outlookcalendar`\n     * - Outlook Email: `connector_outlookemail`\n     * - SharePoint: `connector_sharepoint`\n     */\n    connector_id?:\n      | 'connector_dropbox'\n      | 'connector_gmail'\n      | 'connector_googlecalendar'\n      | 'connector_googledrive'\n      | 'connector_microsoftteams'\n      | 'connector_outlookcalendar'\n      | 'connector_outlookemail'\n      | 'connector_sharepoint';\n\n    /**\n     * Optional HTTP headers to send to the MCP server. Use for authentication or other\n     * purposes.\n     */\n    headers?: { [key: string]: string } | null;\n\n    /**\n     * Specify which of the MCP server's tools require approval.\n     */\n    require_approval?: McpTool.McpToolApprovalFilter | 'always' | 'never' | null;\n\n    /**\n     * Optional description of the MCP server, used to provide more context.\n     */\n    server_description?: string;\n\n    /**\n     * The URL for the MCP server. One of `server_url` or `connector_id` must be\n     * provided.\n     */\n    server_url?: string;\n  }\n\n  export namespace McpTool {\n    /**\n     * A filter object to specify which tools are allowed.\n     */\n    export interface McpToolFilter {\n      /**\n       * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n       * is\n       * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n       * it will match this filter.\n       */\n      read_only?: boolean;\n\n      /**\n       * List of allowed tool names.\n       */\n      tool_names?: Array<string>;\n    }\n\n    /**\n     * Specify which of the MCP server's tools require approval. Can be `always`,\n     * `never`, or a filter object associated with tools that require approval.\n     */\n    export interface McpToolApprovalFilter {\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      always?: McpToolApprovalFilter.Always;\n\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      never?: McpToolApprovalFilter.Never;\n    }\n\n    export namespace McpToolApprovalFilter {\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      export interface Always {\n        /**\n         * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n         * is\n         * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n         * it will match this filter.\n         */\n        read_only?: boolean;\n\n        /**\n         * List of allowed tool names.\n         */\n        tool_names?: Array<string>;\n      }\n\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      export interface Never {\n        /**\n         * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n         * is\n         * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n         * it will match this filter.\n         */\n        read_only?: boolean;\n\n        /**\n         * List of allowed tool names.\n         */\n        tool_names?: Array<string>;\n      }\n    }\n  }\n\n  /**\n   * Granular configuration for tracing.\n   */\n  export interface TracingConfiguration {\n    /**\n     * The group id to attach to this trace to enable filtering and grouping in the\n     * Traces Dashboard.\n     */\n    group_id?: string;\n\n    /**\n     * The arbitrary metadata to attach to this trace to enable filtering in the Traces\n     * Dashboard.\n     */\n    metadata?: unknown;\n\n    /**\n     * The name of the workflow to attach to this trace. This is used to name the trace\n     * in the Traces Dashboard.\n     */\n    workflow_name?: string;\n  }\n}\n\n/**\n * A Realtime transcription session configuration object.\n */\nexport interface RealtimeTranscriptionSessionCreateResponse {\n  /**\n   * Unique identifier for the session that looks like `sess_1234567890abcdef`.\n   */\n  id: string;\n\n  /**\n   * The object type. Always `realtime.transcription_session`.\n   */\n  object: string;\n\n  /**\n   * The type of session. Always `transcription` for transcription sessions.\n   */\n  type: 'transcription';\n\n  /**\n   * Configuration for input audio for the session.\n   */\n  audio?: RealtimeTranscriptionSessionCreateResponse.Audio;\n\n  /**\n   * Expiration timestamp for the session, in seconds since epoch.\n   */\n  expires_at?: number;\n\n  /**\n   * Additional fields to include in server outputs.\n   *\n   * - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n   *   transcription.\n   */\n  include?: Array<'item.input_audio_transcription.logprobs'>;\n}\n\nexport namespace RealtimeTranscriptionSessionCreateResponse {\n  /**\n   * Configuration for input audio for the session.\n   */\n  export interface Audio {\n    input?: Audio.Input;\n  }\n\n  export namespace Audio {\n    export interface Input {\n      /**\n       * The PCM audio format. Only a 24kHz sample rate is supported.\n       */\n      format?: RealtimeAPI.RealtimeAudioFormats;\n\n      /**\n       * Configuration for input audio noise reduction.\n       */\n      noise_reduction?: Input.NoiseReduction;\n\n      /**\n       * Configuration of the transcription model.\n       */\n      transcription?: RealtimeAPI.AudioTranscription;\n\n      /**\n       * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n       * means that the model will detect the start and end of speech based on audio\n       * volume and respond at the end of user speech.\n       */\n      turn_detection?: ClientSecretsAPI.RealtimeTranscriptionSessionTurnDetection;\n    }\n\n    export namespace Input {\n      /**\n       * Configuration for input audio noise reduction.\n       */\n      export interface NoiseReduction {\n        /**\n         * Type of noise reduction. `near_field` is for close-talking microphones such as\n         * headphones, `far_field` is for far-field microphones such as laptop or\n         * conference room microphones.\n         */\n        type?: RealtimeAPI.NoiseReductionType;\n      }\n    }\n  }\n}\n\n/**\n * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n * means that the model will detect the start and end of speech based on audio\n * volume and respond at the end of user speech.\n */\nexport interface RealtimeTranscriptionSessionTurnDetection {\n  /**\n   * Amount of audio to include before the VAD detected speech (in milliseconds).\n   * Defaults to 300ms.\n   */\n  prefix_padding_ms?: number;\n\n  /**\n   * Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n   * With shorter values the model will respond more quickly, but may jump in on\n   * short pauses from the user.\n   */\n  silence_duration_ms?: number;\n\n  /**\n   * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n   * threshold will require louder audio to activate the model, and thus might\n   * perform better in noisy environments.\n   */\n  threshold?: number;\n\n  /**\n   * Type of turn detection, only `server_vad` is currently supported.\n   */\n  type?: string;\n}\n\n/**\n * Response from creating a session and client secret for the Realtime API.\n */\nexport interface ClientSecretCreateResponse {\n  /**\n   * Expiration timestamp for the client secret, in seconds since epoch.\n   */\n  expires_at: number;\n\n  /**\n   * The session configuration for either a realtime or transcription session.\n   */\n  session: RealtimeSessionCreateResponse | RealtimeTranscriptionSessionCreateResponse;\n\n  /**\n   * The generated client secret value.\n   */\n  value: string;\n}\n\nexport interface ClientSecretCreateParams {\n  /**\n   * Configuration for the client secret expiration. Expiration refers to the time\n   * after which a client secret will no longer be valid for creating sessions. The\n   * session itself may continue after that time once started. A secret can be used\n   * to create multiple sessions until it expires.\n   */\n  expires_after?: ClientSecretCreateParams.ExpiresAfter;\n\n  /**\n   * Session configuration to use for the client secret. Choose either a realtime\n   * session or a transcription session.\n   */\n  session?: RealtimeAPI.RealtimeSessionCreateRequest | RealtimeAPI.RealtimeTranscriptionSessionCreateRequest;\n}\n\nexport namespace ClientSecretCreateParams {\n  /**\n   * Configuration for the client secret expiration. Expiration refers to the time\n   * after which a client secret will no longer be valid for creating sessions. The\n   * session itself may continue after that time once started. A secret can be used\n   * to create multiple sessions until it expires.\n   */\n  export interface ExpiresAfter {\n    /**\n     * The anchor point for the client secret expiration, meaning that `seconds` will\n     * be added to the `created_at` time of the client secret to produce an expiration\n     * timestamp. Only `created_at` is currently supported.\n     */\n    anchor?: 'created_at';\n\n    /**\n     * The number of seconds from the anchor point to the expiration. Select a value\n     * between `10` and `7200` (2 hours). This default to 600 seconds (10 minutes) if\n     * not specified.\n     */\n    seconds?: number;\n  }\n}\n\nexport declare namespace ClientSecrets {\n  export {\n    type RealtimeSessionClientSecret as RealtimeSessionClientSecret,\n    type RealtimeSessionCreateResponse as RealtimeSessionCreateResponse,\n    type RealtimeTranscriptionSessionCreateResponse as RealtimeTranscriptionSessionCreateResponse,\n    type RealtimeTranscriptionSessionTurnDetection as RealtimeTranscriptionSessionTurnDetection,\n    type ClientSecretCreateResponse as ClientSecretCreateResponse,\n    type ClientSecretCreateParams as ClientSecretCreateParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as RealtimeAPI from './realtime';\nimport * as Shared from '../shared';\nimport * as CallsAPI from './calls';\nimport { CallAcceptParams, CallReferParams, CallRejectParams, Calls } from './calls';\nimport * as ClientSecretsAPI from './client-secrets';\nimport {\n  ClientSecretCreateParams,\n  ClientSecretCreateResponse,\n  ClientSecrets,\n  RealtimeSessionClientSecret,\n  RealtimeSessionCreateResponse,\n  RealtimeTranscriptionSessionCreateResponse,\n  RealtimeTranscriptionSessionTurnDetection,\n} from './client-secrets';\nimport * as ResponsesAPI from '../responses/responses';\n\nexport class Realtime extends APIResource {\n  clientSecrets: ClientSecretsAPI.ClientSecrets = new ClientSecretsAPI.ClientSecrets(this._client);\n  calls: CallsAPI.Calls = new CallsAPI.Calls(this._client);\n}\n\nexport interface AudioTranscription {\n  /**\n   * The language of the input audio. Supplying the input language in\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n   * format will improve accuracy and latency.\n   */\n  language?: string;\n\n  /**\n   * The model to use for transcription. Current options are `whisper-1`,\n   * `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`,\n   * `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use\n   * `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n   */\n  model?:\n    | (string & {})\n    | 'whisper-1'\n    | 'gpt-4o-mini-transcribe'\n    | 'gpt-4o-mini-transcribe-2025-12-15'\n    | 'gpt-4o-transcribe'\n    | 'gpt-4o-transcribe-diarize';\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. For `whisper-1`, the\n   * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n   * For `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the\n   * prompt is a free text string, for example \"expect words related to technology\".\n   */\n  prompt?: string;\n}\n\n/**\n * Returned when a conversation is created. Emitted right after session creation.\n */\nexport interface ConversationCreatedEvent {\n  /**\n   * The conversation resource.\n   */\n  conversation: ConversationCreatedEvent.Conversation;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The event type, must be `conversation.created`.\n   */\n  type: 'conversation.created';\n}\n\nexport namespace ConversationCreatedEvent {\n  /**\n   * The conversation resource.\n   */\n  export interface Conversation {\n    /**\n     * The unique ID of the conversation.\n     */\n    id?: string;\n\n    /**\n     * The object type, must be `realtime.conversation`.\n     */\n    object?: 'realtime.conversation';\n  }\n}\n\n/**\n * A single item within a Realtime conversation.\n */\nexport type ConversationItem =\n  | RealtimeConversationItemSystemMessage\n  | RealtimeConversationItemUserMessage\n  | RealtimeConversationItemAssistantMessage\n  | RealtimeConversationItemFunctionCall\n  | RealtimeConversationItemFunctionCallOutput\n  | RealtimeMcpApprovalResponse\n  | RealtimeMcpListTools\n  | RealtimeMcpToolCall\n  | RealtimeMcpApprovalRequest;\n\n/**\n * Sent by the server when an Item is added to the default Conversation. This can\n * happen in several cases:\n *\n * - When the client sends a `conversation.item.create` event.\n * - When the input audio buffer is committed. In this case the item will be a user\n *   message containing the audio from the buffer.\n * - When the model is generating a Response. In this case the\n *   `conversation.item.added` event will be sent when the model starts generating\n *   a specific Item, and thus it will not yet have any content (and `status` will\n *   be `in_progress`).\n *\n * The event will include the full content of the Item (except when model is\n * generating a Response) except for audio data, which can be retrieved separately\n * with a `conversation.item.retrieve` event if necessary.\n */\nexport interface ConversationItemAdded {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * A single item within a Realtime conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The event type, must be `conversation.item.added`.\n   */\n  type: 'conversation.item.added';\n\n  /**\n   * The ID of the item that precedes this one, if any. This is used to maintain\n   * ordering when items are inserted.\n   */\n  previous_item_id?: string | null;\n}\n\n/**\n * Add a new Item to the Conversation's context, including messages, function\n * calls, and function call responses. This event can be used both to populate a\n * \"history\" of the conversation and to add new items mid-stream, but has the\n * current limitation that it cannot populate assistant audio messages.\n *\n * If successful, the server will respond with a `conversation.item.created` event,\n * otherwise an `error` event will be sent.\n */\nexport interface ConversationItemCreateEvent {\n  /**\n   * A single item within a Realtime conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The event type, must be `conversation.item.create`.\n   */\n  type: 'conversation.item.create';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n\n  /**\n   * The ID of the preceding item after which the new item will be inserted. If not\n   * set, the new item will be appended to the end of the conversation.\n   *\n   * If set to `root`, the new item will be added to the beginning of the\n   * conversation.\n   *\n   * If set to an existing ID, it allows an item to be inserted mid-conversation. If\n   * the ID cannot be found, an error will be returned and the item will not be\n   * added.\n   */\n  previous_item_id?: string;\n}\n\n/**\n * Returned when a conversation item is created. There are several scenarios that\n * produce this event:\n *\n * - The server is generating a Response, which if successful will produce either\n *   one or two Items, which will be of type `message` (role `assistant`) or type\n *   `function_call`.\n * - The input audio buffer has been committed, either by the client or the server\n *   (in `server_vad` mode). The server will take the content of the input audio\n *   buffer and add it to a new user message Item.\n * - The client has sent a `conversation.item.create` event to add a new Item to\n *   the Conversation.\n */\nexport interface ConversationItemCreatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * A single item within a Realtime conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The event type, must be `conversation.item.created`.\n   */\n  type: 'conversation.item.created';\n\n  /**\n   * The ID of the preceding item in the Conversation context, allows the client to\n   * understand the order of the conversation. Can be `null` if the item has no\n   * predecessor.\n   */\n  previous_item_id?: string | null;\n}\n\n/**\n * Send this event when you want to remove any item from the conversation history.\n * The server will respond with a `conversation.item.deleted` event, unless the\n * item does not exist in the conversation history, in which case the server will\n * respond with an error.\n */\nexport interface ConversationItemDeleteEvent {\n  /**\n   * The ID of the item to delete.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.delete`.\n   */\n  type: 'conversation.item.delete';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when an item in the conversation is deleted by the client with a\n * `conversation.item.delete` event. This event is used to synchronize the server's\n * understanding of the conversation history with the client's view.\n */\nexport interface ConversationItemDeletedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item that was deleted.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.deleted`.\n   */\n  type: 'conversation.item.deleted';\n}\n\n/**\n * Returned when a conversation item is finalized.\n *\n * The event will include the full content of the Item except for audio data, which\n * can be retrieved separately with a `conversation.item.retrieve` event if needed.\n */\nexport interface ConversationItemDone {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * A single item within a Realtime conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The event type, must be `conversation.item.done`.\n   */\n  type: 'conversation.item.done';\n\n  /**\n   * The ID of the item that precedes this one, if any. This is used to maintain\n   * ordering when items are inserted.\n   */\n  previous_item_id?: string | null;\n}\n\n/**\n * This event is the output of audio transcription for user audio written to the\n * user audio buffer. Transcription begins when the input audio buffer is committed\n * by the client or server (when VAD is enabled). Transcription runs asynchronously\n * with Response creation, so this event may come before or after the Response\n * events.\n *\n * Realtime API models accept audio natively, and thus input transcription is a\n * separate process run on a separate ASR (Automatic Speech Recognition) model. The\n * transcript may diverge somewhat from the model's interpretation, and should be\n * treated as a rough guide.\n */\nexport interface ConversationItemInputAudioTranscriptionCompletedEvent {\n  /**\n   * The index of the content part containing the audio.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item containing the audio that is being transcribed.\n   */\n  item_id: string;\n\n  /**\n   * The transcribed text.\n   */\n  transcript: string;\n\n  /**\n   * The event type, must be `conversation.item.input_audio_transcription.completed`.\n   */\n  type: 'conversation.item.input_audio_transcription.completed';\n\n  /**\n   * Usage statistics for the transcription, this is billed according to the ASR\n   * model's pricing rather than the realtime model's pricing.\n   */\n  usage:\n    | ConversationItemInputAudioTranscriptionCompletedEvent.TranscriptTextUsageTokens\n    | ConversationItemInputAudioTranscriptionCompletedEvent.TranscriptTextUsageDuration;\n\n  /**\n   * The log probabilities of the transcription.\n   */\n  logprobs?: Array<LogProbProperties> | null;\n}\n\nexport namespace ConversationItemInputAudioTranscriptionCompletedEvent {\n  /**\n   * Usage statistics for models billed by token usage.\n   */\n  export interface TranscriptTextUsageTokens {\n    /**\n     * Number of input tokens billed for this request.\n     */\n    input_tokens: number;\n\n    /**\n     * Number of output tokens generated.\n     */\n    output_tokens: number;\n\n    /**\n     * Total number of tokens used (input + output).\n     */\n    total_tokens: number;\n\n    /**\n     * The type of the usage object. Always `tokens` for this variant.\n     */\n    type: 'tokens';\n\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    input_token_details?: TranscriptTextUsageTokens.InputTokenDetails;\n  }\n\n  export namespace TranscriptTextUsageTokens {\n    /**\n     * Details about the input tokens billed for this request.\n     */\n    export interface InputTokenDetails {\n      /**\n       * Number of audio tokens billed for this request.\n       */\n      audio_tokens?: number;\n\n      /**\n       * Number of text tokens billed for this request.\n       */\n      text_tokens?: number;\n    }\n  }\n\n  /**\n   * Usage statistics for models billed by audio input duration.\n   */\n  export interface TranscriptTextUsageDuration {\n    /**\n     * Duration of the input audio in seconds.\n     */\n    seconds: number;\n\n    /**\n     * The type of the usage object. Always `duration` for this variant.\n     */\n    type: 'duration';\n  }\n}\n\n/**\n * Returned when the text value of an input audio transcription content part is\n * updated with incremental transcription results.\n */\nexport interface ConversationItemInputAudioTranscriptionDeltaEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item containing the audio that is being transcribed.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.input_audio_transcription.delta`.\n   */\n  type: 'conversation.item.input_audio_transcription.delta';\n\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index?: number;\n\n  /**\n   * The text delta.\n   */\n  delta?: string;\n\n  /**\n   * The log probabilities of the transcription. These can be enabled by\n   * configurating the session with\n   * `\"include\": [\"item.input_audio_transcription.logprobs\"]`. Each entry in the\n   * array corresponds a log probability of which token would be selected for this\n   * chunk of transcription. This can help to identify if it was possible there were\n   * multiple valid options for a given chunk of transcription.\n   */\n  logprobs?: Array<LogProbProperties> | null;\n}\n\n/**\n * Returned when input audio transcription is configured, and a transcription\n * request for a user message failed. These events are separate from other `error`\n * events so that the client can identify the related Item.\n */\nexport interface ConversationItemInputAudioTranscriptionFailedEvent {\n  /**\n   * The index of the content part containing the audio.\n   */\n  content_index: number;\n\n  /**\n   * Details of the transcription error.\n   */\n  error: ConversationItemInputAudioTranscriptionFailedEvent.Error;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.input_audio_transcription.failed`.\n   */\n  type: 'conversation.item.input_audio_transcription.failed';\n}\n\nexport namespace ConversationItemInputAudioTranscriptionFailedEvent {\n  /**\n   * Details of the transcription error.\n   */\n  export interface Error {\n    /**\n     * Error code, if any.\n     */\n    code?: string;\n\n    /**\n     * A human-readable error message.\n     */\n    message?: string;\n\n    /**\n     * Parameter related to the error, if any.\n     */\n    param?: string;\n\n    /**\n     * The type of error.\n     */\n    type?: string;\n  }\n}\n\n/**\n * Returned when an input audio transcription segment is identified for an item.\n */\nexport interface ConversationItemInputAudioTranscriptionSegment {\n  /**\n   * The segment identifier.\n   */\n  id: string;\n\n  /**\n   * The index of the input audio content part within the item.\n   */\n  content_index: number;\n\n  /**\n   * End time of the segment in seconds.\n   */\n  end: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item containing the input audio content.\n   */\n  item_id: string;\n\n  /**\n   * The detected speaker label for this segment.\n   */\n  speaker: string;\n\n  /**\n   * Start time of the segment in seconds.\n   */\n  start: number;\n\n  /**\n   * The text for this segment.\n   */\n  text: string;\n\n  /**\n   * The event type, must be `conversation.item.input_audio_transcription.segment`.\n   */\n  type: 'conversation.item.input_audio_transcription.segment';\n}\n\n/**\n * Send this event when you want to retrieve the server's representation of a\n * specific item in the conversation history. This is useful, for example, to\n * inspect user audio after noise cancellation and VAD. The server will respond\n * with a `conversation.item.retrieved` event, unless the item does not exist in\n * the conversation history, in which case the server will respond with an error.\n */\nexport interface ConversationItemRetrieveEvent {\n  /**\n   * The ID of the item to retrieve.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.retrieve`.\n   */\n  type: 'conversation.item.retrieve';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Send this event to truncate a previous assistant messages audio. The server\n * will produce audio faster than realtime, so this event is useful when the user\n * interrupts to truncate audio that has already been sent to the client but not\n * yet played. This will synchronize the server's understanding of the audio with\n * the client's playback.\n *\n * Truncating audio will delete the server-side text transcript to ensure there is\n * not text in the context that hasn't been heard by the user.\n *\n * If successful, the server will respond with a `conversation.item.truncated`\n * event.\n */\nexport interface ConversationItemTruncateEvent {\n  /**\n   * Inclusive duration up to which audio is truncated, in milliseconds. If the\n   * audio_end_ms is greater than the actual audio duration, the server will respond\n   * with an error.\n   */\n  audio_end_ms: number;\n\n  /**\n   * The index of the content part to truncate. Set this to `0`.\n   */\n  content_index: number;\n\n  /**\n   * The ID of the assistant message item to truncate. Only assistant message items\n   * can be truncated.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.truncate`.\n   */\n  type: 'conversation.item.truncate';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when an earlier assistant audio message item is truncated by the client\n * with a `conversation.item.truncate` event. This event is used to synchronize the\n * server's understanding of the audio with the client's playback.\n *\n * This action will truncate the audio and remove the server-side text transcript\n * to ensure there is no text in the context that hasn't been heard by the user.\n */\nexport interface ConversationItemTruncatedEvent {\n  /**\n   * The duration up to which the audio was truncated, in milliseconds.\n   */\n  audio_end_ms: number;\n\n  /**\n   * The index of the content part that was truncated.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the assistant message item that was truncated.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `conversation.item.truncated`.\n   */\n  type: 'conversation.item.truncated';\n}\n\n/**\n * The item to add to the conversation.\n */\nexport interface ConversationItemWithReference {\n  /**\n   * For an item of type (`message` | `function_call` | `function_call_output`) this\n   * field allows the client to assign the unique ID of the item. It is not required\n   * because the server will generate one if not provided.\n   *\n   * For an item of type `item_reference`, this field is required and is a reference\n   * to any item that has previously existed in the conversation.\n   */\n  id?: string;\n\n  /**\n   * The arguments of the function call (for `function_call` items).\n   */\n  arguments?: string;\n\n  /**\n   * The ID of the function call (for `function_call` and `function_call_output`\n   * items). If passed on a `function_call_output` item, the server will check that a\n   * `function_call` item with the same ID exists in the conversation history.\n   */\n  call_id?: string;\n\n  /**\n   * The content of the message, applicable for `message` items.\n   *\n   * - Message items of role `system` support only `input_text` content\n   * - Message items of role `user` support `input_text` and `input_audio` content\n   * - Message items of role `assistant` support `text` content.\n   */\n  content?: Array<ConversationItemWithReference.Content>;\n\n  /**\n   * The name of the function being called (for `function_call` items).\n   */\n  name?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The output of the function call (for `function_call_output` items).\n   */\n  output?: string;\n\n  /**\n   * The role of the message sender (`user`, `assistant`, `system`), only applicable\n   * for `message` items.\n   */\n  role?: 'user' | 'assistant' | 'system';\n\n  /**\n   * The status of the item (`completed`, `incomplete`, `in_progress`). These have no\n   * effect on the conversation, but are accepted for consistency with the\n   * `conversation.item.created` event.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n\n  /**\n   * The type of the item (`message`, `function_call`, `function_call_output`,\n   * `item_reference`).\n   */\n  type?: 'message' | 'function_call' | 'function_call_output' | 'item_reference';\n}\n\nexport namespace ConversationItemWithReference {\n  export interface Content {\n    /**\n     * ID of a previous conversation item to reference (for `item_reference` content\n     * types in `response.create` events). These can reference both client and server\n     * created items.\n     */\n    id?: string;\n\n    /**\n     * Base64-encoded audio bytes, used for `input_audio` content type.\n     */\n    audio?: string;\n\n    /**\n     * The text content, used for `input_text` and `text` content types.\n     */\n    text?: string;\n\n    /**\n     * The transcript of the audio, used for `input_audio` content type.\n     */\n    transcript?: string;\n\n    /**\n     * The content type (`input_text`, `input_audio`, `item_reference`, `text`).\n     */\n    type?: 'input_text' | 'input_audio' | 'item_reference' | 'text';\n  }\n}\n\n/**\n * Send this event to append audio bytes to the input audio buffer. The audio\n * buffer is temporary storage you can write to and later commit. A \"commit\" will\n * create a new user message item in the conversation history from the buffer\n * content and clear the buffer. Input audio transcription (if enabled) will be\n * generated when the buffer is committed.\n *\n * If VAD is enabled the audio buffer is used to detect speech and the server will\n * decide when to commit. When Server VAD is disabled, you must commit the audio\n * buffer manually. Input audio noise reduction operates on writes to the audio\n * buffer.\n *\n * The client may choose how much audio to place in each event up to a maximum of\n * 15 MiB, for example streaming smaller chunks from the client may allow the VAD\n * to be more responsive. Unlike most other client events, the server will not send\n * a confirmation response to this event.\n */\nexport interface InputAudioBufferAppendEvent {\n  /**\n   * Base64-encoded audio bytes. This must be in the format specified by the\n   * `input_audio_format` field in the session configuration.\n   */\n  audio: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.append`.\n   */\n  type: 'input_audio_buffer.append';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Send this event to clear the audio bytes in the buffer. The server will respond\n * with an `input_audio_buffer.cleared` event.\n */\nexport interface InputAudioBufferClearEvent {\n  /**\n   * The event type, must be `input_audio_buffer.clear`.\n   */\n  type: 'input_audio_buffer.clear';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when the input audio buffer is cleared by the client with a\n * `input_audio_buffer.clear` event.\n */\nexport interface InputAudioBufferClearedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.cleared`.\n   */\n  type: 'input_audio_buffer.cleared';\n}\n\n/**\n * Send this event to commit the user input audio buffer, which will create a new\n * user message item in the conversation. This event will produce an error if the\n * input audio buffer is empty. When in Server VAD mode, the client does not need\n * to send this event, the server will commit the audio buffer automatically.\n *\n * Committing the input audio buffer will trigger input audio transcription (if\n * enabled in session configuration), but it will not create a response from the\n * model. The server will respond with an `input_audio_buffer.committed` event.\n */\nexport interface InputAudioBufferCommitEvent {\n  /**\n   * The event type, must be `input_audio_buffer.commit`.\n   */\n  type: 'input_audio_buffer.commit';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when an input audio buffer is committed, either by the client or\n * automatically in server VAD mode. The `item_id` property is the ID of the user\n * message item that will be created, thus a `conversation.item.created` event will\n * also be sent to the client.\n */\nexport interface InputAudioBufferCommittedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item that will be created.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.committed`.\n   */\n  type: 'input_audio_buffer.committed';\n\n  /**\n   * The ID of the preceding item after which the new item will be inserted. Can be\n   * `null` if the item has no predecessor.\n   */\n  previous_item_id?: string | null;\n}\n\n/**\n * **SIP Only:** Returned when an DTMF event is received. A DTMF event is a message\n * that represents a telephone keypad press (09, \\*, #, AD). The `event` property\n * is the keypad that the user press. The `received_at` is the UTC Unix Timestamp\n * that the server received the event.\n */\nexport interface InputAudioBufferDtmfEventReceivedEvent {\n  /**\n   * The telephone keypad that was pressed by the user.\n   */\n  event: string;\n\n  /**\n   * UTC Unix Timestamp when DTMF Event was received by server.\n   */\n  received_at: number;\n\n  /**\n   * The event type, must be `input_audio_buffer.dtmf_event_received`.\n   */\n  type: 'input_audio_buffer.dtmf_event_received';\n}\n\n/**\n * Sent by the server when in `server_vad` mode to indicate that speech has been\n * detected in the audio buffer. This can happen any time audio is added to the\n * buffer (unless speech is already detected). The client may want to use this\n * event to interrupt audio playback or provide visual feedback to the user.\n *\n * The client should expect to receive a `input_audio_buffer.speech_stopped` event\n * when speech stops. The `item_id` property is the ID of the user message item\n * that will be created when speech stops and will also be included in the\n * `input_audio_buffer.speech_stopped` event (unless the client manually commits\n * the audio buffer during VAD activation).\n */\nexport interface InputAudioBufferSpeechStartedEvent {\n  /**\n   * Milliseconds from the start of all audio written to the buffer during the\n   * session when speech was first detected. This will correspond to the beginning of\n   * audio sent to the model, and thus includes the `prefix_padding_ms` configured in\n   * the Session.\n   */\n  audio_start_ms: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item that will be created when speech stops.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.speech_started`.\n   */\n  type: 'input_audio_buffer.speech_started';\n}\n\n/**\n * Returned in `server_vad` mode when the server detects the end of speech in the\n * audio buffer. The server will also send an `conversation.item.created` event\n * with the user message item that is created from the audio buffer.\n */\nexport interface InputAudioBufferSpeechStoppedEvent {\n  /**\n   * Milliseconds since the session started when speech stopped. This will correspond\n   * to the end of audio sent to the model, and thus includes the\n   * `min_silence_duration_ms` configured in the Session.\n   */\n  audio_end_ms: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the user message item that will be created.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.speech_stopped`.\n   */\n  type: 'input_audio_buffer.speech_stopped';\n}\n\n/**\n * Returned when the Server VAD timeout is triggered for the input audio buffer.\n * This is configured with `idle_timeout_ms` in the `turn_detection` settings of\n * the session, and it indicates that there hasn't been any speech detected for the\n * configured duration.\n *\n * The `audio_start_ms` and `audio_end_ms` fields indicate the segment of audio\n * after the last model response up to the triggering time, as an offset from the\n * beginning of audio written to the input audio buffer. This means it demarcates\n * the segment of audio that was silent and the difference between the start and\n * end values will roughly match the configured timeout.\n *\n * The empty audio will be committed to the conversation as an `input_audio` item\n * (there will be a `input_audio_buffer.committed` event) and a model response will\n * be generated. There may be speech that didn't trigger VAD but is still detected\n * by the model, so the model may respond with something relevant to the\n * conversation or a prompt to continue speaking.\n */\nexport interface InputAudioBufferTimeoutTriggered {\n  /**\n   * Millisecond offset of audio written to the input audio buffer at the time the\n   * timeout was triggered.\n   */\n  audio_end_ms: number;\n\n  /**\n   * Millisecond offset of audio written to the input audio buffer that was after the\n   * playback time of the last model response.\n   */\n  audio_start_ms: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item associated with this segment.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `input_audio_buffer.timeout_triggered`.\n   */\n  type: 'input_audio_buffer.timeout_triggered';\n}\n\n/**\n * A log probability object.\n */\nexport interface LogProbProperties {\n  /**\n   * The token that was used to generate the log probability.\n   */\n  token: string;\n\n  /**\n   * The bytes that were used to generate the log probability.\n   */\n  bytes: Array<number>;\n\n  /**\n   * The log probability of the token.\n   */\n  logprob: number;\n}\n\n/**\n * Returned when listing MCP tools has completed for an item.\n */\nexport interface McpListToolsCompleted {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP list tools item.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `mcp_list_tools.completed`.\n   */\n  type: 'mcp_list_tools.completed';\n}\n\n/**\n * Returned when listing MCP tools has failed for an item.\n */\nexport interface McpListToolsFailed {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP list tools item.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `mcp_list_tools.failed`.\n   */\n  type: 'mcp_list_tools.failed';\n}\n\n/**\n * Returned when listing MCP tools is in progress for an item.\n */\nexport interface McpListToolsInProgress {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP list tools item.\n   */\n  item_id: string;\n\n  /**\n   * The event type, must be `mcp_list_tools.in_progress`.\n   */\n  type: 'mcp_list_tools.in_progress';\n}\n\n/**\n * Type of noise reduction. `near_field` is for close-talking microphones such as\n * headphones, `far_field` is for far-field microphones such as laptop or\n * conference room microphones.\n */\nexport type NoiseReductionType = 'near_field' | 'far_field';\n\n/**\n * **WebRTC/SIP Only:** Emit to cut off the current audio response. This will\n * trigger the server to stop generating audio and emit a\n * `output_audio_buffer.cleared` event. This event should be preceded by a\n * `response.cancel` client event to stop the generation of the current response.\n * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n */\nexport interface OutputAudioBufferClearEvent {\n  /**\n   * The event type, must be `output_audio_buffer.clear`.\n   */\n  type: 'output_audio_buffer.clear';\n\n  /**\n   * The unique ID of the client event used for error handling.\n   */\n  event_id?: string;\n}\n\n/**\n * Emitted at the beginning of a Response to indicate the updated rate limits. When\n * a Response is created some tokens will be \"reserved\" for the output tokens, the\n * rate limits shown here reflect that reservation, which is then adjusted\n * accordingly once the Response is completed.\n */\nexport interface RateLimitsUpdatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * List of rate limit information.\n   */\n  rate_limits: Array<RateLimitsUpdatedEvent.RateLimit>;\n\n  /**\n   * The event type, must be `rate_limits.updated`.\n   */\n  type: 'rate_limits.updated';\n}\n\nexport namespace RateLimitsUpdatedEvent {\n  export interface RateLimit {\n    /**\n     * The maximum allowed value for the rate limit.\n     */\n    limit?: number;\n\n    /**\n     * The name of the rate limit (`requests`, `tokens`).\n     */\n    name?: 'requests' | 'tokens';\n\n    /**\n     * The remaining value before the limit is reached.\n     */\n    remaining?: number;\n\n    /**\n     * Seconds until the rate limit resets.\n     */\n    reset_seconds?: number;\n  }\n}\n\n/**\n * Configuration for input and output audio.\n */\nexport interface RealtimeAudioConfig {\n  input?: RealtimeAudioConfigInput;\n\n  output?: RealtimeAudioConfigOutput;\n}\n\nexport interface RealtimeAudioConfigInput {\n  /**\n   * The format of the input audio.\n   */\n  format?: RealtimeAudioFormats;\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  noise_reduction?: RealtimeAudioConfigInput.NoiseReduction;\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously through\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n   * and should be treated as guidance of input audio content rather than precisely\n   * what the model heard. The client can optionally set the language and prompt for\n   * transcription, these offer additional guidance to the transcription service.\n   */\n  transcription?: AudioTranscription;\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response.\n   *\n   * Server VAD means that the model will detect the start and end of speech based on\n   * audio volume and respond at the end of user speech.\n   *\n   * Semantic VAD is more advanced and uses a turn detection model (in conjunction\n   * with VAD) to semantically estimate whether the user has finished speaking, then\n   * dynamically sets a timeout based on this probability. For example, if user audio\n   * trails off with \"uhhm\", the model will score a low probability of turn end and\n   * wait longer for the user to continue speaking. This can be useful for more\n   * natural conversations, but may have a higher latency.\n   */\n  turn_detection?: RealtimeAudioInputTurnDetection | null;\n}\n\nexport namespace RealtimeAudioConfigInput {\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  export interface NoiseReduction {\n    /**\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\n     * headphones, `far_field` is for far-field microphones such as laptop or\n     * conference room microphones.\n     */\n    type?: RealtimeAPI.NoiseReductionType;\n  }\n}\n\nexport interface RealtimeAudioConfigOutput {\n  /**\n   * The format of the output audio.\n   */\n  format?: RealtimeAudioFormats;\n\n  /**\n   * The speed of the model's spoken response as a multiple of the original speed.\n   * 1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed.\n   * This value can only be changed in between model turns, not while a response is\n   * in progress.\n   *\n   * This parameter is a post-processing adjustment to the audio after it is\n   * generated, it's also possible to prompt the model to speak faster or slower.\n   */\n  speed?: number;\n\n  /**\n   * The voice the model uses to respond. Supported built-in voices are `alloy`,\n   * `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`, and\n   * `cedar`. Voice cannot be changed during the session once the model has responded\n   * with audio at least once. We recommend `marin` and `cedar` for best quality.\n   */\n  voice?:\n    | (string & {})\n    | 'alloy'\n    | 'ash'\n    | 'ballad'\n    | 'coral'\n    | 'echo'\n    | 'sage'\n    | 'shimmer'\n    | 'verse'\n    | 'marin'\n    | 'cedar';\n}\n\n/**\n * The PCM audio format. Only a 24kHz sample rate is supported.\n */\nexport type RealtimeAudioFormats =\n  | RealtimeAudioFormats.AudioPCM\n  | RealtimeAudioFormats.AudioPCMU\n  | RealtimeAudioFormats.AudioPCMA;\n\nexport namespace RealtimeAudioFormats {\n  /**\n   * The PCM audio format. Only a 24kHz sample rate is supported.\n   */\n  export interface AudioPCM {\n    /**\n     * The sample rate of the audio. Always `24000`.\n     */\n    rate?: 24000;\n\n    /**\n     * The audio format. Always `audio/pcm`.\n     */\n    type?: 'audio/pcm';\n  }\n\n  /**\n   * The G.711 -law format.\n   */\n  export interface AudioPCMU {\n    /**\n     * The audio format. Always `audio/pcmu`.\n     */\n    type?: 'audio/pcmu';\n  }\n\n  /**\n   * The G.711 A-law format.\n   */\n  export interface AudioPCMA {\n    /**\n     * The audio format. Always `audio/pcma`.\n     */\n    type?: 'audio/pcma';\n  }\n}\n\n/**\n * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n * set to `null` to turn off, in which case the client must manually trigger model\n * response.\n *\n * Server VAD means that the model will detect the start and end of speech based on\n * audio volume and respond at the end of user speech.\n *\n * Semantic VAD is more advanced and uses a turn detection model (in conjunction\n * with VAD) to semantically estimate whether the user has finished speaking, then\n * dynamically sets a timeout based on this probability. For example, if user audio\n * trails off with \"uhhm\", the model will score a low probability of turn end and\n * wait longer for the user to continue speaking. This can be useful for more\n * natural conversations, but may have a higher latency.\n */\nexport type RealtimeAudioInputTurnDetection =\n  | RealtimeAudioInputTurnDetection.ServerVad\n  | RealtimeAudioInputTurnDetection.SemanticVad;\n\nexport namespace RealtimeAudioInputTurnDetection {\n  /**\n   * Server-side voice activity detection (VAD) which flips on when user speech is\n   * detected and off after a period of silence.\n   */\n  export interface ServerVad {\n    /**\n     * Type of turn detection, `server_vad` to turn on simple Server VAD.\n     */\n    type: 'server_vad';\n\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs. If `interrupt_response` is set to `false` this may fail to create a\n     * response if the model is already responding.\n     *\n     * If both `create_response` and `interrupt_response` are set to `false`, the model\n     * will never respond automatically but VAD events will still be emitted.\n     */\n    create_response?: boolean;\n\n    /**\n     * Optional timeout after which a model response will be triggered automatically.\n     * This is useful for situations in which a long pause from the user is unexpected,\n     * such as a phone call. The model will effectively prompt the user to continue the\n     * conversation based on the current context.\n     *\n     * The timeout value will be applied after the last model response's audio has\n     * finished playing, i.e. it's set to the `response.done` time plus audio playback\n     * duration.\n     *\n     * An `input_audio_buffer.timeout_triggered` event (plus events associated with the\n     * Response) will be emitted when the timeout is reached. Idle timeout is currently\n     * only supported for `server_vad` mode.\n     */\n    idle_timeout_ms?: number | null;\n\n    /**\n     * Whether or not to automatically interrupt (cancel) any ongoing response with\n     * output to the default conversation (i.e. `conversation` of `auto`) when a VAD\n     * start event occurs. If `true` then the response will be cancelled, otherwise it\n     * will continue until complete.\n     *\n     * If both `create_response` and `interrupt_response` are set to `false`, the model\n     * will never respond automatically but VAD events will still be emitted.\n     */\n    interrupt_response?: boolean;\n\n    /**\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\n     * detected speech (in milliseconds). Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\n     * more quickly, but may jump in on short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\n     * model, and thus might perform better in noisy environments.\n     */\n    threshold?: number;\n  }\n\n  /**\n   * Server-side semantic turn detection which uses a model to determine when the\n   * user has finished speaking.\n   */\n  export interface SemanticVad {\n    /**\n     * Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n     */\n    type: 'semantic_vad';\n\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs.\n     */\n    create_response?: boolean;\n\n    /**\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n     * will wait longer for the user to continue speaking, `high` will respond more\n     * quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`,\n     * and `high` have max timeouts of 8s, 4s, and 2s respectively.\n     */\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n    /**\n     * Whether or not to automatically interrupt any ongoing response with output to\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n     * occurs.\n     */\n    interrupt_response?: boolean;\n  }\n}\n\n/**\n * A realtime client event.\n */\nexport type RealtimeClientEvent =\n  | ConversationItemCreateEvent\n  | ConversationItemDeleteEvent\n  | ConversationItemRetrieveEvent\n  | ConversationItemTruncateEvent\n  | InputAudioBufferAppendEvent\n  | InputAudioBufferClearEvent\n  | OutputAudioBufferClearEvent\n  | InputAudioBufferCommitEvent\n  | ResponseCancelEvent\n  | ResponseCreateEvent\n  | SessionUpdateEvent;\n\n/**\n * An assistant message item in a Realtime conversation.\n */\nexport interface RealtimeConversationItemAssistantMessage {\n  /**\n   * The content of the message.\n   */\n  content: Array<RealtimeConversationItemAssistantMessage.Content>;\n\n  /**\n   * The role of the message sender. Always `assistant`.\n   */\n  role: 'assistant';\n\n  /**\n   * The type of the item. Always `message`.\n   */\n  type: 'message';\n\n  /**\n   * The unique ID of the item. This may be provided by the client or generated by\n   * the server.\n   */\n  id?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`. Optional\n   * when creating a new item.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The status of the item. Has no effect on the conversation.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n}\n\nexport namespace RealtimeConversationItemAssistantMessage {\n  export interface Content {\n    /**\n     * Base64-encoded audio bytes, these will be parsed as the format specified in the\n     * session output audio type configuration. This defaults to PCM 16-bit 24kHz mono\n     * if not specified.\n     */\n    audio?: string;\n\n    /**\n     * The text content.\n     */\n    text?: string;\n\n    /**\n     * The transcript of the audio content, this will always be present if the output\n     * type is `audio`.\n     */\n    transcript?: string;\n\n    /**\n     * The content type, `output_text` or `output_audio` depending on the session\n     * `output_modalities` configuration.\n     */\n    type?: 'output_text' | 'output_audio';\n  }\n}\n\n/**\n * A function call item in a Realtime conversation.\n */\nexport interface RealtimeConversationItemFunctionCall {\n  /**\n   * The arguments of the function call. This is a JSON-encoded string representing\n   * the arguments passed to the function, for example\n   * `{\"arg1\": \"value1\", \"arg2\": 42}`.\n   */\n  arguments: string;\n\n  /**\n   * The name of the function being called.\n   */\n  name: string;\n\n  /**\n   * The type of the item. Always `function_call`.\n   */\n  type: 'function_call';\n\n  /**\n   * The unique ID of the item. This may be provided by the client or generated by\n   * the server.\n   */\n  id?: string;\n\n  /**\n   * The ID of the function call.\n   */\n  call_id?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`. Optional\n   * when creating a new item.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The status of the item. Has no effect on the conversation.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n}\n\n/**\n * A function call output item in a Realtime conversation.\n */\nexport interface RealtimeConversationItemFunctionCallOutput {\n  /**\n   * The ID of the function call this output is for.\n   */\n  call_id: string;\n\n  /**\n   * The output of the function call, this is free text and can contain any\n   * information or simply be empty.\n   */\n  output: string;\n\n  /**\n   * The type of the item. Always `function_call_output`.\n   */\n  type: 'function_call_output';\n\n  /**\n   * The unique ID of the item. This may be provided by the client or generated by\n   * the server.\n   */\n  id?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`. Optional\n   * when creating a new item.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The status of the item. Has no effect on the conversation.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n}\n\n/**\n * A system message in a Realtime conversation can be used to provide additional\n * context or instructions to the model. This is similar but distinct from the\n * instruction prompt provided at the start of a conversation, as system messages\n * can be added at any point in the conversation. For major changes to the\n * conversation's behavior, use instructions, but for smaller updates (e.g. \"the\n * user is now asking about a different topic\"), use system messages.\n */\nexport interface RealtimeConversationItemSystemMessage {\n  /**\n   * The content of the message.\n   */\n  content: Array<RealtimeConversationItemSystemMessage.Content>;\n\n  /**\n   * The role of the message sender. Always `system`.\n   */\n  role: 'system';\n\n  /**\n   * The type of the item. Always `message`.\n   */\n  type: 'message';\n\n  /**\n   * The unique ID of the item. This may be provided by the client or generated by\n   * the server.\n   */\n  id?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`. Optional\n   * when creating a new item.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The status of the item. Has no effect on the conversation.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n}\n\nexport namespace RealtimeConversationItemSystemMessage {\n  export interface Content {\n    /**\n     * The text content.\n     */\n    text?: string;\n\n    /**\n     * The content type. Always `input_text` for system messages.\n     */\n    type?: 'input_text';\n  }\n}\n\n/**\n * A user message item in a Realtime conversation.\n */\nexport interface RealtimeConversationItemUserMessage {\n  /**\n   * The content of the message.\n   */\n  content: Array<RealtimeConversationItemUserMessage.Content>;\n\n  /**\n   * The role of the message sender. Always `user`.\n   */\n  role: 'user';\n\n  /**\n   * The type of the item. Always `message`.\n   */\n  type: 'message';\n\n  /**\n   * The unique ID of the item. This may be provided by the client or generated by\n   * the server.\n   */\n  id?: string;\n\n  /**\n   * Identifier for the API object being returned - always `realtime.item`. Optional\n   * when creating a new item.\n   */\n  object?: 'realtime.item';\n\n  /**\n   * The status of the item. Has no effect on the conversation.\n   */\n  status?: 'completed' | 'incomplete' | 'in_progress';\n}\n\nexport namespace RealtimeConversationItemUserMessage {\n  export interface Content {\n    /**\n     * Base64-encoded audio bytes (for `input_audio`), these will be parsed as the\n     * format specified in the session input audio type configuration. This defaults to\n     * PCM 16-bit 24kHz mono if not specified.\n     */\n    audio?: string;\n\n    /**\n     * The detail level of the image (for `input_image`). `auto` will default to\n     * `high`.\n     */\n    detail?: 'auto' | 'low' | 'high';\n\n    /**\n     * Base64-encoded image bytes (for `input_image`) as a data URI. For example\n     * `data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...`. Supported formats are PNG\n     * and JPEG.\n     */\n    image_url?: string;\n\n    /**\n     * The text content (for `input_text`).\n     */\n    text?: string;\n\n    /**\n     * Transcript of the audio (for `input_audio`). This is not sent to the model, but\n     * will be attached to the message item for reference.\n     */\n    transcript?: string;\n\n    /**\n     * The content type (`input_text`, `input_audio`, or `input_image`).\n     */\n    type?: 'input_text' | 'input_audio' | 'input_image';\n  }\n}\n\n/**\n * Details of the error.\n */\nexport interface RealtimeError {\n  /**\n   * A human-readable error message.\n   */\n  message: string;\n\n  /**\n   * The type of error (e.g., \"invalid_request_error\", \"server_error\").\n   */\n  type: string;\n\n  /**\n   * Error code, if any.\n   */\n  code?: string | null;\n\n  /**\n   * The event_id of the client event that caused the error, if applicable.\n   */\n  event_id?: string | null;\n\n  /**\n   * Parameter related to the error, if any.\n   */\n  param?: string | null;\n}\n\n/**\n * Returned when an error occurs, which could be a client problem or a server\n * problem. Most errors are recoverable and the session will stay open, we\n * recommend to implementors to monitor and log error messages by default.\n */\nexport interface RealtimeErrorEvent {\n  /**\n   * Details of the error.\n   */\n  error: RealtimeError;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The event type, must be `error`.\n   */\n  type: 'error';\n}\n\nexport interface RealtimeFunctionTool {\n  /**\n   * The description of the function, including guidance on when and how to call it,\n   * and guidance about what to tell the user when calling (if anything).\n   */\n  description?: string;\n\n  /**\n   * The name of the function.\n   */\n  name?: string;\n\n  /**\n   * Parameters of the function in JSON Schema.\n   */\n  parameters?: unknown;\n\n  /**\n   * The type of the tool, i.e. `function`.\n   */\n  type?: 'function';\n}\n\n/**\n * A Realtime item requesting human approval of a tool invocation.\n */\nexport interface RealtimeMcpApprovalRequest {\n  /**\n   * The unique ID of the approval request.\n   */\n  id: string;\n\n  /**\n   * A JSON string of arguments for the tool.\n   */\n  arguments: string;\n\n  /**\n   * The name of the tool to run.\n   */\n  name: string;\n\n  /**\n   * The label of the MCP server making the request.\n   */\n  server_label: string;\n\n  /**\n   * The type of the item. Always `mcp_approval_request`.\n   */\n  type: 'mcp_approval_request';\n}\n\n/**\n * A Realtime item responding to an MCP approval request.\n */\nexport interface RealtimeMcpApprovalResponse {\n  /**\n   * The unique ID of the approval response.\n   */\n  id: string;\n\n  /**\n   * The ID of the approval request being answered.\n   */\n  approval_request_id: string;\n\n  /**\n   * Whether the request was approved.\n   */\n  approve: boolean;\n\n  /**\n   * The type of the item. Always `mcp_approval_response`.\n   */\n  type: 'mcp_approval_response';\n\n  /**\n   * Optional reason for the decision.\n   */\n  reason?: string | null;\n}\n\n/**\n * A Realtime item listing tools available on an MCP server.\n */\nexport interface RealtimeMcpListTools {\n  /**\n   * The label of the MCP server.\n   */\n  server_label: string;\n\n  /**\n   * The tools available on the server.\n   */\n  tools: Array<RealtimeMcpListTools.Tool>;\n\n  /**\n   * The type of the item. Always `mcp_list_tools`.\n   */\n  type: 'mcp_list_tools';\n\n  /**\n   * The unique ID of the list.\n   */\n  id?: string;\n}\n\nexport namespace RealtimeMcpListTools {\n  /**\n   * A tool available on an MCP server.\n   */\n  export interface Tool {\n    /**\n     * The JSON schema describing the tool's input.\n     */\n    input_schema: unknown;\n\n    /**\n     * The name of the tool.\n     */\n    name: string;\n\n    /**\n     * Additional annotations about the tool.\n     */\n    annotations?: unknown | null;\n\n    /**\n     * The description of the tool.\n     */\n    description?: string | null;\n  }\n}\n\nexport interface RealtimeMcpProtocolError {\n  code: number;\n\n  message: string;\n\n  type: 'protocol_error';\n}\n\n/**\n * A Realtime item representing an invocation of a tool on an MCP server.\n */\nexport interface RealtimeMcpToolCall {\n  /**\n   * The unique ID of the tool call.\n   */\n  id: string;\n\n  /**\n   * A JSON string of the arguments passed to the tool.\n   */\n  arguments: string;\n\n  /**\n   * The name of the tool that was run.\n   */\n  name: string;\n\n  /**\n   * The label of the MCP server running the tool.\n   */\n  server_label: string;\n\n  /**\n   * The type of the item. Always `mcp_call`.\n   */\n  type: 'mcp_call';\n\n  /**\n   * The ID of an associated approval request, if any.\n   */\n  approval_request_id?: string | null;\n\n  /**\n   * The error from the tool call, if any.\n   */\n  error?: RealtimeMcpProtocolError | RealtimeMcpToolExecutionError | RealtimeMcphttpError | null;\n\n  /**\n   * The output from the tool call.\n   */\n  output?: string | null;\n}\n\nexport interface RealtimeMcpToolExecutionError {\n  message: string;\n\n  type: 'tool_execution_error';\n}\n\nexport interface RealtimeMcphttpError {\n  code: number;\n\n  message: string;\n\n  type: 'http_error';\n}\n\n/**\n * The response resource.\n */\nexport interface RealtimeResponse {\n  /**\n   * The unique ID of the response, will look like `resp_1234`.\n   */\n  id?: string;\n\n  /**\n   * Configuration for audio output.\n   */\n  audio?: RealtimeResponse.Audio;\n\n  /**\n   * Which conversation the response is added to, determined by the `conversation`\n   * field in the `response.create` event. If `auto`, the response will be added to\n   * the default conversation and the value of `conversation_id` will be an id like\n   * `conv_1234`. If `none`, the response will not be added to any conversation and\n   * the value of `conversation_id` will be `null`. If responses are being triggered\n   * automatically by VAD the response will be added to the default conversation\n   */\n  conversation_id?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls, that was used in this response.\n   */\n  max_output_tokens?: number | 'inf';\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The object type, must be `realtime.response`.\n   */\n  object?: 'realtime.response';\n\n  /**\n   * The list of output items generated by the response.\n   */\n  output?: Array<ConversationItem>;\n\n  /**\n   * The set of modalities the model used to respond, currently the only possible\n   * values are `[\\\"audio\\\"]`, `[\\\"text\\\"]`. Audio output always include a text\n   * transcript. Setting the output to mode `text` will disable audio output from the\n   * model.\n   */\n  output_modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * The final status of the response (`completed`, `cancelled`, `failed`, or\n   * `incomplete`, `in_progress`).\n   */\n  status?: 'completed' | 'cancelled' | 'failed' | 'incomplete' | 'in_progress';\n\n  /**\n   * Additional details about the status.\n   */\n  status_details?: RealtimeResponseStatus;\n\n  /**\n   * Usage statistics for the Response, this will correspond to billing. A Realtime\n   * API session will maintain a conversation context and append new Items to the\n   * Conversation, thus output from previous turns (text and audio tokens) will\n   * become the input for later turns.\n   */\n  usage?: RealtimeResponseUsage;\n}\n\nexport namespace RealtimeResponse {\n  /**\n   * Configuration for audio output.\n   */\n  export interface Audio {\n    output?: Audio.Output;\n  }\n\n  export namespace Audio {\n    export interface Output {\n      /**\n       * The format of the output audio.\n       */\n      format?: RealtimeAPI.RealtimeAudioFormats;\n\n      /**\n       * The voice the model uses to respond. Voice cannot be changed during the session\n       * once the model has responded with audio at least once. Current voice options are\n       * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`,\n       * and `cedar`. We recommend `marin` and `cedar` for best quality.\n       */\n      voice?:\n        | (string & {})\n        | 'alloy'\n        | 'ash'\n        | 'ballad'\n        | 'coral'\n        | 'echo'\n        | 'sage'\n        | 'shimmer'\n        | 'verse'\n        | 'marin'\n        | 'cedar';\n    }\n  }\n}\n\n/**\n * Configuration for audio input and output.\n */\nexport interface RealtimeResponseCreateAudioOutput {\n  output?: RealtimeResponseCreateAudioOutput.Output;\n}\n\nexport namespace RealtimeResponseCreateAudioOutput {\n  export interface Output {\n    /**\n     * The format of the output audio.\n     */\n    format?: RealtimeAPI.RealtimeAudioFormats;\n\n    /**\n     * The voice the model uses to respond. Supported built-in voices are `alloy`,\n     * `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`, and\n     * `cedar`. Voice cannot be changed during the session once the model has responded\n     * with audio at least once.\n     */\n    voice?:\n      | (string & {})\n      | 'alloy'\n      | 'ash'\n      | 'ballad'\n      | 'coral'\n      | 'echo'\n      | 'sage'\n      | 'shimmer'\n      | 'verse'\n      | 'marin'\n      | 'cedar';\n  }\n}\n\n/**\n * Give the model access to additional tools via remote Model Context Protocol\n * (MCP) servers.\n * [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n */\nexport interface RealtimeResponseCreateMcpTool {\n  /**\n   * A label for this MCP server, used to identify it in tool calls.\n   */\n  server_label: string;\n\n  /**\n   * The type of the MCP tool. Always `mcp`.\n   */\n  type: 'mcp';\n\n  /**\n   * List of allowed tool names or a filter object.\n   */\n  allowed_tools?: Array<string> | RealtimeResponseCreateMcpTool.McpToolFilter | null;\n\n  /**\n   * An OAuth access token that can be used with a remote MCP server, either with a\n   * custom MCP server URL or a service connector. Your application must handle the\n   * OAuth authorization flow and provide the token here.\n   */\n  authorization?: string;\n\n  /**\n   * Identifier for service connectors, like those available in ChatGPT. One of\n   * `server_url` or `connector_id` must be provided. Learn more about service\n   * connectors\n   * [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n   *\n   * Currently supported `connector_id` values are:\n   *\n   * - Dropbox: `connector_dropbox`\n   * - Gmail: `connector_gmail`\n   * - Google Calendar: `connector_googlecalendar`\n   * - Google Drive: `connector_googledrive`\n   * - Microsoft Teams: `connector_microsoftteams`\n   * - Outlook Calendar: `connector_outlookcalendar`\n   * - Outlook Email: `connector_outlookemail`\n   * - SharePoint: `connector_sharepoint`\n   */\n  connector_id?:\n    | 'connector_dropbox'\n    | 'connector_gmail'\n    | 'connector_googlecalendar'\n    | 'connector_googledrive'\n    | 'connector_microsoftteams'\n    | 'connector_outlookcalendar'\n    | 'connector_outlookemail'\n    | 'connector_sharepoint';\n\n  /**\n   * Optional HTTP headers to send to the MCP server. Use for authentication or other\n   * purposes.\n   */\n  headers?: { [key: string]: string } | null;\n\n  /**\n   * Specify which of the MCP server's tools require approval.\n   */\n  require_approval?: RealtimeResponseCreateMcpTool.McpToolApprovalFilter | 'always' | 'never' | null;\n\n  /**\n   * Optional description of the MCP server, used to provide more context.\n   */\n  server_description?: string;\n\n  /**\n   * The URL for the MCP server. One of `server_url` or `connector_id` must be\n   * provided.\n   */\n  server_url?: string;\n}\n\nexport namespace RealtimeResponseCreateMcpTool {\n  /**\n   * A filter object to specify which tools are allowed.\n   */\n  export interface McpToolFilter {\n    /**\n     * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n     * is\n     * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n     * it will match this filter.\n     */\n    read_only?: boolean;\n\n    /**\n     * List of allowed tool names.\n     */\n    tool_names?: Array<string>;\n  }\n\n  /**\n   * Specify which of the MCP server's tools require approval. Can be `always`,\n   * `never`, or a filter object associated with tools that require approval.\n   */\n  export interface McpToolApprovalFilter {\n    /**\n     * A filter object to specify which tools are allowed.\n     */\n    always?: McpToolApprovalFilter.Always;\n\n    /**\n     * A filter object to specify which tools are allowed.\n     */\n    never?: McpToolApprovalFilter.Never;\n  }\n\n  export namespace McpToolApprovalFilter {\n    /**\n     * A filter object to specify which tools are allowed.\n     */\n    export interface Always {\n      /**\n       * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n       * is\n       * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n       * it will match this filter.\n       */\n      read_only?: boolean;\n\n      /**\n       * List of allowed tool names.\n       */\n      tool_names?: Array<string>;\n    }\n\n    /**\n     * A filter object to specify which tools are allowed.\n     */\n    export interface Never {\n      /**\n       * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n       * is\n       * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n       * it will match this filter.\n       */\n      read_only?: boolean;\n\n      /**\n       * List of allowed tool names.\n       */\n      tool_names?: Array<string>;\n    }\n  }\n}\n\n/**\n * Create a new Realtime response with these parameters\n */\nexport interface RealtimeResponseCreateParams {\n  /**\n   * Configuration for audio input and output.\n   */\n  audio?: RealtimeResponseCreateAudioOutput;\n\n  /**\n   * Controls which conversation the response is added to. Currently supports `auto`\n   * and `none`, with `auto` as the default value. The `auto` value means that the\n   * contents of the response will be added to the default conversation. Set this to\n   * `none` to create an out-of-band response which will not add items to default\n   * conversation.\n   */\n  conversation?: (string & {}) | 'auto' | 'none';\n\n  /**\n   * Input items to include in the prompt for the model. Using this field creates a\n   * new context for this Response instead of using the default conversation. An\n   * empty array `[]` will clear the context for this Response. Note that this can\n   * include references to items that previously appeared in the session using their\n   * id.\n   */\n  input?: Array<ConversationItem>;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior. Note that the server sets default\n   * instructions which will be used if this field is not set and are visible in the\n   * `session.created` event at the start of the session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_output_tokens?: number | 'inf';\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The set of modalities the model used to respond, currently the only possible\n   * values are `[\\\"audio\\\"]`, `[\\\"text\\\"]`. Audio output always include a text\n   * transcript. Setting the output to mode `text` will disable audio output from the\n   * model.\n   */\n  output_modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsesAPI.ResponsePrompt | null;\n\n  /**\n   * How the model chooses tools. Provide one of the string modes or force a specific\n   * function/MCP tool.\n   */\n  tool_choice?: ResponsesAPI.ToolChoiceOptions | ResponsesAPI.ToolChoiceFunction | ResponsesAPI.ToolChoiceMcp;\n\n  /**\n   * Tools available to the model.\n   */\n  tools?: Array<RealtimeFunctionTool | RealtimeResponseCreateMcpTool>;\n}\n\n/**\n * Additional details about the status.\n */\nexport interface RealtimeResponseStatus {\n  /**\n   * A description of the error that caused the response to fail, populated when the\n   * `status` is `failed`.\n   */\n  error?: RealtimeResponseStatus.Error;\n\n  /**\n   * The reason the Response did not complete. For a `cancelled` Response, one of\n   * `turn_detected` (the server VAD detected a new start of speech) or\n   * `client_cancelled` (the client sent a cancel event). For an `incomplete`\n   * Response, one of `max_output_tokens` or `content_filter` (the server-side safety\n   * filter activated and cut off the response).\n   */\n  reason?: 'turn_detected' | 'client_cancelled' | 'max_output_tokens' | 'content_filter';\n\n  /**\n   * The type of error that caused the response to fail, corresponding with the\n   * `status` field (`completed`, `cancelled`, `incomplete`, `failed`).\n   */\n  type?: 'completed' | 'cancelled' | 'incomplete' | 'failed';\n}\n\nexport namespace RealtimeResponseStatus {\n  /**\n   * A description of the error that caused the response to fail, populated when the\n   * `status` is `failed`.\n   */\n  export interface Error {\n    /**\n     * Error code, if any.\n     */\n    code?: string;\n\n    /**\n     * The type of error.\n     */\n    type?: string;\n  }\n}\n\n/**\n * Usage statistics for the Response, this will correspond to billing. A Realtime\n * API session will maintain a conversation context and append new Items to the\n * Conversation, thus output from previous turns (text and audio tokens) will\n * become the input for later turns.\n */\nexport interface RealtimeResponseUsage {\n  /**\n   * Details about the input tokens used in the Response. Cached tokens are tokens\n   * from previous turns in the conversation that are included as context for the\n   * current response. Cached tokens here are counted as a subset of input tokens,\n   * meaning input tokens will include cached and uncached tokens.\n   */\n  input_token_details?: RealtimeResponseUsageInputTokenDetails;\n\n  /**\n   * The number of input tokens used in the Response, including text and audio\n   * tokens.\n   */\n  input_tokens?: number;\n\n  /**\n   * Details about the output tokens used in the Response.\n   */\n  output_token_details?: RealtimeResponseUsageOutputTokenDetails;\n\n  /**\n   * The number of output tokens sent in the Response, including text and audio\n   * tokens.\n   */\n  output_tokens?: number;\n\n  /**\n   * The total number of tokens in the Response including input and output text and\n   * audio tokens.\n   */\n  total_tokens?: number;\n}\n\n/**\n * Details about the input tokens used in the Response. Cached tokens are tokens\n * from previous turns in the conversation that are included as context for the\n * current response. Cached tokens here are counted as a subset of input tokens,\n * meaning input tokens will include cached and uncached tokens.\n */\nexport interface RealtimeResponseUsageInputTokenDetails {\n  /**\n   * The number of audio tokens used as input for the Response.\n   */\n  audio_tokens?: number;\n\n  /**\n   * The number of cached tokens used as input for the Response.\n   */\n  cached_tokens?: number;\n\n  /**\n   * Details about the cached tokens used as input for the Response.\n   */\n  cached_tokens_details?: RealtimeResponseUsageInputTokenDetails.CachedTokensDetails;\n\n  /**\n   * The number of image tokens used as input for the Response.\n   */\n  image_tokens?: number;\n\n  /**\n   * The number of text tokens used as input for the Response.\n   */\n  text_tokens?: number;\n}\n\nexport namespace RealtimeResponseUsageInputTokenDetails {\n  /**\n   * Details about the cached tokens used as input for the Response.\n   */\n  export interface CachedTokensDetails {\n    /**\n     * The number of cached audio tokens used as input for the Response.\n     */\n    audio_tokens?: number;\n\n    /**\n     * The number of cached image tokens used as input for the Response.\n     */\n    image_tokens?: number;\n\n    /**\n     * The number of cached text tokens used as input for the Response.\n     */\n    text_tokens?: number;\n  }\n}\n\n/**\n * Details about the output tokens used in the Response.\n */\nexport interface RealtimeResponseUsageOutputTokenDetails {\n  /**\n   * The number of audio tokens used in the Response.\n   */\n  audio_tokens?: number;\n\n  /**\n   * The number of text tokens used in the Response.\n   */\n  text_tokens?: number;\n}\n\n/**\n * A realtime server event.\n */\nexport type RealtimeServerEvent =\n  | ConversationCreatedEvent\n  | ConversationItemCreatedEvent\n  | ConversationItemDeletedEvent\n  | ConversationItemInputAudioTranscriptionCompletedEvent\n  | ConversationItemInputAudioTranscriptionDeltaEvent\n  | ConversationItemInputAudioTranscriptionFailedEvent\n  | RealtimeServerEvent.ConversationItemRetrieved\n  | ConversationItemTruncatedEvent\n  | RealtimeErrorEvent\n  | InputAudioBufferClearedEvent\n  | InputAudioBufferCommittedEvent\n  | InputAudioBufferDtmfEventReceivedEvent\n  | InputAudioBufferSpeechStartedEvent\n  | InputAudioBufferSpeechStoppedEvent\n  | RateLimitsUpdatedEvent\n  | ResponseAudioDeltaEvent\n  | ResponseAudioDoneEvent\n  | ResponseAudioTranscriptDeltaEvent\n  | ResponseAudioTranscriptDoneEvent\n  | ResponseContentPartAddedEvent\n  | ResponseContentPartDoneEvent\n  | ResponseCreatedEvent\n  | ResponseDoneEvent\n  | ResponseFunctionCallArgumentsDeltaEvent\n  | ResponseFunctionCallArgumentsDoneEvent\n  | ResponseOutputItemAddedEvent\n  | ResponseOutputItemDoneEvent\n  | ResponseTextDeltaEvent\n  | ResponseTextDoneEvent\n  | SessionCreatedEvent\n  | SessionUpdatedEvent\n  | RealtimeServerEvent.OutputAudioBufferStarted\n  | RealtimeServerEvent.OutputAudioBufferStopped\n  | RealtimeServerEvent.OutputAudioBufferCleared\n  | ConversationItemAdded\n  | ConversationItemDone\n  | InputAudioBufferTimeoutTriggered\n  | ConversationItemInputAudioTranscriptionSegment\n  | McpListToolsInProgress\n  | McpListToolsCompleted\n  | McpListToolsFailed\n  | ResponseMcpCallArgumentsDelta\n  | ResponseMcpCallArgumentsDone\n  | ResponseMcpCallInProgress\n  | ResponseMcpCallCompleted\n  | ResponseMcpCallFailed;\n\nexport namespace RealtimeServerEvent {\n  /**\n   * Returned when a conversation item is retrieved with\n   * `conversation.item.retrieve`. This is provided as a way to fetch the server's\n   * representation of an item, for example to get access to the post-processed audio\n   * data after noise cancellation and VAD. It includes the full content of the Item,\n   * including audio data.\n   */\n  export interface ConversationItemRetrieved {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * A single item within a Realtime conversation.\n     */\n    item: RealtimeAPI.ConversationItem;\n\n    /**\n     * The event type, must be `conversation.item.retrieved`.\n     */\n    type: 'conversation.item.retrieved';\n  }\n\n  /**\n   * **WebRTC/SIP Only:** Emitted when the server begins streaming audio to the\n   * client. This event is emitted after an audio content part has been added\n   * (`response.content_part.added`) to the response.\n   * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n   */\n  export interface OutputAudioBufferStarted {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * The unique ID of the response that produced the audio.\n     */\n    response_id: string;\n\n    /**\n     * The event type, must be `output_audio_buffer.started`.\n     */\n    type: 'output_audio_buffer.started';\n  }\n\n  /**\n   * **WebRTC/SIP Only:** Emitted when the output audio buffer has been completely\n   * drained on the server, and no more audio is forthcoming. This event is emitted\n   * after the full response data has been sent to the client (`response.done`).\n   * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n   */\n  export interface OutputAudioBufferStopped {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * The unique ID of the response that produced the audio.\n     */\n    response_id: string;\n\n    /**\n     * The event type, must be `output_audio_buffer.stopped`.\n     */\n    type: 'output_audio_buffer.stopped';\n  }\n\n  /**\n   * **WebRTC/SIP Only:** Emitted when the output audio buffer is cleared. This\n   * happens either in VAD mode when the user has interrupted\n   * (`input_audio_buffer.speech_started`), or when the client has emitted the\n   * `output_audio_buffer.clear` event to manually cut off the current audio\n   * response.\n   * [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n   */\n  export interface OutputAudioBufferCleared {\n    /**\n     * The unique ID of the server event.\n     */\n    event_id: string;\n\n    /**\n     * The unique ID of the response that produced the audio.\n     */\n    response_id: string;\n\n    /**\n     * The event type, must be `output_audio_buffer.cleared`.\n     */\n    type: 'output_audio_buffer.cleared';\n  }\n}\n\n/**\n * Realtime session object for the beta interface.\n */\nexport interface RealtimeSession {\n  /**\n   * Unique identifier for the session that looks like `sess_1234567890abcdef`.\n   */\n  id?: string;\n\n  /**\n   * Expiration timestamp for the session, in seconds since epoch.\n   */\n  expires_at?: number;\n\n  /**\n   * Additional fields to include in server outputs.\n   *\n   * - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n   *   transcription.\n   */\n  include?: Array<'item.input_audio_transcription.logprobs'> | null;\n\n  /**\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\n   * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\n   * (mono), and little-endian byte order.\n   */\n  input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  input_audio_noise_reduction?: RealtimeSession.InputAudioNoiseReduction;\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously through\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n   * and should be treated as guidance of input audio content rather than precisely\n   * what the model heard. The client can optionally set the language and prompt for\n   * transcription, these offer additional guidance to the transcription service.\n   */\n  input_audio_transcription?: AudioTranscription | null;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior.\n   *\n   * Note that the server sets default instructions which will be used if this field\n   * is not set and are visible in the `session.created` event at the start of the\n   * session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_response_output_tokens?: number | 'inf';\n\n  /**\n   * The set of modalities the model can respond with. To disable audio, set this to\n   * [\"text\"].\n   */\n  modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * The Realtime model used for this session.\n   */\n  model?:\n    | (string & {})\n    | 'gpt-realtime'\n    | 'gpt-realtime-1.5'\n    | 'gpt-realtime-2025-08-28'\n    | 'gpt-4o-realtime-preview'\n    | 'gpt-4o-realtime-preview-2024-10-01'\n    | 'gpt-4o-realtime-preview-2024-12-17'\n    | 'gpt-4o-realtime-preview-2025-06-03'\n    | 'gpt-4o-mini-realtime-preview'\n    | 'gpt-4o-mini-realtime-preview-2024-12-17'\n    | 'gpt-realtime-mini'\n    | 'gpt-realtime-mini-2025-10-06'\n    | 'gpt-realtime-mini-2025-12-15'\n    | 'gpt-audio-1.5'\n    | 'gpt-audio-mini'\n    | 'gpt-audio-mini-2025-10-06'\n    | 'gpt-audio-mini-2025-12-15';\n\n  /**\n   * The object type. Always `realtime.session`.\n   */\n  object?: 'realtime.session';\n\n  /**\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n   * For `pcm16`, output audio is sampled at a rate of 24kHz.\n   */\n  output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsesAPI.ResponsePrompt | null;\n\n  /**\n   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\n   * minimum speed. 1.5 is the maximum speed. This value can only be changed in\n   * between model turns, not while a response is in progress.\n   */\n  speed?: number;\n\n  /**\n   * Sampling temperature for the model, limited to [0.6, 1.2]. For audio models a\n   * temperature of 0.8 is highly recommended for best performance.\n   */\n  temperature?: number;\n\n  /**\n   * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\n   * a function.\n   */\n  tool_choice?: string;\n\n  /**\n   * Tools (functions) available to the model.\n   */\n  tools?: Array<RealtimeFunctionTool>;\n\n  /**\n   * Configuration options for tracing. Set to null to disable tracing. Once tracing\n   * is enabled for a session, the configuration cannot be modified.\n   *\n   * `auto` will create a trace for the session with default values for the workflow\n   * name, group id, and metadata.\n   */\n  tracing?: 'auto' | RealtimeSession.TracingConfiguration | null;\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response.\n   *\n   * Server VAD means that the model will detect the start and end of speech based on\n   * audio volume and respond at the end of user speech.\n   *\n   * Semantic VAD is more advanced and uses a turn detection model (in conjunction\n   * with VAD) to semantically estimate whether the user has finished speaking, then\n   * dynamically sets a timeout based on this probability. For example, if user audio\n   * trails off with \"uhhm\", the model will score a low probability of turn end and\n   * wait longer for the user to continue speaking. This can be useful for more\n   * natural conversations, but may have a higher latency.\n   */\n  turn_detection?: RealtimeSession.ServerVad | RealtimeSession.SemanticVad | null;\n\n  /**\n   * The voice the model uses to respond. Voice cannot be changed during the session\n   * once the model has responded with audio at least once. Current voice options are\n   * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n   */\n  voice?:\n    | (string & {})\n    | 'alloy'\n    | 'ash'\n    | 'ballad'\n    | 'coral'\n    | 'echo'\n    | 'sage'\n    | 'shimmer'\n    | 'verse'\n    | 'marin'\n    | 'cedar';\n}\n\nexport namespace RealtimeSession {\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  export interface InputAudioNoiseReduction {\n    /**\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\n     * headphones, `far_field` is for far-field microphones such as laptop or\n     * conference room microphones.\n     */\n    type?: RealtimeAPI.NoiseReductionType;\n  }\n\n  /**\n   * Granular configuration for tracing.\n   */\n  export interface TracingConfiguration {\n    /**\n     * The group id to attach to this trace to enable filtering and grouping in the\n     * traces dashboard.\n     */\n    group_id?: string;\n\n    /**\n     * The arbitrary metadata to attach to this trace to enable filtering in the traces\n     * dashboard.\n     */\n    metadata?: unknown;\n\n    /**\n     * The name of the workflow to attach to this trace. This is used to name the trace\n     * in the traces dashboard.\n     */\n    workflow_name?: string;\n  }\n\n  /**\n   * Server-side voice activity detection (VAD) which flips on when user speech is\n   * detected and off after a period of silence.\n   */\n  export interface ServerVad {\n    /**\n     * Type of turn detection, `server_vad` to turn on simple Server VAD.\n     */\n    type: 'server_vad';\n\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs. If `interrupt_response` is set to `false` this may fail to create a\n     * response if the model is already responding.\n     *\n     * If both `create_response` and `interrupt_response` are set to `false`, the model\n     * will never respond automatically but VAD events will still be emitted.\n     */\n    create_response?: boolean;\n\n    /**\n     * Optional timeout after which a model response will be triggered automatically.\n     * This is useful for situations in which a long pause from the user is unexpected,\n     * such as a phone call. The model will effectively prompt the user to continue the\n     * conversation based on the current context.\n     *\n     * The timeout value will be applied after the last model response's audio has\n     * finished playing, i.e. it's set to the `response.done` time plus audio playback\n     * duration.\n     *\n     * An `input_audio_buffer.timeout_triggered` event (plus events associated with the\n     * Response) will be emitted when the timeout is reached. Idle timeout is currently\n     * only supported for `server_vad` mode.\n     */\n    idle_timeout_ms?: number | null;\n\n    /**\n     * Whether or not to automatically interrupt (cancel) any ongoing response with\n     * output to the default conversation (i.e. `conversation` of `auto`) when a VAD\n     * start event occurs. If `true` then the response will be cancelled, otherwise it\n     * will continue until complete.\n     *\n     * If both `create_response` and `interrupt_response` are set to `false`, the model\n     * will never respond automatically but VAD events will still be emitted.\n     */\n    interrupt_response?: boolean;\n\n    /**\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\n     * detected speech (in milliseconds). Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\n     * more quickly, but may jump in on short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\n     * model, and thus might perform better in noisy environments.\n     */\n    threshold?: number;\n  }\n\n  /**\n   * Server-side semantic turn detection which uses a model to determine when the\n   * user has finished speaking.\n   */\n  export interface SemanticVad {\n    /**\n     * Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n     */\n    type: 'semantic_vad';\n\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs.\n     */\n    create_response?: boolean;\n\n    /**\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n     * will wait longer for the user to continue speaking, `high` will respond more\n     * quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`,\n     * and `high` have max timeouts of 8s, 4s, and 2s respectively.\n     */\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n    /**\n     * Whether or not to automatically interrupt any ongoing response with output to\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n     * occurs.\n     */\n    interrupt_response?: boolean;\n  }\n}\n\n/**\n * Realtime session object configuration.\n */\nexport interface RealtimeSessionCreateRequest {\n  /**\n   * The type of session to create. Always `realtime` for the Realtime API.\n   */\n  type: 'realtime';\n\n  /**\n   * Configuration for input and output audio.\n   */\n  audio?: RealtimeAudioConfig;\n\n  /**\n   * Additional fields to include in server outputs.\n   *\n   * `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n   * transcription.\n   */\n  include?: Array<'item.input_audio_transcription.logprobs'>;\n\n  /**\n   * The default system instructions (i.e. system message) prepended to model calls.\n   * This field allows the client to guide the model on desired responses. The model\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\n   * instructions are not guaranteed to be followed by the model, but they provide\n   * guidance to the model on the desired behavior.\n   *\n   * Note that the server sets default instructions which will be used if this field\n   * is not set and are visible in the `session.created` event at the start of the\n   * session.\n   */\n  instructions?: string;\n\n  /**\n   * Maximum number of output tokens for a single assistant response, inclusive of\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n   */\n  max_output_tokens?: number | 'inf';\n\n  /**\n   * The Realtime model used for this session.\n   */\n  model?:\n    | (string & {})\n    | 'gpt-realtime'\n    | 'gpt-realtime-1.5'\n    | 'gpt-realtime-2025-08-28'\n    | 'gpt-4o-realtime-preview'\n    | 'gpt-4o-realtime-preview-2024-10-01'\n    | 'gpt-4o-realtime-preview-2024-12-17'\n    | 'gpt-4o-realtime-preview-2025-06-03'\n    | 'gpt-4o-mini-realtime-preview'\n    | 'gpt-4o-mini-realtime-preview-2024-12-17'\n    | 'gpt-realtime-mini'\n    | 'gpt-realtime-mini-2025-10-06'\n    | 'gpt-realtime-mini-2025-12-15'\n    | 'gpt-audio-1.5'\n    | 'gpt-audio-mini'\n    | 'gpt-audio-mini-2025-10-06'\n    | 'gpt-audio-mini-2025-12-15';\n\n  /**\n   * The set of modalities the model can respond with. It defaults to `[\"audio\"]`,\n   * indicating that the model will respond with audio plus a transcript. `[\"text\"]`\n   * can be used to make the model respond with text only. It is not possible to\n   * request both `text` and `audio` at the same time.\n   */\n  output_modalities?: Array<'text' | 'audio'>;\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsesAPI.ResponsePrompt | null;\n\n  /**\n   * How the model chooses tools. Provide one of the string modes or force a specific\n   * function/MCP tool.\n   */\n  tool_choice?: RealtimeToolChoiceConfig;\n\n  /**\n   * Tools available to the model.\n   */\n  tools?: RealtimeToolsConfig;\n\n  /**\n   * Realtime API can write session traces to the\n   * [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\n   * tracing is enabled for a session, the configuration cannot be modified.\n   *\n   * `auto` will create a trace for the session with default values for the workflow\n   * name, group id, and metadata.\n   */\n  tracing?: RealtimeTracingConfig | null;\n\n  /**\n   * When the number of tokens in a conversation exceeds the model's input token\n   * limit, the conversation be truncated, meaning messages (starting from the\n   * oldest) will not be included in the model's context. A 32k context model with\n   * 4,096 max output tokens can only include 28,224 tokens in the context before\n   * truncation occurs.\n   *\n   * Clients can configure truncation behavior to truncate with a lower max token\n   * limit, which is an effective way to control token usage and cost.\n   *\n   * Truncation will reduce the number of cached tokens on the next turn (busting the\n   * cache), since messages are dropped from the beginning of the context. However,\n   * clients can also configure truncation to retain messages up to a fraction of the\n   * maximum context size, which will reduce the need for future truncations and thus\n   * improve the cache rate.\n   *\n   * Truncation can be disabled entirely, which means the server will never truncate\n   * but would instead return an error if the conversation exceeds the model's input\n   * token limit.\n   */\n  truncation?: RealtimeTruncation;\n}\n\n/**\n * How the model chooses tools. Provide one of the string modes or force a specific\n * function/MCP tool.\n */\nexport type RealtimeToolChoiceConfig =\n  | ResponsesAPI.ToolChoiceOptions\n  | ResponsesAPI.ToolChoiceFunction\n  | ResponsesAPI.ToolChoiceMcp;\n\n/**\n * Tools available to the model.\n */\nexport type RealtimeToolsConfig = Array<RealtimeToolsConfigUnion>;\n\n/**\n * Give the model access to additional tools via remote Model Context Protocol\n * (MCP) servers.\n * [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n */\nexport type RealtimeToolsConfigUnion = RealtimeFunctionTool | RealtimeToolsConfigUnion.Mcp;\n\nexport namespace RealtimeToolsConfigUnion {\n  /**\n   * Give the model access to additional tools via remote Model Context Protocol\n   * (MCP) servers.\n   * [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n   */\n  export interface Mcp {\n    /**\n     * A label for this MCP server, used to identify it in tool calls.\n     */\n    server_label: string;\n\n    /**\n     * The type of the MCP tool. Always `mcp`.\n     */\n    type: 'mcp';\n\n    /**\n     * List of allowed tool names or a filter object.\n     */\n    allowed_tools?: Array<string> | Mcp.McpToolFilter | null;\n\n    /**\n     * An OAuth access token that can be used with a remote MCP server, either with a\n     * custom MCP server URL or a service connector. Your application must handle the\n     * OAuth authorization flow and provide the token here.\n     */\n    authorization?: string;\n\n    /**\n     * Identifier for service connectors, like those available in ChatGPT. One of\n     * `server_url` or `connector_id` must be provided. Learn more about service\n     * connectors\n     * [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n     *\n     * Currently supported `connector_id` values are:\n     *\n     * - Dropbox: `connector_dropbox`\n     * - Gmail: `connector_gmail`\n     * - Google Calendar: `connector_googlecalendar`\n     * - Google Drive: `connector_googledrive`\n     * - Microsoft Teams: `connector_microsoftteams`\n     * - Outlook Calendar: `connector_outlookcalendar`\n     * - Outlook Email: `connector_outlookemail`\n     * - SharePoint: `connector_sharepoint`\n     */\n    connector_id?:\n      | 'connector_dropbox'\n      | 'connector_gmail'\n      | 'connector_googlecalendar'\n      | 'connector_googledrive'\n      | 'connector_microsoftteams'\n      | 'connector_outlookcalendar'\n      | 'connector_outlookemail'\n      | 'connector_sharepoint';\n\n    /**\n     * Optional HTTP headers to send to the MCP server. Use for authentication or other\n     * purposes.\n     */\n    headers?: { [key: string]: string } | null;\n\n    /**\n     * Specify which of the MCP server's tools require approval.\n     */\n    require_approval?: Mcp.McpToolApprovalFilter | 'always' | 'never' | null;\n\n    /**\n     * Optional description of the MCP server, used to provide more context.\n     */\n    server_description?: string;\n\n    /**\n     * The URL for the MCP server. One of `server_url` or `connector_id` must be\n     * provided.\n     */\n    server_url?: string;\n  }\n\n  export namespace Mcp {\n    /**\n     * A filter object to specify which tools are allowed.\n     */\n    export interface McpToolFilter {\n      /**\n       * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n       * is\n       * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n       * it will match this filter.\n       */\n      read_only?: boolean;\n\n      /**\n       * List of allowed tool names.\n       */\n      tool_names?: Array<string>;\n    }\n\n    /**\n     * Specify which of the MCP server's tools require approval. Can be `always`,\n     * `never`, or a filter object associated with tools that require approval.\n     */\n    export interface McpToolApprovalFilter {\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      always?: McpToolApprovalFilter.Always;\n\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      never?: McpToolApprovalFilter.Never;\n    }\n\n    export namespace McpToolApprovalFilter {\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      export interface Always {\n        /**\n         * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n         * is\n         * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n         * it will match this filter.\n         */\n        read_only?: boolean;\n\n        /**\n         * List of allowed tool names.\n         */\n        tool_names?: Array<string>;\n      }\n\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      export interface Never {\n        /**\n         * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n         * is\n         * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n         * it will match this filter.\n         */\n        read_only?: boolean;\n\n        /**\n         * List of allowed tool names.\n         */\n        tool_names?: Array<string>;\n      }\n    }\n  }\n}\n\n/**\n * Realtime API can write session traces to the\n * [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\n * tracing is enabled for a session, the configuration cannot be modified.\n *\n * `auto` will create a trace for the session with default values for the workflow\n * name, group id, and metadata.\n */\nexport type RealtimeTracingConfig = 'auto' | RealtimeTracingConfig.TracingConfiguration;\n\nexport namespace RealtimeTracingConfig {\n  /**\n   * Granular configuration for tracing.\n   */\n  export interface TracingConfiguration {\n    /**\n     * The group id to attach to this trace to enable filtering and grouping in the\n     * Traces Dashboard.\n     */\n    group_id?: string;\n\n    /**\n     * The arbitrary metadata to attach to this trace to enable filtering in the Traces\n     * Dashboard.\n     */\n    metadata?: unknown;\n\n    /**\n     * The name of the workflow to attach to this trace. This is used to name the trace\n     * in the Traces Dashboard.\n     */\n    workflow_name?: string;\n  }\n}\n\n/**\n * Configuration for input and output audio.\n */\nexport interface RealtimeTranscriptionSessionAudio {\n  input?: RealtimeTranscriptionSessionAudioInput;\n}\n\nexport interface RealtimeTranscriptionSessionAudioInput {\n  /**\n   * The PCM audio format. Only a 24kHz sample rate is supported.\n   */\n  format?: RealtimeAudioFormats;\n\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  noise_reduction?: RealtimeTranscriptionSessionAudioInput.NoiseReduction;\n\n  /**\n   * Configuration for input audio transcription, defaults to off and can be set to\n   * `null` to turn off once on. Input audio transcription is not native to the\n   * model, since the model consumes audio directly. Transcription runs\n   * asynchronously through\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n   * and should be treated as guidance of input audio content rather than precisely\n   * what the model heard. The client can optionally set the language and prompt for\n   * transcription, these offer additional guidance to the transcription service.\n   */\n  transcription?: AudioTranscription;\n\n  /**\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n   * set to `null` to turn off, in which case the client must manually trigger model\n   * response.\n   *\n   * Server VAD means that the model will detect the start and end of speech based on\n   * audio volume and respond at the end of user speech.\n   *\n   * Semantic VAD is more advanced and uses a turn detection model (in conjunction\n   * with VAD) to semantically estimate whether the user has finished speaking, then\n   * dynamically sets a timeout based on this probability. For example, if user audio\n   * trails off with \"uhhm\", the model will score a low probability of turn end and\n   * wait longer for the user to continue speaking. This can be useful for more\n   * natural conversations, but may have a higher latency.\n   */\n  turn_detection?: RealtimeTranscriptionSessionAudioInputTurnDetection | null;\n}\n\nexport namespace RealtimeTranscriptionSessionAudioInput {\n  /**\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\n   * off. Noise reduction filters audio added to the input audio buffer before it is\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\n   * detection accuracy (reducing false positives) and model performance by improving\n   * perception of the input audio.\n   */\n  export interface NoiseReduction {\n    /**\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\n     * headphones, `far_field` is for far-field microphones such as laptop or\n     * conference room microphones.\n     */\n    type?: RealtimeAPI.NoiseReductionType;\n  }\n}\n\n/**\n * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\n * set to `null` to turn off, in which case the client must manually trigger model\n * response.\n *\n * Server VAD means that the model will detect the start and end of speech based on\n * audio volume and respond at the end of user speech.\n *\n * Semantic VAD is more advanced and uses a turn detection model (in conjunction\n * with VAD) to semantically estimate whether the user has finished speaking, then\n * dynamically sets a timeout based on this probability. For example, if user audio\n * trails off with \"uhhm\", the model will score a low probability of turn end and\n * wait longer for the user to continue speaking. This can be useful for more\n * natural conversations, but may have a higher latency.\n */\nexport type RealtimeTranscriptionSessionAudioInputTurnDetection =\n  | RealtimeTranscriptionSessionAudioInputTurnDetection.ServerVad\n  | RealtimeTranscriptionSessionAudioInputTurnDetection.SemanticVad;\n\nexport namespace RealtimeTranscriptionSessionAudioInputTurnDetection {\n  /**\n   * Server-side voice activity detection (VAD) which flips on when user speech is\n   * detected and off after a period of silence.\n   */\n  export interface ServerVad {\n    /**\n     * Type of turn detection, `server_vad` to turn on simple Server VAD.\n     */\n    type: 'server_vad';\n\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs. If `interrupt_response` is set to `false` this may fail to create a\n     * response if the model is already responding.\n     *\n     * If both `create_response` and `interrupt_response` are set to `false`, the model\n     * will never respond automatically but VAD events will still be emitted.\n     */\n    create_response?: boolean;\n\n    /**\n     * Optional timeout after which a model response will be triggered automatically.\n     * This is useful for situations in which a long pause from the user is unexpected,\n     * such as a phone call. The model will effectively prompt the user to continue the\n     * conversation based on the current context.\n     *\n     * The timeout value will be applied after the last model response's audio has\n     * finished playing, i.e. it's set to the `response.done` time plus audio playback\n     * duration.\n     *\n     * An `input_audio_buffer.timeout_triggered` event (plus events associated with the\n     * Response) will be emitted when the timeout is reached. Idle timeout is currently\n     * only supported for `server_vad` mode.\n     */\n    idle_timeout_ms?: number | null;\n\n    /**\n     * Whether or not to automatically interrupt (cancel) any ongoing response with\n     * output to the default conversation (i.e. `conversation` of `auto`) when a VAD\n     * start event occurs. If `true` then the response will be cancelled, otherwise it\n     * will continue until complete.\n     *\n     * If both `create_response` and `interrupt_response` are set to `false`, the model\n     * will never respond automatically but VAD events will still be emitted.\n     */\n    interrupt_response?: boolean;\n\n    /**\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\n     * detected speech (in milliseconds). Defaults to 300ms.\n     */\n    prefix_padding_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\n     * more quickly, but may jump in on short pauses from the user.\n     */\n    silence_duration_ms?: number;\n\n    /**\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\n     * model, and thus might perform better in noisy environments.\n     */\n    threshold?: number;\n  }\n\n  /**\n   * Server-side semantic turn detection which uses a model to determine when the\n   * user has finished speaking.\n   */\n  export interface SemanticVad {\n    /**\n     * Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n     */\n    type: 'semantic_vad';\n\n    /**\n     * Whether or not to automatically generate a response when a VAD stop event\n     * occurs.\n     */\n    create_response?: boolean;\n\n    /**\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\n     * will wait longer for the user to continue speaking, `high` will respond more\n     * quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`,\n     * and `high` have max timeouts of 8s, 4s, and 2s respectively.\n     */\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\n\n    /**\n     * Whether or not to automatically interrupt any ongoing response with output to\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n     * occurs.\n     */\n    interrupt_response?: boolean;\n  }\n}\n\n/**\n * Realtime transcription session object configuration.\n */\nexport interface RealtimeTranscriptionSessionCreateRequest {\n  /**\n   * The type of session to create. Always `transcription` for transcription\n   * sessions.\n   */\n  type: 'transcription';\n\n  /**\n   * Configuration for input and output audio.\n   */\n  audio?: RealtimeTranscriptionSessionAudio;\n\n  /**\n   * Additional fields to include in server outputs.\n   *\n   * `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n   * transcription.\n   */\n  include?: Array<'item.input_audio_transcription.logprobs'>;\n}\n\n/**\n * When the number of tokens in a conversation exceeds the model's input token\n * limit, the conversation be truncated, meaning messages (starting from the\n * oldest) will not be included in the model's context. A 32k context model with\n * 4,096 max output tokens can only include 28,224 tokens in the context before\n * truncation occurs.\n *\n * Clients can configure truncation behavior to truncate with a lower max token\n * limit, which is an effective way to control token usage and cost.\n *\n * Truncation will reduce the number of cached tokens on the next turn (busting the\n * cache), since messages are dropped from the beginning of the context. However,\n * clients can also configure truncation to retain messages up to a fraction of the\n * maximum context size, which will reduce the need for future truncations and thus\n * improve the cache rate.\n *\n * Truncation can be disabled entirely, which means the server will never truncate\n * but would instead return an error if the conversation exceeds the model's input\n * token limit.\n */\nexport type RealtimeTruncation = 'auto' | 'disabled' | RealtimeTruncationRetentionRatio;\n\n/**\n * Retain a fraction of the conversation tokens when the conversation exceeds the\n * input token limit. This allows you to amortize truncations across multiple\n * turns, which can help improve cached token usage.\n */\nexport interface RealtimeTruncationRetentionRatio {\n  /**\n   * Fraction of post-instruction conversation tokens to retain (`0.0` - `1.0`) when\n   * the conversation exceeds the input token limit. Setting this to `0.8` means that\n   * messages will be dropped until 80% of the maximum allowed tokens are used. This\n   * helps reduce the frequency of truncations and improve cache rates.\n   */\n  retention_ratio: number;\n\n  /**\n   * Use retention ratio truncation.\n   */\n  type: 'retention_ratio';\n\n  /**\n   * Optional custom token limits for this truncation strategy. If not provided, the\n   * model's default token limits will be used.\n   */\n  token_limits?: RealtimeTruncationRetentionRatio.TokenLimits;\n}\n\nexport namespace RealtimeTruncationRetentionRatio {\n  /**\n   * Optional custom token limits for this truncation strategy. If not provided, the\n   * model's default token limits will be used.\n   */\n  export interface TokenLimits {\n    /**\n     * Maximum tokens allowed in the conversation after instructions (which including\n     * tool definitions). For example, setting this to 5,000 would mean that truncation\n     * would occur when the conversation exceeds 5,000 tokens after instructions. This\n     * cannot be higher than the model's context window size minus the maximum output\n     * tokens.\n     */\n    post_instructions?: number;\n  }\n}\n\n/**\n * Returned when the model-generated audio is updated.\n */\nexport interface ResponseAudioDeltaEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * Base64-encoded audio data delta.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_audio.delta`.\n   */\n  type: 'response.output_audio.delta';\n}\n\n/**\n * Returned when the model-generated audio is done. Also emitted when a Response is\n * interrupted, incomplete, or cancelled.\n */\nexport interface ResponseAudioDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_audio.done`.\n   */\n  type: 'response.output_audio.done';\n}\n\n/**\n * Returned when the model-generated transcription of audio output is updated.\n */\nexport interface ResponseAudioTranscriptDeltaEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The transcript delta.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_audio_transcript.delta`.\n   */\n  type: 'response.output_audio_transcript.delta';\n}\n\n/**\n * Returned when the model-generated transcription of audio output is done\n * streaming. Also emitted when a Response is interrupted, incomplete, or\n * cancelled.\n */\nexport interface ResponseAudioTranscriptDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The final transcript of the audio.\n   */\n  transcript: string;\n\n  /**\n   * The event type, must be `response.output_audio_transcript.done`.\n   */\n  type: 'response.output_audio_transcript.done';\n}\n\n/**\n * Send this event to cancel an in-progress response. The server will respond with\n * a `response.done` event with a status of `response.status=cancelled`. If there\n * is no response to cancel, the server will respond with an error. It's safe to\n * call `response.cancel` even if no response is in progress, an error will be\n * returned the session will remain unaffected.\n */\nexport interface ResponseCancelEvent {\n  /**\n   * The event type, must be `response.cancel`.\n   */\n  type: 'response.cancel';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n\n  /**\n   * A specific response ID to cancel - if not provided, will cancel an in-progress\n   * response in the default conversation.\n   */\n  response_id?: string;\n}\n\n/**\n * Returned when a new content part is added to an assistant message item during\n * response generation.\n */\nexport interface ResponseContentPartAddedEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item to which the content part was added.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The content part that was added.\n   */\n  part: ResponseContentPartAddedEvent.Part;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.content_part.added`.\n   */\n  type: 'response.content_part.added';\n}\n\nexport namespace ResponseContentPartAddedEvent {\n  /**\n   * The content part that was added.\n   */\n  export interface Part {\n    /**\n     * Base64-encoded audio data (if type is \"audio\").\n     */\n    audio?: string;\n\n    /**\n     * The text content (if type is \"text\").\n     */\n    text?: string;\n\n    /**\n     * The transcript of the audio (if type is \"audio\").\n     */\n    transcript?: string;\n\n    /**\n     * The content type (\"text\", \"audio\").\n     */\n    type?: 'text' | 'audio';\n  }\n}\n\n/**\n * Returned when a content part is done streaming in an assistant message item.\n * Also emitted when a Response is interrupted, incomplete, or cancelled.\n */\nexport interface ResponseContentPartDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The content part that is done.\n   */\n  part: ResponseContentPartDoneEvent.Part;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.content_part.done`.\n   */\n  type: 'response.content_part.done';\n}\n\nexport namespace ResponseContentPartDoneEvent {\n  /**\n   * The content part that is done.\n   */\n  export interface Part {\n    /**\n     * Base64-encoded audio data (if type is \"audio\").\n     */\n    audio?: string;\n\n    /**\n     * The text content (if type is \"text\").\n     */\n    text?: string;\n\n    /**\n     * The transcript of the audio (if type is \"audio\").\n     */\n    transcript?: string;\n\n    /**\n     * The content type (\"text\", \"audio\").\n     */\n    type?: 'text' | 'audio';\n  }\n}\n\n/**\n * This event instructs the server to create a Response, which means triggering\n * model inference. When in Server VAD mode, the server will create Responses\n * automatically.\n *\n * A Response will include at least one Item, and may have two, in which case the\n * second will be a function call. These Items will be appended to the conversation\n * history by default.\n *\n * The server will respond with a `response.created` event, events for Items and\n * content created, and finally a `response.done` event to indicate the Response is\n * complete.\n *\n * The `response.create` event includes inference configuration like `instructions`\n * and `tools`. If these are set, they will override the Session's configuration\n * for this Response only.\n *\n * Responses can be created out-of-band of the default Conversation, meaning that\n * they can have arbitrary input, and it's possible to disable writing the output\n * to the Conversation. Only one Response can write to the default Conversation at\n * a time, but otherwise multiple Responses can be created in parallel. The\n * `metadata` field is a good way to disambiguate multiple simultaneous Responses.\n *\n * Clients can set `conversation` to `none` to create a Response that does not\n * write to the default Conversation. Arbitrary input can be provided with the\n * `input` field, which is an array accepting raw Items and references to existing\n * Items.\n */\nexport interface ResponseCreateEvent {\n  /**\n   * The event type, must be `response.create`.\n   */\n  type: 'response.create';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n\n  /**\n   * Create a new Realtime response with these parameters\n   */\n  response?: RealtimeResponseCreateParams;\n}\n\n/**\n * Returned when a new Response is created. The first event of response creation,\n * where the response is in an initial state of `in_progress`.\n */\nexport interface ResponseCreatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The response resource.\n   */\n  response: RealtimeResponse;\n\n  /**\n   * The event type, must be `response.created`.\n   */\n  type: 'response.created';\n}\n\n/**\n * Returned when a Response is done streaming. Always emitted, no matter the final\n * state. The Response object included in the `response.done` event will include\n * all output Items in the Response but will omit the raw audio data.\n *\n * Clients should check the `status` field of the Response to determine if it was\n * successful (`completed`) or if there was another outcome: `cancelled`, `failed`,\n * or `incomplete`.\n *\n * A response will contain all output items that were generated during the\n * response, excluding any audio content.\n */\nexport interface ResponseDoneEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The response resource.\n   */\n  response: RealtimeResponse;\n\n  /**\n   * The event type, must be `response.done`.\n   */\n  type: 'response.done';\n}\n\n/**\n * Returned when the model-generated function call arguments are updated.\n */\nexport interface ResponseFunctionCallArgumentsDeltaEvent {\n  /**\n   * The ID of the function call.\n   */\n  call_id: string;\n\n  /**\n   * The arguments delta as a JSON string.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the function call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.function_call_arguments.delta`.\n   */\n  type: 'response.function_call_arguments.delta';\n}\n\n/**\n * Returned when the model-generated function call arguments are done streaming.\n * Also emitted when a Response is interrupted, incomplete, or cancelled.\n */\nexport interface ResponseFunctionCallArgumentsDoneEvent {\n  /**\n   * The final arguments as a JSON string.\n   */\n  arguments: string;\n\n  /**\n   * The ID of the function call.\n   */\n  call_id: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the function call item.\n   */\n  item_id: string;\n\n  /**\n   * The name of the function that was called.\n   */\n  name: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.function_call_arguments.done`.\n   */\n  type: 'response.function_call_arguments.done';\n}\n\n/**\n * Returned when MCP tool call arguments are updated during response generation.\n */\nexport interface ResponseMcpCallArgumentsDelta {\n  /**\n   * The JSON-encoded arguments delta.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.mcp_call_arguments.delta`.\n   */\n  type: 'response.mcp_call_arguments.delta';\n\n  /**\n   * If present, indicates the delta text was obfuscated.\n   */\n  obfuscation?: string | null;\n}\n\n/**\n * Returned when MCP tool call arguments are finalized during response generation.\n */\nexport interface ResponseMcpCallArgumentsDone {\n  /**\n   * The final JSON-encoded arguments string.\n   */\n  arguments: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.mcp_call_arguments.done`.\n   */\n  type: 'response.mcp_call_arguments.done';\n}\n\n/**\n * Returned when an MCP tool call has completed successfully.\n */\nexport interface ResponseMcpCallCompleted {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The event type, must be `response.mcp_call.completed`.\n   */\n  type: 'response.mcp_call.completed';\n}\n\n/**\n * Returned when an MCP tool call has failed.\n */\nexport interface ResponseMcpCallFailed {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The event type, must be `response.mcp_call.failed`.\n   */\n  type: 'response.mcp_call.failed';\n}\n\n/**\n * Returned when an MCP tool call has started and is in progress.\n */\nexport interface ResponseMcpCallInProgress {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the MCP tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The event type, must be `response.mcp_call.in_progress`.\n   */\n  type: 'response.mcp_call.in_progress';\n}\n\n/**\n * Returned when a new Item is created during Response generation.\n */\nexport interface ResponseOutputItemAddedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * A single item within a Realtime conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The index of the output item in the Response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the Response to which the item belongs.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_item.added`.\n   */\n  type: 'response.output_item.added';\n}\n\n/**\n * Returned when an Item is done streaming. Also emitted when a Response is\n * interrupted, incomplete, or cancelled.\n */\nexport interface ResponseOutputItemDoneEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * A single item within a Realtime conversation.\n   */\n  item: ConversationItem;\n\n  /**\n   * The index of the output item in the Response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the Response to which the item belongs.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_item.done`.\n   */\n  type: 'response.output_item.done';\n}\n\n/**\n * Returned when the text value of an \"output_text\" content part is updated.\n */\nexport interface ResponseTextDeltaEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The text delta.\n   */\n  delta: string;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The event type, must be `response.output_text.delta`.\n   */\n  type: 'response.output_text.delta';\n}\n\n/**\n * Returned when the text value of an \"output_text\" content part is done streaming.\n * Also emitted when a Response is interrupted, incomplete, or cancelled.\n */\nexport interface ResponseTextDoneEvent {\n  /**\n   * The index of the content part in the item's content array.\n   */\n  content_index: number;\n\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response.\n   */\n  output_index: number;\n\n  /**\n   * The ID of the response.\n   */\n  response_id: string;\n\n  /**\n   * The final text content.\n   */\n  text: string;\n\n  /**\n   * The event type, must be `response.output_text.done`.\n   */\n  type: 'response.output_text.done';\n}\n\n/**\n * Returned when a Session is created. Emitted automatically when a new connection\n * is established as the first server event. This event will contain the default\n * Session configuration.\n */\nexport interface SessionCreatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The session configuration.\n   */\n  session: RealtimeSessionCreateRequest | RealtimeTranscriptionSessionCreateRequest;\n\n  /**\n   * The event type, must be `session.created`.\n   */\n  type: 'session.created';\n}\n\n/**\n * Send this event to update the sessions configuration. The client may send this\n * event at any time to update any field except for `voice` and `model`. `voice`\n * can be updated only if there have been no other audio outputs yet.\n *\n * When the server receives a `session.update`, it will respond with a\n * `session.updated` event showing the full, effective configuration. Only the\n * fields that are present in the `session.update` are updated. To clear a field\n * like `instructions`, pass an empty string. To clear a field like `tools`, pass\n * an empty array. To clear a field like `turn_detection`, pass `null`.\n */\nexport interface SessionUpdateEvent {\n  /**\n   * Update the Realtime session. Choose either a realtime session or a transcription\n   * session.\n   */\n  session: RealtimeSessionCreateRequest | RealtimeTranscriptionSessionCreateRequest;\n\n  /**\n   * The event type, must be `session.update`.\n   */\n  type: 'session.update';\n\n  /**\n   * Optional client-generated ID used to identify this event. This is an arbitrary\n   * string that a client may assign. It will be passed back if there is an error\n   * with the event, but the corresponding `session.updated` event will not include\n   * it.\n   */\n  event_id?: string;\n}\n\n/**\n * Returned when a session is updated with a `session.update` event, unless there\n * is an error.\n */\nexport interface SessionUpdatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * The session configuration.\n   */\n  session: RealtimeSessionCreateRequest | RealtimeTranscriptionSessionCreateRequest;\n\n  /**\n   * The event type, must be `session.updated`.\n   */\n  type: 'session.updated';\n}\n\n/**\n * Send this event to update a transcription session.\n */\nexport interface TranscriptionSessionUpdate {\n  /**\n   * Realtime transcription session object configuration.\n   */\n  session: TranscriptionSessionUpdate.Session;\n\n  /**\n   * The event type, must be `transcription_session.update`.\n   */\n  type: 'transcription_session.update';\n\n  /**\n   * Optional client-generated ID used to identify this event.\n   */\n  event_id?: string;\n}\n\nexport namespace TranscriptionSessionUpdate {\n  /**\n   * Realtime transcription session object configuration.\n   */\n  export interface Session {\n    /**\n     * The set of items to include in the transcription. Current available items are:\n     * `item.input_audio_transcription.logprobs`\n     */\n    include?: Array<'item.input_audio_transcription.logprobs'>;\n\n    /**\n     * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\n     * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\n     * (mono), and little-endian byte order.\n     */\n    input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\n\n    /**\n     * Configuration for input audio noise reduction. This can be set to `null` to turn\n     * off. Noise reduction filters audio added to the input audio buffer before it is\n     * sent to VAD and the model. Filtering the audio can improve VAD and turn\n     * detection accuracy (reducing false positives) and model performance by improving\n     * perception of the input audio.\n     */\n    input_audio_noise_reduction?: Session.InputAudioNoiseReduction;\n\n    /**\n     * Configuration for input audio transcription. The client can optionally set the\n     * language and prompt for transcription, these offer additional guidance to the\n     * transcription service.\n     */\n    input_audio_transcription?: RealtimeAPI.AudioTranscription;\n\n    /**\n     * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n     * means that the model will detect the start and end of speech based on audio\n     * volume and respond at the end of user speech.\n     */\n    turn_detection?: Session.TurnDetection;\n  }\n\n  export namespace Session {\n    /**\n     * Configuration for input audio noise reduction. This can be set to `null` to turn\n     * off. Noise reduction filters audio added to the input audio buffer before it is\n     * sent to VAD and the model. Filtering the audio can improve VAD and turn\n     * detection accuracy (reducing false positives) and model performance by improving\n     * perception of the input audio.\n     */\n    export interface InputAudioNoiseReduction {\n      /**\n       * Type of noise reduction. `near_field` is for close-talking microphones such as\n       * headphones, `far_field` is for far-field microphones such as laptop or\n       * conference room microphones.\n       */\n      type?: RealtimeAPI.NoiseReductionType;\n    }\n\n    /**\n     * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n     * means that the model will detect the start and end of speech based on audio\n     * volume and respond at the end of user speech.\n     */\n    export interface TurnDetection {\n      /**\n       * Amount of audio to include before the VAD detected speech (in milliseconds).\n       * Defaults to 300ms.\n       */\n      prefix_padding_ms?: number;\n\n      /**\n       * Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n       * With shorter values the model will respond more quickly, but may jump in on\n       * short pauses from the user.\n       */\n      silence_duration_ms?: number;\n\n      /**\n       * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n       * threshold will require louder audio to activate the model, and thus might\n       * perform better in noisy environments.\n       */\n      threshold?: number;\n\n      /**\n       * Type of turn detection. Only `server_vad` is currently supported for\n       * transcription sessions.\n       */\n      type?: 'server_vad';\n    }\n  }\n}\n\n/**\n * Returned when a transcription session is updated with a\n * `transcription_session.update` event, unless there is an error.\n */\nexport interface TranscriptionSessionUpdatedEvent {\n  /**\n   * The unique ID of the server event.\n   */\n  event_id: string;\n\n  /**\n   * A new Realtime transcription session configuration.\n   *\n   * When a session is created on the server via REST API, the session object also\n   * contains an ephemeral key. Default TTL for keys is 10 minutes. This property is\n   * not present when a session is updated via the WebSocket API.\n   */\n  session: TranscriptionSessionUpdatedEvent.Session;\n\n  /**\n   * The event type, must be `transcription_session.updated`.\n   */\n  type: 'transcription_session.updated';\n}\n\nexport namespace TranscriptionSessionUpdatedEvent {\n  /**\n   * A new Realtime transcription session configuration.\n   *\n   * When a session is created on the server via REST API, the session object also\n   * contains an ephemeral key. Default TTL for keys is 10 minutes. This property is\n   * not present when a session is updated via the WebSocket API.\n   */\n  export interface Session {\n    /**\n     * Ephemeral key returned by the API. Only present when the session is created on\n     * the server via REST API.\n     */\n    client_secret: Session.ClientSecret;\n\n    /**\n     * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n     */\n    input_audio_format?: string;\n\n    /**\n     * Configuration of the transcription model.\n     */\n    input_audio_transcription?: RealtimeAPI.AudioTranscription;\n\n    /**\n     * The set of modalities the model can respond with. To disable audio, set this to\n     * [\"text\"].\n     */\n    modalities?: Array<'text' | 'audio'>;\n\n    /**\n     * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n     * means that the model will detect the start and end of speech based on audio\n     * volume and respond at the end of user speech.\n     */\n    turn_detection?: Session.TurnDetection;\n  }\n\n  export namespace Session {\n    /**\n     * Ephemeral key returned by the API. Only present when the session is created on\n     * the server via REST API.\n     */\n    export interface ClientSecret {\n      /**\n       * Timestamp for when the token expires. Currently, all tokens expire after one\n       * minute.\n       */\n      expires_at: number;\n\n      /**\n       * Ephemeral key usable in client environments to authenticate connections to the\n       * Realtime API. Use this in client-side environments rather than a standard API\n       * token, which should only be used server-side.\n       */\n      value: string;\n    }\n\n    /**\n     * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\n     * means that the model will detect the start and end of speech based on audio\n     * volume and respond at the end of user speech.\n     */\n    export interface TurnDetection {\n      /**\n       * Amount of audio to include before the VAD detected speech (in milliseconds).\n       * Defaults to 300ms.\n       */\n      prefix_padding_ms?: number;\n\n      /**\n       * Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n       * With shorter values the model will respond more quickly, but may jump in on\n       * short pauses from the user.\n       */\n      silence_duration_ms?: number;\n\n      /**\n       * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n       * threshold will require louder audio to activate the model, and thus might\n       * perform better in noisy environments.\n       */\n      threshold?: number;\n\n      /**\n       * Type of turn detection, only `server_vad` is currently supported.\n       */\n      type?: string;\n    }\n  }\n}\n\nRealtime.ClientSecrets = ClientSecrets;\nRealtime.Calls = Calls;\n\nexport declare namespace Realtime {\n  export {\n    type AudioTranscription as AudioTranscription,\n    type ConversationCreatedEvent as ConversationCreatedEvent,\n    type ConversationItem as ConversationItem,\n    type ConversationItemAdded as ConversationItemAdded,\n    type ConversationItemCreateEvent as ConversationItemCreateEvent,\n    type ConversationItemCreatedEvent as ConversationItemCreatedEvent,\n    type ConversationItemDeleteEvent as ConversationItemDeleteEvent,\n    type ConversationItemDeletedEvent as ConversationItemDeletedEvent,\n    type ConversationItemDone as ConversationItemDone,\n    type ConversationItemInputAudioTranscriptionCompletedEvent as ConversationItemInputAudioTranscriptionCompletedEvent,\n    type ConversationItemInputAudioTranscriptionDeltaEvent as ConversationItemInputAudioTranscriptionDeltaEvent,\n    type ConversationItemInputAudioTranscriptionFailedEvent as ConversationItemInputAudioTranscriptionFailedEvent,\n    type ConversationItemInputAudioTranscriptionSegment as ConversationItemInputAudioTranscriptionSegment,\n    type ConversationItemRetrieveEvent as ConversationItemRetrieveEvent,\n    type ConversationItemTruncateEvent as ConversationItemTruncateEvent,\n    type ConversationItemTruncatedEvent as ConversationItemTruncatedEvent,\n    type ConversationItemWithReference as ConversationItemWithReference,\n    type InputAudioBufferAppendEvent as InputAudioBufferAppendEvent,\n    type InputAudioBufferClearEvent as InputAudioBufferClearEvent,\n    type InputAudioBufferClearedEvent as InputAudioBufferClearedEvent,\n    type InputAudioBufferCommitEvent as InputAudioBufferCommitEvent,\n    type InputAudioBufferCommittedEvent as InputAudioBufferCommittedEvent,\n    type InputAudioBufferDtmfEventReceivedEvent as InputAudioBufferDtmfEventReceivedEvent,\n    type InputAudioBufferSpeechStartedEvent as InputAudioBufferSpeechStartedEvent,\n    type InputAudioBufferSpeechStoppedEvent as InputAudioBufferSpeechStoppedEvent,\n    type InputAudioBufferTimeoutTriggered as InputAudioBufferTimeoutTriggered,\n    type LogProbProperties as LogProbProperties,\n    type McpListToolsCompleted as McpListToolsCompleted,\n    type McpListToolsFailed as McpListToolsFailed,\n    type McpListToolsInProgress as McpListToolsInProgress,\n    type NoiseReductionType as NoiseReductionType,\n    type OutputAudioBufferClearEvent as OutputAudioBufferClearEvent,\n    type RateLimitsUpdatedEvent as RateLimitsUpdatedEvent,\n    type RealtimeAudioConfig as RealtimeAudioConfig,\n    type RealtimeAudioConfigInput as RealtimeAudioConfigInput,\n    type RealtimeAudioConfigOutput as RealtimeAudioConfigOutput,\n    type RealtimeAudioFormats as RealtimeAudioFormats,\n    type RealtimeAudioInputTurnDetection as RealtimeAudioInputTurnDetection,\n    type RealtimeClientEvent as RealtimeClientEvent,\n    type RealtimeConversationItemAssistantMessage as RealtimeConversationItemAssistantMessage,\n    type RealtimeConversationItemFunctionCall as RealtimeConversationItemFunctionCall,\n    type RealtimeConversationItemFunctionCallOutput as RealtimeConversationItemFunctionCallOutput,\n    type RealtimeConversationItemSystemMessage as RealtimeConversationItemSystemMessage,\n    type RealtimeConversationItemUserMessage as RealtimeConversationItemUserMessage,\n    type RealtimeError as RealtimeError,\n    type RealtimeErrorEvent as RealtimeErrorEvent,\n    type RealtimeFunctionTool as RealtimeFunctionTool,\n    type RealtimeMcpApprovalRequest as RealtimeMcpApprovalRequest,\n    type RealtimeMcpApprovalResponse as RealtimeMcpApprovalResponse,\n    type RealtimeMcpListTools as RealtimeMcpListTools,\n    type RealtimeMcpProtocolError as RealtimeMcpProtocolError,\n    type RealtimeMcpToolCall as RealtimeMcpToolCall,\n    type RealtimeMcpToolExecutionError as RealtimeMcpToolExecutionError,\n    type RealtimeMcphttpError as RealtimeMcphttpError,\n    type RealtimeResponse as RealtimeResponse,\n    type RealtimeResponseCreateAudioOutput as RealtimeResponseCreateAudioOutput,\n    type RealtimeResponseCreateMcpTool as RealtimeResponseCreateMcpTool,\n    type RealtimeResponseCreateParams as RealtimeResponseCreateParams,\n    type RealtimeResponseStatus as RealtimeResponseStatus,\n    type RealtimeResponseUsage as RealtimeResponseUsage,\n    type RealtimeResponseUsageInputTokenDetails as RealtimeResponseUsageInputTokenDetails,\n    type RealtimeResponseUsageOutputTokenDetails as RealtimeResponseUsageOutputTokenDetails,\n    type RealtimeServerEvent as RealtimeServerEvent,\n    type RealtimeSession as RealtimeSession,\n    type RealtimeSessionCreateRequest as RealtimeSessionCreateRequest,\n    type RealtimeToolChoiceConfig as RealtimeToolChoiceConfig,\n    type RealtimeToolsConfig as RealtimeToolsConfig,\n    type RealtimeToolsConfigUnion as RealtimeToolsConfigUnion,\n    type RealtimeTracingConfig as RealtimeTracingConfig,\n    type RealtimeTranscriptionSessionAudio as RealtimeTranscriptionSessionAudio,\n    type RealtimeTranscriptionSessionAudioInput as RealtimeTranscriptionSessionAudioInput,\n    type RealtimeTranscriptionSessionAudioInputTurnDetection as RealtimeTranscriptionSessionAudioInputTurnDetection,\n    type RealtimeTranscriptionSessionCreateRequest as RealtimeTranscriptionSessionCreateRequest,\n    type RealtimeTruncation as RealtimeTruncation,\n    type RealtimeTruncationRetentionRatio as RealtimeTruncationRetentionRatio,\n    type ResponseAudioDeltaEvent as ResponseAudioDeltaEvent,\n    type ResponseAudioDoneEvent as ResponseAudioDoneEvent,\n    type ResponseAudioTranscriptDeltaEvent as ResponseAudioTranscriptDeltaEvent,\n    type ResponseAudioTranscriptDoneEvent as ResponseAudioTranscriptDoneEvent,\n    type ResponseCancelEvent as ResponseCancelEvent,\n    type ResponseContentPartAddedEvent as ResponseContentPartAddedEvent,\n    type ResponseContentPartDoneEvent as ResponseContentPartDoneEvent,\n    type ResponseCreateEvent as ResponseCreateEvent,\n    type ResponseCreatedEvent as ResponseCreatedEvent,\n    type ResponseDoneEvent as ResponseDoneEvent,\n    type ResponseFunctionCallArgumentsDeltaEvent as ResponseFunctionCallArgumentsDeltaEvent,\n    type ResponseFunctionCallArgumentsDoneEvent as ResponseFunctionCallArgumentsDoneEvent,\n    type ResponseMcpCallArgumentsDelta as ResponseMcpCallArgumentsDelta,\n    type ResponseMcpCallArgumentsDone as ResponseMcpCallArgumentsDone,\n    type ResponseMcpCallCompleted as ResponseMcpCallCompleted,\n    type ResponseMcpCallFailed as ResponseMcpCallFailed,\n    type ResponseMcpCallInProgress as ResponseMcpCallInProgress,\n    type ResponseOutputItemAddedEvent as ResponseOutputItemAddedEvent,\n    type ResponseOutputItemDoneEvent as ResponseOutputItemDoneEvent,\n    type ResponseTextDeltaEvent as ResponseTextDeltaEvent,\n    type ResponseTextDoneEvent as ResponseTextDoneEvent,\n    type SessionCreatedEvent as SessionCreatedEvent,\n    type SessionUpdateEvent as SessionUpdateEvent,\n    type SessionUpdatedEvent as SessionUpdatedEvent,\n    type TranscriptionSessionUpdate as TranscriptionSessionUpdate,\n    type TranscriptionSessionUpdatedEvent as TranscriptionSessionUpdatedEvent,\n  };\n\n  export {\n    ClientSecrets as ClientSecrets,\n    type RealtimeSessionClientSecret as RealtimeSessionClientSecret,\n    type RealtimeSessionCreateResponse as RealtimeSessionCreateResponse,\n    type RealtimeTranscriptionSessionCreateResponse as RealtimeTranscriptionSessionCreateResponse,\n    type RealtimeTranscriptionSessionTurnDetection as RealtimeTranscriptionSessionTurnDetection,\n    type ClientSecretCreateResponse as ClientSecretCreateResponse,\n    type ClientSecretCreateParams as ClientSecretCreateParams,\n  };\n\n  export {\n    Calls as Calls,\n    type CallAcceptParams as CallAcceptParams,\n    type CallReferParams as CallReferParams,\n    type CallRejectParams as CallRejectParams,\n  };\n}\n", "import { OpenAIError } from '../error';\nimport type { ChatCompletionTool } from '../resources/chat/completions';\nimport {\n  ResponseTextConfig,\n  type FunctionTool,\n  type ParsedContent,\n  type ParsedResponse,\n  type ParsedResponseFunctionToolCall,\n  type ParsedResponseOutputItem,\n  type Response,\n  type ResponseCreateParamsBase,\n  type ResponseCreateParamsNonStreaming,\n  type ResponseFunctionToolCall,\n  type Tool,\n} from '../resources/responses/responses';\nimport { type AutoParseableTextFormat, isAutoParsableResponseFormat } from '../lib/parser';\n\nexport type ParseableToolsParams = Array<Tool> | ChatCompletionTool | null;\n\nexport type ResponseCreateParamsWithTools = ResponseCreateParamsBase & {\n  tools?: ParseableToolsParams;\n};\n\ntype TextConfigParams = { text?: ResponseTextConfig };\n\nexport type ExtractParsedContentFromParams<Params extends TextConfigParams> =\n  NonNullable<Params['text']>['format'] extends AutoParseableTextFormat<infer P> ? P : null;\n\nexport function maybeParseResponse<\n  Params extends ResponseCreateParamsBase | null,\n  ParsedT = Params extends null ? null : ExtractParsedContentFromParams<NonNullable<Params>>,\n>(response: Response, params: Params): ParsedResponse<ParsedT> {\n  if (!params || !hasAutoParseableInput(params)) {\n    return {\n      ...response,\n      output_parsed: null,\n      output: response.output.map((item) => {\n        if (item.type === 'function_call') {\n          return {\n            ...item,\n            parsed_arguments: null,\n          };\n        }\n\n        if (item.type === 'message') {\n          return {\n            ...item,\n            content: item.content.map((content) => ({\n              ...content,\n              parsed: null,\n            })),\n          };\n        } else {\n          return item;\n        }\n      }),\n    };\n  }\n\n  return parseResponse(response, params);\n}\n\nexport function parseResponse<\n  Params extends ResponseCreateParamsBase,\n  ParsedT = ExtractParsedContentFromParams<Params>,\n>(response: Response, params: Params): ParsedResponse<ParsedT> {\n  const output: Array<ParsedResponseOutputItem<ParsedT>> = response.output.map(\n    (item): ParsedResponseOutputItem<ParsedT> => {\n      if (item.type === 'function_call') {\n        return {\n          ...item,\n          parsed_arguments: parseToolCall(params, item),\n        };\n      }\n      if (item.type === 'message') {\n        const content: Array<ParsedContent<ParsedT>> = item.content.map((content) => {\n          if (content.type === 'output_text') {\n            return {\n              ...content,\n              parsed: parseTextFormat(params, content.text),\n            };\n          }\n\n          return content;\n        });\n\n        return {\n          ...item,\n          content,\n        };\n      }\n\n      return item;\n    },\n  );\n\n  const parsed: Omit<ParsedResponse<ParsedT>, 'output_parsed'> = Object.assign({}, response, { output });\n  if (!Object.getOwnPropertyDescriptor(response, 'output_text')) {\n    addOutputText(parsed);\n  }\n\n  Object.defineProperty(parsed, 'output_parsed', {\n    enumerable: true,\n    get() {\n      for (const output of parsed.output) {\n        if (output.type !== 'message') {\n          continue;\n        }\n\n        for (const content of output.content) {\n          if (content.type === 'output_text' && content.parsed !== null) {\n            return content.parsed;\n          }\n        }\n      }\n\n      return null;\n    },\n  });\n\n  return parsed as ParsedResponse<ParsedT>;\n}\n\nfunction parseTextFormat<\n  Params extends ResponseCreateParamsBase,\n  ParsedT = ExtractParsedContentFromParams<Params>,\n>(params: Params, content: string): ParsedT | null {\n  if (params.text?.format?.type !== 'json_schema') {\n    return null;\n  }\n\n  if ('$parseRaw' in params.text?.format) {\n    const text_format = params.text?.format as unknown as AutoParseableTextFormat<ParsedT>;\n    return text_format.$parseRaw(content);\n  }\n\n  return JSON.parse(content);\n}\n\nexport function hasAutoParseableInput(params: ResponseCreateParamsWithTools): boolean {\n  if (isAutoParsableResponseFormat(params.text?.format)) {\n    return true;\n  }\n\n  return false;\n}\n\ntype ToolOptions = {\n  name: string;\n  arguments: any;\n  function?: ((args: any) => any) | undefined;\n};\n\nexport type AutoParseableResponseTool<\n  OptionsT extends ToolOptions,\n  HasFunction = OptionsT['function'] extends Function ? true : false,\n> = FunctionTool & {\n  __arguments: OptionsT['arguments']; // type-level only\n  __name: OptionsT['name']; // type-level only\n\n  $brand: 'auto-parseable-tool';\n  $callback: ((args: OptionsT['arguments']) => any) | undefined;\n  $parseRaw(args: string): OptionsT['arguments'];\n};\n\nexport function makeParseableResponseTool<OptionsT extends ToolOptions>(\n  tool: FunctionTool,\n  {\n    parser,\n    callback,\n  }: {\n    parser: (content: string) => OptionsT['arguments'];\n    callback: ((args: any) => any) | undefined;\n  },\n): AutoParseableResponseTool<OptionsT['arguments']> {\n  const obj = { ...tool };\n\n  Object.defineProperties(obj, {\n    $brand: {\n      value: 'auto-parseable-tool',\n      enumerable: false,\n    },\n    $parseRaw: {\n      value: parser,\n      enumerable: false,\n    },\n    $callback: {\n      value: callback,\n      enumerable: false,\n    },\n  });\n\n  return obj as AutoParseableResponseTool<OptionsT['arguments']>;\n}\n\nexport function isAutoParsableTool(tool: any): tool is AutoParseableResponseTool<any> {\n  return tool?.['$brand'] === 'auto-parseable-tool';\n}\n\nfunction getInputToolByName(input_tools: Array<Tool>, name: string): FunctionTool | undefined {\n  return input_tools.find((tool) => tool.type === 'function' && tool.name === name) as\n    | FunctionTool\n    | undefined;\n}\n\nfunction parseToolCall<Params extends ResponseCreateParamsBase>(\n  params: Params,\n  toolCall: ResponseFunctionToolCall,\n): ParsedResponseFunctionToolCall {\n  const inputTool = getInputToolByName(params.tools ?? [], toolCall.name);\n\n  return {\n    ...toolCall,\n    ...toolCall,\n    parsed_arguments:\n      isAutoParsableTool(inputTool) ? inputTool.$parseRaw(toolCall.arguments)\n      : inputTool?.strict ? JSON.parse(toolCall.arguments)\n      : null,\n  };\n}\n\nexport function shouldParseToolCall(\n  params: ResponseCreateParamsNonStreaming | null | undefined,\n  toolCall: ResponseFunctionToolCall,\n): boolean {\n  if (!params) {\n    return false;\n  }\n\n  const inputTool = getInputToolByName(params.tools ?? [], toolCall.name);\n  return isAutoParsableTool(inputTool) || inputTool?.strict || false;\n}\n\nexport function validateInputTools(tools: ChatCompletionTool[] | undefined) {\n  for (const tool of tools ?? []) {\n    if (tool.type !== 'function') {\n      throw new OpenAIError(\n        `Currently only \\`function\\` tool types support auto-parsing; Received \\`${tool.type}\\``,\n      );\n    }\n\n    if (tool.function.strict !== true) {\n      throw new OpenAIError(\n        `The \\`${tool.function.name}\\` tool is not marked with \\`strict: true\\`. Only strict function tools can be auto-parsed`,\n      );\n    }\n  }\n}\n\nexport function addOutputText(rsp: Response): void {\n  const texts: string[] = [];\n  for (const output of rsp.output) {\n    if (output.type !== 'message') {\n      continue;\n    }\n\n    for (const content of output.content) {\n      if (content.type === 'output_text') {\n        texts.push(content.text);\n      }\n    }\n  }\n\n  rsp.output_text = texts.join('');\n}\n", "import {\n  ResponseTextConfig,\n  type ParsedResponse,\n  type Response,\n  type ResponseCreateParamsBase,\n  type ResponseCreateParamsStreaming,\n  type ResponseStreamEvent,\n} from '../../resources/responses/responses';\nimport { RequestOptions } from '../../internal/request-options';\nimport { APIUserAbortError, OpenAIError } from '../../error';\nimport OpenAI from '../../index';\nimport { type BaseEvents, EventStream } from '../EventStream';\nimport { type ResponseFunctionCallArgumentsDeltaEvent, type ResponseTextDeltaEvent } from './EventTypes';\nimport { maybeParseResponse, ParseableToolsParams } from '../ResponsesParser';\nimport { Stream } from '../../streaming';\n\nexport type ResponseStreamParams = ResponseCreateAndStreamParams | ResponseStreamByIdParams;\n\nexport type ResponseCreateAndStreamParams = Omit<ResponseCreateParamsBase, 'stream'> & {\n  stream?: true;\n};\n\nexport type ResponseStreamByIdParams = {\n  /**\n   * The ID of the response to stream.\n   */\n  response_id: string;\n  /**\n   * If provided, the stream will start after the event with the given sequence number.\n   */\n  starting_after?: number;\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data. Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n   */\n  text?: ResponseTextConfig;\n\n  /**\n   * An array of tools the model may call while generating a response. When continuing a stream, provide\n   * the same tools as the original request.\n   */\n  tools?: ParseableToolsParams;\n};\n\ntype ResponseEvents = BaseEvents &\n  Omit<\n    {\n      [K in ResponseStreamEvent['type']]: (event: Extract<ResponseStreamEvent, { type: K }>) => void;\n    },\n    'response.output_text.delta' | 'response.function_call_arguments.delta'\n  > & {\n    event: (event: ResponseStreamEvent) => void;\n    'response.output_text.delta': (event: ResponseTextDeltaEvent) => void;\n    'response.function_call_arguments.delta': (event: ResponseFunctionCallArgumentsDeltaEvent) => void;\n  };\n\nexport type ResponseStreamingParams = Omit<ResponseCreateParamsBase, 'stream'> & {\n  stream?: true;\n};\n\nexport class ResponseStream<ParsedT = null>\n  extends EventStream<ResponseEvents>\n  implements AsyncIterable<ResponseStreamEvent>\n{\n  #params: ResponseStreamingParams | null;\n  #currentResponseSnapshot: Response | undefined;\n  #finalResponse: ParsedResponse<ParsedT> | undefined;\n\n  constructor(params: ResponseStreamingParams | null) {\n    super();\n    this.#params = params;\n  }\n\n  static createResponse<ParsedT>(\n    client: OpenAI,\n    params: ResponseStreamParams,\n    options?: RequestOptions,\n  ): ResponseStream<ParsedT> {\n    const runner = new ResponseStream<ParsedT>(params as ResponseCreateParamsStreaming);\n    runner._run(() =>\n      runner._createOrRetrieveResponse(client, params, {\n        ...options,\n        headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' },\n      }),\n    );\n    return runner;\n  }\n\n  #beginRequest() {\n    if (this.ended) return;\n    this.#currentResponseSnapshot = undefined;\n  }\n\n  #addEvent(this: ResponseStream<ParsedT>, event: ResponseStreamEvent, starting_after: number | null) {\n    if (this.ended) return;\n\n    const maybeEmit = (name: string, event: ResponseStreamEvent & { snapshot?: string }) => {\n      if (starting_after == null || event.sequence_number > starting_after) {\n        this._emit(name as any, event);\n      }\n    };\n\n    const response = this.#accumulateResponse(event);\n    maybeEmit('event', event);\n\n    switch (event.type) {\n      case 'response.output_text.delta': {\n        const output = response.output[event.output_index];\n        if (!output) {\n          throw new OpenAIError(`missing output at index ${event.output_index}`);\n        }\n        if (output.type === 'message') {\n          const content = output.content[event.content_index];\n          if (!content) {\n            throw new OpenAIError(`missing content at index ${event.content_index}`);\n          }\n          if (content.type !== 'output_text') {\n            throw new OpenAIError(`expected content to be 'output_text', got ${content.type}`);\n          }\n\n          maybeEmit('response.output_text.delta', {\n            ...event,\n            snapshot: content.text,\n          });\n        }\n        break;\n      }\n      case 'response.function_call_arguments.delta': {\n        const output = response.output[event.output_index];\n        if (!output) {\n          throw new OpenAIError(`missing output at index ${event.output_index}`);\n        }\n        if (output.type === 'function_call') {\n          maybeEmit('response.function_call_arguments.delta', {\n            ...event,\n            snapshot: output.arguments,\n          });\n        }\n        break;\n      }\n      default:\n        maybeEmit(event.type, event);\n        break;\n    }\n  }\n\n  #endRequest(): ParsedResponse<ParsedT> {\n    if (this.ended) {\n      throw new OpenAIError(`stream has ended, this shouldn't happen`);\n    }\n    const snapshot = this.#currentResponseSnapshot;\n    if (!snapshot) {\n      throw new OpenAIError(`request ended without sending any events`);\n    }\n    this.#currentResponseSnapshot = undefined;\n    const parsedResponse = finalizeResponse<ParsedT>(snapshot, this.#params);\n    this.#finalResponse = parsedResponse;\n\n    return parsedResponse;\n  }\n\n  protected async _createOrRetrieveResponse(\n    client: OpenAI,\n    params: ResponseStreamParams,\n    options?: RequestOptions,\n  ): Promise<ParsedResponse<ParsedT>> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this.#beginRequest();\n\n    let stream: Stream<ResponseStreamEvent> | undefined;\n    let starting_after: number | null = null;\n    if ('response_id' in params) {\n      stream = await client.responses.retrieve(\n        params.response_id,\n        { stream: true },\n        { ...options, signal: this.controller.signal, stream: true },\n      );\n      starting_after = params.starting_after ?? null;\n    } else {\n      stream = await client.responses.create(\n        { ...params, stream: true },\n        { ...options, signal: this.controller.signal },\n      );\n    }\n\n    this._connected();\n    for await (const event of stream) {\n      this.#addEvent(event, starting_after);\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n    return this.#endRequest();\n  }\n\n  #accumulateResponse(event: ResponseStreamEvent): Response {\n    let snapshot = this.#currentResponseSnapshot;\n    if (!snapshot) {\n      if (event.type !== 'response.created') {\n        throw new OpenAIError(\n          `When snapshot hasn't been set yet, expected 'response.created' event, got ${event.type}`,\n        );\n      }\n      snapshot = this.#currentResponseSnapshot = event.response;\n      return snapshot;\n    }\n\n    switch (event.type) {\n      case 'response.output_item.added': {\n        snapshot.output.push(event.item);\n        break;\n      }\n      case 'response.content_part.added': {\n        const output = snapshot.output[event.output_index];\n        if (!output) {\n          throw new OpenAIError(`missing output at index ${event.output_index}`);\n        }\n        const type = output.type;\n        const part = event.part;\n        if (type === 'message' && part.type !== 'reasoning_text') {\n          output.content.push(part);\n        } else if (type === 'reasoning' && part.type === 'reasoning_text') {\n          if (!output.content) {\n            output.content = [];\n          }\n          output.content.push(part);\n        }\n        break;\n      }\n      case 'response.output_text.delta': {\n        const output = snapshot.output[event.output_index];\n        if (!output) {\n          throw new OpenAIError(`missing output at index ${event.output_index}`);\n        }\n        if (output.type === 'message') {\n          const content = output.content[event.content_index];\n          if (!content) {\n            throw new OpenAIError(`missing content at index ${event.content_index}`);\n          }\n          if (content.type !== 'output_text') {\n            throw new OpenAIError(`expected content to be 'output_text', got ${content.type}`);\n          }\n          content.text += event.delta;\n        }\n        break;\n      }\n      case 'response.function_call_arguments.delta': {\n        const output = snapshot.output[event.output_index];\n        if (!output) {\n          throw new OpenAIError(`missing output at index ${event.output_index}`);\n        }\n        if (output.type === 'function_call') {\n          output.arguments += event.delta;\n        }\n        break;\n      }\n      case 'response.reasoning_text.delta': {\n        const output = snapshot.output[event.output_index];\n        if (!output) {\n          throw new OpenAIError(`missing output at index ${event.output_index}`);\n        }\n        if (output.type === 'reasoning') {\n          const content = output.content?.[event.content_index];\n          if (!content) {\n            throw new OpenAIError(`missing content at index ${event.content_index}`);\n          }\n          if (content.type !== 'reasoning_text') {\n            throw new OpenAIError(`expected content to be 'reasoning_text', got ${content.type}`);\n          }\n          content.text += event.delta;\n        }\n        break;\n      }\n      case 'response.completed': {\n        this.#currentResponseSnapshot = event.response;\n        break;\n      }\n    }\n\n    return snapshot;\n  }\n\n  [Symbol.asyncIterator](this: ResponseStream<ParsedT>): AsyncIterator<ResponseStreamEvent> {\n    const pushQueue: ResponseStreamEvent[] = [];\n    const readQueue: {\n      resolve: (event: ResponseStreamEvent | undefined) => void;\n      reject: (err: unknown) => void;\n    }[] = [];\n    let done = false;\n\n    this.on('event', (event) => {\n      const reader = readQueue.shift();\n      if (reader) {\n        reader.resolve(event);\n      } else {\n        pushQueue.push(event);\n      }\n    });\n\n    this.on('end', () => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.resolve(undefined);\n      }\n      readQueue.length = 0;\n    });\n\n    this.on('abort', (err) => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.reject(err);\n      }\n      readQueue.length = 0;\n    });\n\n    this.on('error', (err) => {\n      done = true;\n      for (const reader of readQueue) {\n        reader.reject(err);\n      }\n      readQueue.length = 0;\n    });\n\n    return {\n      next: async (): Promise<IteratorResult<ResponseStreamEvent>> => {\n        if (!pushQueue.length) {\n          if (done) {\n            return { value: undefined, done: true };\n          }\n          return new Promise<ResponseStreamEvent | undefined>((resolve, reject) =>\n            readQueue.push({ resolve, reject }),\n          ).then((event) => (event ? { value: event, done: false } : { value: undefined, done: true }));\n        }\n        const event = pushQueue.shift()!;\n        return { value: event, done: false };\n      },\n      return: async () => {\n        this.abort();\n        return { value: undefined, done: true };\n      },\n    };\n  }\n\n  /**\n   * @returns a promise that resolves with the final Response, or rejects\n   * if an error occurred or the stream ended prematurely without producing a REsponse.\n   */\n  async finalResponse(): Promise<ParsedResponse<ParsedT>> {\n    await this.done();\n    const response = this.#finalResponse;\n    if (!response) throw new OpenAIError('stream ended without producing a ChatCompletion');\n    return response;\n  }\n}\n\nfunction finalizeResponse<ParsedT>(\n  snapshot: Response,\n  params: ResponseStreamingParams | null,\n): ParsedResponse<ParsedT> {\n  return maybeParseResponse(snapshot, params);\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as ResponsesAPI from './responses';\nimport { ResponseItemsPage } from './responses';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class InputItems extends APIResource {\n  /**\n   * Returns a list of input items for a given response.\n   *\n   * @example\n   * ```ts\n   * // Automatically fetches more pages as needed.\n   * for await (const responseItem of client.responses.inputItems.list(\n   *   'response_id',\n   * )) {\n   *   // ...\n   * }\n   * ```\n   */\n  list(\n    responseID: string,\n    query: InputItemListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<ResponseItemsPage, ResponsesAPI.ResponseItem> {\n    return this._client.getAPIList(\n      path`/responses/${responseID}/input_items`,\n      CursorPage<ResponsesAPI.ResponseItem>,\n      { query, ...options },\n    );\n  }\n}\n\n/**\n * A list of Response items.\n */\nexport interface ResponseItemList {\n  /**\n   * A list of items used to generate this response.\n   */\n  data: Array<ResponsesAPI.ResponseItem>;\n\n  /**\n   * The ID of the first item in the list.\n   */\n  first_id: string;\n\n  /**\n   * Whether there are more items available.\n   */\n  has_more: boolean;\n\n  /**\n   * The ID of the last item in the list.\n   */\n  last_id: string;\n\n  /**\n   * The type of object returned, must be `list`.\n   */\n  object: 'list';\n}\n\nexport interface InputItemListParams extends CursorPageParams {\n  /**\n   * Additional fields to include in the response. See the `include` parameter for\n   * Response creation above for more information.\n   */\n  include?: Array<ResponsesAPI.ResponseIncludable>;\n\n  /**\n   * The order to return the input items in. Default is `desc`.\n   *\n   * - `asc`: Return the input items in ascending order.\n   * - `desc`: Return the input items in descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport declare namespace InputItems {\n  export { type ResponseItemList as ResponseItemList, type InputItemListParams as InputItemListParams };\n}\n\nexport { type ResponseItemsPage };\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as Shared from '../shared';\nimport * as ResponsesAPI from './responses';\nimport { APIPromise } from '../../core/api-promise';\nimport { RequestOptions } from '../../internal/request-options';\n\nexport class InputTokens extends APIResource {\n  /**\n   * Returns input token counts of the request.\n   *\n   * Returns an object with `object` set to `response.input_tokens` and an\n   * `input_tokens` count.\n   *\n   * @example\n   * ```ts\n   * const response = await client.responses.inputTokens.count();\n   * ```\n   */\n  count(\n    body: InputTokenCountParams | null | undefined = {},\n    options?: RequestOptions,\n  ): APIPromise<InputTokenCountResponse> {\n    return this._client.post('/responses/input_tokens', { body, ...options });\n  }\n}\n\nexport interface InputTokenCountResponse {\n  input_tokens: number;\n\n  object: 'response.input_tokens';\n}\n\nexport interface InputTokenCountParams {\n  /**\n   * The conversation that this response belongs to. Items from this conversation are\n   * prepended to `input_items` for this response request. Input items and output\n   * items from this response are automatically added to this conversation after this\n   * response completes.\n   */\n  conversation?: string | ResponsesAPI.ResponseConversationParam | null;\n\n  /**\n   * Text, image, or file inputs to the model, used to generate a response\n   */\n  input?: string | Array<ResponsesAPI.ResponseInputItem> | null;\n\n  /**\n   * A system (or developer) message inserted into the model's context. When used\n   * along with `previous_response_id`, the instructions from a previous response\n   * will not be carried over to the next response. This makes it simple to swap out\n   * system (or developer) messages in new responses.\n   */\n  instructions?: string | null;\n\n  /**\n   * Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a\n   * wide range of models with different capabilities, performance characteristics,\n   * and price points. Refer to the\n   * [model guide](https://platform.openai.com/docs/models) to browse and compare\n   * available models.\n   */\n  model?: string | null;\n\n  /**\n   * Whether to allow the model to run tool calls in parallel.\n   */\n  parallel_tool_calls?: boolean | null;\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create\n   * multi-turn conversations. Learn more about\n   * [conversation state](https://platform.openai.com/docs/guides/conversation-state).\n   * Cannot be used in conjunction with `conversation`.\n   */\n  previous_response_id?: string | null;\n\n  /**\n   * **gpt-5 and o-series models only** Configuration options for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n   */\n  reasoning?: Shared.Reasoning | null;\n\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data. Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n   */\n  text?: InputTokenCountParams.Text | null;\n\n  /**\n   * Controls which tool the model should use, if any.\n   */\n  tool_choice?:\n    | ResponsesAPI.ToolChoiceOptions\n    | ResponsesAPI.ToolChoiceAllowed\n    | ResponsesAPI.ToolChoiceTypes\n    | ResponsesAPI.ToolChoiceFunction\n    | ResponsesAPI.ToolChoiceMcp\n    | ResponsesAPI.ToolChoiceCustom\n    | ResponsesAPI.ToolChoiceApplyPatch\n    | ResponsesAPI.ToolChoiceShell\n    | null;\n\n  /**\n   * An array of tools the model may call while generating a response. You can\n   * specify which tool to use by setting the `tool_choice` parameter.\n   */\n  tools?: Array<ResponsesAPI.Tool> | null;\n\n  /**\n   * The truncation strategy to use for the model response. - `auto`: If the input to\n   * this Response exceeds the model's context window size, the model will truncate\n   * the response to fit the context window by dropping items from the beginning of\n   * the conversation. - `disabled` (default): If the input size will exceed the\n   * context window size for a model, the request will fail with a 400 error.\n   */\n  truncation?: 'auto' | 'disabled';\n}\n\nexport namespace InputTokenCountParams {\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data. Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n   */\n  export interface Text {\n    /**\n     * An object specifying the format that the model must output.\n     *\n     * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n     * ensures the model will match your supplied JSON schema. Learn more in the\n     * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n     *\n     * The default format is `{ \"type\": \"text\" }` with no additional options.\n     *\n     * **Not recommended for gpt-4o and newer models:**\n     *\n     * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n     * ensures the message the model generates is valid JSON. Using `json_schema` is\n     * preferred for models that support it.\n     */\n    format?: ResponsesAPI.ResponseFormatTextConfig;\n\n    /**\n     * Constrains the verbosity of the model's response. Lower values will result in\n     * more concise responses, while higher values will result in more verbose\n     * responses. Currently supported values are `low`, `medium`, and `high`.\n     */\n    verbosity?: 'low' | 'medium' | 'high' | null;\n  }\n}\n\nexport declare namespace InputTokens {\n  export {\n    type InputTokenCountResponse as InputTokenCountResponse,\n    type InputTokenCountParams as InputTokenCountParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport {\n  type ExtractParsedContentFromParams,\n  parseResponse,\n  type ResponseCreateParamsWithTools,\n  addOutputText,\n} from '../../lib/ResponsesParser';\nimport { ResponseStream, ResponseStreamParams } from '../../lib/responses/ResponseStream';\nimport { APIResource } from '../../core/resource';\nimport * as ResponsesAPI from './responses';\nimport * as Shared from '../shared';\nimport * as InputItemsAPI from './input-items';\nimport { InputItemListParams, InputItems, ResponseItemList } from './input-items';\nimport * as InputTokensAPI from './input-tokens';\nimport { InputTokenCountParams, InputTokenCountResponse, InputTokens } from './input-tokens';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage } from '../../core/pagination';\nimport { Stream } from '../../core/streaming';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport interface ParsedResponseOutputText<ParsedT> extends ResponseOutputText {\n  parsed: ParsedT | null;\n}\n\nexport type ParsedContent<ParsedT> = ParsedResponseOutputText<ParsedT> | ResponseOutputRefusal;\n\nexport interface ParsedResponseOutputMessage<ParsedT> extends ResponseOutputMessage {\n  content: ParsedContent<ParsedT>[];\n}\n\nexport interface ParsedResponseFunctionToolCall extends ResponseFunctionToolCall {\n  parsed_arguments: any;\n}\n\nexport type ParsedResponseOutputItem<ParsedT> =\n  | ParsedResponseOutputMessage<ParsedT>\n  | ParsedResponseFunctionToolCall\n  | ResponseFileSearchToolCall\n  | ResponseFunctionWebSearch\n  | ResponseComputerToolCall\n  | ResponseReasoningItem\n  | ResponseCompactionItem\n  | ResponseOutputItem.ImageGenerationCall\n  | ResponseCodeInterpreterToolCall\n  | ResponseOutputItem.LocalShellCall\n  | ResponseFunctionShellToolCall\n  | ResponseFunctionShellToolCallOutput\n  | ResponseApplyPatchToolCall\n  | ResponseApplyPatchToolCallOutput\n  | ResponseOutputItem.McpCall\n  | ResponseOutputItem.McpListTools\n  | ResponseOutputItem.McpApprovalRequest\n  | ResponseCustomToolCall;\n\nexport interface ParsedResponse<ParsedT> extends Response {\n  output: Array<ParsedResponseOutputItem<ParsedT>>;\n\n  output_parsed: ParsedT | null;\n}\n\nexport type ResponseParseParams = ResponseCreateParamsNonStreaming;\n\nexport class Responses extends APIResource {\n  inputItems: InputItemsAPI.InputItems = new InputItemsAPI.InputItems(this._client);\n  inputTokens: InputTokensAPI.InputTokens = new InputTokensAPI.InputTokens(this._client);\n\n  /**\n   * Creates a model response. Provide\n   * [text](https://platform.openai.com/docs/guides/text) or\n   * [image](https://platform.openai.com/docs/guides/images) inputs to generate\n   * [text](https://platform.openai.com/docs/guides/text) or\n   * [JSON](https://platform.openai.com/docs/guides/structured-outputs) outputs. Have\n   * the model call your own\n   * [custom code](https://platform.openai.com/docs/guides/function-calling) or use\n   * built-in [tools](https://platform.openai.com/docs/guides/tools) like\n   * [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n   * [file search](https://platform.openai.com/docs/guides/tools-file-search) to use\n   * your own data as input for the model's response.\n   *\n   * @example\n   * ```ts\n   * const response = await client.responses.create();\n   * ```\n   */\n  create(body: ResponseCreateParamsNonStreaming, options?: RequestOptions): APIPromise<Response>;\n  create(\n    body: ResponseCreateParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ResponseStreamEvent>>;\n  create(\n    body: ResponseCreateParamsBase,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ResponseStreamEvent> | Response>;\n  create(\n    body: ResponseCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<Response> | APIPromise<Stream<ResponseStreamEvent>> {\n    return (\n      this._client.post('/responses', { body, ...options, stream: body.stream ?? false }) as\n        | APIPromise<Response>\n        | APIPromise<Stream<ResponseStreamEvent>>\n    )._thenUnwrap((rsp) => {\n      if ('object' in rsp && rsp.object === 'response') {\n        addOutputText(rsp as Response);\n      }\n\n      return rsp;\n    }) as APIPromise<Response> | APIPromise<Stream<ResponseStreamEvent>>;\n  }\n\n  /**\n   * Retrieves a model response with the given ID.\n   *\n   * @example\n   * ```ts\n   * const response = await client.responses.retrieve(\n   *   'resp_677efb5139a88190b512bc3fef8e535d',\n   * );\n   * ```\n   */\n  retrieve(\n    responseID: string,\n    query?: ResponseRetrieveParamsNonStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Response>;\n  retrieve(\n    responseID: string,\n    query: ResponseRetrieveParamsStreaming,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ResponseStreamEvent>>;\n  retrieve(\n    responseID: string,\n    query?: ResponseRetrieveParamsBase | undefined,\n    options?: RequestOptions,\n  ): APIPromise<Stream<ResponseStreamEvent> | Response>;\n  retrieve(\n    responseID: string,\n    query: ResponseRetrieveParams | undefined = {},\n    options?: RequestOptions,\n  ): APIPromise<Response> | APIPromise<Stream<ResponseStreamEvent>> {\n    return (\n      this._client.get(path`/responses/${responseID}`, {\n        query,\n        ...options,\n        stream: query?.stream ?? false,\n      }) as APIPromise<Response> | APIPromise<Stream<ResponseStreamEvent>>\n    )._thenUnwrap((rsp) => {\n      if ('object' in rsp && rsp.object === 'response') {\n        addOutputText(rsp as Response);\n      }\n\n      return rsp;\n    }) as APIPromise<Response> | APIPromise<Stream<ResponseStreamEvent>>;\n  }\n\n  /**\n   * Deletes a model response with the given ID.\n   *\n   * @example\n   * ```ts\n   * await client.responses.delete(\n   *   'resp_677efb5139a88190b512bc3fef8e535d',\n   * );\n   * ```\n   */\n  delete(responseID: string, options?: RequestOptions): APIPromise<void> {\n    return this._client.delete(path`/responses/${responseID}`, {\n      ...options,\n      headers: buildHeaders([{ Accept: '*/*' }, options?.headers]),\n    });\n  }\n\n  parse<Params extends ResponseCreateParamsWithTools, ParsedT = ExtractParsedContentFromParams<Params>>(\n    body: Params,\n    options?: RequestOptions,\n  ): APIPromise<ParsedResponse<ParsedT>> {\n    return this._client.responses\n      .create(body, options)\n      ._thenUnwrap((response) => parseResponse(response as Response, body));\n  }\n\n  /**\n   * Creates a model response stream\n   */\n  stream<Params extends ResponseStreamParams, ParsedT = ExtractParsedContentFromParams<Params>>(\n    body: Params,\n    options?: RequestOptions,\n  ): ResponseStream<ParsedT> {\n    return ResponseStream.createResponse<ParsedT>(this._client, body, options);\n  }\n\n  /**\n   * Cancels a model response with the given ID. Only responses created with the\n   * `background` parameter set to `true` can be cancelled.\n   * [Learn more](https://platform.openai.com/docs/guides/background).\n   *\n   * @example\n   * ```ts\n   * const response = await client.responses.cancel(\n   *   'resp_677efb5139a88190b512bc3fef8e535d',\n   * );\n   * ```\n   */\n  cancel(responseID: string, options?: RequestOptions): APIPromise<Response> {\n    return this._client.post(path`/responses/${responseID}/cancel`, options);\n  }\n\n  /**\n   * Compact a conversation. Returns a compacted response object.\n   *\n   * Learn when and how to compact long-running conversations in the\n   * [conversation state guide](https://platform.openai.com/docs/guides/conversation-state#managing-the-context-window).\n   * For ZDR-compatible compaction details, see\n   * [Compaction (advanced)](https://platform.openai.com/docs/guides/conversation-state#compaction-advanced).\n   *\n   * @example\n   * ```ts\n   * const compactedResponse = await client.responses.compact({\n   *   model: 'gpt-5.2',\n   * });\n   * ```\n   */\n  compact(body: ResponseCompactParams, options?: RequestOptions): APIPromise<CompactedResponse> {\n    return this._client.post('/responses/compact', { body, ...options });\n  }\n}\n\nexport type ResponseItemsPage = CursorPage<ResponseItem>;\n\n/**\n * Allows the assistant to create, delete, or update files using unified diffs.\n */\nexport interface ApplyPatchTool {\n  /**\n   * The type of the tool. Always `apply_patch`.\n   */\n  type: 'apply_patch';\n}\n\nexport interface CompactedResponse {\n  /**\n   * The unique identifier for the compacted response.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) when the compacted conversation was created.\n   */\n  created_at: number;\n\n  /**\n   * The object type. Always `response.compaction`.\n   */\n  object: 'response.compaction';\n\n  /**\n   * The compacted list of output items. This is a list of all user messages,\n   * followed by a single compaction item.\n   */\n  output: Array<ResponseOutputItem>;\n\n  /**\n   * Token accounting for the compaction pass, including cached, reasoning, and total\n   * tokens.\n   */\n  usage: ResponseUsage;\n}\n\n/**\n * A tool that controls a virtual computer. Learn more about the\n * [computer tool](https://platform.openai.com/docs/guides/tools-computer-use).\n */\nexport interface ComputerTool {\n  /**\n   * The height of the computer display.\n   */\n  display_height: number;\n\n  /**\n   * The width of the computer display.\n   */\n  display_width: number;\n\n  /**\n   * The type of computer environment to control.\n   */\n  environment: 'windows' | 'mac' | 'linux' | 'ubuntu' | 'browser';\n\n  /**\n   * The type of the computer use tool. Always `computer_use_preview`.\n   */\n  type: 'computer_use_preview';\n}\n\nexport interface ContainerAuto {\n  /**\n   * Automatically creates a container for this request\n   */\n  type: 'container_auto';\n\n  /**\n   * An optional list of uploaded files to make available to your code.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * The memory limit for the container.\n   */\n  memory_limit?: '1g' | '4g' | '16g' | '64g' | null;\n\n  /**\n   * Network access policy for the container.\n   */\n  network_policy?: ContainerNetworkPolicyDisabled | ContainerNetworkPolicyAllowlist;\n\n  /**\n   * An optional list of skills referenced by id or inline data.\n   */\n  skills?: Array<SkillReference | InlineSkill>;\n}\n\nexport interface ContainerNetworkPolicyAllowlist {\n  /**\n   * A list of allowed domains when type is `allowlist`.\n   */\n  allowed_domains: Array<string>;\n\n  /**\n   * Allow outbound network access only to specified domains. Always `allowlist`.\n   */\n  type: 'allowlist';\n\n  /**\n   * Optional domain-scoped secrets for allowlisted domains.\n   */\n  domain_secrets?: Array<ContainerNetworkPolicyDomainSecret>;\n}\n\nexport interface ContainerNetworkPolicyDisabled {\n  /**\n   * Disable outbound network access. Always `disabled`.\n   */\n  type: 'disabled';\n}\n\nexport interface ContainerNetworkPolicyDomainSecret {\n  /**\n   * The domain associated with the secret.\n   */\n  domain: string;\n\n  /**\n   * The name of the secret to inject for the domain.\n   */\n  name: string;\n\n  /**\n   * The secret value to inject for the domain.\n   */\n  value: string;\n}\n\nexport interface ContainerReference {\n  /**\n   * The ID of the referenced container.\n   */\n  container_id: string;\n\n  /**\n   * References a container created with the /v1/containers endpoint\n   */\n  type: 'container_reference';\n}\n\n/**\n * A custom tool that processes input using a specified format. Learn more about\n * [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n */\nexport interface CustomTool {\n  /**\n   * The name of the custom tool, used to identify it in tool calls.\n   */\n  name: string;\n\n  /**\n   * The type of the custom tool. Always `custom`.\n   */\n  type: 'custom';\n\n  /**\n   * Optional description of the custom tool, used to provide more context.\n   */\n  description?: string;\n\n  /**\n   * The input format for the custom tool. Default is unconstrained text.\n   */\n  format?: Shared.CustomToolInputFormat;\n}\n\n/**\n * A message input to the model with a role indicating instruction following\n * hierarchy. Instructions given with the `developer` or `system` role take\n * precedence over instructions given with the `user` role. Messages with the\n * `assistant` role are presumed to have been generated by the model in previous\n * interactions.\n */\nexport interface EasyInputMessage {\n  /**\n   * Text, image, or audio input to the model, used to generate a response. Can also\n   * contain previous assistant responses.\n   */\n  content: string | ResponseInputMessageContentList;\n\n  /**\n   * The role of the message input. One of `user`, `assistant`, `system`, or\n   * `developer`.\n   */\n  role: 'user' | 'assistant' | 'system' | 'developer';\n\n  /**\n   * The phase of an assistant message.\n   *\n   * Use `commentary` for an intermediate assistant message and `final_answer` for\n   * the final assistant message. For follow-up requests with models like\n   * `gpt-5.3-codex` and later, preserve and resend phase on all assistant messages.\n   * Omitting it can degrade performance. Not used for user messages.\n   */\n  phase?: 'commentary' | 'final_answer' | null;\n\n  /**\n   * The type of the message input. Always `message`.\n   */\n  type?: 'message';\n}\n\n/**\n * A tool that searches for relevant content from uploaded files. Learn more about\n * the\n * [file search tool](https://platform.openai.com/docs/guides/tools-file-search).\n */\nexport interface FileSearchTool {\n  /**\n   * The type of the file search tool. Always `file_search`.\n   */\n  type: 'file_search';\n\n  /**\n   * The IDs of the vector stores to search.\n   */\n  vector_store_ids: Array<string>;\n\n  /**\n   * A filter to apply.\n   */\n  filters?: Shared.ComparisonFilter | Shared.CompoundFilter | null;\n\n  /**\n   * The maximum number of results to return. This number should be between 1 and 50\n   * inclusive.\n   */\n  max_num_results?: number;\n\n  /**\n   * Ranking options for search.\n   */\n  ranking_options?: FileSearchTool.RankingOptions;\n}\n\nexport namespace FileSearchTool {\n  /**\n   * Ranking options for search.\n   */\n  export interface RankingOptions {\n    /**\n     * Weights that control how reciprocal rank fusion balances semantic embedding\n     * matches versus sparse keyword matches when hybrid search is enabled.\n     */\n    hybrid_search?: RankingOptions.HybridSearch;\n\n    /**\n     * The ranker to use for the file search.\n     */\n    ranker?: 'auto' | 'default-2024-11-15';\n\n    /**\n     * The score threshold for the file search, a number between 0 and 1. Numbers\n     * closer to 1 will attempt to return only the most relevant results, but may\n     * return fewer results.\n     */\n    score_threshold?: number;\n  }\n\n  export namespace RankingOptions {\n    /**\n     * Weights that control how reciprocal rank fusion balances semantic embedding\n     * matches versus sparse keyword matches when hybrid search is enabled.\n     */\n    export interface HybridSearch {\n      /**\n       * The weight of the embedding in the reciprocal ranking fusion.\n       */\n      embedding_weight: number;\n\n      /**\n       * The weight of the text in the reciprocal ranking fusion.\n       */\n      text_weight: number;\n    }\n  }\n}\n\n/**\n * A tool that allows the model to execute shell commands.\n */\nexport interface FunctionShellTool {\n  /**\n   * The type of the shell tool. Always `shell`.\n   */\n  type: 'shell';\n\n  environment?: ContainerAuto | LocalEnvironment | ContainerReference | null;\n}\n\n/**\n * Defines a function in your own code the model can choose to call. Learn more\n * about\n * [function calling](https://platform.openai.com/docs/guides/function-calling).\n */\nexport interface FunctionTool {\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n\n  /**\n   * A JSON schema object describing the parameters of the function.\n   */\n  parameters: { [key: string]: unknown } | null;\n\n  /**\n   * Whether to enforce strict parameter validation. Default `true`.\n   */\n  strict: boolean | null;\n\n  /**\n   * The type of the function tool. Always `function`.\n   */\n  type: 'function';\n\n  /**\n   * A description of the function. Used by the model to determine whether or not to\n   * call the function.\n   */\n  description?: string | null;\n}\n\nexport interface InlineSkill {\n  /**\n   * The description of the skill.\n   */\n  description: string;\n\n  /**\n   * The name of the skill.\n   */\n  name: string;\n\n  /**\n   * Inline skill payload\n   */\n  source: InlineSkillSource;\n\n  /**\n   * Defines an inline skill for this request.\n   */\n  type: 'inline';\n}\n\n/**\n * Inline skill payload\n */\nexport interface InlineSkillSource {\n  /**\n   * Base64-encoded skill zip bundle.\n   */\n  data: string;\n\n  /**\n   * The media type of the inline skill payload. Must be `application/zip`.\n   */\n  media_type: 'application/zip';\n\n  /**\n   * The type of the inline skill source. Must be `base64`.\n   */\n  type: 'base64';\n}\n\nexport interface LocalEnvironment {\n  /**\n   * Use a local computer environment.\n   */\n  type: 'local';\n\n  /**\n   * An optional list of skills.\n   */\n  skills?: Array<LocalSkill>;\n}\n\nexport interface LocalSkill {\n  /**\n   * The description of the skill.\n   */\n  description: string;\n\n  /**\n   * The name of the skill.\n   */\n  name: string;\n\n  /**\n   * The path to the directory containing the skill.\n   */\n  path: string;\n}\n\nexport interface Response {\n  /**\n   * Unique identifier for this Response.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (in seconds) of when this Response was created.\n   */\n  created_at: number;\n\n  output_text: string;\n\n  /**\n   * An error object returned when the model fails to generate a Response.\n   */\n  error: ResponseError | null;\n\n  /**\n   * Details about why the response is incomplete.\n   */\n  incomplete_details: Response.IncompleteDetails | null;\n\n  /**\n   * A system (or developer) message inserted into the model's context.\n   *\n   * When using along with `previous_response_id`, the instructions from a previous\n   * response will not be carried over to the next response. This makes it simple to\n   * swap out system (or developer) messages in new responses.\n   */\n  instructions: string | Array<ResponseInputItem> | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a\n   * wide range of models with different capabilities, performance characteristics,\n   * and price points. Refer to the\n   * [model guide](https://platform.openai.com/docs/models) to browse and compare\n   * available models.\n   */\n  model: Shared.ResponsesModel;\n\n  /**\n   * The object type of this resource - always set to `response`.\n   */\n  object: 'response';\n\n  /**\n   * An array of content items generated by the model.\n   *\n   * - The length and order of items in the `output` array is dependent on the\n   *   model's response.\n   * - Rather than accessing the first item in the `output` array and assuming it's\n   *   an `assistant` message with the content generated by the model, you might\n   *   consider using the `output_text` property where supported in SDKs.\n   */\n  output: Array<ResponseOutputItem>;\n\n  /**\n   * Whether to allow the model to run tool calls in parallel.\n   */\n  parallel_tool_calls: boolean;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic. We generally recommend altering this or `top_p` but\n   * not both.\n   */\n  temperature: number | null;\n\n  /**\n   * How the model should select which tool (or tools) to use when generating a\n   * response. See the `tools` parameter to see how to specify which tools the model\n   * can call.\n   */\n  tool_choice:\n    | ToolChoiceOptions\n    | ToolChoiceAllowed\n    | ToolChoiceTypes\n    | ToolChoiceFunction\n    | ToolChoiceMcp\n    | ToolChoiceCustom\n    | ToolChoiceApplyPatch\n    | ToolChoiceShell;\n\n  /**\n   * An array of tools the model may call while generating a response. You can\n   * specify which tool to use by setting the `tool_choice` parameter.\n   *\n   * We support the following categories of tools:\n   *\n   * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n   *   capabilities, like\n   *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n   *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n   *   Learn more about\n   *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n   * - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n   *   predefined connectors such as Google Drive and SharePoint. Learn more about\n   *   [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n   * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n   *   the model to call your own code with strongly typed arguments and outputs.\n   *   Learn more about\n   *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n   *   You can also use custom tools to call your own code.\n   */\n  tools: Array<Tool>;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p: number | null;\n\n  /**\n   * Whether to run the model response in the background.\n   * [Learn more](https://platform.openai.com/docs/guides/background).\n   */\n  background?: boolean | null;\n\n  /**\n   * Unix timestamp (in seconds) of when this Response was completed. Only present\n   * when the status is `completed`.\n   */\n  completed_at?: number | null;\n\n  /**\n   * The conversation that this response belonged to. Input items and output items\n   * from this response were automatically added to this conversation.\n   */\n  conversation?: Response.Conversation | null;\n\n  /**\n   * An upper bound for the number of tokens that can be generated for a response,\n   * including visible output tokens and\n   * [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\n   */\n  max_output_tokens?: number | null;\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create\n   * multi-turn conversations. Learn more about\n   * [conversation state](https://platform.openai.com/docs/guides/conversation-state).\n   * Cannot be used in conjunction with `conversation`.\n   */\n  previous_response_id?: string | null;\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsePrompt | null;\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates. Replaces the `user` field.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  prompt_cache_key?: string;\n\n  /**\n   * The retention policy for the prompt cache. Set to `24h` to enable extended\n   * prompt caching, which keeps cached prefixes active for longer, up to a maximum\n   * of 24 hours.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching#prompt-cache-retention).\n   */\n  prompt_cache_retention?: 'in-memory' | '24h' | null;\n\n  /**\n   * **gpt-5 and o-series models only**\n   *\n   * Configuration options for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n   */\n  reasoning?: Shared.Reasoning | null;\n\n  /**\n   * A stable identifier used to help detect users of your application that may be\n   * violating OpenAI's usage policies. The IDs should be a string that uniquely\n   * identifies each user, with a maximum length of 64 characters. We recommend\n   * hashing their username or email address, in order to avoid sending us any\n   * identifying information.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  safety_identifier?: string;\n\n  /**\n   * Specifies the latency tier to use for processing the request. This parameter is\n   * relevant for customers subscribed to the scale tier service:\n   *\n   * - If set to 'auto', then the request will be processed with the service tier\n   *   configured in the Project settings. Unless otherwise configured, the Project\n   *   will use 'default'.\n   * - If set to 'default', then the request will be processed with the standard\n   *   pricing and performance for the selected model.\n   * - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\n   *   '[priority](https://openai.com/api-priority-processing/)', then the request\n   *   will be processed with the corresponding service tier.\n   * - When not set, the default behavior is 'auto'.\n   *\n   * When this parameter is set, the response body will include the `service_tier`\n   * utilized.\n   */\n  service_tier?: 'auto' | 'default' | 'flex' | 'scale' | 'priority' | null;\n\n  /**\n   * The status of the response generation. One of `completed`, `failed`,\n   * `in_progress`, `cancelled`, `queued`, or `incomplete`.\n   */\n  status?: ResponseStatus;\n\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data. Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n   */\n  text?: ResponseTextConfig;\n\n  /**\n   * The truncation strategy to use for the model response.\n   *\n   * - `auto`: If the input to this Response exceeds the model's context window size,\n   *   the model will truncate the response to fit the context window by dropping\n   *   items from the beginning of the conversation.\n   * - `disabled` (default): If the input size will exceed the context window size\n   *   for a model, the request will fail with a 400 error.\n   */\n  truncation?: 'auto' | 'disabled' | null;\n\n  /**\n   * Represents token usage details including input tokens, output tokens, a\n   * breakdown of output tokens, and the total tokens used.\n   */\n  usage?: ResponseUsage;\n\n  /**\n   * @deprecated This field is being replaced by `safety_identifier` and\n   * `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching\n   * optimizations. A stable identifier for your end-users. Used to boost cache hit\n   * rates by better bucketing similar requests and to help OpenAI detect and prevent\n   * abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  user?: string;\n}\n\nexport namespace Response {\n  /**\n   * Details about why the response is incomplete.\n   */\n  export interface IncompleteDetails {\n    /**\n     * The reason why the response is incomplete.\n     */\n    reason?: 'max_output_tokens' | 'content_filter';\n  }\n\n  /**\n   * The conversation that this response belonged to. Input items and output items\n   * from this response were automatically added to this conversation.\n   */\n  export interface Conversation {\n    /**\n     * The unique ID of the conversation that this response was associated with.\n     */\n    id: string;\n  }\n}\n\n/**\n * A tool call that applies file diffs by creating, deleting, or updating files.\n */\nexport interface ResponseApplyPatchToolCall {\n  /**\n   * The unique ID of the apply patch tool call. Populated when this item is returned\n   * via API.\n   */\n  id: string;\n\n  /**\n   * The unique ID of the apply patch tool call generated by the model.\n   */\n  call_id: string;\n\n  /**\n   * One of the create_file, delete_file, or update_file operations applied via\n   * apply_patch.\n   */\n  operation:\n    | ResponseApplyPatchToolCall.CreateFile\n    | ResponseApplyPatchToolCall.DeleteFile\n    | ResponseApplyPatchToolCall.UpdateFile;\n\n  /**\n   * The status of the apply patch tool call. One of `in_progress` or `completed`.\n   */\n  status: 'in_progress' | 'completed';\n\n  /**\n   * The type of the item. Always `apply_patch_call`.\n   */\n  type: 'apply_patch_call';\n\n  /**\n   * The ID of the entity that created this tool call.\n   */\n  created_by?: string;\n}\n\nexport namespace ResponseApplyPatchToolCall {\n  /**\n   * Instruction describing how to create a file via the apply_patch tool.\n   */\n  export interface CreateFile {\n    /**\n     * Diff to apply.\n     */\n    diff: string;\n\n    /**\n     * Path of the file to create.\n     */\n    path: string;\n\n    /**\n     * Create a new file with the provided diff.\n     */\n    type: 'create_file';\n  }\n\n  /**\n   * Instruction describing how to delete a file via the apply_patch tool.\n   */\n  export interface DeleteFile {\n    /**\n     * Path of the file to delete.\n     */\n    path: string;\n\n    /**\n     * Delete the specified file.\n     */\n    type: 'delete_file';\n  }\n\n  /**\n   * Instruction describing how to update a file via the apply_patch tool.\n   */\n  export interface UpdateFile {\n    /**\n     * Diff to apply.\n     */\n    diff: string;\n\n    /**\n     * Path of the file to update.\n     */\n    path: string;\n\n    /**\n     * Update an existing file with the provided diff.\n     */\n    type: 'update_file';\n  }\n}\n\n/**\n * The output emitted by an apply patch tool call.\n */\nexport interface ResponseApplyPatchToolCallOutput {\n  /**\n   * The unique ID of the apply patch tool call output. Populated when this item is\n   * returned via API.\n   */\n  id: string;\n\n  /**\n   * The unique ID of the apply patch tool call generated by the model.\n   */\n  call_id: string;\n\n  /**\n   * The status of the apply patch tool call output. One of `completed` or `failed`.\n   */\n  status: 'completed' | 'failed';\n\n  /**\n   * The type of the item. Always `apply_patch_call_output`.\n   */\n  type: 'apply_patch_call_output';\n\n  /**\n   * The ID of the entity that created this tool call output.\n   */\n  created_by?: string;\n\n  /**\n   * Optional textual output returned by the apply patch tool.\n   */\n  output?: string | null;\n}\n\n/**\n * Emitted when there is a partial audio response.\n */\nexport interface ResponseAudioDeltaEvent {\n  /**\n   * A chunk of Base64 encoded response audio bytes.\n   */\n  delta: string;\n\n  /**\n   * A sequence number for this chunk of the stream response.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.audio.delta`.\n   */\n  type: 'response.audio.delta';\n}\n\n/**\n * Emitted when the audio response is complete.\n */\nexport interface ResponseAudioDoneEvent {\n  /**\n   * The sequence number of the delta.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.audio.done`.\n   */\n  type: 'response.audio.done';\n}\n\n/**\n * Emitted when there is a partial transcript of audio.\n */\nexport interface ResponseAudioTranscriptDeltaEvent {\n  /**\n   * The partial transcript of the audio response.\n   */\n  delta: string;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.audio.transcript.delta`.\n   */\n  type: 'response.audio.transcript.delta';\n}\n\n/**\n * Emitted when the full audio transcript is completed.\n */\nexport interface ResponseAudioTranscriptDoneEvent {\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.audio.transcript.done`.\n   */\n  type: 'response.audio.transcript.done';\n}\n\n/**\n * Emitted when a partial code snippet is streamed by the code interpreter.\n */\nexport interface ResponseCodeInterpreterCallCodeDeltaEvent {\n  /**\n   * The partial code snippet being streamed by the code interpreter.\n   */\n  delta: string;\n\n  /**\n   * The unique identifier of the code interpreter tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response for which the code is being\n   * streamed.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event, used to order streaming events.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.code_interpreter_call_code.delta`.\n   */\n  type: 'response.code_interpreter_call_code.delta';\n}\n\n/**\n * Emitted when the code snippet is finalized by the code interpreter.\n */\nexport interface ResponseCodeInterpreterCallCodeDoneEvent {\n  /**\n   * The final code snippet output by the code interpreter.\n   */\n  code: string;\n\n  /**\n   * The unique identifier of the code interpreter tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response for which the code is finalized.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event, used to order streaming events.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.code_interpreter_call_code.done`.\n   */\n  type: 'response.code_interpreter_call_code.done';\n}\n\n/**\n * Emitted when the code interpreter call is completed.\n */\nexport interface ResponseCodeInterpreterCallCompletedEvent {\n  /**\n   * The unique identifier of the code interpreter tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response for which the code interpreter call\n   * is completed.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event, used to order streaming events.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.code_interpreter_call.completed`.\n   */\n  type: 'response.code_interpreter_call.completed';\n}\n\n/**\n * Emitted when a code interpreter call is in progress.\n */\nexport interface ResponseCodeInterpreterCallInProgressEvent {\n  /**\n   * The unique identifier of the code interpreter tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response for which the code interpreter call\n   * is in progress.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event, used to order streaming events.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.code_interpreter_call.in_progress`.\n   */\n  type: 'response.code_interpreter_call.in_progress';\n}\n\n/**\n * Emitted when the code interpreter is actively interpreting the code snippet.\n */\nexport interface ResponseCodeInterpreterCallInterpretingEvent {\n  /**\n   * The unique identifier of the code interpreter tool call item.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response for which the code interpreter is\n   * interpreting code.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event, used to order streaming events.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.code_interpreter_call.interpreting`.\n   */\n  type: 'response.code_interpreter_call.interpreting';\n}\n\n/**\n * A tool call to run code.\n */\nexport interface ResponseCodeInterpreterToolCall {\n  /**\n   * The unique ID of the code interpreter tool call.\n   */\n  id: string;\n\n  /**\n   * The code to run, or null if not available.\n   */\n  code: string | null;\n\n  /**\n   * The ID of the container used to run the code.\n   */\n  container_id: string;\n\n  /**\n   * The outputs generated by the code interpreter, such as logs or images. Can be\n   * null if no outputs are available.\n   */\n  outputs: Array<ResponseCodeInterpreterToolCall.Logs | ResponseCodeInterpreterToolCall.Image> | null;\n\n  /**\n   * The status of the code interpreter tool call. Valid values are `in_progress`,\n   * `completed`, `incomplete`, `interpreting`, and `failed`.\n   */\n  status: 'in_progress' | 'completed' | 'incomplete' | 'interpreting' | 'failed';\n\n  /**\n   * The type of the code interpreter tool call. Always `code_interpreter_call`.\n   */\n  type: 'code_interpreter_call';\n}\n\nexport namespace ResponseCodeInterpreterToolCall {\n  /**\n   * The logs output from the code interpreter.\n   */\n  export interface Logs {\n    /**\n     * The logs output from the code interpreter.\n     */\n    logs: string;\n\n    /**\n     * The type of the output. Always `logs`.\n     */\n    type: 'logs';\n  }\n\n  /**\n   * The image output from the code interpreter.\n   */\n  export interface Image {\n    /**\n     * The type of the output. Always `image`.\n     */\n    type: 'image';\n\n    /**\n     * The URL of the image output from the code interpreter.\n     */\n    url: string;\n  }\n}\n\n/**\n * A compaction item generated by the\n * [`v1/responses/compact` API](https://platform.openai.com/docs/api-reference/responses/compact).\n */\nexport interface ResponseCompactionItem {\n  /**\n   * The unique ID of the compaction item.\n   */\n  id: string;\n\n  /**\n   * The encrypted content that was produced by compaction.\n   */\n  encrypted_content: string;\n\n  /**\n   * The type of the item. Always `compaction`.\n   */\n  type: 'compaction';\n\n  /**\n   * The identifier of the actor that created the item.\n   */\n  created_by?: string;\n}\n\n/**\n * A compaction item generated by the\n * [`v1/responses/compact` API](https://platform.openai.com/docs/api-reference/responses/compact).\n */\nexport interface ResponseCompactionItemParam {\n  /**\n   * The encrypted content of the compaction summary.\n   */\n  encrypted_content: string;\n\n  /**\n   * The type of the item. Always `compaction`.\n   */\n  type: 'compaction';\n\n  /**\n   * The ID of the compaction item.\n   */\n  id?: string | null;\n}\n\n/**\n * Emitted when the model response is complete.\n */\nexport interface ResponseCompletedEvent {\n  /**\n   * Properties of the completed response.\n   */\n  response: Response;\n\n  /**\n   * The sequence number for this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.completed`.\n   */\n  type: 'response.completed';\n}\n\n/**\n * A tool call to a computer use tool. See the\n * [computer use guide](https://platform.openai.com/docs/guides/tools-computer-use)\n * for more information.\n */\nexport interface ResponseComputerToolCall {\n  /**\n   * The unique ID of the computer call.\n   */\n  id: string;\n\n  /**\n   * A click action.\n   */\n  action:\n    | ResponseComputerToolCall.Click\n    | ResponseComputerToolCall.DoubleClick\n    | ResponseComputerToolCall.Drag\n    | ResponseComputerToolCall.Keypress\n    | ResponseComputerToolCall.Move\n    | ResponseComputerToolCall.Screenshot\n    | ResponseComputerToolCall.Scroll\n    | ResponseComputerToolCall.Type\n    | ResponseComputerToolCall.Wait;\n\n  /**\n   * An identifier used when responding to the tool call with output.\n   */\n  call_id: string;\n\n  /**\n   * The pending safety checks for the computer call.\n   */\n  pending_safety_checks: Array<ResponseComputerToolCall.PendingSafetyCheck>;\n\n  /**\n   * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n   * Populated when items are returned via API.\n   */\n  status: 'in_progress' | 'completed' | 'incomplete';\n\n  /**\n   * The type of the computer call. Always `computer_call`.\n   */\n  type: 'computer_call';\n}\n\nexport namespace ResponseComputerToolCall {\n  /**\n   * A click action.\n   */\n  export interface Click {\n    /**\n     * Indicates which mouse button was pressed during the click. One of `left`,\n     * `right`, `wheel`, `back`, or `forward`.\n     */\n    button: 'left' | 'right' | 'wheel' | 'back' | 'forward';\n\n    /**\n     * Specifies the event type. For a click action, this property is always `click`.\n     */\n    type: 'click';\n\n    /**\n     * The x-coordinate where the click occurred.\n     */\n    x: number;\n\n    /**\n     * The y-coordinate where the click occurred.\n     */\n    y: number;\n  }\n\n  /**\n   * A double click action.\n   */\n  export interface DoubleClick {\n    /**\n     * Specifies the event type. For a double click action, this property is always set\n     * to `double_click`.\n     */\n    type: 'double_click';\n\n    /**\n     * The x-coordinate where the double click occurred.\n     */\n    x: number;\n\n    /**\n     * The y-coordinate where the double click occurred.\n     */\n    y: number;\n  }\n\n  /**\n   * A drag action.\n   */\n  export interface Drag {\n    /**\n     * An array of coordinates representing the path of the drag action. Coordinates\n     * will appear as an array of objects, eg\n     *\n     * ```\n     * [\n     *   { x: 100, y: 200 },\n     *   { x: 200, y: 300 }\n     * ]\n     * ```\n     */\n    path: Array<Drag.Path>;\n\n    /**\n     * Specifies the event type. For a drag action, this property is always set to\n     * `drag`.\n     */\n    type: 'drag';\n  }\n\n  export namespace Drag {\n    /**\n     * An x/y coordinate pair, e.g. `{ x: 100, y: 200 }`.\n     */\n    export interface Path {\n      /**\n       * The x-coordinate.\n       */\n      x: number;\n\n      /**\n       * The y-coordinate.\n       */\n      y: number;\n    }\n  }\n\n  /**\n   * A collection of keypresses the model would like to perform.\n   */\n  export interface Keypress {\n    /**\n     * The combination of keys the model is requesting to be pressed. This is an array\n     * of strings, each representing a key.\n     */\n    keys: Array<string>;\n\n    /**\n     * Specifies the event type. For a keypress action, this property is always set to\n     * `keypress`.\n     */\n    type: 'keypress';\n  }\n\n  /**\n   * A mouse move action.\n   */\n  export interface Move {\n    /**\n     * Specifies the event type. For a move action, this property is always set to\n     * `move`.\n     */\n    type: 'move';\n\n    /**\n     * The x-coordinate to move to.\n     */\n    x: number;\n\n    /**\n     * The y-coordinate to move to.\n     */\n    y: number;\n  }\n\n  /**\n   * A screenshot action.\n   */\n  export interface Screenshot {\n    /**\n     * Specifies the event type. For a screenshot action, this property is always set\n     * to `screenshot`.\n     */\n    type: 'screenshot';\n  }\n\n  /**\n   * A scroll action.\n   */\n  export interface Scroll {\n    /**\n     * The horizontal scroll distance.\n     */\n    scroll_x: number;\n\n    /**\n     * The vertical scroll distance.\n     */\n    scroll_y: number;\n\n    /**\n     * Specifies the event type. For a scroll action, this property is always set to\n     * `scroll`.\n     */\n    type: 'scroll';\n\n    /**\n     * The x-coordinate where the scroll occurred.\n     */\n    x: number;\n\n    /**\n     * The y-coordinate where the scroll occurred.\n     */\n    y: number;\n  }\n\n  /**\n   * An action to type in text.\n   */\n  export interface Type {\n    /**\n     * The text to type.\n     */\n    text: string;\n\n    /**\n     * Specifies the event type. For a type action, this property is always set to\n     * `type`.\n     */\n    type: 'type';\n  }\n\n  /**\n   * A wait action.\n   */\n  export interface Wait {\n    /**\n     * Specifies the event type. For a wait action, this property is always set to\n     * `wait`.\n     */\n    type: 'wait';\n  }\n\n  /**\n   * A pending safety check for the computer call.\n   */\n  export interface PendingSafetyCheck {\n    /**\n     * The ID of the pending safety check.\n     */\n    id: string;\n\n    /**\n     * The type of the pending safety check.\n     */\n    code?: string | null;\n\n    /**\n     * Details about the pending safety check.\n     */\n    message?: string | null;\n  }\n}\n\nexport interface ResponseComputerToolCallOutputItem {\n  /**\n   * The unique ID of the computer call tool output.\n   */\n  id: string;\n\n  /**\n   * The ID of the computer tool call that produced the output.\n   */\n  call_id: string;\n\n  /**\n   * A computer screenshot image used with the computer use tool.\n   */\n  output: ResponseComputerToolCallOutputScreenshot;\n\n  /**\n   * The type of the computer tool call output. Always `computer_call_output`.\n   */\n  type: 'computer_call_output';\n\n  /**\n   * The safety checks reported by the API that have been acknowledged by the\n   * developer.\n   */\n  acknowledged_safety_checks?: Array<ResponseComputerToolCallOutputItem.AcknowledgedSafetyCheck>;\n\n  /**\n   * The status of the message input. One of `in_progress`, `completed`, or\n   * `incomplete`. Populated when input items are returned via API.\n   */\n  status?: 'in_progress' | 'completed' | 'incomplete';\n}\n\nexport namespace ResponseComputerToolCallOutputItem {\n  /**\n   * A pending safety check for the computer call.\n   */\n  export interface AcknowledgedSafetyCheck {\n    /**\n     * The ID of the pending safety check.\n     */\n    id: string;\n\n    /**\n     * The type of the pending safety check.\n     */\n    code?: string | null;\n\n    /**\n     * Details about the pending safety check.\n     */\n    message?: string | null;\n  }\n}\n\n/**\n * A computer screenshot image used with the computer use tool.\n */\nexport interface ResponseComputerToolCallOutputScreenshot {\n  /**\n   * Specifies the event type. For a computer screenshot, this property is always set\n   * to `computer_screenshot`.\n   */\n  type: 'computer_screenshot';\n\n  /**\n   * The identifier of an uploaded file that contains the screenshot.\n   */\n  file_id?: string;\n\n  /**\n   * The URL of the screenshot image.\n   */\n  image_url?: string;\n}\n\n/**\n * Represents a container created with /v1/containers.\n */\nexport interface ResponseContainerReference {\n  container_id: string;\n\n  /**\n   * The environment type. Always `container_reference`.\n   */\n  type: 'container_reference';\n}\n\n/**\n * Multi-modal input and output contents.\n */\nexport type ResponseContent =\n  | ResponseInputText\n  | ResponseInputImage\n  | ResponseInputFile\n  | ResponseOutputText\n  | ResponseOutputRefusal\n  | ResponseContent.ReasoningTextContent;\n\nexport namespace ResponseContent {\n  /**\n   * Reasoning text from the model.\n   */\n  export interface ReasoningTextContent {\n    /**\n     * The reasoning text from the model.\n     */\n    text: string;\n\n    /**\n     * The type of the reasoning text. Always `reasoning_text`.\n     */\n    type: 'reasoning_text';\n  }\n}\n\n/**\n * Emitted when a new content part is added.\n */\nexport interface ResponseContentPartAddedEvent {\n  /**\n   * The index of the content part that was added.\n   */\n  content_index: number;\n\n  /**\n   * The ID of the output item that the content part was added to.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the content part was added to.\n   */\n  output_index: number;\n\n  /**\n   * The content part that was added.\n   */\n  part: ResponseOutputText | ResponseOutputRefusal | ResponseContentPartAddedEvent.ReasoningText;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.content_part.added`.\n   */\n  type: 'response.content_part.added';\n}\n\nexport namespace ResponseContentPartAddedEvent {\n  /**\n   * Reasoning text from the model.\n   */\n  export interface ReasoningText {\n    /**\n     * The reasoning text from the model.\n     */\n    text: string;\n\n    /**\n     * The type of the reasoning text. Always `reasoning_text`.\n     */\n    type: 'reasoning_text';\n  }\n}\n\n/**\n * Emitted when a content part is done.\n */\nexport interface ResponseContentPartDoneEvent {\n  /**\n   * The index of the content part that is done.\n   */\n  content_index: number;\n\n  /**\n   * The ID of the output item that the content part was added to.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the content part was added to.\n   */\n  output_index: number;\n\n  /**\n   * The content part that is done.\n   */\n  part: ResponseOutputText | ResponseOutputRefusal | ResponseContentPartDoneEvent.ReasoningText;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.content_part.done`.\n   */\n  type: 'response.content_part.done';\n}\n\nexport namespace ResponseContentPartDoneEvent {\n  /**\n   * Reasoning text from the model.\n   */\n  export interface ReasoningText {\n    /**\n     * The reasoning text from the model.\n     */\n    text: string;\n\n    /**\n     * The type of the reasoning text. Always `reasoning_text`.\n     */\n    type: 'reasoning_text';\n  }\n}\n\n/**\n * The conversation that this response belongs to.\n */\nexport interface ResponseConversationParam {\n  /**\n   * The unique ID of the conversation.\n   */\n  id: string;\n}\n\n/**\n * An event that is emitted when a response is created.\n */\nexport interface ResponseCreatedEvent {\n  /**\n   * The response that was created.\n   */\n  response: Response;\n\n  /**\n   * The sequence number for this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.created`.\n   */\n  type: 'response.created';\n}\n\n/**\n * A call to a custom tool created by the model.\n */\nexport interface ResponseCustomToolCall {\n  /**\n   * An identifier used to map this custom tool call to a tool call output.\n   */\n  call_id: string;\n\n  /**\n   * The input for the custom tool call generated by the model.\n   */\n  input: string;\n\n  /**\n   * The name of the custom tool being called.\n   */\n  name: string;\n\n  /**\n   * The type of the custom tool call. Always `custom_tool_call`.\n   */\n  type: 'custom_tool_call';\n\n  /**\n   * The unique ID of the custom tool call in the OpenAI platform.\n   */\n  id?: string;\n}\n\n/**\n * Event representing a delta (partial update) to the input of a custom tool call.\n */\nexport interface ResponseCustomToolCallInputDeltaEvent {\n  /**\n   * The incremental input data (delta) for the custom tool call.\n   */\n  delta: string;\n\n  /**\n   * Unique identifier for the API item associated with this event.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output this delta applies to.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The event type identifier.\n   */\n  type: 'response.custom_tool_call_input.delta';\n}\n\n/**\n * Event indicating that input for a custom tool call is complete.\n */\nexport interface ResponseCustomToolCallInputDoneEvent {\n  /**\n   * The complete input data for the custom tool call.\n   */\n  input: string;\n\n  /**\n   * Unique identifier for the API item associated with this event.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output this event applies to.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The event type identifier.\n   */\n  type: 'response.custom_tool_call_input.done';\n}\n\n/**\n * The output of a custom tool call from your code, being sent back to the model.\n */\nexport interface ResponseCustomToolCallOutput {\n  /**\n   * The call ID, used to map this custom tool call output to a custom tool call.\n   */\n  call_id: string;\n\n  /**\n   * The output from the custom tool call generated by your code. Can be a string or\n   * an list of output content.\n   */\n  output: string | Array<ResponseInputText | ResponseInputImage | ResponseInputFile>;\n\n  /**\n   * The type of the custom tool call output. Always `custom_tool_call_output`.\n   */\n  type: 'custom_tool_call_output';\n\n  /**\n   * The unique ID of the custom tool call output in the OpenAI platform.\n   */\n  id?: string;\n}\n\n/**\n * An error object returned when the model fails to generate a Response.\n */\nexport interface ResponseError {\n  /**\n   * The error code for the response.\n   */\n  code:\n    | 'server_error'\n    | 'rate_limit_exceeded'\n    | 'invalid_prompt'\n    | 'vector_store_timeout'\n    | 'invalid_image'\n    | 'invalid_image_format'\n    | 'invalid_base64_image'\n    | 'invalid_image_url'\n    | 'image_too_large'\n    | 'image_too_small'\n    | 'image_parse_error'\n    | 'image_content_policy_violation'\n    | 'invalid_image_mode'\n    | 'image_file_too_large'\n    | 'unsupported_image_media_type'\n    | 'empty_image_file'\n    | 'failed_to_download_image'\n    | 'image_file_not_found';\n\n  /**\n   * A human-readable description of the error.\n   */\n  message: string;\n}\n\n/**\n * Emitted when an error occurs.\n */\nexport interface ResponseErrorEvent {\n  /**\n   * The error code.\n   */\n  code: string | null;\n\n  /**\n   * The error message.\n   */\n  message: string;\n\n  /**\n   * The error parameter.\n   */\n  param: string | null;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `error`.\n   */\n  type: 'error';\n}\n\n/**\n * An event that is emitted when a response fails.\n */\nexport interface ResponseFailedEvent {\n  /**\n   * The response that failed.\n   */\n  response: Response;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.failed`.\n   */\n  type: 'response.failed';\n}\n\n/**\n * Emitted when a file search call is completed (results found).\n */\nexport interface ResponseFileSearchCallCompletedEvent {\n  /**\n   * The ID of the output item that the file search call is initiated.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the file search call is initiated.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.file_search_call.completed`.\n   */\n  type: 'response.file_search_call.completed';\n}\n\n/**\n * Emitted when a file search call is initiated.\n */\nexport interface ResponseFileSearchCallInProgressEvent {\n  /**\n   * The ID of the output item that the file search call is initiated.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the file search call is initiated.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.file_search_call.in_progress`.\n   */\n  type: 'response.file_search_call.in_progress';\n}\n\n/**\n * Emitted when a file search is currently searching.\n */\nexport interface ResponseFileSearchCallSearchingEvent {\n  /**\n   * The ID of the output item that the file search call is initiated.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the file search call is searching.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.file_search_call.searching`.\n   */\n  type: 'response.file_search_call.searching';\n}\n\n/**\n * The results of a file search tool call. See the\n * [file search guide](https://platform.openai.com/docs/guides/tools-file-search)\n * for more information.\n */\nexport interface ResponseFileSearchToolCall {\n  /**\n   * The unique ID of the file search tool call.\n   */\n  id: string;\n\n  /**\n   * The queries used to search for files.\n   */\n  queries: Array<string>;\n\n  /**\n   * The status of the file search tool call. One of `in_progress`, `searching`,\n   * `incomplete` or `failed`,\n   */\n  status: 'in_progress' | 'searching' | 'completed' | 'incomplete' | 'failed';\n\n  /**\n   * The type of the file search tool call. Always `file_search_call`.\n   */\n  type: 'file_search_call';\n\n  /**\n   * The results of the file search tool call.\n   */\n  results?: Array<ResponseFileSearchToolCall.Result> | null;\n}\n\nexport namespace ResponseFileSearchToolCall {\n  export interface Result {\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard. Keys are strings with a maximum\n     * length of 64 characters. Values are strings with a maximum length of 512\n     * characters, booleans, or numbers.\n     */\n    attributes?: { [key: string]: string | number | boolean } | null;\n\n    /**\n     * The unique ID of the file.\n     */\n    file_id?: string;\n\n    /**\n     * The name of the file.\n     */\n    filename?: string;\n\n    /**\n     * The relevance score of the file - a value between 0 and 1.\n     */\n    score?: number;\n\n    /**\n     * The text that was retrieved from the file.\n     */\n    text?: string;\n  }\n}\n\n/**\n * An object specifying the format that the model must output.\n *\n * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n * ensures the model will match your supplied JSON schema. Learn more in the\n * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n *\n * The default format is `{ \"type\": \"text\" }` with no additional options.\n *\n * **Not recommended for gpt-4o and newer models:**\n *\n * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n * ensures the message the model generates is valid JSON. Using `json_schema` is\n * preferred for models that support it.\n */\nexport type ResponseFormatTextConfig =\n  | Shared.ResponseFormatText\n  | ResponseFormatTextJSONSchemaConfig\n  | Shared.ResponseFormatJSONObject;\n\n/**\n * JSON Schema response format. Used to generate structured JSON responses. Learn\n * more about\n * [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs).\n */\nexport interface ResponseFormatTextJSONSchemaConfig {\n  /**\n   * The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores\n   * and dashes, with a maximum length of 64.\n   */\n  name: string;\n\n  /**\n   * The schema for the response format, described as a JSON Schema object. Learn how\n   * to build JSON schemas [here](https://json-schema.org/).\n   */\n  schema: { [key: string]: unknown };\n\n  /**\n   * The type of response format being defined. Always `json_schema`.\n   */\n  type: 'json_schema';\n\n  /**\n   * A description of what the response format is for, used by the model to determine\n   * how to respond in the format.\n   */\n  description?: string;\n\n  /**\n   * Whether to enable strict schema adherence when generating the output. If set to\n   * true, the model will always follow the exact schema defined in the `schema`\n   * field. Only a subset of JSON Schema is supported when `strict` is `true`. To\n   * learn more, read the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   */\n  strict?: boolean | null;\n}\n\n/**\n * Emitted when there is a partial function-call arguments delta.\n */\nexport interface ResponseFunctionCallArgumentsDeltaEvent {\n  /**\n   * The function-call arguments delta that is added.\n   */\n  delta: string;\n\n  /**\n   * The ID of the output item that the function-call arguments delta is added to.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the function-call arguments delta is added to.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.function_call_arguments.delta`.\n   */\n  type: 'response.function_call_arguments.delta';\n}\n\n/**\n * Emitted when function-call arguments are finalized.\n */\nexport interface ResponseFunctionCallArgumentsDoneEvent {\n  /**\n   * The function-call arguments.\n   */\n  arguments: string;\n\n  /**\n   * The ID of the item.\n   */\n  item_id: string;\n\n  /**\n   * The name of the function that was called.\n   */\n  name: string;\n\n  /**\n   * The index of the output item.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  type: 'response.function_call_arguments.done';\n}\n\n/**\n * A piece of message content, such as text, an image, or a file.\n */\nexport type ResponseFunctionCallOutputItem =\n  | ResponseInputTextContent\n  | ResponseInputImageContent\n  | ResponseInputFileContent;\n\n/**\n * An array of content outputs (text, image, file) for the function tool call.\n */\nexport type ResponseFunctionCallOutputItemList = Array<ResponseFunctionCallOutputItem>;\n\n/**\n * Captured stdout and stderr for a portion of a shell tool call output.\n */\nexport interface ResponseFunctionShellCallOutputContent {\n  /**\n   * The exit or timeout outcome associated with this shell call.\n   */\n  outcome: ResponseFunctionShellCallOutputContent.Timeout | ResponseFunctionShellCallOutputContent.Exit;\n\n  /**\n   * Captured stderr output for the shell call.\n   */\n  stderr: string;\n\n  /**\n   * Captured stdout output for the shell call.\n   */\n  stdout: string;\n}\n\nexport namespace ResponseFunctionShellCallOutputContent {\n  /**\n   * Indicates that the shell call exceeded its configured time limit.\n   */\n  export interface Timeout {\n    /**\n     * The outcome type. Always `timeout`.\n     */\n    type: 'timeout';\n  }\n\n  /**\n   * Indicates that the shell commands finished and returned an exit code.\n   */\n  export interface Exit {\n    /**\n     * The exit code returned by the shell process.\n     */\n    exit_code: number;\n\n    /**\n     * The outcome type. Always `exit`.\n     */\n    type: 'exit';\n  }\n}\n\n/**\n * A tool call that executes one or more shell commands in a managed environment.\n */\nexport interface ResponseFunctionShellToolCall {\n  /**\n   * The unique ID of the shell tool call. Populated when this item is returned via\n   * API.\n   */\n  id: string;\n\n  /**\n   * The shell commands and limits that describe how to run the tool call.\n   */\n  action: ResponseFunctionShellToolCall.Action;\n\n  /**\n   * The unique ID of the shell tool call generated by the model.\n   */\n  call_id: string;\n\n  /**\n   * Represents the use of a local environment to perform shell actions.\n   */\n  environment: ResponseLocalEnvironment | ResponseContainerReference | null;\n\n  /**\n   * The status of the shell call. One of `in_progress`, `completed`, or\n   * `incomplete`.\n   */\n  status: 'in_progress' | 'completed' | 'incomplete';\n\n  /**\n   * The type of the item. Always `shell_call`.\n   */\n  type: 'shell_call';\n\n  /**\n   * The ID of the entity that created this tool call.\n   */\n  created_by?: string;\n}\n\nexport namespace ResponseFunctionShellToolCall {\n  /**\n   * The shell commands and limits that describe how to run the tool call.\n   */\n  export interface Action {\n    commands: Array<string>;\n\n    /**\n     * Optional maximum number of characters to return from each command.\n     */\n    max_output_length: number | null;\n\n    /**\n     * Optional timeout in milliseconds for the commands.\n     */\n    timeout_ms: number | null;\n  }\n}\n\n/**\n * The output of a shell tool call that was emitted.\n */\nexport interface ResponseFunctionShellToolCallOutput {\n  /**\n   * The unique ID of the shell call output. Populated when this item is returned via\n   * API.\n   */\n  id: string;\n\n  /**\n   * The unique ID of the shell tool call generated by the model.\n   */\n  call_id: string;\n\n  /**\n   * The maximum length of the shell command output. This is generated by the model\n   * and should be passed back with the raw output.\n   */\n  max_output_length: number | null;\n\n  /**\n   * An array of shell call output contents\n   */\n  output: Array<ResponseFunctionShellToolCallOutput.Output>;\n\n  /**\n   * The status of the shell call output. One of `in_progress`, `completed`, or\n   * `incomplete`.\n   */\n  status: 'in_progress' | 'completed' | 'incomplete';\n\n  /**\n   * The type of the shell call output. Always `shell_call_output`.\n   */\n  type: 'shell_call_output';\n\n  /**\n   * The identifier of the actor that created the item.\n   */\n  created_by?: string;\n}\n\nexport namespace ResponseFunctionShellToolCallOutput {\n  /**\n   * The content of a shell tool call output that was emitted.\n   */\n  export interface Output {\n    /**\n     * Represents either an exit outcome (with an exit code) or a timeout outcome for a\n     * shell call output chunk.\n     */\n    outcome: Output.Timeout | Output.Exit;\n\n    /**\n     * The standard error output that was captured.\n     */\n    stderr: string;\n\n    /**\n     * The standard output that was captured.\n     */\n    stdout: string;\n\n    /**\n     * The identifier of the actor that created the item.\n     */\n    created_by?: string;\n  }\n\n  export namespace Output {\n    /**\n     * Indicates that the shell call exceeded its configured time limit.\n     */\n    export interface Timeout {\n      /**\n       * The outcome type. Always `timeout`.\n       */\n      type: 'timeout';\n    }\n\n    /**\n     * Indicates that the shell commands finished and returned an exit code.\n     */\n    export interface Exit {\n      /**\n       * Exit code from the shell process.\n       */\n      exit_code: number;\n\n      /**\n       * The outcome type. Always `exit`.\n       */\n      type: 'exit';\n    }\n  }\n}\n\n/**\n * A tool call to run a function. See the\n * [function calling guide](https://platform.openai.com/docs/guides/function-calling)\n * for more information.\n */\nexport interface ResponseFunctionToolCall {\n  /**\n   * A JSON string of the arguments to pass to the function.\n   */\n  arguments: string;\n\n  /**\n   * The unique ID of the function tool call generated by the model.\n   */\n  call_id: string;\n\n  /**\n   * The name of the function to run.\n   */\n  name: string;\n\n  /**\n   * The type of the function tool call. Always `function_call`.\n   */\n  type: 'function_call';\n\n  /**\n   * The unique ID of the function tool call.\n   */\n  id?: string;\n\n  /**\n   * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n   * Populated when items are returned via API.\n   */\n  status?: 'in_progress' | 'completed' | 'incomplete';\n}\n\n/**\n * A tool call to run a function. See the\n * [function calling guide](https://platform.openai.com/docs/guides/function-calling)\n * for more information.\n */\nexport interface ResponseFunctionToolCallItem extends ResponseFunctionToolCall {\n  /**\n   * The unique ID of the function tool call.\n   */\n  id: string;\n}\n\nexport interface ResponseFunctionToolCallOutputItem {\n  /**\n   * The unique ID of the function call tool output.\n   */\n  id: string;\n\n  /**\n   * The unique ID of the function tool call generated by the model.\n   */\n  call_id: string;\n\n  /**\n   * The output from the function call generated by your code. Can be a string or an\n   * list of output content.\n   */\n  output: string | Array<ResponseInputText | ResponseInputImage | ResponseInputFile>;\n\n  /**\n   * The type of the function tool call output. Always `function_call_output`.\n   */\n  type: 'function_call_output';\n\n  /**\n   * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n   * Populated when items are returned via API.\n   */\n  status?: 'in_progress' | 'completed' | 'incomplete';\n}\n\n/**\n * The results of a web search tool call. See the\n * [web search guide](https://platform.openai.com/docs/guides/tools-web-search) for\n * more information.\n */\nexport interface ResponseFunctionWebSearch {\n  /**\n   * The unique ID of the web search tool call.\n   */\n  id: string;\n\n  /**\n   * An object describing the specific action taken in this web search call. Includes\n   * details on how the model used the web (search, open_page, find_in_page).\n   */\n  action:\n    | ResponseFunctionWebSearch.Search\n    | ResponseFunctionWebSearch.OpenPage\n    | ResponseFunctionWebSearch.Find;\n\n  /**\n   * The status of the web search tool call.\n   */\n  status: 'in_progress' | 'searching' | 'completed' | 'failed';\n\n  /**\n   * The type of the web search tool call. Always `web_search_call`.\n   */\n  type: 'web_search_call';\n}\n\nexport namespace ResponseFunctionWebSearch {\n  /**\n   * Action type \"search\" - Performs a web search query.\n   */\n  export interface Search {\n    /**\n     * [DEPRECATED] The search query.\n     */\n    query: string;\n\n    /**\n     * The action type.\n     */\n    type: 'search';\n\n    /**\n     * The search queries.\n     */\n    queries?: Array<string>;\n\n    /**\n     * The sources used in the search.\n     */\n    sources?: Array<Search.Source>;\n  }\n\n  export namespace Search {\n    /**\n     * A source used in the search.\n     */\n    export interface Source {\n      /**\n       * The type of source. Always `url`.\n       */\n      type: 'url';\n\n      /**\n       * The URL of the source.\n       */\n      url: string;\n    }\n  }\n\n  /**\n   * Action type \"open_page\" - Opens a specific URL from search results.\n   */\n  export interface OpenPage {\n    /**\n     * The action type.\n     */\n    type: 'open_page';\n\n    /**\n     * The URL opened by the model.\n     */\n    url?: string | null;\n  }\n\n  /**\n   * Action type \"find_in_page\": Searches for a pattern within a loaded page.\n   */\n  export interface Find {\n    /**\n     * The pattern or text to search for within the page.\n     */\n    pattern: string;\n\n    /**\n     * The action type.\n     */\n    type: 'find_in_page';\n\n    /**\n     * The URL of the page searched for the pattern.\n     */\n    url: string;\n  }\n}\n\n/**\n * Emitted when an image generation tool call has completed and the final image is\n * available.\n */\nexport interface ResponseImageGenCallCompletedEvent {\n  /**\n   * The unique identifier of the image generation item being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.image_generation_call.completed'.\n   */\n  type: 'response.image_generation_call.completed';\n}\n\n/**\n * Emitted when an image generation tool call is actively generating an image\n * (intermediate state).\n */\nexport interface ResponseImageGenCallGeneratingEvent {\n  /**\n   * The unique identifier of the image generation item being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of the image generation item being processed.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.image_generation_call.generating'.\n   */\n  type: 'response.image_generation_call.generating';\n}\n\n/**\n * Emitted when an image generation tool call is in progress.\n */\nexport interface ResponseImageGenCallInProgressEvent {\n  /**\n   * The unique identifier of the image generation item being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of the image generation item being processed.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.image_generation_call.in_progress'.\n   */\n  type: 'response.image_generation_call.in_progress';\n}\n\n/**\n * Emitted when a partial image is available during image generation streaming.\n */\nexport interface ResponseImageGenCallPartialImageEvent {\n  /**\n   * The unique identifier of the image generation item being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * Base64-encoded partial image data, suitable for rendering as an image.\n   */\n  partial_image_b64: string;\n\n  /**\n   * 0-based index for the partial image (backend is 1-based, but this is 0-based for\n   * the user).\n   */\n  partial_image_index: number;\n\n  /**\n   * The sequence number of the image generation item being processed.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.image_generation_call.partial_image'.\n   */\n  type: 'response.image_generation_call.partial_image';\n}\n\n/**\n * Emitted when the response is in progress.\n */\nexport interface ResponseInProgressEvent {\n  /**\n   * The response that is in progress.\n   */\n  response: Response;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.in_progress`.\n   */\n  type: 'response.in_progress';\n}\n\n/**\n * Specify additional output data to include in the model response. Currently\n * supported values are:\n *\n * - `web_search_call.action.sources`: Include the sources of the web search tool\n *   call.\n * - `code_interpreter_call.outputs`: Includes the outputs of python code execution\n *   in code interpreter tool call items.\n * - `computer_call_output.output.image_url`: Include image urls from the computer\n *   call output.\n * - `file_search_call.results`: Include the search results of the file search tool\n *   call.\n * - `message.input_image.image_url`: Include image urls from the input message.\n * - `computer_call_output.output.image_url`: Include image urls from the computer\n *   call output.\n * - `reasoning.encrypted_content`: Includes an encrypted version of reasoning\n *   tokens in reasoning item outputs. This enables reasoning items to be used in\n *   multi-turn conversations when using the Responses API statelessly (like when\n *   the `store` parameter is set to `false`, or when an organization is enrolled\n *   in the zero data retention program).\n * - `code_interpreter_call.outputs`: Includes the outputs of python code execution\n *   in code interpreter tool call items.\n */\nexport type ResponseIncludable =\n  | 'file_search_call.results'\n  | 'web_search_call.results'\n  | 'web_search_call.action.sources'\n  | 'message.input_image.image_url'\n  | 'computer_call_output.output.image_url'\n  | 'code_interpreter_call.outputs'\n  | 'reasoning.encrypted_content'\n  | 'message.output_text.logprobs';\n\n/**\n * An event that is emitted when a response finishes as incomplete.\n */\nexport interface ResponseIncompleteEvent {\n  /**\n   * The response that was incomplete.\n   */\n  response: Response;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.incomplete`.\n   */\n  type: 'response.incomplete';\n}\n\n/**\n * A list of one or many input items to the model, containing different content\n * types.\n */\nexport type ResponseInput = Array<ResponseInputItem>;\n\n/**\n * An audio input to the model.\n */\nexport interface ResponseInputAudio {\n  input_audio: ResponseInputAudio.InputAudio;\n\n  /**\n   * The type of the input item. Always `input_audio`.\n   */\n  type: 'input_audio';\n}\n\nexport namespace ResponseInputAudio {\n  export interface InputAudio {\n    /**\n     * Base64-encoded audio data.\n     */\n    data: string;\n\n    /**\n     * The format of the audio data. Currently supported formats are `mp3` and `wav`.\n     */\n    format: 'mp3' | 'wav';\n  }\n}\n\n/**\n * A text input to the model.\n */\nexport type ResponseInputContent = ResponseInputText | ResponseInputImage | ResponseInputFile;\n\n/**\n * A file input to the model.\n */\nexport interface ResponseInputFile {\n  /**\n   * The type of the input item. Always `input_file`.\n   */\n  type: 'input_file';\n\n  /**\n   * The content of the file to be sent to the model.\n   */\n  file_data?: string;\n\n  /**\n   * The ID of the file to be sent to the model.\n   */\n  file_id?: string | null;\n\n  /**\n   * The URL of the file to be sent to the model.\n   */\n  file_url?: string;\n\n  /**\n   * The name of the file to be sent to the model.\n   */\n  filename?: string;\n}\n\n/**\n * A file input to the model.\n */\nexport interface ResponseInputFileContent {\n  /**\n   * The type of the input item. Always `input_file`.\n   */\n  type: 'input_file';\n\n  /**\n   * The base64-encoded data of the file to be sent to the model.\n   */\n  file_data?: string | null;\n\n  /**\n   * The ID of the file to be sent to the model.\n   */\n  file_id?: string | null;\n\n  /**\n   * The URL of the file to be sent to the model.\n   */\n  file_url?: string | null;\n\n  /**\n   * The name of the file to be sent to the model.\n   */\n  filename?: string | null;\n}\n\n/**\n * An image input to the model. Learn about\n * [image inputs](https://platform.openai.com/docs/guides/vision).\n */\nexport interface ResponseInputImage {\n  /**\n   * The detail level of the image to be sent to the model. One of `high`, `low`, or\n   * `auto`. Defaults to `auto`.\n   */\n  detail: 'low' | 'high' | 'auto';\n\n  /**\n   * The type of the input item. Always `input_image`.\n   */\n  type: 'input_image';\n\n  /**\n   * The ID of the file to be sent to the model.\n   */\n  file_id?: string | null;\n\n  /**\n   * The URL of the image to be sent to the model. A fully qualified URL or base64\n   * encoded image in a data URL.\n   */\n  image_url?: string | null;\n}\n\n/**\n * An image input to the model. Learn about\n * [image inputs](https://platform.openai.com/docs/guides/vision)\n */\nexport interface ResponseInputImageContent {\n  /**\n   * The type of the input item. Always `input_image`.\n   */\n  type: 'input_image';\n\n  /**\n   * The detail level of the image to be sent to the model. One of `high`, `low`, or\n   * `auto`. Defaults to `auto`.\n   */\n  detail?: 'low' | 'high' | 'auto' | null;\n\n  /**\n   * The ID of the file to be sent to the model.\n   */\n  file_id?: string | null;\n\n  /**\n   * The URL of the image to be sent to the model. A fully qualified URL or base64\n   * encoded image in a data URL.\n   */\n  image_url?: string | null;\n}\n\n/**\n * A message input to the model with a role indicating instruction following\n * hierarchy. Instructions given with the `developer` or `system` role take\n * precedence over instructions given with the `user` role. Messages with the\n * `assistant` role are presumed to have been generated by the model in previous\n * interactions.\n */\nexport type ResponseInputItem =\n  | EasyInputMessage\n  | ResponseInputItem.Message\n  | ResponseOutputMessage\n  | ResponseFileSearchToolCall\n  | ResponseComputerToolCall\n  | ResponseInputItem.ComputerCallOutput\n  | ResponseFunctionWebSearch\n  | ResponseFunctionToolCall\n  | ResponseInputItem.FunctionCallOutput\n  | ResponseReasoningItem\n  | ResponseCompactionItemParam\n  | ResponseInputItem.ImageGenerationCall\n  | ResponseCodeInterpreterToolCall\n  | ResponseInputItem.LocalShellCall\n  | ResponseInputItem.LocalShellCallOutput\n  | ResponseInputItem.ShellCall\n  | ResponseInputItem.ShellCallOutput\n  | ResponseInputItem.ApplyPatchCall\n  | ResponseInputItem.ApplyPatchCallOutput\n  | ResponseInputItem.McpListTools\n  | ResponseInputItem.McpApprovalRequest\n  | ResponseInputItem.McpApprovalResponse\n  | ResponseInputItem.McpCall\n  | ResponseCustomToolCallOutput\n  | ResponseCustomToolCall\n  | ResponseInputItem.ItemReference;\n\nexport namespace ResponseInputItem {\n  /**\n   * A message input to the model with a role indicating instruction following\n   * hierarchy. Instructions given with the `developer` or `system` role take\n   * precedence over instructions given with the `user` role.\n   */\n  export interface Message {\n    /**\n     * A list of one or many input items to the model, containing different content\n     * types.\n     */\n    content: ResponsesAPI.ResponseInputMessageContentList;\n\n    /**\n     * The role of the message input. One of `user`, `system`, or `developer`.\n     */\n    role: 'user' | 'system' | 'developer';\n\n    /**\n     * The status of item. One of `in_progress`, `completed`, or `incomplete`.\n     * Populated when items are returned via API.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete';\n\n    /**\n     * The type of the message input. Always set to `message`.\n     */\n    type?: 'message';\n  }\n\n  /**\n   * The output of a computer tool call.\n   */\n  export interface ComputerCallOutput {\n    /**\n     * The ID of the computer tool call that produced the output.\n     */\n    call_id: string;\n\n    /**\n     * A computer screenshot image used with the computer use tool.\n     */\n    output: ResponsesAPI.ResponseComputerToolCallOutputScreenshot;\n\n    /**\n     * The type of the computer tool call output. Always `computer_call_output`.\n     */\n    type: 'computer_call_output';\n\n    /**\n     * The ID of the computer tool call output.\n     */\n    id?: string | null;\n\n    /**\n     * The safety checks reported by the API that have been acknowledged by the\n     * developer.\n     */\n    acknowledged_safety_checks?: Array<ComputerCallOutput.AcknowledgedSafetyCheck> | null;\n\n    /**\n     * The status of the message input. One of `in_progress`, `completed`, or\n     * `incomplete`. Populated when input items are returned via API.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | null;\n  }\n\n  export namespace ComputerCallOutput {\n    /**\n     * A pending safety check for the computer call.\n     */\n    export interface AcknowledgedSafetyCheck {\n      /**\n       * The ID of the pending safety check.\n       */\n      id: string;\n\n      /**\n       * The type of the pending safety check.\n       */\n      code?: string | null;\n\n      /**\n       * Details about the pending safety check.\n       */\n      message?: string | null;\n    }\n  }\n\n  /**\n   * The output of a function tool call.\n   */\n  export interface FunctionCallOutput {\n    /**\n     * The unique ID of the function tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * Text, image, or file output of the function tool call.\n     */\n    output: string | ResponsesAPI.ResponseFunctionCallOutputItemList;\n\n    /**\n     * The type of the function tool call output. Always `function_call_output`.\n     */\n    type: 'function_call_output';\n\n    /**\n     * The unique ID of the function tool call output. Populated when this item is\n     * returned via API.\n     */\n    id?: string | null;\n\n    /**\n     * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n     * Populated when items are returned via API.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | null;\n  }\n\n  /**\n   * An image generation request made by the model.\n   */\n  export interface ImageGenerationCall {\n    /**\n     * The unique ID of the image generation call.\n     */\n    id: string;\n\n    /**\n     * The generated image encoded in base64.\n     */\n    result: string | null;\n\n    /**\n     * The status of the image generation call.\n     */\n    status: 'in_progress' | 'completed' | 'generating' | 'failed';\n\n    /**\n     * The type of the image generation call. Always `image_generation_call`.\n     */\n    type: 'image_generation_call';\n  }\n\n  /**\n   * A tool call to run a command on the local shell.\n   */\n  export interface LocalShellCall {\n    /**\n     * The unique ID of the local shell call.\n     */\n    id: string;\n\n    /**\n     * Execute a shell command on the server.\n     */\n    action: LocalShellCall.Action;\n\n    /**\n     * The unique ID of the local shell tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * The status of the local shell call.\n     */\n    status: 'in_progress' | 'completed' | 'incomplete';\n\n    /**\n     * The type of the local shell call. Always `local_shell_call`.\n     */\n    type: 'local_shell_call';\n  }\n\n  export namespace LocalShellCall {\n    /**\n     * Execute a shell command on the server.\n     */\n    export interface Action {\n      /**\n       * The command to run.\n       */\n      command: Array<string>;\n\n      /**\n       * Environment variables to set for the command.\n       */\n      env: { [key: string]: string };\n\n      /**\n       * The type of the local shell action. Always `exec`.\n       */\n      type: 'exec';\n\n      /**\n       * Optional timeout in milliseconds for the command.\n       */\n      timeout_ms?: number | null;\n\n      /**\n       * Optional user to run the command as.\n       */\n      user?: string | null;\n\n      /**\n       * Optional working directory to run the command in.\n       */\n      working_directory?: string | null;\n    }\n  }\n\n  /**\n   * The output of a local shell tool call.\n   */\n  export interface LocalShellCallOutput {\n    /**\n     * The unique ID of the local shell tool call generated by the model.\n     */\n    id: string;\n\n    /**\n     * A JSON string of the output of the local shell tool call.\n     */\n    output: string;\n\n    /**\n     * The type of the local shell tool call output. Always `local_shell_call_output`.\n     */\n    type: 'local_shell_call_output';\n\n    /**\n     * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | null;\n  }\n\n  /**\n   * A tool representing a request to execute one or more shell commands.\n   */\n  export interface ShellCall {\n    /**\n     * The shell commands and limits that describe how to run the tool call.\n     */\n    action: ShellCall.Action;\n\n    /**\n     * The unique ID of the shell tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * The type of the item. Always `shell_call`.\n     */\n    type: 'shell_call';\n\n    /**\n     * The unique ID of the shell tool call. Populated when this item is returned via\n     * API.\n     */\n    id?: string | null;\n\n    /**\n     * The environment to execute the shell commands in.\n     */\n    environment?: ResponsesAPI.LocalEnvironment | ResponsesAPI.ContainerReference | null;\n\n    /**\n     * The status of the shell call. One of `in_progress`, `completed`, or\n     * `incomplete`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | null;\n  }\n\n  export namespace ShellCall {\n    /**\n     * The shell commands and limits that describe how to run the tool call.\n     */\n    export interface Action {\n      /**\n       * Ordered shell commands for the execution environment to run.\n       */\n      commands: Array<string>;\n\n      /**\n       * Maximum number of UTF-8 characters to capture from combined stdout and stderr\n       * output.\n       */\n      max_output_length?: number | null;\n\n      /**\n       * Maximum wall-clock time in milliseconds to allow the shell commands to run.\n       */\n      timeout_ms?: number | null;\n    }\n  }\n\n  /**\n   * The streamed output items emitted by a shell tool call.\n   */\n  export interface ShellCallOutput {\n    /**\n     * The unique ID of the shell tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * Captured chunks of stdout and stderr output, along with their associated\n     * outcomes.\n     */\n    output: Array<ResponsesAPI.ResponseFunctionShellCallOutputContent>;\n\n    /**\n     * The type of the item. Always `shell_call_output`.\n     */\n    type: 'shell_call_output';\n\n    /**\n     * The unique ID of the shell tool call output. Populated when this item is\n     * returned via API.\n     */\n    id?: string | null;\n\n    /**\n     * The maximum number of UTF-8 characters captured for this shell call's combined\n     * output.\n     */\n    max_output_length?: number | null;\n\n    /**\n     * The status of the shell call output.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | null;\n  }\n\n  /**\n   * A tool call representing a request to create, delete, or update files using diff\n   * patches.\n   */\n  export interface ApplyPatchCall {\n    /**\n     * The unique ID of the apply patch tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * The specific create, delete, or update instruction for the apply_patch tool\n     * call.\n     */\n    operation: ApplyPatchCall.CreateFile | ApplyPatchCall.DeleteFile | ApplyPatchCall.UpdateFile;\n\n    /**\n     * The status of the apply patch tool call. One of `in_progress` or `completed`.\n     */\n    status: 'in_progress' | 'completed';\n\n    /**\n     * The type of the item. Always `apply_patch_call`.\n     */\n    type: 'apply_patch_call';\n\n    /**\n     * The unique ID of the apply patch tool call. Populated when this item is returned\n     * via API.\n     */\n    id?: string | null;\n  }\n\n  export namespace ApplyPatchCall {\n    /**\n     * Instruction for creating a new file via the apply_patch tool.\n     */\n    export interface CreateFile {\n      /**\n       * Unified diff content to apply when creating the file.\n       */\n      diff: string;\n\n      /**\n       * Path of the file to create relative to the workspace root.\n       */\n      path: string;\n\n      /**\n       * The operation type. Always `create_file`.\n       */\n      type: 'create_file';\n    }\n\n    /**\n     * Instruction for deleting an existing file via the apply_patch tool.\n     */\n    export interface DeleteFile {\n      /**\n       * Path of the file to delete relative to the workspace root.\n       */\n      path: string;\n\n      /**\n       * The operation type. Always `delete_file`.\n       */\n      type: 'delete_file';\n    }\n\n    /**\n     * Instruction for updating an existing file via the apply_patch tool.\n     */\n    export interface UpdateFile {\n      /**\n       * Unified diff content to apply to the existing file.\n       */\n      diff: string;\n\n      /**\n       * Path of the file to update relative to the workspace root.\n       */\n      path: string;\n\n      /**\n       * The operation type. Always `update_file`.\n       */\n      type: 'update_file';\n    }\n  }\n\n  /**\n   * The streamed output emitted by an apply patch tool call.\n   */\n  export interface ApplyPatchCallOutput {\n    /**\n     * The unique ID of the apply patch tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * The status of the apply patch tool call output. One of `completed` or `failed`.\n     */\n    status: 'completed' | 'failed';\n\n    /**\n     * The type of the item. Always `apply_patch_call_output`.\n     */\n    type: 'apply_patch_call_output';\n\n    /**\n     * The unique ID of the apply patch tool call output. Populated when this item is\n     * returned via API.\n     */\n    id?: string | null;\n\n    /**\n     * Optional human-readable log text from the apply patch tool (e.g., patch results\n     * or errors).\n     */\n    output?: string | null;\n  }\n\n  /**\n   * A list of tools available on an MCP server.\n   */\n  export interface McpListTools {\n    /**\n     * The unique ID of the list.\n     */\n    id: string;\n\n    /**\n     * The label of the MCP server.\n     */\n    server_label: string;\n\n    /**\n     * The tools available on the server.\n     */\n    tools: Array<McpListTools.Tool>;\n\n    /**\n     * The type of the item. Always `mcp_list_tools`.\n     */\n    type: 'mcp_list_tools';\n\n    /**\n     * Error message if the server could not list tools.\n     */\n    error?: string | null;\n  }\n\n  export namespace McpListTools {\n    /**\n     * A tool available on an MCP server.\n     */\n    export interface Tool {\n      /**\n       * The JSON schema describing the tool's input.\n       */\n      input_schema: unknown;\n\n      /**\n       * The name of the tool.\n       */\n      name: string;\n\n      /**\n       * Additional annotations about the tool.\n       */\n      annotations?: unknown | null;\n\n      /**\n       * The description of the tool.\n       */\n      description?: string | null;\n    }\n  }\n\n  /**\n   * A request for human approval of a tool invocation.\n   */\n  export interface McpApprovalRequest {\n    /**\n     * The unique ID of the approval request.\n     */\n    id: string;\n\n    /**\n     * A JSON string of arguments for the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool to run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server making the request.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_approval_request`.\n     */\n    type: 'mcp_approval_request';\n  }\n\n  /**\n   * A response to an MCP approval request.\n   */\n  export interface McpApprovalResponse {\n    /**\n     * The ID of the approval request being answered.\n     */\n    approval_request_id: string;\n\n    /**\n     * Whether the request was approved.\n     */\n    approve: boolean;\n\n    /**\n     * The type of the item. Always `mcp_approval_response`.\n     */\n    type: 'mcp_approval_response';\n\n    /**\n     * The unique ID of the approval response\n     */\n    id?: string | null;\n\n    /**\n     * Optional reason for the decision.\n     */\n    reason?: string | null;\n  }\n\n  /**\n   * An invocation of a tool on an MCP server.\n   */\n  export interface McpCall {\n    /**\n     * The unique ID of the tool call.\n     */\n    id: string;\n\n    /**\n     * A JSON string of the arguments passed to the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool that was run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server running the tool.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_call`.\n     */\n    type: 'mcp_call';\n\n    /**\n     * Unique identifier for the MCP tool call approval request. Include this value in\n     * a subsequent `mcp_approval_response` input to approve or reject the\n     * corresponding tool call.\n     */\n    approval_request_id?: string | null;\n\n    /**\n     * The error from the tool call, if any.\n     */\n    error?: string | null;\n\n    /**\n     * The output from the tool call.\n     */\n    output?: string | null;\n\n    /**\n     * The status of the tool call. One of `in_progress`, `completed`, `incomplete`,\n     * `calling`, or `failed`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | 'calling' | 'failed';\n  }\n\n  /**\n   * An internal identifier for an item to reference.\n   */\n  export interface ItemReference {\n    /**\n     * The ID of the item to reference.\n     */\n    id: string;\n\n    /**\n     * The type of item to reference. Always `item_reference`.\n     */\n    type?: 'item_reference' | null;\n  }\n}\n\n/**\n * A list of one or many input items to the model, containing different content\n * types.\n */\nexport type ResponseInputMessageContentList = Array<ResponseInputContent>;\n\nexport interface ResponseInputMessageItem {\n  /**\n   * The unique ID of the message input.\n   */\n  id: string;\n\n  /**\n   * A list of one or many input items to the model, containing different content\n   * types.\n   */\n  content: ResponseInputMessageContentList;\n\n  /**\n   * The role of the message input. One of `user`, `system`, or `developer`.\n   */\n  role: 'user' | 'system' | 'developer';\n\n  /**\n   * The status of item. One of `in_progress`, `completed`, or `incomplete`.\n   * Populated when items are returned via API.\n   */\n  status?: 'in_progress' | 'completed' | 'incomplete';\n\n  /**\n   * The type of the message input. Always set to `message`.\n   */\n  type?: 'message';\n}\n\n/**\n * A text input to the model.\n */\nexport interface ResponseInputText {\n  /**\n   * The text input to the model.\n   */\n  text: string;\n\n  /**\n   * The type of the input item. Always `input_text`.\n   */\n  type: 'input_text';\n}\n\n/**\n * A text input to the model.\n */\nexport interface ResponseInputTextContent {\n  /**\n   * The text input to the model.\n   */\n  text: string;\n\n  /**\n   * The type of the input item. Always `input_text`.\n   */\n  type: 'input_text';\n}\n\n/**\n * Content item used to generate a response.\n */\nexport type ResponseItem =\n  | ResponseInputMessageItem\n  | ResponseOutputMessage\n  | ResponseFileSearchToolCall\n  | ResponseComputerToolCall\n  | ResponseComputerToolCallOutputItem\n  | ResponseFunctionWebSearch\n  | ResponseFunctionToolCallItem\n  | ResponseFunctionToolCallOutputItem\n  | ResponseItem.ImageGenerationCall\n  | ResponseCodeInterpreterToolCall\n  | ResponseItem.LocalShellCall\n  | ResponseItem.LocalShellCallOutput\n  | ResponseFunctionShellToolCall\n  | ResponseFunctionShellToolCallOutput\n  | ResponseApplyPatchToolCall\n  | ResponseApplyPatchToolCallOutput\n  | ResponseItem.McpListTools\n  | ResponseItem.McpApprovalRequest\n  | ResponseItem.McpApprovalResponse\n  | ResponseItem.McpCall;\n\nexport namespace ResponseItem {\n  /**\n   * An image generation request made by the model.\n   */\n  export interface ImageGenerationCall {\n    /**\n     * The unique ID of the image generation call.\n     */\n    id: string;\n\n    /**\n     * The generated image encoded in base64.\n     */\n    result: string | null;\n\n    /**\n     * The status of the image generation call.\n     */\n    status: 'in_progress' | 'completed' | 'generating' | 'failed';\n\n    /**\n     * The type of the image generation call. Always `image_generation_call`.\n     */\n    type: 'image_generation_call';\n  }\n\n  /**\n   * A tool call to run a command on the local shell.\n   */\n  export interface LocalShellCall {\n    /**\n     * The unique ID of the local shell call.\n     */\n    id: string;\n\n    /**\n     * Execute a shell command on the server.\n     */\n    action: LocalShellCall.Action;\n\n    /**\n     * The unique ID of the local shell tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * The status of the local shell call.\n     */\n    status: 'in_progress' | 'completed' | 'incomplete';\n\n    /**\n     * The type of the local shell call. Always `local_shell_call`.\n     */\n    type: 'local_shell_call';\n  }\n\n  export namespace LocalShellCall {\n    /**\n     * Execute a shell command on the server.\n     */\n    export interface Action {\n      /**\n       * The command to run.\n       */\n      command: Array<string>;\n\n      /**\n       * Environment variables to set for the command.\n       */\n      env: { [key: string]: string };\n\n      /**\n       * The type of the local shell action. Always `exec`.\n       */\n      type: 'exec';\n\n      /**\n       * Optional timeout in milliseconds for the command.\n       */\n      timeout_ms?: number | null;\n\n      /**\n       * Optional user to run the command as.\n       */\n      user?: string | null;\n\n      /**\n       * Optional working directory to run the command in.\n       */\n      working_directory?: string | null;\n    }\n  }\n\n  /**\n   * The output of a local shell tool call.\n   */\n  export interface LocalShellCallOutput {\n    /**\n     * The unique ID of the local shell tool call generated by the model.\n     */\n    id: string;\n\n    /**\n     * A JSON string of the output of the local shell tool call.\n     */\n    output: string;\n\n    /**\n     * The type of the local shell tool call output. Always `local_shell_call_output`.\n     */\n    type: 'local_shell_call_output';\n\n    /**\n     * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | null;\n  }\n\n  /**\n   * A list of tools available on an MCP server.\n   */\n  export interface McpListTools {\n    /**\n     * The unique ID of the list.\n     */\n    id: string;\n\n    /**\n     * The label of the MCP server.\n     */\n    server_label: string;\n\n    /**\n     * The tools available on the server.\n     */\n    tools: Array<McpListTools.Tool>;\n\n    /**\n     * The type of the item. Always `mcp_list_tools`.\n     */\n    type: 'mcp_list_tools';\n\n    /**\n     * Error message if the server could not list tools.\n     */\n    error?: string | null;\n  }\n\n  export namespace McpListTools {\n    /**\n     * A tool available on an MCP server.\n     */\n    export interface Tool {\n      /**\n       * The JSON schema describing the tool's input.\n       */\n      input_schema: unknown;\n\n      /**\n       * The name of the tool.\n       */\n      name: string;\n\n      /**\n       * Additional annotations about the tool.\n       */\n      annotations?: unknown | null;\n\n      /**\n       * The description of the tool.\n       */\n      description?: string | null;\n    }\n  }\n\n  /**\n   * A request for human approval of a tool invocation.\n   */\n  export interface McpApprovalRequest {\n    /**\n     * The unique ID of the approval request.\n     */\n    id: string;\n\n    /**\n     * A JSON string of arguments for the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool to run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server making the request.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_approval_request`.\n     */\n    type: 'mcp_approval_request';\n  }\n\n  /**\n   * A response to an MCP approval request.\n   */\n  export interface McpApprovalResponse {\n    /**\n     * The unique ID of the approval response\n     */\n    id: string;\n\n    /**\n     * The ID of the approval request being answered.\n     */\n    approval_request_id: string;\n\n    /**\n     * Whether the request was approved.\n     */\n    approve: boolean;\n\n    /**\n     * The type of the item. Always `mcp_approval_response`.\n     */\n    type: 'mcp_approval_response';\n\n    /**\n     * Optional reason for the decision.\n     */\n    reason?: string | null;\n  }\n\n  /**\n   * An invocation of a tool on an MCP server.\n   */\n  export interface McpCall {\n    /**\n     * The unique ID of the tool call.\n     */\n    id: string;\n\n    /**\n     * A JSON string of the arguments passed to the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool that was run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server running the tool.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_call`.\n     */\n    type: 'mcp_call';\n\n    /**\n     * Unique identifier for the MCP tool call approval request. Include this value in\n     * a subsequent `mcp_approval_response` input to approve or reject the\n     * corresponding tool call.\n     */\n    approval_request_id?: string | null;\n\n    /**\n     * The error from the tool call, if any.\n     */\n    error?: string | null;\n\n    /**\n     * The output from the tool call.\n     */\n    output?: string | null;\n\n    /**\n     * The status of the tool call. One of `in_progress`, `completed`, `incomplete`,\n     * `calling`, or `failed`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | 'calling' | 'failed';\n  }\n}\n\n/**\n * Represents the use of a local environment to perform shell actions.\n */\nexport interface ResponseLocalEnvironment {\n  /**\n   * The environment type. Always `local`.\n   */\n  type: 'local';\n}\n\n/**\n * Emitted when there is a delta (partial update) to the arguments of an MCP tool\n * call.\n */\nexport interface ResponseMcpCallArgumentsDeltaEvent {\n  /**\n   * A JSON string containing the partial update to the arguments for the MCP tool\n   * call.\n   */\n  delta: string;\n\n  /**\n   * The unique identifier of the MCP tool call item being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_call_arguments.delta'.\n   */\n  type: 'response.mcp_call_arguments.delta';\n}\n\n/**\n * Emitted when the arguments for an MCP tool call are finalized.\n */\nexport interface ResponseMcpCallArgumentsDoneEvent {\n  /**\n   * A JSON string containing the finalized arguments for the MCP tool call.\n   */\n  arguments: string;\n\n  /**\n   * The unique identifier of the MCP tool call item being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_call_arguments.done'.\n   */\n  type: 'response.mcp_call_arguments.done';\n}\n\n/**\n * Emitted when an MCP tool call has completed successfully.\n */\nexport interface ResponseMcpCallCompletedEvent {\n  /**\n   * The ID of the MCP tool call item that completed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that completed.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_call.completed'.\n   */\n  type: 'response.mcp_call.completed';\n}\n\n/**\n * Emitted when an MCP tool call has failed.\n */\nexport interface ResponseMcpCallFailedEvent {\n  /**\n   * The ID of the MCP tool call item that failed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that failed.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_call.failed'.\n   */\n  type: 'response.mcp_call.failed';\n}\n\n/**\n * Emitted when an MCP tool call is in progress.\n */\nexport interface ResponseMcpCallInProgressEvent {\n  /**\n   * The unique identifier of the MCP tool call item being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_call.in_progress'.\n   */\n  type: 'response.mcp_call.in_progress';\n}\n\n/**\n * Emitted when the list of available MCP tools has been successfully retrieved.\n */\nexport interface ResponseMcpListToolsCompletedEvent {\n  /**\n   * The ID of the MCP tool call item that produced this output.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that was processed.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_list_tools.completed'.\n   */\n  type: 'response.mcp_list_tools.completed';\n}\n\n/**\n * Emitted when the attempt to list available MCP tools has failed.\n */\nexport interface ResponseMcpListToolsFailedEvent {\n  /**\n   * The ID of the MCP tool call item that failed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that failed.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_list_tools.failed'.\n   */\n  type: 'response.mcp_list_tools.failed';\n}\n\n/**\n * Emitted when the system is in the process of retrieving the list of available\n * MCP tools.\n */\nexport interface ResponseMcpListToolsInProgressEvent {\n  /**\n   * The ID of the MCP tool call item that is being processed.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that is being processed.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.mcp_list_tools.in_progress'.\n   */\n  type: 'response.mcp_list_tools.in_progress';\n}\n\n/**\n * An audio output from the model.\n */\nexport interface ResponseOutputAudio {\n  /**\n   * Base64-encoded audio data from the model.\n   */\n  data: string;\n\n  /**\n   * The transcript of the audio data from the model.\n   */\n  transcript: string;\n\n  /**\n   * The type of the output audio. Always `output_audio`.\n   */\n  type: 'output_audio';\n}\n\n/**\n * An output message from the model.\n */\nexport type ResponseOutputItem =\n  | ResponseOutputMessage\n  | ResponseFileSearchToolCall\n  | ResponseFunctionToolCall\n  | ResponseFunctionWebSearch\n  | ResponseComputerToolCall\n  | ResponseReasoningItem\n  | ResponseCompactionItem\n  | ResponseOutputItem.ImageGenerationCall\n  | ResponseCodeInterpreterToolCall\n  | ResponseOutputItem.LocalShellCall\n  | ResponseFunctionShellToolCall\n  | ResponseFunctionShellToolCallOutput\n  | ResponseApplyPatchToolCall\n  | ResponseApplyPatchToolCallOutput\n  | ResponseOutputItem.McpCall\n  | ResponseOutputItem.McpListTools\n  | ResponseOutputItem.McpApprovalRequest\n  | ResponseCustomToolCall;\n\nexport namespace ResponseOutputItem {\n  /**\n   * An image generation request made by the model.\n   */\n  export interface ImageGenerationCall {\n    /**\n     * The unique ID of the image generation call.\n     */\n    id: string;\n\n    /**\n     * The generated image encoded in base64.\n     */\n    result: string | null;\n\n    /**\n     * The status of the image generation call.\n     */\n    status: 'in_progress' | 'completed' | 'generating' | 'failed';\n\n    /**\n     * The type of the image generation call. Always `image_generation_call`.\n     */\n    type: 'image_generation_call';\n  }\n\n  /**\n   * A tool call to run a command on the local shell.\n   */\n  export interface LocalShellCall {\n    /**\n     * The unique ID of the local shell call.\n     */\n    id: string;\n\n    /**\n     * Execute a shell command on the server.\n     */\n    action: LocalShellCall.Action;\n\n    /**\n     * The unique ID of the local shell tool call generated by the model.\n     */\n    call_id: string;\n\n    /**\n     * The status of the local shell call.\n     */\n    status: 'in_progress' | 'completed' | 'incomplete';\n\n    /**\n     * The type of the local shell call. Always `local_shell_call`.\n     */\n    type: 'local_shell_call';\n  }\n\n  export namespace LocalShellCall {\n    /**\n     * Execute a shell command on the server.\n     */\n    export interface Action {\n      /**\n       * The command to run.\n       */\n      command: Array<string>;\n\n      /**\n       * Environment variables to set for the command.\n       */\n      env: { [key: string]: string };\n\n      /**\n       * The type of the local shell action. Always `exec`.\n       */\n      type: 'exec';\n\n      /**\n       * Optional timeout in milliseconds for the command.\n       */\n      timeout_ms?: number | null;\n\n      /**\n       * Optional user to run the command as.\n       */\n      user?: string | null;\n\n      /**\n       * Optional working directory to run the command in.\n       */\n      working_directory?: string | null;\n    }\n  }\n\n  /**\n   * An invocation of a tool on an MCP server.\n   */\n  export interface McpCall {\n    /**\n     * The unique ID of the tool call.\n     */\n    id: string;\n\n    /**\n     * A JSON string of the arguments passed to the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool that was run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server running the tool.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_call`.\n     */\n    type: 'mcp_call';\n\n    /**\n     * Unique identifier for the MCP tool call approval request. Include this value in\n     * a subsequent `mcp_approval_response` input to approve or reject the\n     * corresponding tool call.\n     */\n    approval_request_id?: string | null;\n\n    /**\n     * The error from the tool call, if any.\n     */\n    error?: string | null;\n\n    /**\n     * The output from the tool call.\n     */\n    output?: string | null;\n\n    /**\n     * The status of the tool call. One of `in_progress`, `completed`, `incomplete`,\n     * `calling`, or `failed`.\n     */\n    status?: 'in_progress' | 'completed' | 'incomplete' | 'calling' | 'failed';\n  }\n\n  /**\n   * A list of tools available on an MCP server.\n   */\n  export interface McpListTools {\n    /**\n     * The unique ID of the list.\n     */\n    id: string;\n\n    /**\n     * The label of the MCP server.\n     */\n    server_label: string;\n\n    /**\n     * The tools available on the server.\n     */\n    tools: Array<McpListTools.Tool>;\n\n    /**\n     * The type of the item. Always `mcp_list_tools`.\n     */\n    type: 'mcp_list_tools';\n\n    /**\n     * Error message if the server could not list tools.\n     */\n    error?: string | null;\n  }\n\n  export namespace McpListTools {\n    /**\n     * A tool available on an MCP server.\n     */\n    export interface Tool {\n      /**\n       * The JSON schema describing the tool's input.\n       */\n      input_schema: unknown;\n\n      /**\n       * The name of the tool.\n       */\n      name: string;\n\n      /**\n       * Additional annotations about the tool.\n       */\n      annotations?: unknown | null;\n\n      /**\n       * The description of the tool.\n       */\n      description?: string | null;\n    }\n  }\n\n  /**\n   * A request for human approval of a tool invocation.\n   */\n  export interface McpApprovalRequest {\n    /**\n     * The unique ID of the approval request.\n     */\n    id: string;\n\n    /**\n     * A JSON string of arguments for the tool.\n     */\n    arguments: string;\n\n    /**\n     * The name of the tool to run.\n     */\n    name: string;\n\n    /**\n     * The label of the MCP server making the request.\n     */\n    server_label: string;\n\n    /**\n     * The type of the item. Always `mcp_approval_request`.\n     */\n    type: 'mcp_approval_request';\n  }\n}\n\n/**\n * Emitted when a new output item is added.\n */\nexport interface ResponseOutputItemAddedEvent {\n  /**\n   * The output item that was added.\n   */\n  item: ResponseOutputItem;\n\n  /**\n   * The index of the output item that was added.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.output_item.added`.\n   */\n  type: 'response.output_item.added';\n}\n\n/**\n * Emitted when an output item is marked done.\n */\nexport interface ResponseOutputItemDoneEvent {\n  /**\n   * The output item that was marked done.\n   */\n  item: ResponseOutputItem;\n\n  /**\n   * The index of the output item that was marked done.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.output_item.done`.\n   */\n  type: 'response.output_item.done';\n}\n\n/**\n * An output message from the model.\n */\nexport interface ResponseOutputMessage {\n  /**\n   * The unique ID of the output message.\n   */\n  id: string;\n\n  /**\n   * The content of the output message.\n   */\n  content: Array<ResponseOutputText | ResponseOutputRefusal>;\n\n  /**\n   * The role of the output message. Always `assistant`.\n   */\n  role: 'assistant';\n\n  /**\n   * The status of the message input. One of `in_progress`, `completed`, or\n   * `incomplete`. Populated when input items are returned via API.\n   */\n  status: 'in_progress' | 'completed' | 'incomplete';\n\n  /**\n   * The type of the output message. Always `message`.\n   */\n  type: 'message';\n\n  /**\n   * The phase of an assistant message.\n   *\n   * Use `commentary` for an intermediate assistant message and `final_answer` for\n   * the final assistant message. For follow-up requests with models like\n   * `gpt-5.3-codex` and later, preserve and resend phase on all assistant messages.\n   * Omitting it can degrade performance. Not used for user messages.\n   */\n  phase?: 'commentary' | 'final_answer' | null;\n}\n\n/**\n * A refusal from the model.\n */\nexport interface ResponseOutputRefusal {\n  /**\n   * The refusal explanation from the model.\n   */\n  refusal: string;\n\n  /**\n   * The type of the refusal. Always `refusal`.\n   */\n  type: 'refusal';\n}\n\n/**\n * A text output from the model.\n */\nexport interface ResponseOutputText {\n  /**\n   * The annotations of the text output.\n   */\n  annotations: Array<\n    | ResponseOutputText.FileCitation\n    | ResponseOutputText.URLCitation\n    | ResponseOutputText.ContainerFileCitation\n    | ResponseOutputText.FilePath\n  >;\n\n  /**\n   * The text output from the model.\n   */\n  text: string;\n\n  /**\n   * The type of the output text. Always `output_text`.\n   */\n  type: 'output_text';\n\n  logprobs?: Array<ResponseOutputText.Logprob>;\n}\n\nexport namespace ResponseOutputText {\n  /**\n   * A citation to a file.\n   */\n  export interface FileCitation {\n    /**\n     * The ID of the file.\n     */\n    file_id: string;\n\n    /**\n     * The filename of the file cited.\n     */\n    filename: string;\n\n    /**\n     * The index of the file in the list of files.\n     */\n    index: number;\n\n    /**\n     * The type of the file citation. Always `file_citation`.\n     */\n    type: 'file_citation';\n  }\n\n  /**\n   * A citation for a web resource used to generate a model response.\n   */\n  export interface URLCitation {\n    /**\n     * The index of the last character of the URL citation in the message.\n     */\n    end_index: number;\n\n    /**\n     * The index of the first character of the URL citation in the message.\n     */\n    start_index: number;\n\n    /**\n     * The title of the web resource.\n     */\n    title: string;\n\n    /**\n     * The type of the URL citation. Always `url_citation`.\n     */\n    type: 'url_citation';\n\n    /**\n     * The URL of the web resource.\n     */\n    url: string;\n  }\n\n  /**\n   * A citation for a container file used to generate a model response.\n   */\n  export interface ContainerFileCitation {\n    /**\n     * The ID of the container file.\n     */\n    container_id: string;\n\n    /**\n     * The index of the last character of the container file citation in the message.\n     */\n    end_index: number;\n\n    /**\n     * The ID of the file.\n     */\n    file_id: string;\n\n    /**\n     * The filename of the container file cited.\n     */\n    filename: string;\n\n    /**\n     * The index of the first character of the container file citation in the message.\n     */\n    start_index: number;\n\n    /**\n     * The type of the container file citation. Always `container_file_citation`.\n     */\n    type: 'container_file_citation';\n  }\n\n  /**\n   * A path to a file.\n   */\n  export interface FilePath {\n    /**\n     * The ID of the file.\n     */\n    file_id: string;\n\n    /**\n     * The index of the file in the list of files.\n     */\n    index: number;\n\n    /**\n     * The type of the file path. Always `file_path`.\n     */\n    type: 'file_path';\n  }\n\n  /**\n   * The log probability of a token.\n   */\n  export interface Logprob {\n    token: string;\n\n    bytes: Array<number>;\n\n    logprob: number;\n\n    top_logprobs: Array<Logprob.TopLogprob>;\n  }\n\n  export namespace Logprob {\n    /**\n     * The top log probability of a token.\n     */\n    export interface TopLogprob {\n      token: string;\n\n      bytes: Array<number>;\n\n      logprob: number;\n    }\n  }\n}\n\n/**\n * Emitted when an annotation is added to output text content.\n */\nexport interface ResponseOutputTextAnnotationAddedEvent {\n  /**\n   * The annotation object being added. (See annotation schema for details.)\n   */\n  annotation: unknown;\n\n  /**\n   * The index of the annotation within the content part.\n   */\n  annotation_index: number;\n\n  /**\n   * The index of the content part within the output item.\n   */\n  content_index: number;\n\n  /**\n   * The unique identifier of the item to which the annotation is being added.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item in the response's output array.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.output_text.annotation.added'.\n   */\n  type: 'response.output_text.annotation.added';\n}\n\n/**\n * Reference to a prompt template and its variables.\n * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n */\nexport interface ResponsePrompt {\n  /**\n   * The unique identifier of the prompt template to use.\n   */\n  id: string;\n\n  /**\n   * Optional map of values to substitute in for variables in your prompt. The\n   * substitution values can either be strings, or other Response input types like\n   * images or files.\n   */\n  variables?: { [key: string]: string | ResponseInputText | ResponseInputImage | ResponseInputFile } | null;\n\n  /**\n   * Optional version of the prompt template.\n   */\n  version?: string | null;\n}\n\n/**\n * Emitted when a response is queued and waiting to be processed.\n */\nexport interface ResponseQueuedEvent {\n  /**\n   * The full response object that is queued.\n   */\n  response: Response;\n\n  /**\n   * The sequence number for this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always 'response.queued'.\n   */\n  type: 'response.queued';\n}\n\n/**\n * A description of the chain of thought used by a reasoning model while generating\n * a response. Be sure to include these items in your `input` to the Responses API\n * for subsequent turns of a conversation if you are manually\n * [managing context](https://platform.openai.com/docs/guides/conversation-state).\n */\nexport interface ResponseReasoningItem {\n  /**\n   * The unique identifier of the reasoning content.\n   */\n  id: string;\n\n  /**\n   * Reasoning summary content.\n   */\n  summary: Array<ResponseReasoningItem.Summary>;\n\n  /**\n   * The type of the object. Always `reasoning`.\n   */\n  type: 'reasoning';\n\n  /**\n   * Reasoning text content.\n   */\n  content?: Array<ResponseReasoningItem.Content>;\n\n  /**\n   * The encrypted content of the reasoning item - populated when a response is\n   * generated with `reasoning.encrypted_content` in the `include` parameter.\n   */\n  encrypted_content?: string | null;\n\n  /**\n   * The status of the item. One of `in_progress`, `completed`, or `incomplete`.\n   * Populated when items are returned via API.\n   */\n  status?: 'in_progress' | 'completed' | 'incomplete';\n}\n\nexport namespace ResponseReasoningItem {\n  /**\n   * A summary text from the model.\n   */\n  export interface Summary {\n    /**\n     * A summary of the reasoning output from the model so far.\n     */\n    text: string;\n\n    /**\n     * The type of the object. Always `summary_text`.\n     */\n    type: 'summary_text';\n  }\n\n  /**\n   * Reasoning text from the model.\n   */\n  export interface Content {\n    /**\n     * The reasoning text from the model.\n     */\n    text: string;\n\n    /**\n     * The type of the reasoning text. Always `reasoning_text`.\n     */\n    type: 'reasoning_text';\n  }\n}\n\n/**\n * Emitted when a new reasoning summary part is added.\n */\nexport interface ResponseReasoningSummaryPartAddedEvent {\n  /**\n   * The ID of the item this summary part is associated with.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item this summary part is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The summary part that was added.\n   */\n  part: ResponseReasoningSummaryPartAddedEvent.Part;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The index of the summary part within the reasoning summary.\n   */\n  summary_index: number;\n\n  /**\n   * The type of the event. Always `response.reasoning_summary_part.added`.\n   */\n  type: 'response.reasoning_summary_part.added';\n}\n\nexport namespace ResponseReasoningSummaryPartAddedEvent {\n  /**\n   * The summary part that was added.\n   */\n  export interface Part {\n    /**\n     * The text of the summary part.\n     */\n    text: string;\n\n    /**\n     * The type of the summary part. Always `summary_text`.\n     */\n    type: 'summary_text';\n  }\n}\n\n/**\n * Emitted when a reasoning summary part is completed.\n */\nexport interface ResponseReasoningSummaryPartDoneEvent {\n  /**\n   * The ID of the item this summary part is associated with.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item this summary part is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The completed summary part.\n   */\n  part: ResponseReasoningSummaryPartDoneEvent.Part;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The index of the summary part within the reasoning summary.\n   */\n  summary_index: number;\n\n  /**\n   * The type of the event. Always `response.reasoning_summary_part.done`.\n   */\n  type: 'response.reasoning_summary_part.done';\n}\n\nexport namespace ResponseReasoningSummaryPartDoneEvent {\n  /**\n   * The completed summary part.\n   */\n  export interface Part {\n    /**\n     * The text of the summary part.\n     */\n    text: string;\n\n    /**\n     * The type of the summary part. Always `summary_text`.\n     */\n    type: 'summary_text';\n  }\n}\n\n/**\n * Emitted when a delta is added to a reasoning summary text.\n */\nexport interface ResponseReasoningSummaryTextDeltaEvent {\n  /**\n   * The text delta that was added to the summary.\n   */\n  delta: string;\n\n  /**\n   * The ID of the item this summary text delta is associated with.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item this summary text delta is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The index of the summary part within the reasoning summary.\n   */\n  summary_index: number;\n\n  /**\n   * The type of the event. Always `response.reasoning_summary_text.delta`.\n   */\n  type: 'response.reasoning_summary_text.delta';\n}\n\n/**\n * Emitted when a reasoning summary text is completed.\n */\nexport interface ResponseReasoningSummaryTextDoneEvent {\n  /**\n   * The ID of the item this summary text is associated with.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item this summary text is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The index of the summary part within the reasoning summary.\n   */\n  summary_index: number;\n\n  /**\n   * The full text of the completed reasoning summary.\n   */\n  text: string;\n\n  /**\n   * The type of the event. Always `response.reasoning_summary_text.done`.\n   */\n  type: 'response.reasoning_summary_text.done';\n}\n\n/**\n * Emitted when a delta is added to a reasoning text.\n */\nexport interface ResponseReasoningTextDeltaEvent {\n  /**\n   * The index of the reasoning content part this delta is associated with.\n   */\n  content_index: number;\n\n  /**\n   * The text delta that was added to the reasoning content.\n   */\n  delta: string;\n\n  /**\n   * The ID of the item this reasoning text delta is associated with.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item this reasoning text delta is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.reasoning_text.delta`.\n   */\n  type: 'response.reasoning_text.delta';\n}\n\n/**\n * Emitted when a reasoning text is completed.\n */\nexport interface ResponseReasoningTextDoneEvent {\n  /**\n   * The index of the reasoning content part.\n   */\n  content_index: number;\n\n  /**\n   * The ID of the item this reasoning text is associated with.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item this reasoning text is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The full text of the completed reasoning content.\n   */\n  text: string;\n\n  /**\n   * The type of the event. Always `response.reasoning_text.done`.\n   */\n  type: 'response.reasoning_text.done';\n}\n\n/**\n * Emitted when there is a partial refusal text.\n */\nexport interface ResponseRefusalDeltaEvent {\n  /**\n   * The index of the content part that the refusal text is added to.\n   */\n  content_index: number;\n\n  /**\n   * The refusal text that is added.\n   */\n  delta: string;\n\n  /**\n   * The ID of the output item that the refusal text is added to.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the refusal text is added to.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.refusal.delta`.\n   */\n  type: 'response.refusal.delta';\n}\n\n/**\n * Emitted when refusal text is finalized.\n */\nexport interface ResponseRefusalDoneEvent {\n  /**\n   * The index of the content part that the refusal text is finalized.\n   */\n  content_index: number;\n\n  /**\n   * The ID of the output item that the refusal text is finalized.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the refusal text is finalized.\n   */\n  output_index: number;\n\n  /**\n   * The refusal text that is finalized.\n   */\n  refusal: string;\n\n  /**\n   * The sequence number of this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.refusal.done`.\n   */\n  type: 'response.refusal.done';\n}\n\n/**\n * The status of the response generation. One of `completed`, `failed`,\n * `in_progress`, `cancelled`, `queued`, or `incomplete`.\n */\nexport type ResponseStatus = 'completed' | 'failed' | 'in_progress' | 'cancelled' | 'queued' | 'incomplete';\n\n/**\n * Emitted when there is a partial audio response.\n */\nexport type ResponseStreamEvent =\n  | ResponseAudioDeltaEvent\n  | ResponseAudioDoneEvent\n  | ResponseAudioTranscriptDeltaEvent\n  | ResponseAudioTranscriptDoneEvent\n  | ResponseCodeInterpreterCallCodeDeltaEvent\n  | ResponseCodeInterpreterCallCodeDoneEvent\n  | ResponseCodeInterpreterCallCompletedEvent\n  | ResponseCodeInterpreterCallInProgressEvent\n  | ResponseCodeInterpreterCallInterpretingEvent\n  | ResponseCompletedEvent\n  | ResponseContentPartAddedEvent\n  | ResponseContentPartDoneEvent\n  | ResponseCreatedEvent\n  | ResponseErrorEvent\n  | ResponseFileSearchCallCompletedEvent\n  | ResponseFileSearchCallInProgressEvent\n  | ResponseFileSearchCallSearchingEvent\n  | ResponseFunctionCallArgumentsDeltaEvent\n  | ResponseFunctionCallArgumentsDoneEvent\n  | ResponseInProgressEvent\n  | ResponseFailedEvent\n  | ResponseIncompleteEvent\n  | ResponseOutputItemAddedEvent\n  | ResponseOutputItemDoneEvent\n  | ResponseReasoningSummaryPartAddedEvent\n  | ResponseReasoningSummaryPartDoneEvent\n  | ResponseReasoningSummaryTextDeltaEvent\n  | ResponseReasoningSummaryTextDoneEvent\n  | ResponseReasoningTextDeltaEvent\n  | ResponseReasoningTextDoneEvent\n  | ResponseRefusalDeltaEvent\n  | ResponseRefusalDoneEvent\n  | ResponseTextDeltaEvent\n  | ResponseTextDoneEvent\n  | ResponseWebSearchCallCompletedEvent\n  | ResponseWebSearchCallInProgressEvent\n  | ResponseWebSearchCallSearchingEvent\n  | ResponseImageGenCallCompletedEvent\n  | ResponseImageGenCallGeneratingEvent\n  | ResponseImageGenCallInProgressEvent\n  | ResponseImageGenCallPartialImageEvent\n  | ResponseMcpCallArgumentsDeltaEvent\n  | ResponseMcpCallArgumentsDoneEvent\n  | ResponseMcpCallCompletedEvent\n  | ResponseMcpCallFailedEvent\n  | ResponseMcpCallInProgressEvent\n  | ResponseMcpListToolsCompletedEvent\n  | ResponseMcpListToolsFailedEvent\n  | ResponseMcpListToolsInProgressEvent\n  | ResponseOutputTextAnnotationAddedEvent\n  | ResponseQueuedEvent\n  | ResponseCustomToolCallInputDeltaEvent\n  | ResponseCustomToolCallInputDoneEvent;\n\n/**\n * Configuration options for a text response from the model. Can be plain text or\n * structured JSON data. Learn more:\n *\n * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n */\nexport interface ResponseTextConfig {\n  /**\n   * An object specifying the format that the model must output.\n   *\n   * Configuring `{ \"type\": \"json_schema\" }` enables Structured Outputs, which\n   * ensures the model will match your supplied JSON schema. Learn more in the\n   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\n   *\n   * The default format is `{ \"type\": \"text\" }` with no additional options.\n   *\n   * **Not recommended for gpt-4o and newer models:**\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n   * ensures the message the model generates is valid JSON. Using `json_schema` is\n   * preferred for models that support it.\n   */\n  format?: ResponseFormatTextConfig;\n\n  /**\n   * Constrains the verbosity of the model's response. Lower values will result in\n   * more concise responses, while higher values will result in more verbose\n   * responses. Currently supported values are `low`, `medium`, and `high`.\n   */\n  verbosity?: 'low' | 'medium' | 'high' | null;\n}\n\n/**\n * Emitted when there is an additional text delta.\n */\nexport interface ResponseTextDeltaEvent {\n  /**\n   * The index of the content part that the text delta was added to.\n   */\n  content_index: number;\n\n  /**\n   * The text delta that was added.\n   */\n  delta: string;\n\n  /**\n   * The ID of the output item that the text delta was added to.\n   */\n  item_id: string;\n\n  /**\n   * The log probabilities of the tokens in the delta.\n   */\n  logprobs: Array<ResponseTextDeltaEvent.Logprob>;\n\n  /**\n   * The index of the output item that the text delta was added to.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number for this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.output_text.delta`.\n   */\n  type: 'response.output_text.delta';\n}\n\nexport namespace ResponseTextDeltaEvent {\n  /**\n   * A logprob is the logarithmic probability that the model assigns to producing a\n   * particular token at a given position in the sequence. Less-negative (higher)\n   * logprob values indicate greater model confidence in that token choice.\n   */\n  export interface Logprob {\n    /**\n     * A possible text token.\n     */\n    token: string;\n\n    /**\n     * The log probability of this token.\n     */\n    logprob: number;\n\n    /**\n     * The log probability of the top 20 most likely tokens.\n     */\n    top_logprobs?: Array<Logprob.TopLogprob>;\n  }\n\n  export namespace Logprob {\n    export interface TopLogprob {\n      /**\n       * A possible text token.\n       */\n      token?: string;\n\n      /**\n       * The log probability of this token.\n       */\n      logprob?: number;\n    }\n  }\n}\n\n/**\n * Emitted when text content is finalized.\n */\nexport interface ResponseTextDoneEvent {\n  /**\n   * The index of the content part that the text content is finalized.\n   */\n  content_index: number;\n\n  /**\n   * The ID of the output item that the text content is finalized.\n   */\n  item_id: string;\n\n  /**\n   * The log probabilities of the tokens in the delta.\n   */\n  logprobs: Array<ResponseTextDoneEvent.Logprob>;\n\n  /**\n   * The index of the output item that the text content is finalized.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number for this event.\n   */\n  sequence_number: number;\n\n  /**\n   * The text content that is finalized.\n   */\n  text: string;\n\n  /**\n   * The type of the event. Always `response.output_text.done`.\n   */\n  type: 'response.output_text.done';\n}\n\nexport namespace ResponseTextDoneEvent {\n  /**\n   * A logprob is the logarithmic probability that the model assigns to producing a\n   * particular token at a given position in the sequence. Less-negative (higher)\n   * logprob values indicate greater model confidence in that token choice.\n   */\n  export interface Logprob {\n    /**\n     * A possible text token.\n     */\n    token: string;\n\n    /**\n     * The log probability of this token.\n     */\n    logprob: number;\n\n    /**\n     * The log probability of the top 20 most likely tokens.\n     */\n    top_logprobs?: Array<Logprob.TopLogprob>;\n  }\n\n  export namespace Logprob {\n    export interface TopLogprob {\n      /**\n       * A possible text token.\n       */\n      token?: string;\n\n      /**\n       * The log probability of this token.\n       */\n      logprob?: number;\n    }\n  }\n}\n\n/**\n * Represents token usage details including input tokens, output tokens, a\n * breakdown of output tokens, and the total tokens used.\n */\nexport interface ResponseUsage {\n  /**\n   * The number of input tokens.\n   */\n  input_tokens: number;\n\n  /**\n   * A detailed breakdown of the input tokens.\n   */\n  input_tokens_details: ResponseUsage.InputTokensDetails;\n\n  /**\n   * The number of output tokens.\n   */\n  output_tokens: number;\n\n  /**\n   * A detailed breakdown of the output tokens.\n   */\n  output_tokens_details: ResponseUsage.OutputTokensDetails;\n\n  /**\n   * The total number of tokens used.\n   */\n  total_tokens: number;\n}\n\nexport namespace ResponseUsage {\n  /**\n   * A detailed breakdown of the input tokens.\n   */\n  export interface InputTokensDetails {\n    /**\n     * The number of tokens that were retrieved from the cache.\n     * [More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    cached_tokens: number;\n  }\n\n  /**\n   * A detailed breakdown of the output tokens.\n   */\n  export interface OutputTokensDetails {\n    /**\n     * The number of reasoning tokens.\n     */\n    reasoning_tokens: number;\n  }\n}\n\n/**\n * Emitted when a web search call is completed.\n */\nexport interface ResponseWebSearchCallCompletedEvent {\n  /**\n   * Unique ID for the output item associated with the web search call.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the web search call is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of the web search call being processed.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.web_search_call.completed`.\n   */\n  type: 'response.web_search_call.completed';\n}\n\n/**\n * Emitted when a web search call is initiated.\n */\nexport interface ResponseWebSearchCallInProgressEvent {\n  /**\n   * Unique ID for the output item associated with the web search call.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the web search call is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of the web search call being processed.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.web_search_call.in_progress`.\n   */\n  type: 'response.web_search_call.in_progress';\n}\n\n/**\n * Emitted when a web search call is executing.\n */\nexport interface ResponseWebSearchCallSearchingEvent {\n  /**\n   * Unique ID for the output item associated with the web search call.\n   */\n  item_id: string;\n\n  /**\n   * The index of the output item that the web search call is associated with.\n   */\n  output_index: number;\n\n  /**\n   * The sequence number of the web search call being processed.\n   */\n  sequence_number: number;\n\n  /**\n   * The type of the event. Always `response.web_search_call.searching`.\n   */\n  type: 'response.web_search_call.searching';\n}\n\nexport interface ResponsesClientEvent {\n  /**\n   * The type of the client event. Always `response.create`.\n   */\n  type: 'response.create';\n\n  /**\n   * Whether to run the model response in the background.\n   * [Learn more](https://platform.openai.com/docs/guides/background).\n   */\n  background?: boolean | null;\n\n  /**\n   * Context management configuration for this request.\n   */\n  context_management?: Array<ResponsesClientEvent.ContextManagement> | null;\n\n  /**\n   * The conversation that this response belongs to. Items from this conversation are\n   * prepended to `input_items` for this response request. Input items and output\n   * items from this response are automatically added to this conversation after this\n   * response completes.\n   */\n  conversation?: string | ResponseConversationParam | null;\n\n  /**\n   * Specify additional output data to include in the model response. Currently\n   * supported values are:\n   *\n   * - `web_search_call.action.sources`: Include the sources of the web search tool\n   *   call.\n   * - `code_interpreter_call.outputs`: Includes the outputs of python code execution\n   *   in code interpreter tool call items.\n   * - `computer_call_output.output.image_url`: Include image urls from the computer\n   *   call output.\n   * - `file_search_call.results`: Include the search results of the file search tool\n   *   call.\n   * - `message.input_image.image_url`: Include image urls from the input message.\n   * - `message.output_text.logprobs`: Include logprobs with assistant messages.\n   * - `reasoning.encrypted_content`: Includes an encrypted version of reasoning\n   *   tokens in reasoning item outputs. This enables reasoning items to be used in\n   *   multi-turn conversations when using the Responses API statelessly (like when\n   *   the `store` parameter is set to `false`, or when an organization is enrolled\n   *   in the zero data retention program).\n   */\n  include?: Array<ResponseIncludable> | null;\n\n  /**\n   * Text, image, or file inputs to the model, used to generate a response.\n   *\n   * Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Image inputs](https://platform.openai.com/docs/guides/images)\n   * - [File inputs](https://platform.openai.com/docs/guides/pdf-files)\n   * - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)\n   * - [Function calling](https://platform.openai.com/docs/guides/function-calling)\n   */\n  input?: string | ResponseInput;\n\n  /**\n   * A system (or developer) message inserted into the model's context.\n   *\n   * When using along with `previous_response_id`, the instructions from a previous\n   * response will not be carried over to the next response. This makes it simple to\n   * swap out system (or developer) messages in new responses.\n   */\n  instructions?: string | null;\n\n  /**\n   * An upper bound for the number of tokens that can be generated for a response,\n   * including visible output tokens and\n   * [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\n   */\n  max_output_tokens?: number | null;\n\n  /**\n   * The maximum number of total calls to built-in tools that can be processed in a\n   * response. This maximum number applies across all built-in tool calls, not per\n   * individual tool. Any further attempts to call a tool by the model will be\n   * ignored.\n   */\n  max_tool_calls?: number | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a\n   * wide range of models with different capabilities, performance characteristics,\n   * and price points. Refer to the\n   * [model guide](https://platform.openai.com/docs/models) to browse and compare\n   * available models.\n   */\n  model?: Shared.ResponsesModel;\n\n  /**\n   * Whether to allow the model to run tool calls in parallel.\n   */\n  parallel_tool_calls?: boolean | null;\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create\n   * multi-turn conversations. Learn more about\n   * [conversation state](https://platform.openai.com/docs/guides/conversation-state).\n   * Cannot be used in conjunction with `conversation`.\n   */\n  previous_response_id?: string | null;\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsePrompt | null;\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates. Replaces the `user` field.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  prompt_cache_key?: string;\n\n  /**\n   * The retention policy for the prompt cache. Set to `24h` to enable extended\n   * prompt caching, which keeps cached prefixes active for longer, up to a maximum\n   * of 24 hours.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching#prompt-cache-retention).\n   */\n  prompt_cache_retention?: 'in-memory' | '24h' | null;\n\n  /**\n   * **gpt-5 and o-series models only**\n   *\n   * Configuration options for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n   */\n  reasoning?: Shared.Reasoning | null;\n\n  /**\n   * A stable identifier used to help detect users of your application that may be\n   * violating OpenAI's usage policies. The IDs should be a string that uniquely\n   * identifies each user, with a maximum length of 64 characters. We recommend\n   * hashing their username or email address, in order to avoid sending us any\n   * identifying information.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  safety_identifier?: string;\n\n  /**\n   * Specifies the processing type used for serving the request.\n   *\n   * - If set to 'auto', then the request will be processed with the service tier\n   *   configured in the Project settings. Unless otherwise configured, the Project\n   *   will use 'default'.\n   * - If set to 'default', then the request will be processed with the standard\n   *   pricing and performance for the selected model.\n   * - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\n   *   '[priority](https://openai.com/api-priority-processing/)', then the request\n   *   will be processed with the corresponding service tier.\n   * - When not set, the default behavior is 'auto'.\n   *\n   * When the `service_tier` parameter is set, the response body will include the\n   * `service_tier` value based on the processing mode actually used to serve the\n   * request. This response value may be different from the value set in the\n   * parameter.\n   */\n  service_tier?: 'auto' | 'default' | 'flex' | 'scale' | 'priority' | null;\n\n  /**\n   * Whether to store the generated model response for later retrieval via API.\n   */\n  store?: boolean | null;\n\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n   * for more information.\n   */\n  stream?: boolean | null;\n\n  /**\n   * Options for streaming responses. Only set this when you set `stream: true`.\n   */\n  stream_options?: ResponsesClientEvent.StreamOptions | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic. We generally recommend altering this or `top_p` but\n   * not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data. Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n   */\n  text?: ResponseTextConfig;\n\n  /**\n   * How the model should select which tool (or tools) to use when generating a\n   * response. See the `tools` parameter to see how to specify which tools the model\n   * can call.\n   */\n  tool_choice?:\n    | ToolChoiceOptions\n    | ToolChoiceAllowed\n    | ToolChoiceTypes\n    | ToolChoiceFunction\n    | ToolChoiceMcp\n    | ToolChoiceCustom\n    | ToolChoiceApplyPatch\n    | ToolChoiceShell;\n\n  /**\n   * An array of tools the model may call while generating a response. You can\n   * specify which tool to use by setting the `tool_choice` parameter.\n   *\n   * We support the following categories of tools:\n   *\n   * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n   *   capabilities, like\n   *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n   *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n   *   Learn more about\n   *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n   * - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n   *   predefined connectors such as Google Drive and SharePoint. Learn more about\n   *   [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n   * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n   *   the model to call your own code with strongly typed arguments and outputs.\n   *   Learn more about\n   *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n   *   You can also use custom tools to call your own code.\n   */\n  tools?: Array<Tool>;\n\n  /**\n   * An integer between 0 and 20 specifying the number of most likely tokens to\n   * return at each token position, each with an associated log probability.\n   */\n  top_logprobs?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * The truncation strategy to use for the model response.\n   *\n   * - `auto`: If the input to this Response exceeds the model's context window size,\n   *   the model will truncate the response to fit the context window by dropping\n   *   items from the beginning of the conversation.\n   * - `disabled` (default): If the input size will exceed the context window size\n   *   for a model, the request will fail with a 400 error.\n   */\n  truncation?: 'auto' | 'disabled' | null;\n\n  /**\n   * @deprecated This field is being replaced by `safety_identifier` and\n   * `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching\n   * optimizations. A stable identifier for your end-users. Used to boost cache hit\n   * rates by better bucketing similar requests and to help OpenAI detect and prevent\n   * abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  user?: string;\n}\n\nexport namespace ResponsesClientEvent {\n  export interface ContextManagement {\n    /**\n     * The context management entry type. Currently only 'compaction' is supported.\n     */\n    type: string;\n\n    /**\n     * Token threshold at which compaction should be triggered for this entry.\n     */\n    compact_threshold?: number | null;\n  }\n\n  /**\n   * Options for streaming responses. Only set this when you set `stream: true`.\n   */\n  export interface StreamOptions {\n    /**\n     * When true, stream obfuscation will be enabled. Stream obfuscation adds random\n     * characters to an `obfuscation` field on streaming delta events to normalize\n     * payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n     * fields are included by default, but add a small amount of overhead to the data\n     * stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n     * you trust the network links between your application and the OpenAI API.\n     */\n    include_obfuscation?: boolean;\n  }\n}\n\n/**\n * Server events emitted by the Responses WebSocket server.\n */\nexport type ResponsesServerEvent =\n  | ResponseAudioDeltaEvent\n  | ResponseAudioDoneEvent\n  | ResponseAudioTranscriptDeltaEvent\n  | ResponseAudioTranscriptDoneEvent\n  | ResponseCodeInterpreterCallCodeDeltaEvent\n  | ResponseCodeInterpreterCallCodeDoneEvent\n  | ResponseCodeInterpreterCallCompletedEvent\n  | ResponseCodeInterpreterCallInProgressEvent\n  | ResponseCodeInterpreterCallInterpretingEvent\n  | ResponseCompletedEvent\n  | ResponseContentPartAddedEvent\n  | ResponseContentPartDoneEvent\n  | ResponseCreatedEvent\n  | ResponseErrorEvent\n  | ResponseFileSearchCallCompletedEvent\n  | ResponseFileSearchCallInProgressEvent\n  | ResponseFileSearchCallSearchingEvent\n  | ResponseFunctionCallArgumentsDeltaEvent\n  | ResponseFunctionCallArgumentsDoneEvent\n  | ResponseInProgressEvent\n  | ResponseFailedEvent\n  | ResponseIncompleteEvent\n  | ResponseOutputItemAddedEvent\n  | ResponseOutputItemDoneEvent\n  | ResponseReasoningSummaryPartAddedEvent\n  | ResponseReasoningSummaryPartDoneEvent\n  | ResponseReasoningSummaryTextDeltaEvent\n  | ResponseReasoningSummaryTextDoneEvent\n  | ResponseReasoningTextDeltaEvent\n  | ResponseReasoningTextDoneEvent\n  | ResponseRefusalDeltaEvent\n  | ResponseRefusalDoneEvent\n  | ResponseTextDeltaEvent\n  | ResponseTextDoneEvent\n  | ResponseWebSearchCallCompletedEvent\n  | ResponseWebSearchCallInProgressEvent\n  | ResponseWebSearchCallSearchingEvent\n  | ResponseImageGenCallCompletedEvent\n  | ResponseImageGenCallGeneratingEvent\n  | ResponseImageGenCallInProgressEvent\n  | ResponseImageGenCallPartialImageEvent\n  | ResponseMcpCallArgumentsDeltaEvent\n  | ResponseMcpCallArgumentsDoneEvent\n  | ResponseMcpCallCompletedEvent\n  | ResponseMcpCallFailedEvent\n  | ResponseMcpCallInProgressEvent\n  | ResponseMcpListToolsCompletedEvent\n  | ResponseMcpListToolsFailedEvent\n  | ResponseMcpListToolsInProgressEvent\n  | ResponseOutputTextAnnotationAddedEvent\n  | ResponseQueuedEvent\n  | ResponseCustomToolCallInputDeltaEvent\n  | ResponseCustomToolCallInputDoneEvent;\n\nexport interface SkillReference {\n  /**\n   * The ID of the referenced skill.\n   */\n  skill_id: string;\n\n  /**\n   * References a skill created with the /v1/skills endpoint.\n   */\n  type: 'skill_reference';\n\n  /**\n   * Optional skill version. Use a positive integer or 'latest'. Omit for default.\n   */\n  version?: string;\n}\n\n/**\n * A tool that can be used to generate a response.\n */\nexport type Tool =\n  | FunctionTool\n  | FileSearchTool\n  | ComputerTool\n  | WebSearchTool\n  | Tool.Mcp\n  | Tool.CodeInterpreter\n  | Tool.ImageGeneration\n  | Tool.LocalShell\n  | FunctionShellTool\n  | CustomTool\n  | WebSearchPreviewTool\n  | ApplyPatchTool;\n\nexport namespace Tool {\n  /**\n   * Give the model access to additional tools via remote Model Context Protocol\n   * (MCP) servers.\n   * [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n   */\n  export interface Mcp {\n    /**\n     * A label for this MCP server, used to identify it in tool calls.\n     */\n    server_label: string;\n\n    /**\n     * The type of the MCP tool. Always `mcp`.\n     */\n    type: 'mcp';\n\n    /**\n     * List of allowed tool names or a filter object.\n     */\n    allowed_tools?: Array<string> | Mcp.McpToolFilter | null;\n\n    /**\n     * An OAuth access token that can be used with a remote MCP server, either with a\n     * custom MCP server URL or a service connector. Your application must handle the\n     * OAuth authorization flow and provide the token here.\n     */\n    authorization?: string;\n\n    /**\n     * Identifier for service connectors, like those available in ChatGPT. One of\n     * `server_url` or `connector_id` must be provided. Learn more about service\n     * connectors\n     * [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n     *\n     * Currently supported `connector_id` values are:\n     *\n     * - Dropbox: `connector_dropbox`\n     * - Gmail: `connector_gmail`\n     * - Google Calendar: `connector_googlecalendar`\n     * - Google Drive: `connector_googledrive`\n     * - Microsoft Teams: `connector_microsoftteams`\n     * - Outlook Calendar: `connector_outlookcalendar`\n     * - Outlook Email: `connector_outlookemail`\n     * - SharePoint: `connector_sharepoint`\n     */\n    connector_id?:\n      | 'connector_dropbox'\n      | 'connector_gmail'\n      | 'connector_googlecalendar'\n      | 'connector_googledrive'\n      | 'connector_microsoftteams'\n      | 'connector_outlookcalendar'\n      | 'connector_outlookemail'\n      | 'connector_sharepoint';\n\n    /**\n     * Optional HTTP headers to send to the MCP server. Use for authentication or other\n     * purposes.\n     */\n    headers?: { [key: string]: string } | null;\n\n    /**\n     * Specify which of the MCP server's tools require approval.\n     */\n    require_approval?: Mcp.McpToolApprovalFilter | 'always' | 'never' | null;\n\n    /**\n     * Optional description of the MCP server, used to provide more context.\n     */\n    server_description?: string;\n\n    /**\n     * The URL for the MCP server. One of `server_url` or `connector_id` must be\n     * provided.\n     */\n    server_url?: string;\n  }\n\n  export namespace Mcp {\n    /**\n     * A filter object to specify which tools are allowed.\n     */\n    export interface McpToolFilter {\n      /**\n       * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n       * is\n       * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n       * it will match this filter.\n       */\n      read_only?: boolean;\n\n      /**\n       * List of allowed tool names.\n       */\n      tool_names?: Array<string>;\n    }\n\n    /**\n     * Specify which of the MCP server's tools require approval. Can be `always`,\n     * `never`, or a filter object associated with tools that require approval.\n     */\n    export interface McpToolApprovalFilter {\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      always?: McpToolApprovalFilter.Always;\n\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      never?: McpToolApprovalFilter.Never;\n    }\n\n    export namespace McpToolApprovalFilter {\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      export interface Always {\n        /**\n         * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n         * is\n         * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n         * it will match this filter.\n         */\n        read_only?: boolean;\n\n        /**\n         * List of allowed tool names.\n         */\n        tool_names?: Array<string>;\n      }\n\n      /**\n       * A filter object to specify which tools are allowed.\n       */\n      export interface Never {\n        /**\n         * Indicates whether or not a tool modifies data or is read-only. If an MCP server\n         * is\n         * [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n         * it will match this filter.\n         */\n        read_only?: boolean;\n\n        /**\n         * List of allowed tool names.\n         */\n        tool_names?: Array<string>;\n      }\n    }\n  }\n\n  /**\n   * A tool that runs Python code to help generate a response to a prompt.\n   */\n  export interface CodeInterpreter {\n    /**\n     * The code interpreter container. Can be a container ID or an object that\n     * specifies uploaded file IDs to make available to your code, along with an\n     * optional `memory_limit` setting.\n     */\n    container: string | CodeInterpreter.CodeInterpreterToolAuto;\n\n    /**\n     * The type of the code interpreter tool. Always `code_interpreter`.\n     */\n    type: 'code_interpreter';\n  }\n\n  export namespace CodeInterpreter {\n    /**\n     * Configuration for a code interpreter container. Optionally specify the IDs of\n     * the files to run the code on.\n     */\n    export interface CodeInterpreterToolAuto {\n      /**\n       * Always `auto`.\n       */\n      type: 'auto';\n\n      /**\n       * An optional list of uploaded files to make available to your code.\n       */\n      file_ids?: Array<string>;\n\n      /**\n       * The memory limit for the code interpreter container.\n       */\n      memory_limit?: '1g' | '4g' | '16g' | '64g' | null;\n\n      /**\n       * Network access policy for the container.\n       */\n      network_policy?:\n        | ResponsesAPI.ContainerNetworkPolicyDisabled\n        | ResponsesAPI.ContainerNetworkPolicyAllowlist;\n    }\n  }\n\n  /**\n   * A tool that generates images using the GPT image models.\n   */\n  export interface ImageGeneration {\n    /**\n     * The type of the image generation tool. Always `image_generation`.\n     */\n    type: 'image_generation';\n\n    /**\n     * Whether to generate a new image or edit an existing image. Default: `auto`.\n     */\n    action?: 'generate' | 'edit' | 'auto';\n\n    /**\n     * Background type for the generated image. One of `transparent`, `opaque`, or\n     * `auto`. Default: `auto`.\n     */\n    background?: 'transparent' | 'opaque' | 'auto';\n\n    /**\n     * Control how much effort the model will exert to match the style and features,\n     * especially facial features, of input images. This parameter is only supported\n     * for `gpt-image-1` and `gpt-image-1.5` and later models, unsupported for\n     * `gpt-image-1-mini`. Supports `high` and `low`. Defaults to `low`.\n     */\n    input_fidelity?: 'high' | 'low' | null;\n\n    /**\n     * Optional mask for inpainting. Contains `image_url` (string, optional) and\n     * `file_id` (string, optional).\n     */\n    input_image_mask?: ImageGeneration.InputImageMask;\n\n    /**\n     * The image generation model to use. Default: `gpt-image-1`.\n     */\n    model?: (string & {}) | 'gpt-image-1' | 'gpt-image-1-mini' | 'gpt-image-1.5';\n\n    /**\n     * Moderation level for the generated image. Default: `auto`.\n     */\n    moderation?: 'auto' | 'low';\n\n    /**\n     * Compression level for the output image. Default: 100.\n     */\n    output_compression?: number;\n\n    /**\n     * The output format of the generated image. One of `png`, `webp`, or `jpeg`.\n     * Default: `png`.\n     */\n    output_format?: 'png' | 'webp' | 'jpeg';\n\n    /**\n     * Number of partial images to generate in streaming mode, from 0 (default value)\n     * to 3.\n     */\n    partial_images?: number;\n\n    /**\n     * The quality of the generated image. One of `low`, `medium`, `high`, or `auto`.\n     * Default: `auto`.\n     */\n    quality?: 'low' | 'medium' | 'high' | 'auto';\n\n    /**\n     * The size of the generated image. One of `1024x1024`, `1024x1536`, `1536x1024`,\n     * or `auto`. Default: `auto`.\n     */\n    size?: '1024x1024' | '1024x1536' | '1536x1024' | 'auto';\n  }\n\n  export namespace ImageGeneration {\n    /**\n     * Optional mask for inpainting. Contains `image_url` (string, optional) and\n     * `file_id` (string, optional).\n     */\n    export interface InputImageMask {\n      /**\n       * File ID for the mask image.\n       */\n      file_id?: string;\n\n      /**\n       * Base64-encoded mask image.\n       */\n      image_url?: string;\n    }\n  }\n\n  /**\n   * A tool that allows the model to execute shell commands in a local environment.\n   */\n  export interface LocalShell {\n    /**\n     * The type of the local shell tool. Always `local_shell`.\n     */\n    type: 'local_shell';\n  }\n}\n\n/**\n * Constrains the tools available to the model to a pre-defined set.\n */\nexport interface ToolChoiceAllowed {\n  /**\n   * Constrains the tools available to the model to a pre-defined set.\n   *\n   * `auto` allows the model to pick from among the allowed tools and generate a\n   * message.\n   *\n   * `required` requires the model to call one or more of the allowed tools.\n   */\n  mode: 'auto' | 'required';\n\n  /**\n   * A list of tool definitions that the model should be allowed to call.\n   *\n   * For the Responses API, the list of tool definitions might look like:\n   *\n   * ```json\n   * [\n   *   { \"type\": \"function\", \"name\": \"get_weather\" },\n   *   { \"type\": \"mcp\", \"server_label\": \"deepwiki\" },\n   *   { \"type\": \"image_generation\" }\n   * ]\n   * ```\n   */\n  tools: Array<{ [key: string]: unknown }>;\n\n  /**\n   * Allowed tool configuration type. Always `allowed_tools`.\n   */\n  type: 'allowed_tools';\n}\n\n/**\n * Forces the model to call the apply_patch tool when executing a tool call.\n */\nexport interface ToolChoiceApplyPatch {\n  /**\n   * The tool to call. Always `apply_patch`.\n   */\n  type: 'apply_patch';\n}\n\n/**\n * Use this option to force the model to call a specific custom tool.\n */\nexport interface ToolChoiceCustom {\n  /**\n   * The name of the custom tool to call.\n   */\n  name: string;\n\n  /**\n   * For custom tool calling, the type is always `custom`.\n   */\n  type: 'custom';\n}\n\n/**\n * Use this option to force the model to call a specific function.\n */\nexport interface ToolChoiceFunction {\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n\n  /**\n   * For function calling, the type is always `function`.\n   */\n  type: 'function';\n}\n\n/**\n * Use this option to force the model to call a specific tool on a remote MCP\n * server.\n */\nexport interface ToolChoiceMcp {\n  /**\n   * The label of the MCP server to use.\n   */\n  server_label: string;\n\n  /**\n   * For MCP tools, the type is always `mcp`.\n   */\n  type: 'mcp';\n\n  /**\n   * The name of the tool to call on the server.\n   */\n  name?: string | null;\n}\n\n/**\n * Controls which (if any) tool is called by the model.\n *\n * `none` means the model will not call any tool and instead generates a message.\n *\n * `auto` means the model can pick between generating a message or calling one or\n * more tools.\n *\n * `required` means the model must call one or more tools.\n */\nexport type ToolChoiceOptions = 'none' | 'auto' | 'required';\n\n/**\n * Forces the model to call the shell tool when a tool call is required.\n */\nexport interface ToolChoiceShell {\n  /**\n   * The tool to call. Always `shell`.\n   */\n  type: 'shell';\n}\n\n/**\n * Indicates that the model should use a built-in tool to generate a response.\n * [Learn more about built-in tools](https://platform.openai.com/docs/guides/tools).\n */\nexport interface ToolChoiceTypes {\n  /**\n   * The type of hosted tool the model should to use. Learn more about\n   * [built-in tools](https://platform.openai.com/docs/guides/tools).\n   *\n   * Allowed values are:\n   *\n   * - `file_search`\n   * - `web_search_preview`\n   * - `computer_use_preview`\n   * - `code_interpreter`\n   * - `mcp`\n   * - `image_generation`\n   */\n  type:\n    | 'file_search'\n    | 'web_search_preview'\n    | 'computer_use_preview'\n    | 'web_search_preview_2025_03_11'\n    | 'image_generation'\n    | 'code_interpreter'\n    | 'mcp';\n}\n\n/**\n * This tool searches the web for relevant results to use in a response. Learn more\n * about the\n * [web search tool](https://platform.openai.com/docs/guides/tools-web-search).\n */\nexport interface WebSearchPreviewTool {\n  /**\n   * The type of the web search tool. One of `web_search_preview` or\n   * `web_search_preview_2025_03_11`.\n   */\n  type: 'web_search_preview' | 'web_search_preview_2025_03_11';\n\n  /**\n   * High level guidance for the amount of context window space to use for the\n   * search. One of `low`, `medium`, or `high`. `medium` is the default.\n   */\n  search_context_size?: 'low' | 'medium' | 'high';\n\n  /**\n   * The user's location.\n   */\n  user_location?: WebSearchPreviewTool.UserLocation | null;\n}\n\nexport namespace WebSearchPreviewTool {\n  /**\n   * The user's location.\n   */\n  export interface UserLocation {\n    /**\n     * The type of location approximation. Always `approximate`.\n     */\n    type: 'approximate';\n\n    /**\n     * Free text input for the city of the user, e.g. `San Francisco`.\n     */\n    city?: string | null;\n\n    /**\n     * The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of\n     * the user, e.g. `US`.\n     */\n    country?: string | null;\n\n    /**\n     * Free text input for the region of the user, e.g. `California`.\n     */\n    region?: string | null;\n\n    /**\n     * The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the\n     * user, e.g. `America/Los_Angeles`.\n     */\n    timezone?: string | null;\n  }\n}\n\n/**\n * Search the Internet for sources related to the prompt. Learn more about the\n * [web search tool](https://platform.openai.com/docs/guides/tools-web-search).\n */\nexport interface WebSearchTool {\n  /**\n   * The type of the web search tool. One of `web_search` or `web_search_2025_08_26`.\n   */\n  type: 'web_search' | 'web_search_2025_08_26';\n\n  /**\n   * Filters for the search.\n   */\n  filters?: WebSearchTool.Filters | null;\n\n  /**\n   * High level guidance for the amount of context window space to use for the\n   * search. One of `low`, `medium`, or `high`. `medium` is the default.\n   */\n  search_context_size?: 'low' | 'medium' | 'high';\n\n  /**\n   * The approximate location of the user.\n   */\n  user_location?: WebSearchTool.UserLocation | null;\n}\n\nexport namespace WebSearchTool {\n  /**\n   * Filters for the search.\n   */\n  export interface Filters {\n    /**\n     * Allowed domains for the search. If not provided, all domains are allowed.\n     * Subdomains of the provided domains are allowed as well.\n     *\n     * Example: `[\"pubmed.ncbi.nlm.nih.gov\"]`\n     */\n    allowed_domains?: Array<string> | null;\n  }\n\n  /**\n   * The approximate location of the user.\n   */\n  export interface UserLocation {\n    /**\n     * Free text input for the city of the user, e.g. `San Francisco`.\n     */\n    city?: string | null;\n\n    /**\n     * The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of\n     * the user, e.g. `US`.\n     */\n    country?: string | null;\n\n    /**\n     * Free text input for the region of the user, e.g. `California`.\n     */\n    region?: string | null;\n\n    /**\n     * The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the\n     * user, e.g. `America/Los_Angeles`.\n     */\n    timezone?: string | null;\n\n    /**\n     * The type of location approximation. Always `approximate`.\n     */\n    type?: 'approximate';\n  }\n}\n\nexport type ResponseCreateParams = ResponseCreateParamsNonStreaming | ResponseCreateParamsStreaming;\n\nexport interface ResponseCreateParamsBase {\n  /**\n   * Whether to run the model response in the background.\n   * [Learn more](https://platform.openai.com/docs/guides/background).\n   */\n  background?: boolean | null;\n\n  /**\n   * Context management configuration for this request.\n   */\n  context_management?: Array<ResponseCreateParams.ContextManagement> | null;\n\n  /**\n   * The conversation that this response belongs to. Items from this conversation are\n   * prepended to `input_items` for this response request. Input items and output\n   * items from this response are automatically added to this conversation after this\n   * response completes.\n   */\n  conversation?: string | ResponseConversationParam | null;\n\n  /**\n   * Specify additional output data to include in the model response. Currently\n   * supported values are:\n   *\n   * - `web_search_call.action.sources`: Include the sources of the web search tool\n   *   call.\n   * - `code_interpreter_call.outputs`: Includes the outputs of python code execution\n   *   in code interpreter tool call items.\n   * - `computer_call_output.output.image_url`: Include image urls from the computer\n   *   call output.\n   * - `file_search_call.results`: Include the search results of the file search tool\n   *   call.\n   * - `message.input_image.image_url`: Include image urls from the input message.\n   * - `computer_call_output.output.image_url`: Include image urls from the computer\n   *   call output.\n   * - `reasoning.encrypted_content`: Includes an encrypted version of reasoning\n   *   tokens in reasoning item outputs. This enables reasoning items to be used in\n   *   multi-turn conversations when using the Responses API statelessly (like when\n   *   the `store` parameter is set to `false`, or when an organization is enrolled\n   *   in the zero data retention program).\n   * - `code_interpreter_call.outputs`: Includes the outputs of python code execution\n   *   in code interpreter tool call items.\n   */\n  include?: Array<ResponseIncludable> | null;\n\n  /**\n   * Text, image, or file inputs to the model, used to generate a response.\n   *\n   * Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Image inputs](https://platform.openai.com/docs/guides/images)\n   * - [File inputs](https://platform.openai.com/docs/guides/pdf-files)\n   * - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)\n   * - [Function calling](https://platform.openai.com/docs/guides/function-calling)\n   */\n  input?: string | ResponseInput;\n\n  /**\n   * A system (or developer) message inserted into the model's context.\n   *\n   * When using along with `previous_response_id`, the instructions from a previous\n   * response will not be carried over to the next response. This makes it simple to\n   * swap out system (or developer) messages in new responses.\n   */\n  instructions?: string | null;\n\n  /**\n   * An upper bound for the number of tokens that can be generated for a response,\n   * including visible output tokens and\n   * [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\n   */\n  max_output_tokens?: number | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a\n   * wide range of models with different capabilities, performance characteristics,\n   * and price points. Refer to the\n   * [model guide](https://platform.openai.com/docs/models) to browse and compare\n   * available models.\n   */\n  model?: Shared.ResponsesModel;\n\n  /**\n   * Whether to allow the model to run tool calls in parallel.\n   */\n  parallel_tool_calls?: boolean | null;\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create\n   * multi-turn conversations. Learn more about\n   * [conversation state](https://platform.openai.com/docs/guides/conversation-state).\n   * Cannot be used in conjunction with `conversation`.\n   */\n  previous_response_id?: string | null;\n\n  /**\n   * Reference to a prompt template and its variables.\n   * [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n   */\n  prompt?: ResponsePrompt | null;\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates. Replaces the `user` field.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  prompt_cache_key?: string;\n\n  /**\n   * The retention policy for the prompt cache. Set to `24h` to enable extended\n   * prompt caching, which keeps cached prefixes active for longer, up to a maximum\n   * of 24 hours.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching#prompt-cache-retention).\n   */\n  prompt_cache_retention?: 'in-memory' | '24h' | null;\n\n  /**\n   * **gpt-5 and o-series models only**\n   *\n   * Configuration options for\n   * [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n   */\n  reasoning?: Shared.Reasoning | null;\n\n  /**\n   * A stable identifier used to help detect users of your application that may be\n   * violating OpenAI's usage policies. The IDs should be a string that uniquely\n   * identifies each user, with a maximum length of 64 characters. We recommend\n   * hashing their username or email address, in order to avoid sending us any\n   * identifying information.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  safety_identifier?: string;\n\n  /**\n   * Specifies the latency tier to use for processing the request. This parameter is\n   * relevant for customers subscribed to the scale tier service:\n   *\n   * - If set to 'auto', then the request will be processed with the service tier\n   *   configured in the Project settings. Unless otherwise configured, the Project\n   *   will use 'default'.\n   * - If set to 'default', then the request will be processed with the standard\n   *   pricing and performance for the selected model.\n   * - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\n   *   '[priority](https://openai.com/api-priority-processing/)', then the request\n   *   will be processed with the corresponding service tier.\n   * - When not set, the default behavior is 'auto'.\n   *\n   * When this parameter is set, the response body will include the `service_tier`\n   * utilized.\n   */\n  service_tier?: 'auto' | 'default' | 'flex' | 'scale' | 'priority' | null;\n\n  /**\n   * Whether to store the generated model response for later retrieval via API.\n   */\n  store?: boolean | null;\n\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n   * for more information.\n   */\n  stream?: boolean | null;\n\n  /**\n   * Options for streaming responses. Only set this when you set `stream: true`.\n   */\n  stream_options?: ResponseCreateParams.StreamOptions | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic. We generally recommend altering this or `top_p` but\n   * not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data. Learn more:\n   *\n   * - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n   * - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)\n   */\n  text?: ResponseTextConfig;\n\n  /**\n   * How the model should select which tool (or tools) to use when generating a\n   * response. See the `tools` parameter to see how to specify which tools the model\n   * can call.\n   */\n  tool_choice?:\n    | ToolChoiceOptions\n    | ToolChoiceAllowed\n    | ToolChoiceTypes\n    | ToolChoiceFunction\n    | ToolChoiceMcp\n    | ToolChoiceCustom\n    | ToolChoiceApplyPatch\n    | ToolChoiceShell;\n\n  /**\n   * An array of tools the model may call while generating a response. You can\n   * specify which tool to use by setting the `tool_choice` parameter.\n   *\n   * We support the following categories of tools:\n   *\n   * - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n   *   capabilities, like\n   *   [web search](https://platform.openai.com/docs/guides/tools-web-search) or\n   *   [file search](https://platform.openai.com/docs/guides/tools-file-search).\n   *   Learn more about\n   *   [built-in tools](https://platform.openai.com/docs/guides/tools).\n   * - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n   *   predefined connectors such as Google Drive and SharePoint. Learn more about\n   *   [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n   * - **Function calls (custom tools)**: Functions that are defined by you, enabling\n   *   the model to call your own code with strongly typed arguments and outputs.\n   *   Learn more about\n   *   [function calling](https://platform.openai.com/docs/guides/function-calling).\n   *   You can also use custom tools to call your own code.\n   */\n  tools?: Array<Tool>;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * The truncation strategy to use for the model response.\n   *\n   * - `auto`: If the input to this Response exceeds the model's context window size,\n   *   the model will truncate the response to fit the context window by dropping\n   *   items from the beginning of the conversation.\n   * - `disabled` (default): If the input size will exceed the context window size\n   *   for a model, the request will fail with a 400 error.\n   */\n  truncation?: 'auto' | 'disabled' | null;\n\n  /**\n   * @deprecated This field is being replaced by `safety_identifier` and\n   * `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching\n   * optimizations. A stable identifier for your end-users. Used to boost cache hit\n   * rates by better bucketing similar requests and to help OpenAI detect and prevent\n   * abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n   */\n  user?: string;\n}\n\nexport namespace ResponseCreateParams {\n  export interface ContextManagement {\n    /**\n     * The context management entry type. Currently only 'compaction' is supported.\n     */\n    type: string;\n\n    /**\n     * Token threshold at which compaction should be triggered for this entry.\n     */\n    compact_threshold?: number | null;\n  }\n\n  /**\n   * Options for streaming responses. Only set this when you set `stream: true`.\n   */\n  export interface StreamOptions {\n    /**\n     * When true, stream obfuscation will be enabled. Stream obfuscation adds random\n     * characters to an `obfuscation` field on streaming delta events to normalize\n     * payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n     * fields are included by default, but add a small amount of overhead to the data\n     * stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n     * you trust the network links between your application and the OpenAI API.\n     */\n    include_obfuscation?: boolean;\n  }\n\n  export type ResponseCreateParamsNonStreaming = ResponsesAPI.ResponseCreateParamsNonStreaming;\n  export type ResponseCreateParamsStreaming = ResponsesAPI.ResponseCreateParamsStreaming;\n}\n\nexport interface ResponseCreateParamsNonStreaming extends ResponseCreateParamsBase {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n   * for more information.\n   */\n  stream?: false | null;\n}\n\nexport interface ResponseCreateParamsStreaming extends ResponseCreateParamsBase {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n   * for more information.\n   */\n  stream: true;\n}\n\nexport type ResponseRetrieveParams = ResponseRetrieveParamsNonStreaming | ResponseRetrieveParamsStreaming;\n\nexport interface ResponseRetrieveParamsBase {\n  /**\n   * Additional fields to include in the response. See the `include` parameter for\n   * Response creation above for more information.\n   */\n  include?: Array<ResponseIncludable>;\n\n  /**\n   * When true, stream obfuscation will be enabled. Stream obfuscation adds random\n   * characters to an `obfuscation` field on streaming delta events to normalize\n   * payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n   * fields are included by default, but add a small amount of overhead to the data\n   * stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n   * you trust the network links between your application and the OpenAI API.\n   */\n  include_obfuscation?: boolean;\n\n  /**\n   * The sequence number of the event after which to start streaming.\n   */\n  starting_after?: number;\n\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n   * for more information.\n   */\n  stream?: boolean;\n}\n\nexport namespace ResponseRetrieveParams {\n  export type ResponseRetrieveParamsNonStreaming = ResponsesAPI.ResponseRetrieveParamsNonStreaming;\n  export type ResponseRetrieveParamsStreaming = ResponsesAPI.ResponseRetrieveParamsStreaming;\n}\n\nexport interface ResponseRetrieveParamsNonStreaming extends ResponseRetrieveParamsBase {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n   * for more information.\n   */\n  stream?: false;\n}\n\nexport interface ResponseRetrieveParamsStreaming extends ResponseRetrieveParamsBase {\n  /**\n   * If set to true, the model response data will be streamed to the client as it is\n   * generated using\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\n   * See the\n   * [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n   * for more information.\n   */\n  stream: true;\n}\n\nexport interface ResponseCompactParams {\n  /**\n   * Model ID used to generate the response, like `gpt-5` or `o3`. OpenAI offers a\n   * wide range of models with different capabilities, performance characteristics,\n   * and price points. Refer to the\n   * [model guide](https://platform.openai.com/docs/models) to browse and compare\n   * available models.\n   */\n  model:\n    | 'gpt-5.2'\n    | 'gpt-5.2-2025-12-11'\n    | 'gpt-5.2-chat-latest'\n    | 'gpt-5.2-pro'\n    | 'gpt-5.2-pro-2025-12-11'\n    | 'gpt-5.1'\n    | 'gpt-5.1-2025-11-13'\n    | 'gpt-5.1-codex'\n    | 'gpt-5.1-mini'\n    | 'gpt-5.1-chat-latest'\n    | 'gpt-5'\n    | 'gpt-5-mini'\n    | 'gpt-5-nano'\n    | 'gpt-5-2025-08-07'\n    | 'gpt-5-mini-2025-08-07'\n    | 'gpt-5-nano-2025-08-07'\n    | 'gpt-5-chat-latest'\n    | 'gpt-4.1'\n    | 'gpt-4.1-mini'\n    | 'gpt-4.1-nano'\n    | 'gpt-4.1-2025-04-14'\n    | 'gpt-4.1-mini-2025-04-14'\n    | 'gpt-4.1-nano-2025-04-14'\n    | 'o4-mini'\n    | 'o4-mini-2025-04-16'\n    | 'o3'\n    | 'o3-2025-04-16'\n    | 'o3-mini'\n    | 'o3-mini-2025-01-31'\n    | 'o1'\n    | 'o1-2024-12-17'\n    | 'o1-preview'\n    | 'o1-preview-2024-09-12'\n    | 'o1-mini'\n    | 'o1-mini-2024-09-12'\n    | 'gpt-4o'\n    | 'gpt-4o-2024-11-20'\n    | 'gpt-4o-2024-08-06'\n    | 'gpt-4o-2024-05-13'\n    | 'gpt-4o-audio-preview'\n    | 'gpt-4o-audio-preview-2024-10-01'\n    | 'gpt-4o-audio-preview-2024-12-17'\n    | 'gpt-4o-audio-preview-2025-06-03'\n    | 'gpt-4o-mini-audio-preview'\n    | 'gpt-4o-mini-audio-preview-2024-12-17'\n    | 'gpt-4o-search-preview'\n    | 'gpt-4o-mini-search-preview'\n    | 'gpt-4o-search-preview-2025-03-11'\n    | 'gpt-4o-mini-search-preview-2025-03-11'\n    | 'chatgpt-4o-latest'\n    | 'codex-mini-latest'\n    | 'gpt-4o-mini'\n    | 'gpt-4o-mini-2024-07-18'\n    | 'gpt-4-turbo'\n    | 'gpt-4-turbo-2024-04-09'\n    | 'gpt-4-0125-preview'\n    | 'gpt-4-turbo-preview'\n    | 'gpt-4-1106-preview'\n    | 'gpt-4-vision-preview'\n    | 'gpt-4'\n    | 'gpt-4-0314'\n    | 'gpt-4-0613'\n    | 'gpt-4-32k'\n    | 'gpt-4-32k-0314'\n    | 'gpt-4-32k-0613'\n    | 'gpt-3.5-turbo'\n    | 'gpt-3.5-turbo-16k'\n    | 'gpt-3.5-turbo-0301'\n    | 'gpt-3.5-turbo-0613'\n    | 'gpt-3.5-turbo-1106'\n    | 'gpt-3.5-turbo-0125'\n    | 'gpt-3.5-turbo-16k-0613'\n    | 'o1-pro'\n    | 'o1-pro-2025-03-19'\n    | 'o3-pro'\n    | 'o3-pro-2025-06-10'\n    | 'o3-deep-research'\n    | 'o3-deep-research-2025-06-26'\n    | 'o4-mini-deep-research'\n    | 'o4-mini-deep-research-2025-06-26'\n    | 'computer-use-preview'\n    | 'computer-use-preview-2025-03-11'\n    | 'gpt-5-codex'\n    | 'gpt-5-pro'\n    | 'gpt-5-pro-2025-10-06'\n    | 'gpt-5.1-codex-max'\n    | (string & {})\n    | null;\n\n  /**\n   * Text, image, or file inputs to the model, used to generate a response\n   */\n  input?: string | Array<ResponseInputItem> | null;\n\n  /**\n   * A system (or developer) message inserted into the model's context. When used\n   * along with `previous_response_id`, the instructions from a previous response\n   * will not be carried over to the next response. This makes it simple to swap out\n   * system (or developer) messages in new responses.\n   */\n  instructions?: string | null;\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create\n   * multi-turn conversations. Learn more about\n   * [conversation state](https://platform.openai.com/docs/guides/conversation-state).\n   * Cannot be used in conjunction with `conversation`.\n   */\n  previous_response_id?: string | null;\n\n  /**\n   * A key to use when reading from or writing to the prompt cache.\n   */\n  prompt_cache_key?: string | null;\n}\n\nResponses.InputItems = InputItems;\nResponses.InputTokens = InputTokens;\n\nexport declare namespace Responses {\n  export {\n    type ApplyPatchTool as ApplyPatchTool,\n    type CompactedResponse as CompactedResponse,\n    type ComputerTool as ComputerTool,\n    type ContainerAuto as ContainerAuto,\n    type ContainerNetworkPolicyAllowlist as ContainerNetworkPolicyAllowlist,\n    type ContainerNetworkPolicyDisabled as ContainerNetworkPolicyDisabled,\n    type ContainerNetworkPolicyDomainSecret as ContainerNetworkPolicyDomainSecret,\n    type ContainerReference as ContainerReference,\n    type CustomTool as CustomTool,\n    type EasyInputMessage as EasyInputMessage,\n    type FileSearchTool as FileSearchTool,\n    type FunctionShellTool as FunctionShellTool,\n    type FunctionTool as FunctionTool,\n    type InlineSkill as InlineSkill,\n    type InlineSkillSource as InlineSkillSource,\n    type LocalEnvironment as LocalEnvironment,\n    type LocalSkill as LocalSkill,\n    type Response as Response,\n    type ResponseApplyPatchToolCall as ResponseApplyPatchToolCall,\n    type ResponseApplyPatchToolCallOutput as ResponseApplyPatchToolCallOutput,\n    type ResponseAudioDeltaEvent as ResponseAudioDeltaEvent,\n    type ResponseAudioDoneEvent as ResponseAudioDoneEvent,\n    type ResponseAudioTranscriptDeltaEvent as ResponseAudioTranscriptDeltaEvent,\n    type ResponseAudioTranscriptDoneEvent as ResponseAudioTranscriptDoneEvent,\n    type ResponseCodeInterpreterCallCodeDeltaEvent as ResponseCodeInterpreterCallCodeDeltaEvent,\n    type ResponseCodeInterpreterCallCodeDoneEvent as ResponseCodeInterpreterCallCodeDoneEvent,\n    type ResponseCodeInterpreterCallCompletedEvent as ResponseCodeInterpreterCallCompletedEvent,\n    type ResponseCodeInterpreterCallInProgressEvent as ResponseCodeInterpreterCallInProgressEvent,\n    type ResponseCodeInterpreterCallInterpretingEvent as ResponseCodeInterpreterCallInterpretingEvent,\n    type ResponseCodeInterpreterToolCall as ResponseCodeInterpreterToolCall,\n    type ResponseCompactionItem as ResponseCompactionItem,\n    type ResponseCompactionItemParam as ResponseCompactionItemParam,\n    type ResponseCompletedEvent as ResponseCompletedEvent,\n    type ResponseComputerToolCall as ResponseComputerToolCall,\n    type ResponseComputerToolCallOutputItem as ResponseComputerToolCallOutputItem,\n    type ResponseComputerToolCallOutputScreenshot as ResponseComputerToolCallOutputScreenshot,\n    type ResponseContainerReference as ResponseContainerReference,\n    type ResponseContent as ResponseContent,\n    type ResponseContentPartAddedEvent as ResponseContentPartAddedEvent,\n    type ResponseContentPartDoneEvent as ResponseContentPartDoneEvent,\n    type ResponseConversationParam as ResponseConversationParam,\n    type ResponseCreatedEvent as ResponseCreatedEvent,\n    type ResponseCustomToolCall as ResponseCustomToolCall,\n    type ResponseCustomToolCallInputDeltaEvent as ResponseCustomToolCallInputDeltaEvent,\n    type ResponseCustomToolCallInputDoneEvent as ResponseCustomToolCallInputDoneEvent,\n    type ResponseCustomToolCallOutput as ResponseCustomToolCallOutput,\n    type ResponseError as ResponseError,\n    type ResponseErrorEvent as ResponseErrorEvent,\n    type ResponseFailedEvent as ResponseFailedEvent,\n    type ResponseFileSearchCallCompletedEvent as ResponseFileSearchCallCompletedEvent,\n    type ResponseFileSearchCallInProgressEvent as ResponseFileSearchCallInProgressEvent,\n    type ResponseFileSearchCallSearchingEvent as ResponseFileSearchCallSearchingEvent,\n    type ResponseFileSearchToolCall as ResponseFileSearchToolCall,\n    type ResponseFormatTextConfig as ResponseFormatTextConfig,\n    type ResponseFormatTextJSONSchemaConfig as ResponseFormatTextJSONSchemaConfig,\n    type ResponseFunctionCallArgumentsDeltaEvent as ResponseFunctionCallArgumentsDeltaEvent,\n    type ResponseFunctionCallArgumentsDoneEvent as ResponseFunctionCallArgumentsDoneEvent,\n    type ResponseFunctionCallOutputItem as ResponseFunctionCallOutputItem,\n    type ResponseFunctionCallOutputItemList as ResponseFunctionCallOutputItemList,\n    type ResponseFunctionShellCallOutputContent as ResponseFunctionShellCallOutputContent,\n    type ResponseFunctionShellToolCall as ResponseFunctionShellToolCall,\n    type ResponseFunctionShellToolCallOutput as ResponseFunctionShellToolCallOutput,\n    type ResponseFunctionToolCall as ResponseFunctionToolCall,\n    type ResponseFunctionToolCallItem as ResponseFunctionToolCallItem,\n    type ResponseFunctionToolCallOutputItem as ResponseFunctionToolCallOutputItem,\n    type ResponseFunctionWebSearch as ResponseFunctionWebSearch,\n    type ResponseImageGenCallCompletedEvent as ResponseImageGenCallCompletedEvent,\n    type ResponseImageGenCallGeneratingEvent as ResponseImageGenCallGeneratingEvent,\n    type ResponseImageGenCallInProgressEvent as ResponseImageGenCallInProgressEvent,\n    type ResponseImageGenCallPartialImageEvent as ResponseImageGenCallPartialImageEvent,\n    type ResponseInProgressEvent as ResponseInProgressEvent,\n    type ResponseIncludable as ResponseIncludable,\n    type ResponseIncompleteEvent as ResponseIncompleteEvent,\n    type ResponseInput as ResponseInput,\n    type ResponseInputAudio as ResponseInputAudio,\n    type ResponseInputContent as ResponseInputContent,\n    type ResponseInputFile as ResponseInputFile,\n    type ResponseInputFileContent as ResponseInputFileContent,\n    type ResponseInputImage as ResponseInputImage,\n    type ResponseInputImageContent as ResponseInputImageContent,\n    type ResponseInputItem as ResponseInputItem,\n    type ResponseInputMessageContentList as ResponseInputMessageContentList,\n    type ResponseInputMessageItem as ResponseInputMessageItem,\n    type ResponseInputText as ResponseInputText,\n    type ResponseInputTextContent as ResponseInputTextContent,\n    type ResponseItem as ResponseItem,\n    type ResponseLocalEnvironment as ResponseLocalEnvironment,\n    type ResponseMcpCallArgumentsDeltaEvent as ResponseMcpCallArgumentsDeltaEvent,\n    type ResponseMcpCallArgumentsDoneEvent as ResponseMcpCallArgumentsDoneEvent,\n    type ResponseMcpCallCompletedEvent as ResponseMcpCallCompletedEvent,\n    type ResponseMcpCallFailedEvent as ResponseMcpCallFailedEvent,\n    type ResponseMcpCallInProgressEvent as ResponseMcpCallInProgressEvent,\n    type ResponseMcpListToolsCompletedEvent as ResponseMcpListToolsCompletedEvent,\n    type ResponseMcpListToolsFailedEvent as ResponseMcpListToolsFailedEvent,\n    type ResponseMcpListToolsInProgressEvent as ResponseMcpListToolsInProgressEvent,\n    type ResponseOutputAudio as ResponseOutputAudio,\n    type ResponseOutputItem as ResponseOutputItem,\n    type ResponseOutputItemAddedEvent as ResponseOutputItemAddedEvent,\n    type ResponseOutputItemDoneEvent as ResponseOutputItemDoneEvent,\n    type ResponseOutputMessage as ResponseOutputMessage,\n    type ResponseOutputRefusal as ResponseOutputRefusal,\n    type ResponseOutputText as ResponseOutputText,\n    type ResponseOutputTextAnnotationAddedEvent as ResponseOutputTextAnnotationAddedEvent,\n    type ResponsePrompt as ResponsePrompt,\n    type ResponseQueuedEvent as ResponseQueuedEvent,\n    type ResponseReasoningItem as ResponseReasoningItem,\n    type ResponseReasoningSummaryPartAddedEvent as ResponseReasoningSummaryPartAddedEvent,\n    type ResponseReasoningSummaryPartDoneEvent as ResponseReasoningSummaryPartDoneEvent,\n    type ResponseReasoningSummaryTextDeltaEvent as ResponseReasoningSummaryTextDeltaEvent,\n    type ResponseReasoningSummaryTextDoneEvent as ResponseReasoningSummaryTextDoneEvent,\n    type ResponseReasoningTextDeltaEvent as ResponseReasoningTextDeltaEvent,\n    type ResponseReasoningTextDoneEvent as ResponseReasoningTextDoneEvent,\n    type ResponseRefusalDeltaEvent as ResponseRefusalDeltaEvent,\n    type ResponseRefusalDoneEvent as ResponseRefusalDoneEvent,\n    type ResponseStatus as ResponseStatus,\n    type ResponseStreamEvent as ResponseStreamEvent,\n    type ResponseTextConfig as ResponseTextConfig,\n    type ResponseTextDeltaEvent as ResponseTextDeltaEvent,\n    type ResponseTextDoneEvent as ResponseTextDoneEvent,\n    type ResponseUsage as ResponseUsage,\n    type ResponseWebSearchCallCompletedEvent as ResponseWebSearchCallCompletedEvent,\n    type ResponseWebSearchCallInProgressEvent as ResponseWebSearchCallInProgressEvent,\n    type ResponseWebSearchCallSearchingEvent as ResponseWebSearchCallSearchingEvent,\n    type ResponsesClientEvent as ResponsesClientEvent,\n    type ResponsesServerEvent as ResponsesServerEvent,\n    type SkillReference as SkillReference,\n    type Tool as Tool,\n    type ToolChoiceAllowed as ToolChoiceAllowed,\n    type ToolChoiceApplyPatch as ToolChoiceApplyPatch,\n    type ToolChoiceCustom as ToolChoiceCustom,\n    type ToolChoiceFunction as ToolChoiceFunction,\n    type ToolChoiceMcp as ToolChoiceMcp,\n    type ToolChoiceOptions as ToolChoiceOptions,\n    type ToolChoiceShell as ToolChoiceShell,\n    type ToolChoiceTypes as ToolChoiceTypes,\n    type WebSearchPreviewTool as WebSearchPreviewTool,\n    type WebSearchTool as WebSearchTool,\n    type ResponseCreateParams as ResponseCreateParams,\n    type ResponseCreateParamsNonStreaming as ResponseCreateParamsNonStreaming,\n    type ResponseCreateParamsStreaming as ResponseCreateParamsStreaming,\n    type ResponseRetrieveParams as ResponseRetrieveParams,\n    type ResponseRetrieveParamsNonStreaming as ResponseRetrieveParamsNonStreaming,\n    type ResponseRetrieveParamsStreaming as ResponseRetrieveParamsStreaming,\n    type ResponseCompactParams as ResponseCompactParams,\n  };\n\n  export {\n    InputItems as InputItems,\n    type ResponseItemList as ResponseItemList,\n    type InputItemListParams as InputItemListParams,\n  };\n\n  export {\n    InputTokens as InputTokens,\n    type InputTokenCountResponse as InputTokenCountResponse,\n    type InputTokenCountParams as InputTokenCountParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport { APIPromise } from '../../core/api-promise';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class Content extends APIResource {\n  /**\n   * Download a skill zip bundle by its ID.\n   */\n  retrieve(skillID: string, options?: RequestOptions): APIPromise<Response> {\n    return this._client.get(path`/skills/${skillID}/content`, {\n      ...options,\n      headers: buildHeaders([{ Accept: 'application/binary' }, options?.headers]),\n      __binaryResponse: true,\n    });\n  }\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport { APIPromise } from '../../../core/api-promise';\nimport { buildHeaders } from '../../../internal/headers';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { path } from '../../../internal/utils/path';\n\nexport class Content extends APIResource {\n  /**\n   * Download a skill version zip bundle.\n   */\n  retrieve(version: string, params: ContentRetrieveParams, options?: RequestOptions): APIPromise<Response> {\n    const { skill_id } = params;\n    return this._client.get(path`/skills/${skill_id}/versions/${version}/content`, {\n      ...options,\n      headers: buildHeaders([{ Accept: 'application/binary' }, options?.headers]),\n      __binaryResponse: true,\n    });\n  }\n}\n\nexport interface ContentRetrieveParams {\n  /**\n   * The identifier of the skill.\n   */\n  skill_id: string;\n}\n\nexport declare namespace Content {\n  export { type ContentRetrieveParams as ContentRetrieveParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../core/resource';\nimport * as ContentAPI from './content';\nimport { Content, ContentRetrieveParams } from './content';\nimport { APIPromise } from '../../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../../core/pagination';\nimport { type Uploadable } from '../../../core/uploads';\nimport { RequestOptions } from '../../../internal/request-options';\nimport { maybeMultipartFormRequestOptions } from '../../../internal/uploads';\nimport { path } from '../../../internal/utils/path';\n\nexport class Versions extends APIResource {\n  content: ContentAPI.Content = new ContentAPI.Content(this._client);\n\n  /**\n   * Create a new immutable skill version.\n   */\n  create(\n    skillID: string,\n    body: VersionCreateParams | null | undefined = {},\n    options?: RequestOptions,\n  ): APIPromise<SkillVersion> {\n    return this._client.post(\n      path`/skills/${skillID}/versions`,\n      maybeMultipartFormRequestOptions({ body, ...options }, this._client),\n    );\n  }\n\n  /**\n   * Get a specific skill version.\n   */\n  retrieve(\n    version: string,\n    params: VersionRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<SkillVersion> {\n    const { skill_id } = params;\n    return this._client.get(path`/skills/${skill_id}/versions/${version}`, options);\n  }\n\n  /**\n   * List skill versions for a skill.\n   */\n  list(\n    skillID: string,\n    query: VersionListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<SkillVersionsPage, SkillVersion> {\n    return this._client.getAPIList(path`/skills/${skillID}/versions`, CursorPage<SkillVersion>, {\n      query,\n      ...options,\n    });\n  }\n\n  /**\n   * Delete a skill version.\n   */\n  delete(\n    version: string,\n    params: VersionDeleteParams,\n    options?: RequestOptions,\n  ): APIPromise<DeletedSkillVersion> {\n    const { skill_id } = params;\n    return this._client.delete(path`/skills/${skill_id}/versions/${version}`, options);\n  }\n}\n\nexport type SkillVersionsPage = CursorPage<SkillVersion>;\n\nexport interface DeletedSkillVersion {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'skill.version.deleted';\n\n  /**\n   * The deleted skill version.\n   */\n  version: string;\n}\n\nexport interface SkillVersion {\n  /**\n   * Unique identifier for the skill version.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (seconds) for when the version was created.\n   */\n  created_at: number;\n\n  /**\n   * Description of the skill version.\n   */\n  description: string;\n\n  /**\n   * Name of the skill version.\n   */\n  name: string;\n\n  /**\n   * The object type, which is `skill.version`.\n   */\n  object: 'skill.version';\n\n  /**\n   * Identifier of the skill for this version.\n   */\n  skill_id: string;\n\n  /**\n   * Version number for this skill.\n   */\n  version: string;\n}\n\nexport interface SkillVersionList {\n  /**\n   * A list of items\n   */\n  data: Array<SkillVersion>;\n\n  /**\n   * The ID of the first item in the list.\n   */\n  first_id: string | null;\n\n  /**\n   * Whether there are more items available.\n   */\n  has_more: boolean;\n\n  /**\n   * The ID of the last item in the list.\n   */\n  last_id: string | null;\n\n  /**\n   * The type of object returned, must be `list`.\n   */\n  object: 'list';\n}\n\nexport interface VersionCreateParams {\n  /**\n   * Whether to set this version as the default.\n   */\n  default?: boolean;\n\n  /**\n   * Skill files to upload (directory upload) or a single zip file.\n   */\n  files?: Array<Uploadable> | Uploadable;\n}\n\nexport interface VersionRetrieveParams {\n  /**\n   * The identifier of the skill.\n   */\n  skill_id: string;\n}\n\nexport interface VersionListParams extends CursorPageParams {\n  /**\n   * Sort order of results by version number.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface VersionDeleteParams {\n  /**\n   * The identifier of the skill.\n   */\n  skill_id: string;\n}\n\nVersions.Content = Content;\n\nexport declare namespace Versions {\n  export {\n    type DeletedSkillVersion as DeletedSkillVersion,\n    type SkillVersion as SkillVersion,\n    type SkillVersionList as SkillVersionList,\n    type SkillVersionsPage as SkillVersionsPage,\n    type VersionCreateParams as VersionCreateParams,\n    type VersionRetrieveParams as VersionRetrieveParams,\n    type VersionListParams as VersionListParams,\n    type VersionDeleteParams as VersionDeleteParams,\n  };\n\n  export { Content as Content, type ContentRetrieveParams as ContentRetrieveParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as ContentAPI from './content';\nimport { Content } from './content';\nimport * as VersionsAPI from './versions/versions';\nimport {\n  DeletedSkillVersion,\n  SkillVersion,\n  SkillVersionList,\n  SkillVersionsPage,\n  VersionCreateParams,\n  VersionDeleteParams,\n  VersionListParams,\n  VersionRetrieveParams,\n  Versions,\n} from './versions/versions';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\nimport { type Uploadable } from '../../core/uploads';\nimport { RequestOptions } from '../../internal/request-options';\nimport { maybeMultipartFormRequestOptions } from '../../internal/uploads';\nimport { path } from '../../internal/utils/path';\n\nexport class Skills extends APIResource {\n  content: ContentAPI.Content = new ContentAPI.Content(this._client);\n  versions: VersionsAPI.Versions = new VersionsAPI.Versions(this._client);\n\n  /**\n   * Create a new skill.\n   */\n  create(body: SkillCreateParams | null | undefined = {}, options?: RequestOptions): APIPromise<Skill> {\n    return this._client.post('/skills', maybeMultipartFormRequestOptions({ body, ...options }, this._client));\n  }\n\n  /**\n   * Get a skill by its ID.\n   */\n  retrieve(skillID: string, options?: RequestOptions): APIPromise<Skill> {\n    return this._client.get(path`/skills/${skillID}`, options);\n  }\n\n  /**\n   * Update the default version pointer for a skill.\n   */\n  update(skillID: string, body: SkillUpdateParams, options?: RequestOptions): APIPromise<Skill> {\n    return this._client.post(path`/skills/${skillID}`, { body, ...options });\n  }\n\n  /**\n   * List all skills for the current project.\n   */\n  list(\n    query: SkillListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<SkillsPage, Skill> {\n    return this._client.getAPIList('/skills', CursorPage<Skill>, { query, ...options });\n  }\n\n  /**\n   * Delete a skill by its ID.\n   */\n  delete(skillID: string, options?: RequestOptions): APIPromise<DeletedSkill> {\n    return this._client.delete(path`/skills/${skillID}`, options);\n  }\n}\n\nexport type SkillsPage = CursorPage<Skill>;\n\nexport interface DeletedSkill {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'skill.deleted';\n}\n\nexport interface Skill {\n  /**\n   * Unique identifier for the skill.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (seconds) for when the skill was created.\n   */\n  created_at: number;\n\n  /**\n   * Default version for the skill.\n   */\n  default_version: string;\n\n  /**\n   * Description of the skill.\n   */\n  description: string;\n\n  /**\n   * Latest version for the skill.\n   */\n  latest_version: string;\n\n  /**\n   * Name of the skill.\n   */\n  name: string;\n\n  /**\n   * The object type, which is `skill`.\n   */\n  object: 'skill';\n}\n\nexport interface SkillList {\n  /**\n   * A list of items\n   */\n  data: Array<Skill>;\n\n  /**\n   * The ID of the first item in the list.\n   */\n  first_id: string | null;\n\n  /**\n   * Whether there are more items available.\n   */\n  has_more: boolean;\n\n  /**\n   * The ID of the last item in the list.\n   */\n  last_id: string | null;\n\n  /**\n   * The type of object returned, must be `list`.\n   */\n  object: 'list';\n}\n\nexport interface SkillCreateParams {\n  /**\n   * Skill files to upload (directory upload) or a single zip file.\n   */\n  files?: Array<Uploadable> | Uploadable;\n}\n\nexport interface SkillUpdateParams {\n  /**\n   * The skill version number to set as default.\n   */\n  default_version: string;\n}\n\nexport interface SkillListParams extends CursorPageParams {\n  /**\n   * Sort order of results by timestamp. Use `asc` for ascending order or `desc` for\n   * descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nSkills.Content = Content;\nSkills.Versions = Versions;\n\nexport declare namespace Skills {\n  export {\n    type DeletedSkill as DeletedSkill,\n    type Skill as Skill,\n    type SkillList as SkillList,\n    type SkillsPage as SkillsPage,\n    type SkillCreateParams as SkillCreateParams,\n    type SkillUpdateParams as SkillUpdateParams,\n    type SkillListParams as SkillListParams,\n  };\n\n  export { Content as Content };\n\n  export {\n    Versions as Versions,\n    type DeletedSkillVersion as DeletedSkillVersion,\n    type SkillVersion as SkillVersion,\n    type SkillVersionList as SkillVersionList,\n    type SkillVersionsPage as SkillVersionsPage,\n    type VersionCreateParams as VersionCreateParams,\n    type VersionRetrieveParams as VersionRetrieveParams,\n    type VersionListParams as VersionListParams,\n    type VersionDeleteParams as VersionDeleteParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport { APIPromise } from '../../core/api-promise';\nimport { type Uploadable } from '../../core/uploads';\nimport { RequestOptions } from '../../internal/request-options';\nimport { multipartFormRequestOptions } from '../../internal/uploads';\nimport { path } from '../../internal/utils/path';\n\nexport class Parts extends APIResource {\n  /**\n   * Adds a\n   * [Part](https://platform.openai.com/docs/api-reference/uploads/part-object) to an\n   * [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object.\n   * A Part represents a chunk of bytes from the file you are trying to upload.\n   *\n   * Each Part can be at most 64 MB, and you can add Parts until you hit the Upload\n   * maximum of 8 GB.\n   *\n   * It is possible to add multiple Parts in parallel. You can decide the intended\n   * order of the Parts when you\n   * [complete the Upload](https://platform.openai.com/docs/api-reference/uploads/complete).\n   */\n  create(uploadID: string, body: PartCreateParams, options?: RequestOptions): APIPromise<UploadPart> {\n    return this._client.post(\n      path`/uploads/${uploadID}/parts`,\n      multipartFormRequestOptions({ body, ...options }, this._client),\n    );\n  }\n}\n\n/**\n * The upload Part represents a chunk of bytes we can add to an Upload object.\n */\nexport interface UploadPart {\n  /**\n   * The upload Part unique identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the Part was created.\n   */\n  created_at: number;\n\n  /**\n   * The object type, which is always `upload.part`.\n   */\n  object: 'upload.part';\n\n  /**\n   * The ID of the Upload object that this Part was added to.\n   */\n  upload_id: string;\n}\n\nexport interface PartCreateParams {\n  /**\n   * The chunk of bytes for this Part.\n   */\n  data: Uploadable;\n}\n\nexport declare namespace Parts {\n  export { type UploadPart as UploadPart, type PartCreateParams as PartCreateParams };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as FilesAPI from '../files';\nimport * as PartsAPI from './parts';\nimport { PartCreateParams, Parts, UploadPart } from './parts';\nimport { APIPromise } from '../../core/api-promise';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class Uploads extends APIResource {\n  parts: PartsAPI.Parts = new PartsAPI.Parts(this._client);\n\n  /**\n   * Creates an intermediate\n   * [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object\n   * that you can add\n   * [Parts](https://platform.openai.com/docs/api-reference/uploads/part-object) to.\n   * Currently, an Upload can accept at most 8 GB in total and expires after an hour\n   * after you create it.\n   *\n   * Once you complete the Upload, we will create a\n   * [File](https://platform.openai.com/docs/api-reference/files/object) object that\n   * contains all the parts you uploaded. This File is usable in the rest of our\n   * platform as a regular File object.\n   *\n   * For certain `purpose` values, the correct `mime_type` must be specified. Please\n   * refer to documentation for the\n   * [supported MIME types for your use case](https://platform.openai.com/docs/assistants/tools/file-search#supported-files).\n   *\n   * For guidance on the proper filename extensions for each purpose, please follow\n   * the documentation on\n   * [creating a File](https://platform.openai.com/docs/api-reference/files/create).\n   *\n   * Returns the Upload object with status `pending`.\n   */\n  create(body: UploadCreateParams, options?: RequestOptions): APIPromise<Upload> {\n    return this._client.post('/uploads', { body, ...options });\n  }\n\n  /**\n   * Cancels the Upload. No Parts may be added after an Upload is cancelled.\n   *\n   * Returns the Upload object with status `cancelled`.\n   */\n  cancel(uploadID: string, options?: RequestOptions): APIPromise<Upload> {\n    return this._client.post(path`/uploads/${uploadID}/cancel`, options);\n  }\n\n  /**\n   * Completes the\n   * [Upload](https://platform.openai.com/docs/api-reference/uploads/object).\n   *\n   * Within the returned Upload object, there is a nested\n   * [File](https://platform.openai.com/docs/api-reference/files/object) object that\n   * is ready to use in the rest of the platform.\n   *\n   * You can specify the order of the Parts by passing in an ordered list of the Part\n   * IDs.\n   *\n   * The number of bytes uploaded upon completion must match the number of bytes\n   * initially specified when creating the Upload object. No Parts may be added after\n   * an Upload is completed. Returns the Upload object with status `completed`,\n   * including an additional `file` property containing the created usable File\n   * object.\n   */\n  complete(uploadID: string, body: UploadCompleteParams, options?: RequestOptions): APIPromise<Upload> {\n    return this._client.post(path`/uploads/${uploadID}/complete`, { body, ...options });\n  }\n}\n\n/**\n * The Upload object can accept byte chunks in the form of Parts.\n */\nexport interface Upload {\n  /**\n   * The Upload unique identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The intended number of bytes to be uploaded.\n   */\n  bytes: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the Upload was created.\n   */\n  created_at: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the Upload will expire.\n   */\n  expires_at: number;\n\n  /**\n   * The name of the file to be uploaded.\n   */\n  filename: string;\n\n  /**\n   * The object type, which is always \"upload\".\n   */\n  object: 'upload';\n\n  /**\n   * The intended purpose of the file.\n   * [Please refer here](https://platform.openai.com/docs/api-reference/files/object#files/object-purpose)\n   * for acceptable values.\n   */\n  purpose: string;\n\n  /**\n   * The status of the Upload.\n   */\n  status: 'pending' | 'completed' | 'cancelled' | 'expired';\n\n  /**\n   * The `File` object represents a document that has been uploaded to OpenAI.\n   */\n  file?: FilesAPI.FileObject | null;\n}\n\nexport interface UploadCreateParams {\n  /**\n   * The number of bytes in the file you are uploading.\n   */\n  bytes: number;\n\n  /**\n   * The name of the file to upload.\n   */\n  filename: string;\n\n  /**\n   * The MIME type of the file.\n   *\n   * This must fall within the supported MIME types for your file purpose. See the\n   * supported MIME types for assistants and vision.\n   */\n  mime_type: string;\n\n  /**\n   * The intended purpose of the uploaded file.\n   *\n   * See the\n   * [documentation on File purposes](https://platform.openai.com/docs/api-reference/files/create#files-create-purpose).\n   */\n  purpose: FilesAPI.FilePurpose;\n\n  /**\n   * The expiration policy for a file. By default, files with `purpose=batch` expire\n   * after 30 days and all other files are persisted until they are manually deleted.\n   */\n  expires_after?: UploadCreateParams.ExpiresAfter;\n}\n\nexport namespace UploadCreateParams {\n  /**\n   * The expiration policy for a file. By default, files with `purpose=batch` expire\n   * after 30 days and all other files are persisted until they are manually deleted.\n   */\n  export interface ExpiresAfter {\n    /**\n     * Anchor timestamp after which the expiration policy applies. Supported anchors:\n     * `created_at`.\n     */\n    anchor: 'created_at';\n\n    /**\n     * The number of seconds after the anchor time that the file will expire. Must be\n     * between 3600 (1 hour) and 2592000 (30 days).\n     */\n    seconds: number;\n  }\n}\n\nexport interface UploadCompleteParams {\n  /**\n   * The ordered list of Part IDs.\n   */\n  part_ids: Array<string>;\n\n  /**\n   * The optional md5 checksum for the file contents to verify if the bytes uploaded\n   * matches what you expect.\n   */\n  md5?: string;\n}\n\nUploads.Parts = Parts;\n\nexport declare namespace Uploads {\n  export {\n    type Upload as Upload,\n    type UploadCreateParams as UploadCreateParams,\n    type UploadCompleteParams as UploadCompleteParams,\n  };\n\n  export { Parts as Parts, type UploadPart as UploadPart, type PartCreateParams as PartCreateParams };\n}\n", "/**\n * Like `Promise.allSettled()` but throws an error if any promises are rejected.\n */\nexport const allSettledWithThrow = async <R>(promises: Promise<R>[]): Promise<R[]> => {\n  const results = await Promise.allSettled(promises);\n  const rejected = results.filter((result): result is PromiseRejectedResult => result.status === 'rejected');\n  if (rejected.length) {\n    for (const result of rejected) {\n      console.error(result.reason);\n    }\n\n    throw new Error(`${rejected.length} promise(s) failed - see the above errors`);\n  }\n\n  // Note: TS was complaining about using `.filter().map()` here for some reason\n  const values: R[] = [];\n  for (const result of results) {\n    if (result.status === 'fulfilled') {\n      values.push(result.value);\n    }\n  }\n  return values;\n};\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as FilesAPI from './files';\nimport { VectorStoreFilesPage } from './files';\nimport * as VectorStoresAPI from './vector-stores';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { sleep } from '../../internal/utils/sleep';\nimport { type Uploadable } from '../../uploads';\nimport { allSettledWithThrow } from '../../lib/Util';\nimport { path } from '../../internal/utils/path';\n\nexport class FileBatches extends APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(\n    vectorStoreID: string,\n    body: FileBatchCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFileBatch> {\n    return this._client.post(path`/vector_stores/${vectorStoreID}/file_batches`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(\n    batchID: string,\n    params: FileBatchRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFileBatch> {\n    const { vector_store_id } = params;\n    return this._client.get(path`/vector_stores/${vector_store_id}/file_batches/${batchID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(\n    batchID: string,\n    params: FileBatchCancelParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFileBatch> {\n    const { vector_store_id } = params;\n    return this._client.post(path`/vector_stores/${vector_store_id}/file_batches/${batchID}/cancel`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(\n    vectorStoreId: string,\n    body: FileBatchCreateParams,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n\n  /**\n   * Returns a list of vector store files in a batch.\n   */\n  listFiles(\n    batchID: string,\n    params: FileBatchListFilesParams,\n    options?: RequestOptions,\n  ): PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile> {\n    const { vector_store_id, ...query } = params;\n    return this._client.getAPIList(\n      path`/vector_stores/${vector_store_id}/file_batches/${batchID}/files`,\n      CursorPage<FilesAPI.VectorStoreFile>,\n      { query, ...options, headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]) },\n    );\n  }\n\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(\n    vectorStoreID: string,\n    batchID: string,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const headers = buildHeaders([\n      options?.headers,\n      {\n        'X-Stainless-Poll-Helper': 'true',\n        'X-Stainless-Custom-Poll-Interval': options?.pollIntervalMs?.toString() ?? undefined,\n      },\n    ]);\n\n    while (true) {\n      const { data: batch, response } = await this.retrieve(\n        batchID,\n        { vector_store_id: vectorStoreID },\n        {\n          ...options,\n          headers,\n        },\n      ).withResponse();\n\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(\n    vectorStoreId: string,\n    { files, fileIds = [] }: { files: Uploadable[]; fileIds?: string[] },\n    options?: RequestOptions & { pollIntervalMs?: number; maxConcurrency?: number },\n  ): Promise<VectorStoreFileBatch> {\n    if (files == null || files.length == 0) {\n      throw new Error(\n        `No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`,\n      );\n    }\n\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds: string[] = [...fileIds];\n\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator: IterableIterator<Uploadable>) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n\n    // Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds,\n    });\n  }\n}\n\n/**\n * A batch of files attached to a vector store.\n */\nexport interface VectorStoreFileBatch {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store files batch was\n   * created.\n   */\n  created_at: number;\n\n  file_counts: VectorStoreFileBatch.FileCounts;\n\n  /**\n   * The object type, which is always `vector_store.file_batch`.\n   */\n  object: 'vector_store.files_batch';\n\n  /**\n   * The status of the vector store files batch, which can be either `in_progress`,\n   * `completed`, `cancelled` or `failed`.\n   */\n  status: 'in_progress' | 'completed' | 'cancelled' | 'failed';\n\n  /**\n   * The ID of the\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\n   * attached to.\n   */\n  vector_store_id: string;\n}\n\nexport namespace VectorStoreFileBatch {\n  export interface FileCounts {\n    /**\n     * The number of files that where cancelled.\n     */\n    cancelled: number;\n\n    /**\n     * The number of files that have been processed.\n     */\n    completed: number;\n\n    /**\n     * The number of files that have failed to process.\n     */\n    failed: number;\n\n    /**\n     * The number of files that are currently being processed.\n     */\n    in_progress: number;\n\n    /**\n     * The total number of files.\n     */\n    total: number;\n  }\n}\n\nexport interface FileBatchCreateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\n   * length of 64 characters. Values are strings with a maximum length of 512\n   * characters, booleans, or numbers.\n   */\n  attributes?: { [key: string]: string | number | boolean } | null;\n\n  /**\n   * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n   * strategy. Only applicable if `file_ids` is non-empty.\n   */\n  chunking_strategy?: VectorStoresAPI.FileChunkingStrategyParam;\n\n  /**\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n   * the vector store should use. Useful for tools like `file_search` that can access\n   * files. If `attributes` or `chunking_strategy` are provided, they will be applied\n   * to all files in the batch. The maximum batch size is 2000 files. Mutually\n   * exclusive with `files`.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * A list of objects that each include a `file_id` plus optional `attributes` or\n   * `chunking_strategy`. Use this when you need to override metadata for specific\n   * files. The global `attributes` or `chunking_strategy` will be ignored and must\n   * be specified for each file. The maximum batch size is 2000 files. Mutually\n   * exclusive with `file_ids`.\n   */\n  files?: Array<FileBatchCreateParams.File>;\n}\n\nexport namespace FileBatchCreateParams {\n  export interface File {\n    /**\n     * A [File](https://platform.openai.com/docs/api-reference/files) ID that the\n     * vector store should use. Useful for tools like `file_search` that can access\n     * files.\n     */\n    file_id: string;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard. Keys are strings with a maximum\n     * length of 64 characters. Values are strings with a maximum length of 512\n     * characters, booleans, or numbers.\n     */\n    attributes?: { [key: string]: string | number | boolean } | null;\n\n    /**\n     * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n     * strategy. Only applicable if `file_ids` is non-empty.\n     */\n    chunking_strategy?: VectorStoresAPI.FileChunkingStrategyParam;\n  }\n}\n\nexport interface FileBatchRetrieveParams {\n  /**\n   * The ID of the vector store that the file batch belongs to.\n   */\n  vector_store_id: string;\n}\n\nexport interface FileBatchCancelParams {\n  /**\n   * The ID of the vector store that the file batch belongs to.\n   */\n  vector_store_id: string;\n}\n\nexport interface FileBatchListFilesParams extends CursorPageParams {\n  /**\n   * Path param: The ID of the vector store that the files belong to.\n   */\n  vector_store_id: string;\n\n  /**\n   * Query param: A cursor for use in pagination. `before` is an object ID that\n   * defines your place in the list. For instance, if you make a list request and\n   * receive 100 objects, starting with obj_foo, your subsequent call can include\n   * before=obj_foo in order to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Query param: Filter by file status. One of `in_progress`, `completed`, `failed`,\n   * `cancelled`.\n   */\n  filter?: 'in_progress' | 'completed' | 'failed' | 'cancelled';\n\n  /**\n   * Query param: Sort order by the `created_at` timestamp of the objects. `asc` for\n   * ascending order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport declare namespace FileBatches {\n  export {\n    type VectorStoreFileBatch as VectorStoreFileBatch,\n    type FileBatchCreateParams as FileBatchCreateParams,\n    type FileBatchRetrieveParams as FileBatchRetrieveParams,\n    type FileBatchCancelParams as FileBatchCancelParams,\n    type FileBatchListFilesParams as FileBatchListFilesParams,\n  };\n}\n\nexport { type VectorStoreFilesPage };\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as VectorStoresAPI from './vector-stores';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise, Page } from '../../core/pagination';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { sleep } from '../../internal/utils';\nimport { Uploadable } from '../../uploads';\nimport { path } from '../../internal/utils/path';\n\nexport class Files extends APIResource {\n  /**\n   * Create a vector store file by attaching a\n   * [File](https://platform.openai.com/docs/api-reference/files) to a\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object).\n   */\n  create(\n    vectorStoreID: string,\n    body: FileCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFile> {\n    return this._client.post(path`/vector_stores/${vectorStoreID}/files`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Retrieves a vector store file.\n   */\n  retrieve(\n    fileID: string,\n    params: FileRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFile> {\n    const { vector_store_id } = params;\n    return this._client.get(path`/vector_stores/${vector_store_id}/files/${fileID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Update attributes on a vector store file.\n   */\n  update(fileID: string, params: FileUpdateParams, options?: RequestOptions): APIPromise<VectorStoreFile> {\n    const { vector_store_id, ...body } = params;\n    return this._client.post(path`/vector_stores/${vector_store_id}/files/${fileID}`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Returns a list of vector store files.\n   */\n  list(\n    vectorStoreID: string,\n    query: FileListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<VectorStoreFilesPage, VectorStoreFile> {\n    return this._client.getAPIList(path`/vector_stores/${vectorStoreID}/files`, CursorPage<VectorStoreFile>, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Delete a vector store file. This will remove the file from the vector store but\n   * the file itself will not be deleted. To delete the file, use the\n   * [delete file](https://platform.openai.com/docs/api-reference/files/delete)\n   * endpoint.\n   */\n  delete(\n    fileID: string,\n    params: FileDeleteParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFileDeleted> {\n    const { vector_store_id } = params;\n    return this._client.delete(path`/vector_stores/${vector_store_id}/files/${fileID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Attach a file to the given vector store and wait for it to be processed.\n   */\n  async createAndPoll(\n    vectorStoreId: string,\n    body: FileCreateParams,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFile> {\n    const file = await this.create(vectorStoreId, body, options);\n    return await this.poll(vectorStoreId, file.id, options);\n  }\n  /**\n   * Wait for the vector store file to finish processing.\n   *\n   * Note: this will return even if the file failed to process, you need to check\n   * file.last_error and file.status to handle these cases\n   */\n  async poll(\n    vectorStoreID: string,\n    fileID: string,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFile> {\n    const headers = buildHeaders([\n      options?.headers,\n      {\n        'X-Stainless-Poll-Helper': 'true',\n        'X-Stainless-Custom-Poll-Interval': options?.pollIntervalMs?.toString() ?? undefined,\n      },\n    ]);\n\n    while (true) {\n      const fileResponse = await this.retrieve(\n        fileID,\n        {\n          vector_store_id: vectorStoreID,\n        },\n        { ...options, headers },\n      ).withResponse();\n\n      const file = fileResponse.data;\n\n      switch (file.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = fileResponse.response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'completed':\n          return file;\n      }\n    }\n  }\n  /**\n   * Upload a file to the `files` API and then attach it to the given vector store.\n   *\n   * Note the file will be asynchronously processed (you can use the alternative\n   * polling helper method to wait for processing to complete).\n   */\n  async upload(vectorStoreId: string, file: Uploadable, options?: RequestOptions): Promise<VectorStoreFile> {\n    const fileInfo = await this._client.files.create({ file: file, purpose: 'assistants' }, options);\n    return this.create(vectorStoreId, { file_id: fileInfo.id }, options);\n  }\n  /**\n   * Add a file to a vector store and poll until processing is complete.\n   */\n  async uploadAndPoll(\n    vectorStoreId: string,\n    file: Uploadable,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFile> {\n    const fileInfo = await this.upload(vectorStoreId, file, options);\n    return await this.poll(vectorStoreId, fileInfo.id, options);\n  }\n\n  /**\n   * Retrieve the parsed contents of a vector store file.\n   */\n  content(\n    fileID: string,\n    params: FileContentParams,\n    options?: RequestOptions,\n  ): PagePromise<FileContentResponsesPage, FileContentResponse> {\n    const { vector_store_id } = params;\n    return this._client.getAPIList(\n      path`/vector_stores/${vector_store_id}/files/${fileID}/content`,\n      Page<FileContentResponse>,\n      { ...options, headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]) },\n    );\n  }\n}\n\nexport type VectorStoreFilesPage = CursorPage<VectorStoreFile>;\n\n// Note: no pagination actually occurs yet, this is for forwards-compatibility.\nexport type FileContentResponsesPage = Page<FileContentResponse>;\n\n/**\n * A list of files attached to a vector store.\n */\nexport interface VectorStoreFile {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store file was created.\n   */\n  created_at: number;\n\n  /**\n   * The last error associated with this vector store file. Will be `null` if there\n   * are no errors.\n   */\n  last_error: VectorStoreFile.LastError | null;\n\n  /**\n   * The object type, which is always `vector_store.file`.\n   */\n  object: 'vector_store.file';\n\n  /**\n   * The status of the vector store file, which can be either `in_progress`,\n   * `completed`, `cancelled`, or `failed`. The status `completed` indicates that the\n   * vector store file is ready for use.\n   */\n  status: 'in_progress' | 'completed' | 'cancelled' | 'failed';\n\n  /**\n   * The total vector store usage in bytes. Note that this may be different from the\n   * original file size.\n   */\n  usage_bytes: number;\n\n  /**\n   * The ID of the\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\n   * attached to.\n   */\n  vector_store_id: string;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\n   * length of 64 characters. Values are strings with a maximum length of 512\n   * characters, booleans, or numbers.\n   */\n  attributes?: { [key: string]: string | number | boolean } | null;\n\n  /**\n   * The strategy used to chunk the file.\n   */\n  chunking_strategy?: VectorStoresAPI.FileChunkingStrategy;\n}\n\nexport namespace VectorStoreFile {\n  /**\n   * The last error associated with this vector store file. Will be `null` if there\n   * are no errors.\n   */\n  export interface LastError {\n    /**\n     * One of `server_error`, `unsupported_file`, or `invalid_file`.\n     */\n    code: 'server_error' | 'unsupported_file' | 'invalid_file';\n\n    /**\n     * A human-readable description of the error.\n     */\n    message: string;\n  }\n}\n\nexport interface VectorStoreFileDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'vector_store.file.deleted';\n}\n\nexport interface FileContentResponse {\n  /**\n   * The text content\n   */\n  text?: string;\n\n  /**\n   * The content type (currently only `\"text\"`)\n   */\n  type?: string;\n}\n\nexport interface FileCreateParams {\n  /**\n   * A [File](https://platform.openai.com/docs/api-reference/files) ID that the\n   * vector store should use. Useful for tools like `file_search` that can access\n   * files.\n   */\n  file_id: string;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\n   * length of 64 characters. Values are strings with a maximum length of 512\n   * characters, booleans, or numbers.\n   */\n  attributes?: { [key: string]: string | number | boolean } | null;\n\n  /**\n   * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n   * strategy. Only applicable if `file_ids` is non-empty.\n   */\n  chunking_strategy?: VectorStoresAPI.FileChunkingStrategyParam;\n}\n\nexport interface FileRetrieveParams {\n  /**\n   * The ID of the vector store that the file belongs to.\n   */\n  vector_store_id: string;\n}\n\nexport interface FileUpdateParams {\n  /**\n   * Path param: The ID of the vector store the file belongs to.\n   */\n  vector_store_id: string;\n\n  /**\n   * Body param: Set of 16 key-value pairs that can be attached to an object. This\n   * can be useful for storing additional information about the object in a\n   * structured format, and querying for objects via API or the dashboard. Keys are\n   * strings with a maximum length of 64 characters. Values are strings with a\n   * maximum length of 512 characters, booleans, or numbers.\n   */\n  attributes: { [key: string]: string | number | boolean } | null;\n}\n\nexport interface FileListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * starting with obj_foo, your subsequent call can include before=obj_foo in order\n   * to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.\n   */\n  filter?: 'in_progress' | 'completed' | 'failed' | 'cancelled';\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface FileDeleteParams {\n  /**\n   * The ID of the vector store that the file belongs to.\n   */\n  vector_store_id: string;\n}\n\nexport interface FileContentParams {\n  /**\n   * The ID of the vector store.\n   */\n  vector_store_id: string;\n}\n\nexport declare namespace Files {\n  export {\n    type VectorStoreFile as VectorStoreFile,\n    type VectorStoreFileDeleted as VectorStoreFileDeleted,\n    type FileContentResponse as FileContentResponse,\n    type VectorStoreFilesPage as VectorStoreFilesPage,\n    type FileContentResponsesPage as FileContentResponsesPage,\n    type FileCreateParams as FileCreateParams,\n    type FileRetrieveParams as FileRetrieveParams,\n    type FileUpdateParams as FileUpdateParams,\n    type FileListParams as FileListParams,\n    type FileDeleteParams as FileDeleteParams,\n    type FileContentParams as FileContentParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as Shared from '../shared';\nimport * as FileBatchesAPI from './file-batches';\nimport {\n  FileBatchCancelParams,\n  FileBatchCreateParams,\n  FileBatchListFilesParams,\n  FileBatchRetrieveParams,\n  FileBatches,\n  VectorStoreFileBatch,\n} from './file-batches';\nimport * as FilesAPI from './files';\nimport {\n  FileContentParams,\n  FileContentResponse,\n  FileContentResponsesPage,\n  FileCreateParams,\n  FileDeleteParams,\n  FileListParams,\n  FileRetrieveParams,\n  FileUpdateParams,\n  Files,\n  VectorStoreFile,\n  VectorStoreFileDeleted,\n  VectorStoreFilesPage,\n} from './files';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, Page, PagePromise } from '../../core/pagination';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { path } from '../../internal/utils/path';\n\nexport class VectorStores extends APIResource {\n  files: FilesAPI.Files = new FilesAPI.Files(this._client);\n  fileBatches: FileBatchesAPI.FileBatches = new FileBatchesAPI.FileBatches(this._client);\n\n  /**\n   * Create a vector store.\n   */\n  create(body: VectorStoreCreateParams, options?: RequestOptions): APIPromise<VectorStore> {\n    return this._client.post('/vector_stores', {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Retrieves a vector store.\n   */\n  retrieve(vectorStoreID: string, options?: RequestOptions): APIPromise<VectorStore> {\n    return this._client.get(path`/vector_stores/${vectorStoreID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Modifies a vector store.\n   */\n  update(\n    vectorStoreID: string,\n    body: VectorStoreUpdateParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStore> {\n    return this._client.post(path`/vector_stores/${vectorStoreID}`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Returns a list of vector stores.\n   */\n  list(\n    query: VectorStoreListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<VectorStoresPage, VectorStore> {\n    return this._client.getAPIList('/vector_stores', CursorPage<VectorStore>, {\n      query,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Delete a vector store.\n   */\n  delete(vectorStoreID: string, options?: RequestOptions): APIPromise<VectorStoreDeleted> {\n    return this._client.delete(path`/vector_stores/${vectorStoreID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Search a vector store for relevant chunks based on a query and file attributes\n   * filter.\n   */\n  search(\n    vectorStoreID: string,\n    body: VectorStoreSearchParams,\n    options?: RequestOptions,\n  ): PagePromise<VectorStoreSearchResponsesPage, VectorStoreSearchResponse> {\n    return this._client.getAPIList(\n      path`/vector_stores/${vectorStoreID}/search`,\n      Page<VectorStoreSearchResponse>,\n      {\n        body,\n        method: 'post',\n        ...options,\n        headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n      },\n    );\n  }\n}\n\nexport type VectorStoresPage = CursorPage<VectorStore>;\n\n// Note: no pagination actually occurs yet, this is for forwards-compatibility.\nexport type VectorStoreSearchResponsesPage = Page<VectorStoreSearchResponse>;\n\n/**\n * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of\n * `800` and `chunk_overlap_tokens` of `400`.\n */\nexport interface AutoFileChunkingStrategyParam {\n  /**\n   * Always `auto`.\n   */\n  type: 'auto';\n}\n\n/**\n * The strategy used to chunk the file.\n */\nexport type FileChunkingStrategy = StaticFileChunkingStrategyObject | OtherFileChunkingStrategyObject;\n\n/**\n * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n * strategy. Only applicable if `file_ids` is non-empty.\n */\nexport type FileChunkingStrategyParam = AutoFileChunkingStrategyParam | StaticFileChunkingStrategyObjectParam;\n\n/**\n * This is returned when the chunking strategy is unknown. Typically, this is\n * because the file was indexed before the `chunking_strategy` concept was\n * introduced in the API.\n */\nexport interface OtherFileChunkingStrategyObject {\n  /**\n   * Always `other`.\n   */\n  type: 'other';\n}\n\nexport interface StaticFileChunkingStrategy {\n  /**\n   * The number of tokens that overlap between chunks. The default value is `400`.\n   *\n   * Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n   */\n  chunk_overlap_tokens: number;\n\n  /**\n   * The maximum number of tokens in each chunk. The default value is `800`. The\n   * minimum value is `100` and the maximum value is `4096`.\n   */\n  max_chunk_size_tokens: number;\n}\n\nexport interface StaticFileChunkingStrategyObject {\n  static: StaticFileChunkingStrategy;\n\n  /**\n   * Always `static`.\n   */\n  type: 'static';\n}\n\n/**\n * Customize your own chunking strategy by setting chunk size and chunk overlap.\n */\nexport interface StaticFileChunkingStrategyObjectParam {\n  static: StaticFileChunkingStrategy;\n\n  /**\n   * Always `static`.\n   */\n  type: 'static';\n}\n\n/**\n * A vector store is a collection of processed files can be used by the\n * `file_search` tool.\n */\nexport interface VectorStore {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store was created.\n   */\n  created_at: number;\n\n  file_counts: VectorStore.FileCounts;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store was last active.\n   */\n  last_active_at: number | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the vector store.\n   */\n  name: string;\n\n  /**\n   * The object type, which is always `vector_store`.\n   */\n  object: 'vector_store';\n\n  /**\n   * The status of the vector store, which can be either `expired`, `in_progress`, or\n   * `completed`. A status of `completed` indicates that the vector store is ready\n   * for use.\n   */\n  status: 'expired' | 'in_progress' | 'completed';\n\n  /**\n   * The total number of bytes used by the files in the vector store.\n   */\n  usage_bytes: number;\n\n  /**\n   * The expiration policy for a vector store.\n   */\n  expires_after?: VectorStore.ExpiresAfter;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store will expire.\n   */\n  expires_at?: number | null;\n}\n\nexport namespace VectorStore {\n  export interface FileCounts {\n    /**\n     * The number of files that were cancelled.\n     */\n    cancelled: number;\n\n    /**\n     * The number of files that have been successfully processed.\n     */\n    completed: number;\n\n    /**\n     * The number of files that have failed to process.\n     */\n    failed: number;\n\n    /**\n     * The number of files that are currently being processed.\n     */\n    in_progress: number;\n\n    /**\n     * The total number of files.\n     */\n    total: number;\n  }\n\n  /**\n   * The expiration policy for a vector store.\n   */\n  export interface ExpiresAfter {\n    /**\n     * Anchor timestamp after which the expiration policy applies. Supported anchors:\n     * `last_active_at`.\n     */\n    anchor: 'last_active_at';\n\n    /**\n     * The number of days after the anchor time that the vector store will expire.\n     */\n    days: number;\n  }\n}\n\nexport interface VectorStoreDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'vector_store.deleted';\n}\n\nexport interface VectorStoreSearchResponse {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\n   * length of 64 characters. Values are strings with a maximum length of 512\n   * characters, booleans, or numbers.\n   */\n  attributes: { [key: string]: string | number | boolean } | null;\n\n  /**\n   * Content chunks from the file.\n   */\n  content: Array<VectorStoreSearchResponse.Content>;\n\n  /**\n   * The ID of the vector store file.\n   */\n  file_id: string;\n\n  /**\n   * The name of the vector store file.\n   */\n  filename: string;\n\n  /**\n   * The similarity score for the result.\n   */\n  score: number;\n}\n\nexport namespace VectorStoreSearchResponse {\n  export interface Content {\n    /**\n     * The text content returned from search.\n     */\n    text: string;\n\n    /**\n     * The type of content.\n     */\n    type: 'text';\n  }\n}\n\nexport interface VectorStoreCreateParams {\n  /**\n   * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n   * strategy. Only applicable if `file_ids` is non-empty.\n   */\n  chunking_strategy?: FileChunkingStrategyParam;\n\n  /**\n   * A description for the vector store. Can be used to describe the vector store's\n   * purpose.\n   */\n  description?: string;\n\n  /**\n   * The expiration policy for a vector store.\n   */\n  expires_after?: VectorStoreCreateParams.ExpiresAfter;\n\n  /**\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n   * the vector store should use. Useful for tools like `file_search` that can access\n   * files.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The name of the vector store.\n   */\n  name?: string;\n}\n\nexport namespace VectorStoreCreateParams {\n  /**\n   * The expiration policy for a vector store.\n   */\n  export interface ExpiresAfter {\n    /**\n     * Anchor timestamp after which the expiration policy applies. Supported anchors:\n     * `last_active_at`.\n     */\n    anchor: 'last_active_at';\n\n    /**\n     * The number of days after the anchor time that the vector store will expire.\n     */\n    days: number;\n  }\n}\n\nexport interface VectorStoreUpdateParams {\n  /**\n   * The expiration policy for a vector store.\n   */\n  expires_after?: VectorStoreUpdateParams.ExpiresAfter | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The name of the vector store.\n   */\n  name?: string | null;\n}\n\nexport namespace VectorStoreUpdateParams {\n  /**\n   * The expiration policy for a vector store.\n   */\n  export interface ExpiresAfter {\n    /**\n     * Anchor timestamp after which the expiration policy applies. Supported anchors:\n     * `last_active_at`.\n     */\n    anchor: 'last_active_at';\n\n    /**\n     * The number of days after the anchor time that the vector store will expire.\n     */\n    days: number;\n  }\n}\n\nexport interface VectorStoreListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * starting with obj_foo, your subsequent call can include before=obj_foo in order\n   * to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface VectorStoreSearchParams {\n  /**\n   * A query string for a search\n   */\n  query: string | Array<string>;\n\n  /**\n   * A filter to apply based on file attributes.\n   */\n  filters?: Shared.ComparisonFilter | Shared.CompoundFilter;\n\n  /**\n   * The maximum number of results to return. This number should be between 1 and 50\n   * inclusive.\n   */\n  max_num_results?: number;\n\n  /**\n   * Ranking options for search.\n   */\n  ranking_options?: VectorStoreSearchParams.RankingOptions;\n\n  /**\n   * Whether to rewrite the natural language query for vector search.\n   */\n  rewrite_query?: boolean;\n}\n\nexport namespace VectorStoreSearchParams {\n  /**\n   * Ranking options for search.\n   */\n  export interface RankingOptions {\n    /**\n     * Enable re-ranking; set to `none` to disable, which can help reduce latency.\n     */\n    ranker?: 'none' | 'auto' | 'default-2024-11-15';\n\n    score_threshold?: number;\n  }\n}\n\nVectorStores.Files = Files;\nVectorStores.FileBatches = FileBatches;\n\nexport declare namespace VectorStores {\n  export {\n    type AutoFileChunkingStrategyParam as AutoFileChunkingStrategyParam,\n    type FileChunkingStrategy as FileChunkingStrategy,\n    type FileChunkingStrategyParam as FileChunkingStrategyParam,\n    type OtherFileChunkingStrategyObject as OtherFileChunkingStrategyObject,\n    type StaticFileChunkingStrategy as StaticFileChunkingStrategy,\n    type StaticFileChunkingStrategyObject as StaticFileChunkingStrategyObject,\n    type StaticFileChunkingStrategyObjectParam as StaticFileChunkingStrategyObjectParam,\n    type VectorStore as VectorStore,\n    type VectorStoreDeleted as VectorStoreDeleted,\n    type VectorStoreSearchResponse as VectorStoreSearchResponse,\n    type VectorStoresPage as VectorStoresPage,\n    type VectorStoreSearchResponsesPage as VectorStoreSearchResponsesPage,\n    type VectorStoreCreateParams as VectorStoreCreateParams,\n    type VectorStoreUpdateParams as VectorStoreUpdateParams,\n    type VectorStoreListParams as VectorStoreListParams,\n    type VectorStoreSearchParams as VectorStoreSearchParams,\n  };\n\n  export {\n    Files as Files,\n    type VectorStoreFile as VectorStoreFile,\n    type VectorStoreFileDeleted as VectorStoreFileDeleted,\n    type FileContentResponse as FileContentResponse,\n    type VectorStoreFilesPage as VectorStoreFilesPage,\n    type FileContentResponsesPage as FileContentResponsesPage,\n    type FileCreateParams as FileCreateParams,\n    type FileRetrieveParams as FileRetrieveParams,\n    type FileUpdateParams as FileUpdateParams,\n    type FileListParams as FileListParams,\n    type FileDeleteParams as FileDeleteParams,\n    type FileContentParams as FileContentParams,\n  };\n\n  export {\n    FileBatches as FileBatches,\n    type VectorStoreFileBatch as VectorStoreFileBatch,\n    type FileBatchCreateParams as FileBatchCreateParams,\n    type FileBatchRetrieveParams as FileBatchRetrieveParams,\n    type FileBatchCancelParams as FileBatchCancelParams,\n    type FileBatchListFilesParams as FileBatchListFilesParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../core/resource';\nimport { APIPromise } from '../core/api-promise';\nimport { ConversationCursorPage, type ConversationCursorPageParams, PagePromise } from '../core/pagination';\nimport { type Uploadable } from '../core/uploads';\nimport { buildHeaders } from '../internal/headers';\nimport { RequestOptions } from '../internal/request-options';\nimport { maybeMultipartFormRequestOptions } from '../internal/uploads';\nimport { path } from '../internal/utils/path';\n\nexport class Videos extends APIResource {\n  /**\n   * Create a new video generation job from a prompt and optional reference assets.\n   */\n  create(body: VideoCreateParams, options?: RequestOptions): APIPromise<Video> {\n    return this._client.post('/videos', maybeMultipartFormRequestOptions({ body, ...options }, this._client));\n  }\n\n  /**\n   * Fetch the latest metadata for a generated video.\n   */\n  retrieve(videoID: string, options?: RequestOptions): APIPromise<Video> {\n    return this._client.get(path`/videos/${videoID}`, options);\n  }\n\n  /**\n   * List recently generated videos for the current project.\n   */\n  list(\n    query: VideoListParams | null | undefined = {},\n    options?: RequestOptions,\n  ): PagePromise<VideosPage, Video> {\n    return this._client.getAPIList('/videos', ConversationCursorPage<Video>, { query, ...options });\n  }\n\n  /**\n   * Permanently delete a completed or failed video and its stored assets.\n   */\n  delete(videoID: string, options?: RequestOptions): APIPromise<VideoDeleteResponse> {\n    return this._client.delete(path`/videos/${videoID}`, options);\n  }\n\n  /**\n   * Download the generated video bytes or a derived preview asset.\n   *\n   * Streams the rendered video content for the specified video job.\n   */\n  downloadContent(\n    videoID: string,\n    query: VideoDownloadContentParams | null | undefined = {},\n    options?: RequestOptions,\n  ): APIPromise<Response> {\n    return this._client.get(path`/videos/${videoID}/content`, {\n      query,\n      ...options,\n      headers: buildHeaders([{ Accept: 'application/binary' }, options?.headers]),\n      __binaryResponse: true,\n    });\n  }\n\n  /**\n   * Create a remix of a completed video using a refreshed prompt.\n   */\n  remix(videoID: string, body: VideoRemixParams, options?: RequestOptions): APIPromise<Video> {\n    return this._client.post(\n      path`/videos/${videoID}/remix`,\n      maybeMultipartFormRequestOptions({ body, ...options }, this._client),\n    );\n  }\n}\n\nexport type VideosPage = ConversationCursorPage<Video>;\n\n/**\n * Structured information describing a generated video job.\n */\nexport interface Video {\n  /**\n   * Unique identifier for the video job.\n   */\n  id: string;\n\n  /**\n   * Unix timestamp (seconds) for when the job completed, if finished.\n   */\n  completed_at: number | null;\n\n  /**\n   * Unix timestamp (seconds) for when the job was created.\n   */\n  created_at: number;\n\n  /**\n   * Error payload that explains why generation failed, if applicable.\n   */\n  error: VideoCreateError | null;\n\n  /**\n   * Unix timestamp (seconds) for when the downloadable assets expire, if set.\n   */\n  expires_at: number | null;\n\n  /**\n   * The video generation model that produced the job.\n   */\n  model: VideoModel;\n\n  /**\n   * The object type, which is always `video`.\n   */\n  object: 'video';\n\n  /**\n   * Approximate completion percentage for the generation task.\n   */\n  progress: number;\n\n  /**\n   * The prompt that was used to generate the video.\n   */\n  prompt: string | null;\n\n  /**\n   * Identifier of the source video if this video is a remix.\n   */\n  remixed_from_video_id: string | null;\n\n  /**\n   * Duration of the generated clip in seconds.\n   */\n  seconds: VideoSeconds;\n\n  /**\n   * The resolution of the generated video.\n   */\n  size: VideoSize;\n\n  /**\n   * Current lifecycle status of the video job.\n   */\n  status: 'queued' | 'in_progress' | 'completed' | 'failed';\n}\n\n/**\n * An error that occurred while generating the response.\n */\nexport interface VideoCreateError {\n  /**\n   * A machine-readable error code that was returned.\n   */\n  code: string;\n\n  /**\n   * A human-readable description of the error that was returned.\n   */\n  message: string;\n}\n\nexport type VideoModel =\n  | (string & {})\n  | 'sora-2'\n  | 'sora-2-pro'\n  | 'sora-2-2025-10-06'\n  | 'sora-2-pro-2025-10-06'\n  | 'sora-2-2025-12-08';\n\nexport type VideoSeconds = '4' | '8' | '12';\n\nexport type VideoSize = '720x1280' | '1280x720' | '1024x1792' | '1792x1024';\n\n/**\n * Confirmation payload returned after deleting a video.\n */\nexport interface VideoDeleteResponse {\n  /**\n   * Identifier of the deleted video.\n   */\n  id: string;\n\n  /**\n   * Indicates that the video resource was deleted.\n   */\n  deleted: boolean;\n\n  /**\n   * The object type that signals the deletion response.\n   */\n  object: 'video.deleted';\n}\n\nexport interface VideoCreateParams {\n  /**\n   * Text prompt that describes the video to generate.\n   */\n  prompt: string;\n\n  /**\n   * Optional image reference that guides generation.\n   */\n  input_reference?: Uploadable;\n\n  /**\n   * The video generation model to use (allowed values: sora-2, sora-2-pro). Defaults\n   * to `sora-2`.\n   */\n  model?: VideoModel;\n\n  /**\n   * Clip duration in seconds (allowed values: 4, 8, 12). Defaults to 4 seconds.\n   */\n  seconds?: VideoSeconds;\n\n  /**\n   * Output resolution formatted as width x height (allowed values: 720x1280,\n   * 1280x720, 1024x1792, 1792x1024). Defaults to 720x1280.\n   */\n  size?: VideoSize;\n}\n\nexport interface VideoListParams extends ConversationCursorPageParams {\n  /**\n   * Sort order of results by timestamp. Use `asc` for ascending order or `desc` for\n   * descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface VideoDownloadContentParams {\n  /**\n   * Which downloadable asset to return. Defaults to the MP4 video.\n   */\n  variant?: 'video' | 'thumbnail' | 'spritesheet';\n}\n\nexport interface VideoRemixParams {\n  /**\n   * Updated text prompt that directs the remix generation.\n   */\n  prompt: string;\n}\n\nexport declare namespace Videos {\n  export {\n    type Video as Video,\n    type VideoCreateError as VideoCreateError,\n    type VideoModel as VideoModel,\n    type VideoSeconds as VideoSeconds,\n    type VideoSize as VideoSize,\n    type VideoDeleteResponse as VideoDeleteResponse,\n    type VideosPage as VideosPage,\n    type VideoCreateParams as VideoCreateParams,\n    type VideoListParams as VideoListParams,\n    type VideoDownloadContentParams as VideoDownloadContentParams,\n    type VideoRemixParams as VideoRemixParams,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { InvalidWebhookSignatureError } from '../../error';\nimport { APIResource } from '../../core/resource';\nimport { buildHeaders, HeadersLike } from '../../internal/headers';\n\nexport class Webhooks extends APIResource {\n  /**\n   * Validates that the given payload was sent by OpenAI and parses the payload.\n   */\n  async unwrap(\n    payload: string,\n    headers: HeadersLike,\n    secret: string | undefined | null = this._client.webhookSecret,\n    tolerance: number = 300,\n  ): Promise<UnwrapWebhookEvent> {\n    await this.verifySignature(payload, headers, secret, tolerance);\n\n    return JSON.parse(payload) as UnwrapWebhookEvent;\n  }\n\n  /**\n   * Validates whether or not the webhook payload was sent by OpenAI.\n   *\n   * An error will be raised if the webhook payload was not sent by OpenAI.\n   *\n   * @param payload - The webhook payload\n   * @param headers - The webhook headers\n   * @param secret - The webhook secret (optional, will use client secret if not provided)\n   * @param tolerance - Maximum age of the webhook in seconds (default: 300 = 5 minutes)\n   */\n  async verifySignature(\n    payload: string,\n    headers: HeadersLike,\n    secret: string | undefined | null = this._client.webhookSecret,\n    tolerance: number = 300,\n  ): Promise<void> {\n    if (\n      typeof crypto === 'undefined' ||\n      typeof crypto.subtle.importKey !== 'function' ||\n      typeof crypto.subtle.verify !== 'function'\n    ) {\n      throw new Error('Webhook signature verification is only supported when the `crypto` global is defined');\n    }\n\n    this.#validateSecret(secret);\n\n    const headersObj = buildHeaders([headers]).values;\n    const signatureHeader = this.#getRequiredHeader(headersObj, 'webhook-signature');\n    const timestamp = this.#getRequiredHeader(headersObj, 'webhook-timestamp');\n    const webhookId = this.#getRequiredHeader(headersObj, 'webhook-id');\n\n    // Validate timestamp to prevent replay attacks\n    const timestampSeconds = parseInt(timestamp, 10);\n    if (isNaN(timestampSeconds)) {\n      throw new InvalidWebhookSignatureError('Invalid webhook timestamp format');\n    }\n\n    const nowSeconds = Math.floor(Date.now() / 1000);\n\n    if (nowSeconds - timestampSeconds > tolerance) {\n      throw new InvalidWebhookSignatureError('Webhook timestamp is too old');\n    }\n\n    if (timestampSeconds > nowSeconds + tolerance) {\n      throw new InvalidWebhookSignatureError('Webhook timestamp is too new');\n    }\n\n    // Extract signatures from v1,<base64> format\n    // The signature header can have multiple values, separated by spaces.\n    // Each value is in the format v1,<base64>. We should accept if any match.\n    const signatures = signatureHeader\n      .split(' ')\n      .map((part) => (part.startsWith('v1,') ? part.substring(3) : part));\n\n    // Decode the secret if it starts with whsec_\n    const decodedSecret =\n      secret.startsWith('whsec_') ?\n        Buffer.from(secret.replace('whsec_', ''), 'base64')\n      : Buffer.from(secret, 'utf-8');\n\n    // Create the signed payload: {webhook_id}.{timestamp}.{payload}\n    const signedPayload = webhookId ? `${webhookId}.${timestamp}.${payload}` : `${timestamp}.${payload}`;\n\n    // Import the secret as a cryptographic key for HMAC\n    const key = await crypto.subtle.importKey(\n      'raw',\n      decodedSecret,\n      { name: 'HMAC', hash: 'SHA-256' },\n      false,\n      ['verify'],\n    );\n\n    // Check if any signature matches using timing-safe WebCrypto verify\n    for (const signature of signatures) {\n      try {\n        const signatureBytes = Buffer.from(signature, 'base64');\n        const isValid = await crypto.subtle.verify(\n          'HMAC',\n          key,\n          signatureBytes,\n          new TextEncoder().encode(signedPayload),\n        );\n\n        if (isValid) {\n          return; // Valid signature found\n        }\n      } catch {\n        // Invalid base64 or signature format, continue to next signature\n        continue;\n      }\n    }\n\n    throw new InvalidWebhookSignatureError(\n      'The given webhook signature does not match the expected signature',\n    );\n  }\n\n  #validateSecret(secret: string | null | undefined): asserts secret is string {\n    if (typeof secret !== 'string' || secret.length === 0) {\n      throw new Error(\n        `The webhook secret must either be set using the env var, OPENAI_WEBHOOK_SECRET, on the client class, OpenAI({ webhookSecret: '123' }), or passed to this function`,\n      );\n    }\n  }\n\n  #getRequiredHeader(headers: Headers, name: string): string {\n    if (!headers) {\n      throw new Error(`Headers are required`);\n    }\n\n    const value = headers.get(name);\n\n    if (value === null || value === undefined) {\n      throw new Error(`Missing required header: ${name}`);\n    }\n\n    return value;\n  }\n}\n\n/**\n * Sent when a batch API request has been cancelled.\n */\nexport interface BatchCancelledWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the batch API request was cancelled.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: BatchCancelledWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `batch.cancelled`.\n   */\n  type: 'batch.cancelled';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace BatchCancelledWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the batch API request.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a batch API request has been completed.\n */\nexport interface BatchCompletedWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the batch API request was completed.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: BatchCompletedWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `batch.completed`.\n   */\n  type: 'batch.completed';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace BatchCompletedWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the batch API request.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a batch API request has expired.\n */\nexport interface BatchExpiredWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the batch API request expired.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: BatchExpiredWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `batch.expired`.\n   */\n  type: 'batch.expired';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace BatchExpiredWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the batch API request.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a batch API request has failed.\n */\nexport interface BatchFailedWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the batch API request failed.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: BatchFailedWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `batch.failed`.\n   */\n  type: 'batch.failed';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace BatchFailedWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the batch API request.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when an eval run has been canceled.\n */\nexport interface EvalRunCanceledWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the eval run was canceled.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: EvalRunCanceledWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `eval.run.canceled`.\n   */\n  type: 'eval.run.canceled';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace EvalRunCanceledWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the eval run.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when an eval run has failed.\n */\nexport interface EvalRunFailedWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the eval run failed.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: EvalRunFailedWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `eval.run.failed`.\n   */\n  type: 'eval.run.failed';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace EvalRunFailedWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the eval run.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when an eval run has succeeded.\n */\nexport interface EvalRunSucceededWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the eval run succeeded.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: EvalRunSucceededWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `eval.run.succeeded`.\n   */\n  type: 'eval.run.succeeded';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace EvalRunSucceededWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the eval run.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a fine-tuning job has been cancelled.\n */\nexport interface FineTuningJobCancelledWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the fine-tuning job was cancelled.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: FineTuningJobCancelledWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `fine_tuning.job.cancelled`.\n   */\n  type: 'fine_tuning.job.cancelled';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace FineTuningJobCancelledWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the fine-tuning job.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a fine-tuning job has failed.\n */\nexport interface FineTuningJobFailedWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the fine-tuning job failed.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: FineTuningJobFailedWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `fine_tuning.job.failed`.\n   */\n  type: 'fine_tuning.job.failed';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace FineTuningJobFailedWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the fine-tuning job.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a fine-tuning job has succeeded.\n */\nexport interface FineTuningJobSucceededWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the fine-tuning job succeeded.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: FineTuningJobSucceededWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `fine_tuning.job.succeeded`.\n   */\n  type: 'fine_tuning.job.succeeded';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace FineTuningJobSucceededWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the fine-tuning job.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when Realtime API Receives a incoming SIP call.\n */\nexport interface RealtimeCallIncomingWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the model response was completed.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: RealtimeCallIncomingWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `realtime.call.incoming`.\n   */\n  type: 'realtime.call.incoming';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace RealtimeCallIncomingWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of this call.\n     */\n    call_id: string;\n\n    /**\n     * Headers from the SIP Invite.\n     */\n    sip_headers: Array<Data.SipHeader>;\n  }\n\n  export namespace Data {\n    /**\n     * A header from the SIP Invite.\n     */\n    export interface SipHeader {\n      /**\n       * Name of the SIP Header.\n       */\n      name: string;\n\n      /**\n       * Value of the SIP Header.\n       */\n      value: string;\n    }\n  }\n}\n\n/**\n * Sent when a background response has been cancelled.\n */\nexport interface ResponseCancelledWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the model response was cancelled.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: ResponseCancelledWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `response.cancelled`.\n   */\n  type: 'response.cancelled';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace ResponseCancelledWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the model response.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a background response has been completed.\n */\nexport interface ResponseCompletedWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the model response was completed.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: ResponseCompletedWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `response.completed`.\n   */\n  type: 'response.completed';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace ResponseCompletedWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the model response.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a background response has failed.\n */\nexport interface ResponseFailedWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the model response failed.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: ResponseFailedWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `response.failed`.\n   */\n  type: 'response.failed';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace ResponseFailedWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the model response.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a background response has been interrupted.\n */\nexport interface ResponseIncompleteWebhookEvent {\n  /**\n   * The unique ID of the event.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) of when the model response was interrupted.\n   */\n  created_at: number;\n\n  /**\n   * Event data payload.\n   */\n  data: ResponseIncompleteWebhookEvent.Data;\n\n  /**\n   * The type of the event. Always `response.incomplete`.\n   */\n  type: 'response.incomplete';\n\n  /**\n   * The object of the event. Always `event`.\n   */\n  object?: 'event';\n}\n\nexport namespace ResponseIncompleteWebhookEvent {\n  /**\n   * Event data payload.\n   */\n  export interface Data {\n    /**\n     * The unique ID of the model response.\n     */\n    id: string;\n  }\n}\n\n/**\n * Sent when a batch API request has been cancelled.\n */\nexport type UnwrapWebhookEvent =\n  | BatchCancelledWebhookEvent\n  | BatchCompletedWebhookEvent\n  | BatchExpiredWebhookEvent\n  | BatchFailedWebhookEvent\n  | EvalRunCanceledWebhookEvent\n  | EvalRunFailedWebhookEvent\n  | EvalRunSucceededWebhookEvent\n  | FineTuningJobCancelledWebhookEvent\n  | FineTuningJobFailedWebhookEvent\n  | FineTuningJobSucceededWebhookEvent\n  | RealtimeCallIncomingWebhookEvent\n  | ResponseCancelledWebhookEvent\n  | ResponseCompletedWebhookEvent\n  | ResponseFailedWebhookEvent\n  | ResponseIncompleteWebhookEvent;\n\nexport declare namespace Webhooks {\n  export {\n    type BatchCancelledWebhookEvent as BatchCancelledWebhookEvent,\n    type BatchCompletedWebhookEvent as BatchCompletedWebhookEvent,\n    type BatchExpiredWebhookEvent as BatchExpiredWebhookEvent,\n    type BatchFailedWebhookEvent as BatchFailedWebhookEvent,\n    type EvalRunCanceledWebhookEvent as EvalRunCanceledWebhookEvent,\n    type EvalRunFailedWebhookEvent as EvalRunFailedWebhookEvent,\n    type EvalRunSucceededWebhookEvent as EvalRunSucceededWebhookEvent,\n    type FineTuningJobCancelledWebhookEvent as FineTuningJobCancelledWebhookEvent,\n    type FineTuningJobFailedWebhookEvent as FineTuningJobFailedWebhookEvent,\n    type FineTuningJobSucceededWebhookEvent as FineTuningJobSucceededWebhookEvent,\n    type RealtimeCallIncomingWebhookEvent as RealtimeCallIncomingWebhookEvent,\n    type ResponseCancelledWebhookEvent as ResponseCancelledWebhookEvent,\n    type ResponseCompletedWebhookEvent as ResponseCompletedWebhookEvent,\n    type ResponseFailedWebhookEvent as ResponseFailedWebhookEvent,\n    type ResponseIncompleteWebhookEvent as ResponseIncompleteWebhookEvent,\n    type UnwrapWebhookEvent as UnwrapWebhookEvent,\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport type { RequestInit, RequestInfo, BodyInit } from './internal/builtin-types';\nimport type { HTTPMethod, PromiseOrValue, MergedRequestInit, FinalizedRequestInit } from './internal/types';\nimport { uuid4 } from './internal/utils/uuid';\nimport { validatePositiveInteger, isAbsoluteURL, safeJSON } from './internal/utils/values';\nimport { sleep } from './internal/utils/sleep';\nexport type { Logger, LogLevel } from './internal/utils/log';\nimport { castToError, isAbortError } from './internal/errors';\nimport type { APIResponseProps } from './internal/parse';\nimport { getPlatformHeaders } from './internal/detect-platform';\nimport * as Shims from './internal/shims';\nimport * as Opts from './internal/request-options';\nimport * as qs from './internal/qs';\nimport { VERSION } from './version';\nimport * as Errors from './core/error';\nimport * as Pagination from './core/pagination';\nimport {\n  AbstractPage,\n  type ConversationCursorPageParams,\n  ConversationCursorPageResponse,\n  type CursorPageParams,\n  CursorPageResponse,\n  PageResponse,\n} from './core/pagination';\nimport * as Uploads from './core/uploads';\nimport * as API from './resources/index';\nimport { APIPromise } from './core/api-promise';\nimport {\n  Batch,\n  BatchCreateParams,\n  BatchError,\n  BatchListParams,\n  BatchRequestCounts,\n  BatchUsage,\n  Batches,\n  BatchesPage,\n} from './resources/batches';\nimport {\n  Completion,\n  CompletionChoice,\n  CompletionCreateParams,\n  CompletionCreateParamsNonStreaming,\n  CompletionCreateParamsStreaming,\n  CompletionUsage,\n  Completions,\n} from './resources/completions';\nimport {\n  CreateEmbeddingResponse,\n  Embedding,\n  EmbeddingCreateParams,\n  EmbeddingModel,\n  Embeddings,\n} from './resources/embeddings';\nimport {\n  FileContent,\n  FileCreateParams,\n  FileDeleted,\n  FileListParams,\n  FileObject,\n  FileObjectsPage,\n  FilePurpose,\n  Files,\n} from './resources/files';\nimport {\n  Image,\n  ImageCreateVariationParams,\n  ImageEditCompletedEvent,\n  ImageEditParams,\n  ImageEditParamsNonStreaming,\n  ImageEditParamsStreaming,\n  ImageEditPartialImageEvent,\n  ImageEditStreamEvent,\n  ImageGenCompletedEvent,\n  ImageGenPartialImageEvent,\n  ImageGenStreamEvent,\n  ImageGenerateParams,\n  ImageGenerateParamsNonStreaming,\n  ImageGenerateParamsStreaming,\n  ImageModel,\n  Images,\n  ImagesResponse,\n} from './resources/images';\nimport { Model, ModelDeleted, Models, ModelsPage } from './resources/models';\nimport {\n  Moderation,\n  ModerationCreateParams,\n  ModerationCreateResponse,\n  ModerationImageURLInput,\n  ModerationModel,\n  ModerationMultiModalInput,\n  ModerationTextInput,\n  Moderations,\n} from './resources/moderations';\nimport {\n  Video,\n  VideoCreateError,\n  VideoCreateParams,\n  VideoDeleteResponse,\n  VideoDownloadContentParams,\n  VideoListParams,\n  VideoModel,\n  VideoRemixParams,\n  VideoSeconds,\n  VideoSize,\n  Videos,\n  VideosPage,\n} from './resources/videos';\nimport { Audio, AudioModel, AudioResponseFormat } from './resources/audio/audio';\nimport { Beta } from './resources/beta/beta';\nimport { Chat } from './resources/chat/chat';\nimport {\n  ContainerCreateParams,\n  ContainerCreateResponse,\n  ContainerListParams,\n  ContainerListResponse,\n  ContainerListResponsesPage,\n  ContainerRetrieveResponse,\n  Containers,\n} from './resources/containers/containers';\nimport { Conversations } from './resources/conversations/conversations';\nimport {\n  EvalCreateParams,\n  EvalCreateResponse,\n  EvalCustomDataSourceConfig,\n  EvalDeleteResponse,\n  EvalListParams,\n  EvalListResponse,\n  EvalListResponsesPage,\n  EvalRetrieveResponse,\n  EvalStoredCompletionsDataSourceConfig,\n  EvalUpdateParams,\n  EvalUpdateResponse,\n  Evals,\n} from './resources/evals/evals';\nimport { FineTuning } from './resources/fine-tuning/fine-tuning';\nimport { Graders } from './resources/graders/graders';\nimport { Realtime } from './resources/realtime/realtime';\nimport { Responses } from './resources/responses/responses';\nimport {\n  DeletedSkill,\n  Skill,\n  SkillCreateParams,\n  SkillList,\n  SkillListParams,\n  SkillUpdateParams,\n  Skills,\n  SkillsPage,\n} from './resources/skills/skills';\nimport {\n  Upload,\n  UploadCompleteParams,\n  UploadCreateParams,\n  Uploads as UploadsAPIUploads,\n} from './resources/uploads/uploads';\nimport {\n  AutoFileChunkingStrategyParam,\n  FileChunkingStrategy,\n  FileChunkingStrategyParam,\n  OtherFileChunkingStrategyObject,\n  StaticFileChunkingStrategy,\n  StaticFileChunkingStrategyObject,\n  StaticFileChunkingStrategyObjectParam,\n  VectorStore,\n  VectorStoreCreateParams,\n  VectorStoreDeleted,\n  VectorStoreListParams,\n  VectorStoreSearchParams,\n  VectorStoreSearchResponse,\n  VectorStoreSearchResponsesPage,\n  VectorStoreUpdateParams,\n  VectorStores,\n  VectorStoresPage,\n} from './resources/vector-stores/vector-stores';\nimport { Webhooks } from './resources/webhooks/webhooks';\nimport {\n  ChatCompletion,\n  ChatCompletionAllowedToolChoice,\n  ChatCompletionAllowedTools,\n  ChatCompletionAssistantMessageParam,\n  ChatCompletionAudio,\n  ChatCompletionAudioParam,\n  ChatCompletionChunk,\n  ChatCompletionContentPart,\n  ChatCompletionContentPartImage,\n  ChatCompletionContentPartInputAudio,\n  ChatCompletionContentPartRefusal,\n  ChatCompletionContentPartText,\n  ChatCompletionCreateParams,\n  ChatCompletionCreateParamsNonStreaming,\n  ChatCompletionCreateParamsStreaming,\n  ChatCompletionCustomTool,\n  ChatCompletionDeleted,\n  ChatCompletionDeveloperMessageParam,\n  ChatCompletionFunctionCallOption,\n  ChatCompletionFunctionMessageParam,\n  ChatCompletionFunctionTool,\n  ChatCompletionListParams,\n  ChatCompletionMessage,\n  ChatCompletionMessageCustomToolCall,\n  ChatCompletionMessageFunctionToolCall,\n  ChatCompletionMessageParam,\n  ChatCompletionMessageToolCall,\n  ChatCompletionModality,\n  ChatCompletionNamedToolChoice,\n  ChatCompletionNamedToolChoiceCustom,\n  ChatCompletionPredictionContent,\n  ChatCompletionReasoningEffort,\n  ChatCompletionRole,\n  ChatCompletionStoreMessage,\n  ChatCompletionStreamOptions,\n  ChatCompletionSystemMessageParam,\n  ChatCompletionTokenLogprob,\n  ChatCompletionTool,\n  ChatCompletionToolChoiceOption,\n  ChatCompletionToolMessageParam,\n  ChatCompletionUpdateParams,\n  ChatCompletionUserMessageParam,\n  ChatCompletionsPage,\n} from './resources/chat/completions/completions';\nimport { type Fetch } from './internal/builtin-types';\nimport { isRunningInBrowser } from './internal/detect-platform';\nimport { HeadersLike, NullableHeaders, buildHeaders } from './internal/headers';\nimport { FinalRequestOptions, RequestOptions } from './internal/request-options';\nimport { readEnv } from './internal/utils/env';\nimport {\n  type LogLevel,\n  type Logger,\n  formatRequestDetails,\n  loggerFor,\n  parseLogLevel,\n} from './internal/utils/log';\nimport { isEmptyObj } from './internal/utils/values';\n\nexport type ApiKeySetter = () => Promise<string>;\n\nexport interface ClientOptions {\n  /**\n   * API key used for authentication.\n   *\n   * - Accepts either a static string or an async function that resolves to a string.\n   * - Defaults to process.env['OPENAI_API_KEY'].\n   * - When a function is provided, it is invoked before each request so you can rotate\n   *   or refresh credentials at runtime.\n   * - The function must return a non-empty string; otherwise an OpenAIError is thrown.\n   * - If the function throws, the error is wrapped in an OpenAIError with the original\n   *   error available as `cause`.\n   */\n  apiKey?: string | ApiKeySetter | undefined;\n  /**\n   * Defaults to process.env['OPENAI_ORG_ID'].\n   */\n  organization?: string | null | undefined;\n\n  /**\n   * Defaults to process.env['OPENAI_PROJECT_ID'].\n   */\n  project?: string | null | undefined;\n\n  /**\n   * Defaults to process.env['OPENAI_WEBHOOK_SECRET'].\n   */\n  webhookSecret?: string | null | undefined;\n\n  /**\n   * Override the default base URL for the API, e.g., \"https://api.example.com/v2/\"\n   *\n   * Defaults to process.env['OPENAI_BASE_URL'].\n   */\n  baseURL?: string | null | undefined;\n\n  /**\n   * The maximum amount of time (in milliseconds) that the client should wait for a response\n   * from the server before timing out a single request.\n   *\n   * Note that request timeouts are retried by default, so in a worst-case scenario you may wait\n   * much longer than this timeout before the promise succeeds or fails.\n   *\n   * @unit milliseconds\n   */\n  timeout?: number | undefined;\n  /**\n   * Additional `RequestInit` options to be passed to `fetch` calls.\n   * Properties will be overridden by per-request `fetchOptions`.\n   */\n  fetchOptions?: MergedRequestInit | undefined;\n\n  /**\n   * Specify a custom `fetch` function implementation.\n   *\n   * If not provided, we expect that `fetch` is defined globally.\n   */\n  fetch?: Fetch | undefined;\n\n  /**\n   * The maximum number of times that the client will retry a request in case of a\n   * temporary failure, like a network error or a 5XX error from the server.\n   *\n   * @default 2\n   */\n  maxRetries?: number | undefined;\n\n  /**\n   * Default headers to include with every request to the API.\n   *\n   * These can be removed in individual requests by explicitly setting the\n   * header to `null` in request options.\n   */\n  defaultHeaders?: HeadersLike | undefined;\n\n  /**\n   * Default query parameters to include with every request to the API.\n   *\n   * These can be removed in individual requests by explicitly setting the\n   * param to `undefined` in request options.\n   */\n  defaultQuery?: Record<string, string | undefined> | undefined;\n\n  /**\n   * By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n   * Only set this option to `true` if you understand the risks and have appropriate mitigations in place.\n   */\n  dangerouslyAllowBrowser?: boolean | undefined;\n\n  /**\n   * Set the log level.\n   *\n   * Defaults to process.env['OPENAI_LOG'] or 'warn' if it isn't set.\n   */\n  logLevel?: LogLevel | undefined;\n\n  /**\n   * Set the logger.\n   *\n   * Defaults to globalThis.console.\n   */\n  logger?: Logger | undefined;\n}\n\n/**\n * API Client for interfacing with the OpenAI API.\n */\nexport class OpenAI {\n  apiKey: string;\n  organization: string | null;\n  project: string | null;\n  webhookSecret: string | null;\n\n  baseURL: string;\n  maxRetries: number;\n  timeout: number;\n  logger: Logger;\n  logLevel: LogLevel | undefined;\n  fetchOptions: MergedRequestInit | undefined;\n\n  private fetch: Fetch;\n  #encoder: Opts.RequestEncoder;\n  protected idempotencyHeader?: string;\n  protected _options: ClientOptions;\n\n  /**\n   * API Client for interfacing with the OpenAI API.\n   *\n   * @param {string | undefined} [opts.apiKey=process.env['OPENAI_API_KEY'] ?? undefined]\n   * @param {string | null | undefined} [opts.organization=process.env['OPENAI_ORG_ID'] ?? null]\n   * @param {string | null | undefined} [opts.project=process.env['OPENAI_PROJECT_ID'] ?? null]\n   * @param {string | null | undefined} [opts.webhookSecret=process.env['OPENAI_WEBHOOK_SECRET'] ?? null]\n   * @param {string} [opts.baseURL=process.env['OPENAI_BASE_URL'] ?? https://api.openai.com/v1] - Override the default base URL for the API.\n   * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.\n   * @param {MergedRequestInit} [opts.fetchOptions] - Additional `RequestInit` options to be passed to `fetch` calls.\n   * @param {Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.\n   * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.\n   * @param {HeadersLike} opts.defaultHeaders - Default headers to include with every request to the API.\n   * @param {Record<string, string | undefined>} opts.defaultQuery - Default query parameters to include with every request to the API.\n   * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n   */\n  constructor({\n    baseURL = readEnv('OPENAI_BASE_URL'),\n    apiKey = readEnv('OPENAI_API_KEY'),\n    organization = readEnv('OPENAI_ORG_ID') ?? null,\n    project = readEnv('OPENAI_PROJECT_ID') ?? null,\n    webhookSecret = readEnv('OPENAI_WEBHOOK_SECRET') ?? null,\n    ...opts\n  }: ClientOptions = {}) {\n    if (apiKey === undefined) {\n      throw new Errors.OpenAIError(\n        'Missing credentials. Please pass an `apiKey`, or set the `OPENAI_API_KEY` environment variable.',\n      );\n    }\n\n    const options: ClientOptions = {\n      apiKey,\n      organization,\n      project,\n      webhookSecret,\n      ...opts,\n      baseURL: baseURL || `https://api.openai.com/v1`,\n    };\n\n    if (!options.dangerouslyAllowBrowser && isRunningInBrowser()) {\n      throw new Errors.OpenAIError(\n        \"It looks like you're running in a browser-like environment.\\n\\nThis is disabled by default, as it risks exposing your secret API credentials to attackers.\\nIf you understand the risks and have appropriate mitigations in place,\\nyou can set the `dangerouslyAllowBrowser` option to `true`, e.g.,\\n\\nnew OpenAI({ apiKey, dangerouslyAllowBrowser: true });\\n\\nhttps://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\\n\",\n      );\n    }\n\n    this.baseURL = options.baseURL!;\n    this.timeout = options.timeout ?? OpenAI.DEFAULT_TIMEOUT /* 10 minutes */;\n    this.logger = options.logger ?? console;\n    const defaultLogLevel = 'warn';\n    // Set default logLevel early so that we can log a warning in parseLogLevel.\n    this.logLevel = defaultLogLevel;\n    this.logLevel =\n      parseLogLevel(options.logLevel, 'ClientOptions.logLevel', this) ??\n      parseLogLevel(readEnv('OPENAI_LOG'), \"process.env['OPENAI_LOG']\", this) ??\n      defaultLogLevel;\n    this.fetchOptions = options.fetchOptions;\n    this.maxRetries = options.maxRetries ?? 2;\n    this.fetch = options.fetch ?? Shims.getDefaultFetch();\n    this.#encoder = Opts.FallbackEncoder;\n\n    this._options = options;\n\n    this.apiKey = typeof apiKey === 'string' ? apiKey : 'Missing Key';\n    this.organization = organization;\n    this.project = project;\n    this.webhookSecret = webhookSecret;\n  }\n\n  /**\n   * Create a new client instance re-using the same options given to the current client with optional overriding.\n   */\n  withOptions(options: Partial<ClientOptions>): this {\n    const client = new (this.constructor as any as new (props: ClientOptions) => typeof this)({\n      ...this._options,\n      baseURL: this.baseURL,\n      maxRetries: this.maxRetries,\n      timeout: this.timeout,\n      logger: this.logger,\n      logLevel: this.logLevel,\n      fetch: this.fetch,\n      fetchOptions: this.fetchOptions,\n      apiKey: this.apiKey,\n      organization: this.organization,\n      project: this.project,\n      webhookSecret: this.webhookSecret,\n      ...options,\n    });\n    return client;\n  }\n\n  /**\n   * Check whether the base URL is set to its default.\n   */\n  #baseURLOverridden(): boolean {\n    return this.baseURL !== 'https://api.openai.com/v1';\n  }\n\n  protected defaultQuery(): Record<string, string | undefined> | undefined {\n    return this._options.defaultQuery;\n  }\n\n  protected validateHeaders({ values, nulls }: NullableHeaders) {\n    return;\n  }\n\n  protected async authHeaders(opts: FinalRequestOptions): Promise<NullableHeaders | undefined> {\n    return buildHeaders([{ Authorization: `Bearer ${this.apiKey}` }]);\n  }\n\n  protected stringifyQuery(query: Record<string, unknown>): string {\n    return qs.stringify(query, { arrayFormat: 'brackets' });\n  }\n\n  private getUserAgent(): string {\n    return `${this.constructor.name}/JS ${VERSION}`;\n  }\n\n  protected defaultIdempotencyKey(): string {\n    return `stainless-node-retry-${uuid4()}`;\n  }\n\n  protected makeStatusError(\n    status: number,\n    error: Object,\n    message: string | undefined,\n    headers: Headers,\n  ): Errors.APIError {\n    return Errors.APIError.generate(status, error, message, headers);\n  }\n\n  async _callApiKey(): Promise<boolean> {\n    const apiKey = this._options.apiKey;\n    if (typeof apiKey !== 'function') return false;\n\n    let token: unknown;\n    try {\n      token = await apiKey();\n    } catch (err: any) {\n      if (err instanceof Errors.OpenAIError) throw err;\n      throw new Errors.OpenAIError(\n        `Failed to get token from 'apiKey' function: ${err.message}`,\n        // @ts-ignore\n        { cause: err },\n      );\n    }\n\n    if (typeof token !== 'string' || !token) {\n      throw new Errors.OpenAIError(\n        `Expected 'apiKey' function argument to return a string but it returned ${token}`,\n      );\n    }\n    this.apiKey = token;\n    return true;\n  }\n\n  buildURL(\n    path: string,\n    query: Record<string, unknown> | null | undefined,\n    defaultBaseURL?: string | undefined,\n  ): string {\n    const baseURL = (!this.#baseURLOverridden() && defaultBaseURL) || this.baseURL;\n    const url =\n      isAbsoluteURL(path) ?\n        new URL(path)\n      : new URL(baseURL + (baseURL.endsWith('/') && path.startsWith('/') ? path.slice(1) : path));\n\n    const defaultQuery = this.defaultQuery();\n    if (!isEmptyObj(defaultQuery)) {\n      query = { ...defaultQuery, ...query };\n    }\n\n    if (typeof query === 'object' && query && !Array.isArray(query)) {\n      url.search = this.stringifyQuery(query as Record<string, unknown>);\n    }\n\n    return url.toString();\n  }\n\n  /**\n   * Used as a callback for mutating the given `FinalRequestOptions` object.\n   */\n  protected async prepareOptions(options: FinalRequestOptions): Promise<void> {\n    await this._callApiKey();\n  }\n\n  /**\n   * Used as a callback for mutating the given `RequestInit` object.\n   *\n   * This is useful for cases where you want to add certain headers based off of\n   * the request properties, e.g. `method` or `url`.\n   */\n  protected async prepareRequest(\n    request: RequestInit,\n    { url, options }: { url: string; options: FinalRequestOptions },\n  ): Promise<void> {}\n\n  get<Rsp>(path: string, opts?: PromiseOrValue<RequestOptions>): APIPromise<Rsp> {\n    return this.methodRequest('get', path, opts);\n  }\n\n  post<Rsp>(path: string, opts?: PromiseOrValue<RequestOptions>): APIPromise<Rsp> {\n    return this.methodRequest('post', path, opts);\n  }\n\n  patch<Rsp>(path: string, opts?: PromiseOrValue<RequestOptions>): APIPromise<Rsp> {\n    return this.methodRequest('patch', path, opts);\n  }\n\n  put<Rsp>(path: string, opts?: PromiseOrValue<RequestOptions>): APIPromise<Rsp> {\n    return this.methodRequest('put', path, opts);\n  }\n\n  delete<Rsp>(path: string, opts?: PromiseOrValue<RequestOptions>): APIPromise<Rsp> {\n    return this.methodRequest('delete', path, opts);\n  }\n\n  private methodRequest<Rsp>(\n    method: HTTPMethod,\n    path: string,\n    opts?: PromiseOrValue<RequestOptions>,\n  ): APIPromise<Rsp> {\n    return this.request(\n      Promise.resolve(opts).then((opts) => {\n        return { method, path, ...opts };\n      }),\n    );\n  }\n\n  request<Rsp>(\n    options: PromiseOrValue<FinalRequestOptions>,\n    remainingRetries: number | null = null,\n  ): APIPromise<Rsp> {\n    return new APIPromise(this, this.makeRequest(options, remainingRetries, undefined));\n  }\n\n  private async makeRequest(\n    optionsInput: PromiseOrValue<FinalRequestOptions>,\n    retriesRemaining: number | null,\n    retryOfRequestLogID: string | undefined,\n  ): Promise<APIResponseProps> {\n    const options = await optionsInput;\n    const maxRetries = options.maxRetries ?? this.maxRetries;\n    if (retriesRemaining == null) {\n      retriesRemaining = maxRetries;\n    }\n\n    await this.prepareOptions(options);\n\n    const { req, url, timeout } = await this.buildRequest(options, {\n      retryCount: maxRetries - retriesRemaining,\n    });\n\n    await this.prepareRequest(req, { url, options });\n\n    /** Not an API request ID, just for correlating local log entries. */\n    const requestLogID = 'log_' + ((Math.random() * (1 << 24)) | 0).toString(16).padStart(6, '0');\n    const retryLogStr = retryOfRequestLogID === undefined ? '' : `, retryOf: ${retryOfRequestLogID}`;\n    const startTime = Date.now();\n\n    loggerFor(this).debug(\n      `[${requestLogID}] sending request`,\n      formatRequestDetails({\n        retryOfRequestLogID,\n        method: options.method,\n        url,\n        options,\n        headers: req.headers,\n      }),\n    );\n\n    if (options.signal?.aborted) {\n      throw new Errors.APIUserAbortError();\n    }\n\n    const controller = new AbortController();\n    const response = await this.fetchWithTimeout(url, req, timeout, controller).catch(castToError);\n    const headersTime = Date.now();\n\n    if (response instanceof globalThis.Error) {\n      const retryMessage = `retrying, ${retriesRemaining} attempts remaining`;\n      if (options.signal?.aborted) {\n        throw new Errors.APIUserAbortError();\n      }\n      // detect native connection timeout errors\n      // deno throws \"TypeError: error sending request for url (https://example/): client error (Connect): tcp connect error: Operation timed out (os error 60): Operation timed out (os error 60)\"\n      // undici throws \"TypeError: fetch failed\" with cause \"ConnectTimeoutError: Connect Timeout Error (attempted address: example:443, timeout: 1ms)\"\n      // others do not provide enough information to distinguish timeouts from other connection errors\n      const isTimeout =\n        isAbortError(response) ||\n        /timed? ?out/i.test(String(response) + ('cause' in response ? String(response.cause) : ''));\n      if (retriesRemaining) {\n        loggerFor(this).info(\n          `[${requestLogID}] connection ${isTimeout ? 'timed out' : 'failed'} - ${retryMessage}`,\n        );\n        loggerFor(this).debug(\n          `[${requestLogID}] connection ${isTimeout ? 'timed out' : 'failed'} (${retryMessage})`,\n          formatRequestDetails({\n            retryOfRequestLogID,\n            url,\n            durationMs: headersTime - startTime,\n            message: response.message,\n          }),\n        );\n        return this.retryRequest(options, retriesRemaining, retryOfRequestLogID ?? requestLogID);\n      }\n      loggerFor(this).info(\n        `[${requestLogID}] connection ${isTimeout ? 'timed out' : 'failed'} - error; no more retries left`,\n      );\n      loggerFor(this).debug(\n        `[${requestLogID}] connection ${isTimeout ? 'timed out' : 'failed'} (error; no more retries left)`,\n        formatRequestDetails({\n          retryOfRequestLogID,\n          url,\n          durationMs: headersTime - startTime,\n          message: response.message,\n        }),\n      );\n      if (isTimeout) {\n        throw new Errors.APIConnectionTimeoutError();\n      }\n      throw new Errors.APIConnectionError({ cause: response });\n    }\n\n    const specialHeaders = [...response.headers.entries()]\n      .filter(([name]) => name === 'x-request-id')\n      .map(([name, value]) => ', ' + name + ': ' + JSON.stringify(value))\n      .join('');\n    const responseInfo = `[${requestLogID}${retryLogStr}${specialHeaders}] ${req.method} ${url} ${\n      response.ok ? 'succeeded' : 'failed'\n    } with status ${response.status} in ${headersTime - startTime}ms`;\n\n    if (!response.ok) {\n      const shouldRetry = await this.shouldRetry(response);\n      if (retriesRemaining && shouldRetry) {\n        const retryMessage = `retrying, ${retriesRemaining} attempts remaining`;\n\n        // We don't need the body of this response.\n        await Shims.CancelReadableStream(response.body);\n        loggerFor(this).info(`${responseInfo} - ${retryMessage}`);\n        loggerFor(this).debug(\n          `[${requestLogID}] response error (${retryMessage})`,\n          formatRequestDetails({\n            retryOfRequestLogID,\n            url: response.url,\n            status: response.status,\n            headers: response.headers,\n            durationMs: headersTime - startTime,\n          }),\n        );\n        return this.retryRequest(\n          options,\n          retriesRemaining,\n          retryOfRequestLogID ?? requestLogID,\n          response.headers,\n        );\n      }\n\n      const retryMessage = shouldRetry ? `error; no more retries left` : `error; not retryable`;\n\n      loggerFor(this).info(`${responseInfo} - ${retryMessage}`);\n\n      const errText = await response.text().catch((err: any) => castToError(err).message);\n      const errJSON = safeJSON(errText) as any;\n      const errMessage = errJSON ? undefined : errText;\n\n      loggerFor(this).debug(\n        `[${requestLogID}] response error (${retryMessage})`,\n        formatRequestDetails({\n          retryOfRequestLogID,\n          url: response.url,\n          status: response.status,\n          headers: response.headers,\n          message: errMessage,\n          durationMs: Date.now() - startTime,\n        }),\n      );\n\n      const err = this.makeStatusError(response.status, errJSON, errMessage, response.headers);\n      throw err;\n    }\n\n    loggerFor(this).info(responseInfo);\n    loggerFor(this).debug(\n      `[${requestLogID}] response start`,\n      formatRequestDetails({\n        retryOfRequestLogID,\n        url: response.url,\n        status: response.status,\n        headers: response.headers,\n        durationMs: headersTime - startTime,\n      }),\n    );\n\n    return { response, options, controller, requestLogID, retryOfRequestLogID, startTime };\n  }\n\n  getAPIList<Item, PageClass extends Pagination.AbstractPage<Item> = Pagination.AbstractPage<Item>>(\n    path: string,\n    Page: new (...args: any[]) => PageClass,\n    opts?: PromiseOrValue<RequestOptions>,\n  ): Pagination.PagePromise<PageClass, Item> {\n    return this.requestAPIList(\n      Page,\n      opts && 'then' in opts ?\n        opts.then((opts) => ({ method: 'get', path, ...opts }))\n      : { method: 'get', path, ...opts },\n    );\n  }\n\n  requestAPIList<\n    Item = unknown,\n    PageClass extends Pagination.AbstractPage<Item> = Pagination.AbstractPage<Item>,\n  >(\n    Page: new (...args: ConstructorParameters<typeof Pagination.AbstractPage>) => PageClass,\n    options: PromiseOrValue<FinalRequestOptions>,\n  ): Pagination.PagePromise<PageClass, Item> {\n    const request = this.makeRequest(options, null, undefined);\n    return new Pagination.PagePromise<PageClass, Item>(this as any as OpenAI, request, Page);\n  }\n\n  async fetchWithTimeout(\n    url: RequestInfo,\n    init: RequestInit | undefined,\n    ms: number,\n    controller: AbortController,\n  ): Promise<Response> {\n    const { signal, method, ...options } = init || {};\n    const abort = this._makeAbort(controller);\n    if (signal) signal.addEventListener('abort', abort, { once: true });\n\n    const timeout = setTimeout(abort, ms);\n\n    const isReadableBody =\n      ((globalThis as any).ReadableStream && options.body instanceof (globalThis as any).ReadableStream) ||\n      (typeof options.body === 'object' && options.body !== null && Symbol.asyncIterator in options.body);\n\n    const fetchOptions: RequestInit = {\n      signal: controller.signal as any,\n      ...(isReadableBody ? { duplex: 'half' } : {}),\n      method: 'GET',\n      ...options,\n    };\n    if (method) {\n      // Custom methods like 'patch' need to be uppercased\n      // See https://github.com/nodejs/undici/issues/2294\n      fetchOptions.method = method.toUpperCase();\n    }\n\n    try {\n      // use undefined this binding; fetch errors if bound to something else in browser/cloudflare\n      return await this.fetch.call(undefined, url, fetchOptions);\n    } finally {\n      clearTimeout(timeout);\n    }\n  }\n\n  private async shouldRetry(response: Response): Promise<boolean> {\n    // Note this is not a standard header.\n    const shouldRetryHeader = response.headers.get('x-should-retry');\n\n    // If the server explicitly says whether or not to retry, obey.\n    if (shouldRetryHeader === 'true') return true;\n    if (shouldRetryHeader === 'false') return false;\n\n    // Retry on request timeouts.\n    if (response.status === 408) return true;\n\n    // Retry on lock timeouts.\n    if (response.status === 409) return true;\n\n    // Retry on rate limits.\n    if (response.status === 429) return true;\n\n    // Retry internal errors.\n    if (response.status >= 500) return true;\n\n    return false;\n  }\n\n  private async retryRequest(\n    options: FinalRequestOptions,\n    retriesRemaining: number,\n    requestLogID: string,\n    responseHeaders?: Headers | undefined,\n  ): Promise<APIResponseProps> {\n    let timeoutMillis: number | undefined;\n\n    // Note the `retry-after-ms` header may not be standard, but is a good idea and we'd like proactive support for it.\n    const retryAfterMillisHeader = responseHeaders?.get('retry-after-ms');\n    if (retryAfterMillisHeader) {\n      const timeoutMs = parseFloat(retryAfterMillisHeader);\n      if (!Number.isNaN(timeoutMs)) {\n        timeoutMillis = timeoutMs;\n      }\n    }\n\n    // About the Retry-After header: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After\n    const retryAfterHeader = responseHeaders?.get('retry-after');\n    if (retryAfterHeader && !timeoutMillis) {\n      const timeoutSeconds = parseFloat(retryAfterHeader);\n      if (!Number.isNaN(timeoutSeconds)) {\n        timeoutMillis = timeoutSeconds * 1000;\n      } else {\n        timeoutMillis = Date.parse(retryAfterHeader) - Date.now();\n      }\n    }\n\n    // If the API asks us to wait a certain amount of time (and it's a reasonable amount),\n    // just do what it says, but otherwise calculate a default\n    if (!(timeoutMillis && 0 <= timeoutMillis && timeoutMillis < 60 * 1000)) {\n      const maxRetries = options.maxRetries ?? this.maxRetries;\n      timeoutMillis = this.calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries);\n    }\n    await sleep(timeoutMillis);\n\n    return this.makeRequest(options, retriesRemaining - 1, requestLogID);\n  }\n\n  private calculateDefaultRetryTimeoutMillis(retriesRemaining: number, maxRetries: number): number {\n    const initialRetryDelay = 0.5;\n    const maxRetryDelay = 8.0;\n\n    const numRetries = maxRetries - retriesRemaining;\n\n    // Apply exponential backoff, but not more than the max.\n    const sleepSeconds = Math.min(initialRetryDelay * Math.pow(2, numRetries), maxRetryDelay);\n\n    // Apply some jitter, take up to at most 25 percent of the retry time.\n    const jitter = 1 - Math.random() * 0.25;\n\n    return sleepSeconds * jitter * 1000;\n  }\n\n  async buildRequest(\n    inputOptions: FinalRequestOptions,\n    { retryCount = 0 }: { retryCount?: number } = {},\n  ): Promise<{ req: FinalizedRequestInit; url: string; timeout: number }> {\n    const options = { ...inputOptions };\n    const { method, path, query, defaultBaseURL } = options;\n\n    const url = this.buildURL(path!, query as Record<string, unknown>, defaultBaseURL);\n    if ('timeout' in options) validatePositiveInteger('timeout', options.timeout);\n    options.timeout = options.timeout ?? this.timeout;\n    const { bodyHeaders, body } = this.buildBody({ options });\n    const reqHeaders = await this.buildHeaders({ options: inputOptions, method, bodyHeaders, retryCount });\n\n    const req: FinalizedRequestInit = {\n      method,\n      headers: reqHeaders,\n      ...(options.signal && { signal: options.signal }),\n      ...((globalThis as any).ReadableStream &&\n        body instanceof (globalThis as any).ReadableStream && { duplex: 'half' }),\n      ...(body && { body }),\n      ...((this.fetchOptions as any) ?? {}),\n      ...((options.fetchOptions as any) ?? {}),\n    };\n\n    return { req, url, timeout: options.timeout };\n  }\n\n  private async buildHeaders({\n    options,\n    method,\n    bodyHeaders,\n    retryCount,\n  }: {\n    options: FinalRequestOptions;\n    method: HTTPMethod;\n    bodyHeaders: HeadersLike;\n    retryCount: number;\n  }): Promise<Headers> {\n    let idempotencyHeaders: HeadersLike = {};\n    if (this.idempotencyHeader && method !== 'get') {\n      if (!options.idempotencyKey) options.idempotencyKey = this.defaultIdempotencyKey();\n      idempotencyHeaders[this.idempotencyHeader] = options.idempotencyKey;\n    }\n\n    const headers = buildHeaders([\n      idempotencyHeaders,\n      {\n        Accept: 'application/json',\n        'User-Agent': this.getUserAgent(),\n        'X-Stainless-Retry-Count': String(retryCount),\n        ...(options.timeout ? { 'X-Stainless-Timeout': String(Math.trunc(options.timeout / 1000)) } : {}),\n        ...getPlatformHeaders(),\n        'OpenAI-Organization': this.organization,\n        'OpenAI-Project': this.project,\n      },\n      await this.authHeaders(options),\n      this._options.defaultHeaders,\n      bodyHeaders,\n      options.headers,\n    ]);\n\n    this.validateHeaders(headers);\n\n    return headers.values;\n  }\n\n  private _makeAbort(controller: AbortController) {\n    // note: we can't just inline this method inside `fetchWithTimeout()` because then the closure\n    //       would capture all request options, and cause a memory leak.\n    return () => controller.abort();\n  }\n\n  private buildBody({ options: { body, headers: rawHeaders } }: { options: FinalRequestOptions }): {\n    bodyHeaders: HeadersLike;\n    body: BodyInit | undefined;\n  } {\n    if (!body) {\n      return { bodyHeaders: undefined, body: undefined };\n    }\n    const headers = buildHeaders([rawHeaders]);\n    if (\n      // Pass raw type verbatim\n      ArrayBuffer.isView(body) ||\n      body instanceof ArrayBuffer ||\n      body instanceof DataView ||\n      (typeof body === 'string' &&\n        // Preserve legacy string encoding behavior for now\n        headers.values.has('content-type')) ||\n      // `Blob` is superset of `File`\n      ((globalThis as any).Blob && body instanceof (globalThis as any).Blob) ||\n      // `FormData` -> `multipart/form-data`\n      body instanceof FormData ||\n      // `URLSearchParams` -> `application/x-www-form-urlencoded`\n      body instanceof URLSearchParams ||\n      // Send chunked stream (each chunk has own `length`)\n      ((globalThis as any).ReadableStream && body instanceof (globalThis as any).ReadableStream)\n    ) {\n      return { bodyHeaders: undefined, body: body as BodyInit };\n    } else if (\n      typeof body === 'object' &&\n      (Symbol.asyncIterator in body ||\n        (Symbol.iterator in body && 'next' in body && typeof body.next === 'function'))\n    ) {\n      return { bodyHeaders: undefined, body: Shims.ReadableStreamFrom(body as AsyncIterable<Uint8Array>) };\n    } else if (\n      typeof body === 'object' &&\n      headers.values.get('content-type') === 'application/x-www-form-urlencoded'\n    ) {\n      return {\n        bodyHeaders: { 'content-type': 'application/x-www-form-urlencoded' },\n        body: this.stringifyQuery(body as Record<string, unknown>),\n      };\n    } else {\n      return this.#encoder({ body, headers });\n    }\n  }\n\n  static OpenAI = this;\n  static DEFAULT_TIMEOUT = 600000; // 10 minutes\n\n  static OpenAIError = Errors.OpenAIError;\n  static APIError = Errors.APIError;\n  static APIConnectionError = Errors.APIConnectionError;\n  static APIConnectionTimeoutError = Errors.APIConnectionTimeoutError;\n  static APIUserAbortError = Errors.APIUserAbortError;\n  static NotFoundError = Errors.NotFoundError;\n  static ConflictError = Errors.ConflictError;\n  static RateLimitError = Errors.RateLimitError;\n  static BadRequestError = Errors.BadRequestError;\n  static AuthenticationError = Errors.AuthenticationError;\n  static InternalServerError = Errors.InternalServerError;\n  static PermissionDeniedError = Errors.PermissionDeniedError;\n  static UnprocessableEntityError = Errors.UnprocessableEntityError;\n  static InvalidWebhookSignatureError = Errors.InvalidWebhookSignatureError;\n\n  static toFile = Uploads.toFile;\n\n  completions: API.Completions = new API.Completions(this);\n  chat: API.Chat = new API.Chat(this);\n  embeddings: API.Embeddings = new API.Embeddings(this);\n  files: API.Files = new API.Files(this);\n  images: API.Images = new API.Images(this);\n  audio: API.Audio = new API.Audio(this);\n  moderations: API.Moderations = new API.Moderations(this);\n  models: API.Models = new API.Models(this);\n  fineTuning: API.FineTuning = new API.FineTuning(this);\n  graders: API.Graders = new API.Graders(this);\n  vectorStores: API.VectorStores = new API.VectorStores(this);\n  webhooks: API.Webhooks = new API.Webhooks(this);\n  beta: API.Beta = new API.Beta(this);\n  batches: API.Batches = new API.Batches(this);\n  uploads: API.Uploads = new API.Uploads(this);\n  responses: API.Responses = new API.Responses(this);\n  realtime: API.Realtime = new API.Realtime(this);\n  conversations: API.Conversations = new API.Conversations(this);\n  evals: API.Evals = new API.Evals(this);\n  containers: API.Containers = new API.Containers(this);\n  skills: API.Skills = new API.Skills(this);\n  videos: API.Videos = new API.Videos(this);\n}\n\nOpenAI.Completions = Completions;\nOpenAI.Chat = Chat;\nOpenAI.Embeddings = Embeddings;\nOpenAI.Files = Files;\nOpenAI.Images = Images;\nOpenAI.Audio = Audio;\nOpenAI.Moderations = Moderations;\nOpenAI.Models = Models;\nOpenAI.FineTuning = FineTuning;\nOpenAI.Graders = Graders;\nOpenAI.VectorStores = VectorStores;\nOpenAI.Webhooks = Webhooks;\nOpenAI.Beta = Beta;\nOpenAI.Batches = Batches;\nOpenAI.Uploads = UploadsAPIUploads;\nOpenAI.Responses = Responses;\nOpenAI.Realtime = Realtime;\nOpenAI.Conversations = Conversations;\nOpenAI.Evals = Evals;\nOpenAI.Containers = Containers;\nOpenAI.Skills = Skills;\nOpenAI.Videos = Videos;\n\nexport declare namespace OpenAI {\n  export type RequestOptions = Opts.RequestOptions;\n\n  export import Page = Pagination.Page;\n  export { type PageResponse as PageResponse };\n\n  export import CursorPage = Pagination.CursorPage;\n  export { type CursorPageParams as CursorPageParams, type CursorPageResponse as CursorPageResponse };\n\n  export import ConversationCursorPage = Pagination.ConversationCursorPage;\n  export {\n    type ConversationCursorPageParams as ConversationCursorPageParams,\n    type ConversationCursorPageResponse as ConversationCursorPageResponse,\n  };\n\n  export {\n    Completions as Completions,\n    type Completion as Completion,\n    type CompletionChoice as CompletionChoice,\n    type CompletionUsage as CompletionUsage,\n    type CompletionCreateParams as CompletionCreateParams,\n    type CompletionCreateParamsNonStreaming as CompletionCreateParamsNonStreaming,\n    type CompletionCreateParamsStreaming as CompletionCreateParamsStreaming,\n  };\n\n  export {\n    Chat as Chat,\n    type ChatCompletion as ChatCompletion,\n    type ChatCompletionAllowedToolChoice as ChatCompletionAllowedToolChoice,\n    type ChatCompletionAssistantMessageParam as ChatCompletionAssistantMessageParam,\n    type ChatCompletionAudio as ChatCompletionAudio,\n    type ChatCompletionAudioParam as ChatCompletionAudioParam,\n    type ChatCompletionChunk as ChatCompletionChunk,\n    type ChatCompletionContentPart as ChatCompletionContentPart,\n    type ChatCompletionContentPartImage as ChatCompletionContentPartImage,\n    type ChatCompletionContentPartInputAudio as ChatCompletionContentPartInputAudio,\n    type ChatCompletionContentPartRefusal as ChatCompletionContentPartRefusal,\n    type ChatCompletionContentPartText as ChatCompletionContentPartText,\n    type ChatCompletionCustomTool as ChatCompletionCustomTool,\n    type ChatCompletionDeleted as ChatCompletionDeleted,\n    type ChatCompletionDeveloperMessageParam as ChatCompletionDeveloperMessageParam,\n    type ChatCompletionFunctionCallOption as ChatCompletionFunctionCallOption,\n    type ChatCompletionFunctionMessageParam as ChatCompletionFunctionMessageParam,\n    type ChatCompletionFunctionTool as ChatCompletionFunctionTool,\n    type ChatCompletionMessage as ChatCompletionMessage,\n    type ChatCompletionMessageCustomToolCall as ChatCompletionMessageCustomToolCall,\n    type ChatCompletionMessageFunctionToolCall as ChatCompletionMessageFunctionToolCall,\n    type ChatCompletionMessageParam as ChatCompletionMessageParam,\n    type ChatCompletionMessageToolCall as ChatCompletionMessageToolCall,\n    type ChatCompletionModality as ChatCompletionModality,\n    type ChatCompletionNamedToolChoice as ChatCompletionNamedToolChoice,\n    type ChatCompletionNamedToolChoiceCustom as ChatCompletionNamedToolChoiceCustom,\n    type ChatCompletionPredictionContent as ChatCompletionPredictionContent,\n    type ChatCompletionRole as ChatCompletionRole,\n    type ChatCompletionStoreMessage as ChatCompletionStoreMessage,\n    type ChatCompletionStreamOptions as ChatCompletionStreamOptions,\n    type ChatCompletionSystemMessageParam as ChatCompletionSystemMessageParam,\n    type ChatCompletionTokenLogprob as ChatCompletionTokenLogprob,\n    type ChatCompletionTool as ChatCompletionTool,\n    type ChatCompletionToolChoiceOption as ChatCompletionToolChoiceOption,\n    type ChatCompletionToolMessageParam as ChatCompletionToolMessageParam,\n    type ChatCompletionUserMessageParam as ChatCompletionUserMessageParam,\n    type ChatCompletionAllowedTools as ChatCompletionAllowedTools,\n    type ChatCompletionReasoningEffort as ChatCompletionReasoningEffort,\n    type ChatCompletionsPage as ChatCompletionsPage,\n    type ChatCompletionCreateParams as ChatCompletionCreateParams,\n    type ChatCompletionCreateParamsNonStreaming as ChatCompletionCreateParamsNonStreaming,\n    type ChatCompletionCreateParamsStreaming as ChatCompletionCreateParamsStreaming,\n    type ChatCompletionUpdateParams as ChatCompletionUpdateParams,\n    type ChatCompletionListParams as ChatCompletionListParams,\n  };\n\n  export {\n    Embeddings as Embeddings,\n    type CreateEmbeddingResponse as CreateEmbeddingResponse,\n    type Embedding as Embedding,\n    type EmbeddingModel as EmbeddingModel,\n    type EmbeddingCreateParams as EmbeddingCreateParams,\n  };\n\n  export {\n    Files as Files,\n    type FileContent as FileContent,\n    type FileDeleted as FileDeleted,\n    type FileObject as FileObject,\n    type FilePurpose as FilePurpose,\n    type FileObjectsPage as FileObjectsPage,\n    type FileCreateParams as FileCreateParams,\n    type FileListParams as FileListParams,\n  };\n\n  export {\n    Images as Images,\n    type Image as Image,\n    type ImageEditCompletedEvent as ImageEditCompletedEvent,\n    type ImageEditPartialImageEvent as ImageEditPartialImageEvent,\n    type ImageEditStreamEvent as ImageEditStreamEvent,\n    type ImageGenCompletedEvent as ImageGenCompletedEvent,\n    type ImageGenPartialImageEvent as ImageGenPartialImageEvent,\n    type ImageGenStreamEvent as ImageGenStreamEvent,\n    type ImageModel as ImageModel,\n    type ImagesResponse as ImagesResponse,\n    type ImageCreateVariationParams as ImageCreateVariationParams,\n    type ImageEditParams as ImageEditParams,\n    type ImageEditParamsNonStreaming as ImageEditParamsNonStreaming,\n    type ImageEditParamsStreaming as ImageEditParamsStreaming,\n    type ImageGenerateParams as ImageGenerateParams,\n    type ImageGenerateParamsNonStreaming as ImageGenerateParamsNonStreaming,\n    type ImageGenerateParamsStreaming as ImageGenerateParamsStreaming,\n  };\n\n  export { Audio as Audio, type AudioModel as AudioModel, type AudioResponseFormat as AudioResponseFormat };\n\n  export {\n    Moderations as Moderations,\n    type Moderation as Moderation,\n    type ModerationImageURLInput as ModerationImageURLInput,\n    type ModerationModel as ModerationModel,\n    type ModerationMultiModalInput as ModerationMultiModalInput,\n    type ModerationTextInput as ModerationTextInput,\n    type ModerationCreateResponse as ModerationCreateResponse,\n    type ModerationCreateParams as ModerationCreateParams,\n  };\n\n  export {\n    Models as Models,\n    type Model as Model,\n    type ModelDeleted as ModelDeleted,\n    type ModelsPage as ModelsPage,\n  };\n\n  export { FineTuning as FineTuning };\n\n  export { Graders as Graders };\n\n  export {\n    VectorStores as VectorStores,\n    type AutoFileChunkingStrategyParam as AutoFileChunkingStrategyParam,\n    type FileChunkingStrategy as FileChunkingStrategy,\n    type FileChunkingStrategyParam as FileChunkingStrategyParam,\n    type OtherFileChunkingStrategyObject as OtherFileChunkingStrategyObject,\n    type StaticFileChunkingStrategy as StaticFileChunkingStrategy,\n    type StaticFileChunkingStrategyObject as StaticFileChunkingStrategyObject,\n    type StaticFileChunkingStrategyObjectParam as StaticFileChunkingStrategyObjectParam,\n    type VectorStore as VectorStore,\n    type VectorStoreDeleted as VectorStoreDeleted,\n    type VectorStoreSearchResponse as VectorStoreSearchResponse,\n    type VectorStoresPage as VectorStoresPage,\n    type VectorStoreSearchResponsesPage as VectorStoreSearchResponsesPage,\n    type VectorStoreCreateParams as VectorStoreCreateParams,\n    type VectorStoreUpdateParams as VectorStoreUpdateParams,\n    type VectorStoreListParams as VectorStoreListParams,\n    type VectorStoreSearchParams as VectorStoreSearchParams,\n  };\n\n  export { Webhooks as Webhooks };\n\n  export { Beta as Beta };\n\n  export {\n    Batches as Batches,\n    type Batch as Batch,\n    type BatchError as BatchError,\n    type BatchRequestCounts as BatchRequestCounts,\n    type BatchUsage as BatchUsage,\n    type BatchesPage as BatchesPage,\n    type BatchCreateParams as BatchCreateParams,\n    type BatchListParams as BatchListParams,\n  };\n\n  export {\n    UploadsAPIUploads as Uploads,\n    type Upload as Upload,\n    type UploadCreateParams as UploadCreateParams,\n    type UploadCompleteParams as UploadCompleteParams,\n  };\n\n  export { Responses as Responses };\n\n  export { Realtime as Realtime };\n\n  export { Conversations as Conversations };\n\n  export {\n    Evals as Evals,\n    type EvalCustomDataSourceConfig as EvalCustomDataSourceConfig,\n    type EvalStoredCompletionsDataSourceConfig as EvalStoredCompletionsDataSourceConfig,\n    type EvalCreateResponse as EvalCreateResponse,\n    type EvalRetrieveResponse as EvalRetrieveResponse,\n    type EvalUpdateResponse as EvalUpdateResponse,\n    type EvalListResponse as EvalListResponse,\n    type EvalDeleteResponse as EvalDeleteResponse,\n    type EvalListResponsesPage as EvalListResponsesPage,\n    type EvalCreateParams as EvalCreateParams,\n    type EvalUpdateParams as EvalUpdateParams,\n    type EvalListParams as EvalListParams,\n  };\n\n  export {\n    Containers as Containers,\n    type ContainerCreateResponse as ContainerCreateResponse,\n    type ContainerRetrieveResponse as ContainerRetrieveResponse,\n    type ContainerListResponse as ContainerListResponse,\n    type ContainerListResponsesPage as ContainerListResponsesPage,\n    type ContainerCreateParams as ContainerCreateParams,\n    type ContainerListParams as ContainerListParams,\n  };\n\n  export {\n    Skills as Skills,\n    type DeletedSkill as DeletedSkill,\n    type Skill as Skill,\n    type SkillList as SkillList,\n    type SkillsPage as SkillsPage,\n    type SkillCreateParams as SkillCreateParams,\n    type SkillUpdateParams as SkillUpdateParams,\n    type SkillListParams as SkillListParams,\n  };\n\n  export {\n    Videos as Videos,\n    type Video as Video,\n    type VideoCreateError as VideoCreateError,\n    type VideoModel as VideoModel,\n    type VideoSeconds as VideoSeconds,\n    type VideoSize as VideoSize,\n    type VideoDeleteResponse as VideoDeleteResponse,\n    type VideosPage as VideosPage,\n    type VideoCreateParams as VideoCreateParams,\n    type VideoListParams as VideoListParams,\n    type VideoDownloadContentParams as VideoDownloadContentParams,\n    type VideoRemixParams as VideoRemixParams,\n  };\n\n  export type AllModels = API.AllModels;\n  export type ChatModel = API.ChatModel;\n  export type ComparisonFilter = API.ComparisonFilter;\n  export type CompoundFilter = API.CompoundFilter;\n  export type CustomToolInputFormat = API.CustomToolInputFormat;\n  export type ErrorObject = API.ErrorObject;\n  export type FunctionDefinition = API.FunctionDefinition;\n  export type FunctionParameters = API.FunctionParameters;\n  export type Metadata = API.Metadata;\n  export type Reasoning = API.Reasoning;\n  export type ReasoningEffort = API.ReasoningEffort;\n  export type ResponseFormatJSONObject = API.ResponseFormatJSONObject;\n  export type ResponseFormatJSONSchema = API.ResponseFormatJSONSchema;\n  export type ResponseFormatText = API.ResponseFormatText;\n  export type ResponseFormatTextGrammar = API.ResponseFormatTextGrammar;\n  export type ResponseFormatTextPython = API.ResponseFormatTextPython;\n  export type ResponsesModel = API.ResponsesModel;\n}\n", "import type { RequestInit } from './internal/builtin-types';\nimport type { NullableHeaders } from './internal/headers';\nimport { buildHeaders } from './internal/headers';\nimport * as Errors from './error';\nimport { FinalRequestOptions } from './internal/request-options';\nimport { isObj, readEnv } from './internal/utils';\nimport { ClientOptions, OpenAI } from './client';\n\n/** API Client for interfacing with the Azure OpenAI API. */\nexport interface AzureClientOptions extends ClientOptions {\n  /**\n   * Defaults to process.env['OPENAI_API_VERSION'].\n   */\n  apiVersion?: string | undefined;\n\n  /**\n   * Your Azure endpoint, including the resource, e.g. `https://example-resource.azure.openai.com/`\n   */\n  endpoint?: string | undefined;\n\n  /**\n   * A model deployment, if given, sets the base client URL to include `/deployments/{deployment}`.\n   * Note: this means you won't be able to use non-deployment endpoints. Not supported with Assistants APIs.\n   */\n  deployment?: string | undefined;\n\n  /**\n   * Defaults to process.env['AZURE_OPENAI_API_KEY'].\n   */\n  apiKey?: string | undefined;\n\n  /**\n   * A function that returns an access token for Microsoft Entra (formerly known as Azure Active Directory),\n   * which will be invoked on every request.\n   */\n  azureADTokenProvider?: (() => Promise<string>) | undefined;\n}\n\n/** API Client for interfacing with the Azure OpenAI API. */\nexport class AzureOpenAI extends OpenAI {\n  deploymentName: string | undefined;\n  apiVersion: string = '';\n\n  /**\n   * API Client for interfacing with the Azure OpenAI API.\n   *\n   * @param {string | undefined} [opts.apiVersion=process.env['OPENAI_API_VERSION'] ?? undefined]\n   * @param {string | undefined} [opts.endpoint=process.env['AZURE_OPENAI_ENDPOINT'] ?? undefined] - Your Azure endpoint, including the resource, e.g. `https://example-resource.azure.openai.com/`\n   * @param {string | undefined} [opts.apiKey=process.env['AZURE_OPENAI_API_KEY'] ?? undefined]\n   * @param {string | undefined} opts.deployment - A model deployment, if given, sets the base client URL to include `/deployments/{deployment}`.\n   * @param {string | null | undefined} [opts.organization=process.env['OPENAI_ORG_ID'] ?? null]\n   * @param {string} [opts.baseURL=process.env['OPENAI_BASE_URL']] - Sets the base URL for the API, e.g. `https://example-resource.azure.openai.com/openai/`.\n   * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.\n   * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.\n   * @param {Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.\n   * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.\n   * @param {Headers} opts.defaultHeaders - Default headers to include with every request to the API.\n   * @param {DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.\n   * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n   */\n  constructor({\n    baseURL = readEnv('OPENAI_BASE_URL'),\n    apiKey = readEnv('AZURE_OPENAI_API_KEY'),\n    apiVersion = readEnv('OPENAI_API_VERSION'),\n    endpoint,\n    deployment,\n    azureADTokenProvider,\n    dangerouslyAllowBrowser,\n    ...opts\n  }: AzureClientOptions = {}) {\n    if (!apiVersion) {\n      throw new Errors.OpenAIError(\n        \"The OPENAI_API_VERSION environment variable is missing or empty; either provide it, or instantiate the AzureOpenAI client with an apiVersion option, like new AzureOpenAI({ apiVersion: 'My API Version' }).\",\n      );\n    }\n\n    if (typeof azureADTokenProvider === 'function') {\n      dangerouslyAllowBrowser = true;\n    }\n\n    if (!azureADTokenProvider && !apiKey) {\n      throw new Errors.OpenAIError(\n        'Missing credentials. Please pass one of `apiKey` and `azureADTokenProvider`, or set the `AZURE_OPENAI_API_KEY` environment variable.',\n      );\n    }\n\n    if (azureADTokenProvider && apiKey) {\n      throw new Errors.OpenAIError(\n        'The `apiKey` and `azureADTokenProvider` arguments are mutually exclusive; only one can be passed at a time.',\n      );\n    }\n\n    opts.defaultQuery = { ...opts.defaultQuery, 'api-version': apiVersion };\n\n    if (!baseURL) {\n      if (!endpoint) {\n        endpoint = process.env['AZURE_OPENAI_ENDPOINT'];\n      }\n\n      if (!endpoint) {\n        throw new Errors.OpenAIError(\n          'Must provide one of the `baseURL` or `endpoint` arguments, or the `AZURE_OPENAI_ENDPOINT` environment variable',\n        );\n      }\n\n      baseURL = `${endpoint}/openai`;\n    } else {\n      if (endpoint) {\n        throw new Errors.OpenAIError('baseURL and endpoint are mutually exclusive');\n      }\n    }\n\n    super({\n      apiKey: azureADTokenProvider ?? apiKey,\n      baseURL,\n      ...opts,\n      ...(dangerouslyAllowBrowser !== undefined ? { dangerouslyAllowBrowser } : {}),\n    });\n\n    this.apiVersion = apiVersion;\n    this.deploymentName = deployment;\n  }\n\n  override async buildRequest(\n    options: FinalRequestOptions,\n    props: { retryCount?: number } = {},\n  ): Promise<{ req: RequestInit & { headers: Headers }; url: string; timeout: number }> {\n    if (_deployments_endpoints.has(options.path) && options.method === 'post' && options.body !== undefined) {\n      if (!isObj(options.body)) {\n        throw new Error('Expected request body to be an object');\n      }\n      const model = this.deploymentName || options.body['model'] || options.__metadata?.['model'];\n      if (model !== undefined && !this.baseURL.includes('/deployments')) {\n        options.path = `/deployments/${model}${options.path}`;\n      }\n    }\n    return super.buildRequest(options, props);\n  }\n\n  protected override async authHeaders(opts: FinalRequestOptions): Promise<NullableHeaders | undefined> {\n    if (typeof this._options.apiKey === 'string') {\n      return buildHeaders([{ 'api-key': this.apiKey }]);\n    }\n    return super.authHeaders(opts);\n  }\n}\n\nconst _deployments_endpoints = new Set([\n  '/completions',\n  '/chat/completions',\n  '/embeddings',\n  '/audio/transcriptions',\n  '/audio/translations',\n  '/audio/speech',\n  '/images/generations',\n  '/batches',\n  '/images/edits',\n]);\n", "import { APIConnectionTimeoutError, APIUserAbortError } from \"openai\";\nimport { ContextOverflowError } from \"@langchain/core/errors\";\nimport { addLangChainErrorFields } from \"./errors.js\";\n\nfunction _isOpenAIContextOverflowError(e: object): boolean {\n  const errorStr = String(e);\n  if (errorStr.includes(\"context_length_exceeded\")) {\n    return true;\n  }\n  if (\n    \"message\" in e &&\n    typeof e.message === \"string\" &&\n    (e.message.includes(\"Input tokens exceed the configured limit\") ||\n      e.message.includes(\"exceeds the context window\"))\n  ) {\n    return true;\n  }\n  return false;\n}\n\nexport function wrapOpenAIClientError(e: unknown) {\n  if (!e || typeof e !== \"object\") {\n    return e;\n  }\n\n  let error;\n  if (\n    e.constructor.name === APIConnectionTimeoutError.name &&\n    \"message\" in e &&\n    typeof e.message === \"string\"\n  ) {\n    error = new Error(e.message);\n    error.name = \"TimeoutError\";\n  } else if (\n    e.constructor.name === APIUserAbortError.name &&\n    \"message\" in e &&\n    typeof e.message === \"string\"\n  ) {\n    error = new Error(e.message);\n    error.name = \"AbortError\";\n  } else if (_isOpenAIContextOverflowError(e)) {\n    error = ContextOverflowError.fromError(e as Error);\n  } else if (\n    \"status\" in e &&\n    e.status === 400 &&\n    \"message\" in e &&\n    typeof e.message === \"string\" &&\n    e.message.includes(\"tool_calls\")\n  ) {\n    error = addLangChainErrorFields(e, \"INVALID_TOOL_RESULTS\");\n  } else if (\"status\" in e && e.status === 401) {\n    error = addLangChainErrorFields(e, \"MODEL_AUTHENTICATION\");\n  } else if (\"status\" in e && e.status === 429) {\n    error = addLangChainErrorFields(e, \"MODEL_RATE_LIMIT\");\n  } else if (\"status\" in e && e.status === 404) {\n    error = addLangChainErrorFields(e, \"MODEL_NOT_FOUND\");\n  } else {\n    error = e;\n  }\n  return error;\n}\n", "import type { OpenAI as OpenAIClient } from \"openai\";\nimport {\n  BaseMessage,\n  ChatMessage,\n  ContentBlock,\n  Data,\n} from \"@langchain/core/messages\";\n\nexport const iife = <T>(fn: () => T) => fn();\n\nexport function isReasoningModel(model?: string) {\n  if (!model) return false;\n  if (/^o\\d/.test(model ?? \"\")) return true;\n  if (model.startsWith(\"gpt-5\") && !model.startsWith(\"gpt-5-chat\")) return true;\n  return false;\n}\n\nexport function extractGenericMessageCustomRole(message: ChatMessage) {\n  if (\n    message.role !== \"system\" &&\n    message.role !== \"developer\" &&\n    message.role !== \"assistant\" &&\n    message.role !== \"user\" &&\n    message.role !== \"function\" &&\n    message.role !== \"tool\"\n  ) {\n    console.warn(`Unknown message role: ${message.role}`);\n  }\n\n  return message.role as OpenAIClient.ChatCompletionRole;\n}\n\nexport function getFilenameFromMetadata(\n  block:\n    | ContentBlock.Multimodal.File\n    | ContentBlock.Multimodal.Video\n    | Data.StandardFileBlock\n): string | undefined {\n  return (block.metadata?.filename ??\n    block.metadata?.name ??\n    block.metadata?.title) as string;\n}\n\nexport function getRequiredFilenameFromMetadata(\n  block:\n    | ContentBlock.Multimodal.File\n    | ContentBlock.Multimodal.Video\n    | Data.StandardFileBlock\n): string {\n  const filename = (block.metadata?.filename ??\n    block.metadata?.name ??\n    block.metadata?.title) as string;\n\n  if (!filename) {\n    throw new Error(\n      \"a filename or name or title is needed via meta-data for OpenAI when working with multimodal blocks\"\n    );\n  }\n\n  return filename;\n}\nexport function messageToOpenAIRole(\n  message: BaseMessage\n): OpenAIClient.ChatCompletionRole {\n  const type = message._getType();\n  switch (type) {\n    case \"system\":\n      return \"system\";\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"function\":\n      return \"function\";\n    case \"tool\":\n      return \"tool\";\n    case \"generic\": {\n      if (!ChatMessage.isInstance(message))\n        throw new Error(\"Invalid generic chat message\");\n      return extractGenericMessageCustomRole(message);\n    }\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\n\nexport function _modelPrefersResponsesAPI(model: string): boolean {\n  if (model.includes(\"gpt-5.2-pro\")) return true;\n  // Codex models are Responses API only\n  if (model.includes(\"codex\")) return true;\n  return false;\n}\n", "import { getEnv } from \"@langchain/core/utils/env\";\nimport { iife } from \"./misc.js\";\n\nexport interface OpenAIEndpointConfig {\n  azureOpenAIApiDeploymentName?: string;\n  azureOpenAIApiInstanceName?: string;\n  azureOpenAIApiKey?: string;\n  azureADTokenProvider?: () => Promise<string>;\n  azureOpenAIBasePath?: string;\n  baseURL?: string | null;\n  azureOpenAIEndpoint?: string;\n}\n\n/**\n * This function generates an endpoint URL for (Azure) OpenAI\n * based on the configuration parameters provided.\n *\n * @param {OpenAIEndpointConfig} config - The configuration object for the (Azure) endpoint.\n *\n * @property {string} config.azureOpenAIApiDeploymentName - The deployment name of Azure OpenAI.\n * @property {string} config.azureOpenAIApiInstanceName - The instance name of Azure OpenAI, e.g. `example-resource`.\n * @property {string} config.azureOpenAIApiKey - The API Key for Azure OpenAI.\n * @property {string} config.azureOpenAIBasePath - The base path for Azure OpenAI, e.g. `https://example-resource.azure.openai.com/openai/deployments/`.\n * @property {string} config.baseURL - Some other custom base path URL.\n * @property {string} config.azureOpenAIEndpoint - The endpoint for the Azure OpenAI instance, e.g. `https://example-resource.azure.openai.com/`.\n *\n * The function operates as follows:\n * - If both `azureOpenAIBasePath` and `azureOpenAIApiDeploymentName` (plus `azureOpenAIApiKey`) are provided, it returns an URL combining these two parameters (`${azureOpenAIBasePath}/${azureOpenAIApiDeploymentName}`).\n * - If both `azureOpenAIEndpoint` and `azureOpenAIApiDeploymentName` (plus `azureOpenAIApiKey`) are provided, it returns an URL combining these two parameters (`${azureOpenAIEndpoint}/openai/deployments/${azureOpenAIApiDeploymentName}`).\n * - If `azureOpenAIApiKey` is provided, it checks for `azureOpenAIApiInstanceName` and `azureOpenAIApiDeploymentName` and throws an error if any of these is missing. If both are provided, it generates an URL incorporating these parameters.\n * - If none of the above conditions are met, return any custom `baseURL`.\n * - The function returns the generated URL as a string, or undefined if no custom paths are specified.\n *\n * @throws Will throw an error if the necessary parameters for generating the URL are missing.\n *\n * @returns {string | undefined} The generated (Azure) OpenAI endpoint URL.\n */\nexport function getEndpoint(config: OpenAIEndpointConfig) {\n  const {\n    azureOpenAIApiDeploymentName,\n    azureOpenAIApiInstanceName,\n    azureOpenAIApiKey,\n    azureOpenAIBasePath,\n    baseURL,\n    azureADTokenProvider,\n    azureOpenAIEndpoint,\n  } = config;\n\n  if (\n    (azureOpenAIApiKey || azureADTokenProvider) &&\n    azureOpenAIBasePath &&\n    azureOpenAIApiDeploymentName\n  ) {\n    return `${azureOpenAIBasePath}/${azureOpenAIApiDeploymentName}`;\n  }\n  if (\n    (azureOpenAIApiKey || azureADTokenProvider) &&\n    azureOpenAIEndpoint &&\n    azureOpenAIApiDeploymentName\n  ) {\n    return `${azureOpenAIEndpoint}/openai/deployments/${azureOpenAIApiDeploymentName}`;\n  }\n\n  if (azureOpenAIApiKey || azureADTokenProvider) {\n    if (!azureOpenAIApiInstanceName) {\n      throw new Error(\n        \"azureOpenAIApiInstanceName is required when using azureOpenAIApiKey\"\n      );\n    }\n    if (!azureOpenAIApiDeploymentName) {\n      throw new Error(\n        \"azureOpenAIApiDeploymentName is a required parameter when using azureOpenAIApiKey\"\n      );\n    }\n    return `https://${azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${azureOpenAIApiDeploymentName}`;\n  }\n\n  return baseURL;\n}\n\ntype HeaderValue = string | undefined | null;\nexport type HeadersLike =\n  | Headers\n  | readonly HeaderValue[][]\n  | Record<string, HeaderValue | readonly HeaderValue[]>\n  | undefined\n  | null\n  // NullableHeaders\n  | { values: Headers; [key: string]: unknown };\n\nexport function isHeaders(headers: unknown): headers is Headers {\n  return (\n    typeof Headers !== \"undefined\" &&\n    headers !== null &&\n    typeof headers === \"object\" &&\n    Object.prototype.toString.call(headers) === \"[object Headers]\"\n  );\n}\n\n/**\n * Normalizes various header formats into a consistent Record format.\n *\n * This function accepts headers in multiple formats and converts them to a\n * Record<string, HeaderValue | readonly HeaderValue[]> for consistent handling.\n *\n * @param headers - The headers to normalize. Can be:\n *   - A Headers instance\n *   - An array of [key, value] pairs\n *   - A plain object with string keys\n *   - A NullableHeaders-like object with a 'values' property containing Headers\n *   - null or undefined\n * @returns A normalized Record containing the header key-value pairs\n *\n * @example\n * ```ts\n * // With Headers instance\n * const headers1 = new Headers([['content-type', 'application/json']]);\n * const normalized1 = normalizeHeaders(headers1);\n *\n * // With plain object\n * const headers2 = { 'content-type': 'application/json' };\n * const normalized2 = normalizeHeaders(headers2);\n *\n * // With array of pairs\n * const headers3 = [['content-type', 'application/json']];\n * const normalized3 = normalizeHeaders(headers3);\n * ```\n */\nexport function normalizeHeaders(\n  headers: HeadersLike\n): Record<string, HeaderValue | readonly HeaderValue[]> {\n  const output = iife(() => {\n    // If headers is a Headers instance\n    if (isHeaders(headers)) {\n      return headers;\n    }\n    // If headers is an array of [key, value] pairs\n    else if (Array.isArray(headers)) {\n      return new Headers(headers);\n    }\n    // If headers is a NullableHeaders-like object (has 'values' property that is a Headers)\n    else if (\n      typeof headers === \"object\" &&\n      headers !== null &&\n      \"values\" in headers &&\n      isHeaders(headers.values)\n    ) {\n      return headers.values;\n    }\n    // If headers is a plain object\n    else if (typeof headers === \"object\" && headers !== null) {\n      const entries: [string, string][] = Object.entries(headers)\n        .filter(([, v]) => typeof v === \"string\")\n        .map(([k, v]) => [k, v as string]);\n      return new Headers(entries);\n    }\n    return new Headers();\n  });\n\n  return Object.fromEntries(output.entries());\n}\n\nexport function getFormattedEnv() {\n  let env = getEnv();\n  if (env === \"node\" || env === \"deno\") {\n    env = `(${env}/${process.version}; ${process.platform}; ${process.arch})`;\n  }\n  return env;\n}\n\n// Note: ideally version would be imported from package.json, but there's\n// currently no good way to do that for all supported environments (Node, Deno, Browser).\nexport function getHeadersWithUserAgent(\n  headers: HeadersLike,\n  isAzure = false,\n  version = \"1.0.0\"\n): Record<string, string> {\n  const normalizedHeaders = normalizeHeaders(headers);\n  const env = getFormattedEnv();\n  const library = `langchainjs${isAzure ? \"-azure\" : \"\"}-openai`;\n  return {\n    ...normalizedHeaders,\n    \"User-Agent\": normalizedHeaders[\"User-Agent\"]\n      ? `${library}/${version} (${env})${normalizedHeaders[\"User-Agent\"]}`\n      : `${library}/${version} (${env})`,\n  };\n}\n", "import type { z as z3 } from \"zod/v3\";\nimport { CallbackManagerForToolRun } from \"../callbacks/manager.js\";\nimport type {\n  BaseLangChainParams,\n  ToolDefinition,\n} from \"../language_models/base.js\";\nimport type { RunnableConfig } from \"../runnables/config.js\";\nimport {\n  Runnable,\n  RunnableToolLike,\n  type RunnableInterface,\n} from \"../runnables/base.js\";\nimport {\n  type DirectToolOutput,\n  type ToolCall,\n  type ToolMessage,\n} from \"../messages/tool.js\";\nimport type { MessageContent } from \"../messages/base.js\";\nimport {\n  type InferInteropZodInput,\n  type InferInteropZodOutput,\n  type InteropZodType,\n  isInteropZodSchema,\n  type InteropZodObject,\n} from \"../utils/types/zod.js\";\n\nimport { JSONSchema } from \"../utils/json_schema.js\";\nimport type { BaseStore } from \"../stores.js\";\n\nexport type ResponseFormat = \"content\" | \"content_and_artifact\" | string;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type ToolOutputType = any;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type ContentAndArtifact = [MessageContent, any];\n\n/**\n * Conditional type that determines the return type of the {@link StructuredTool.invoke} method.\n * - If the input is a ToolCall, it returns a ToolMessage\n * - If the config is a runnable config and contains a toolCall property, it returns a ToolMessage\n * - Otherwise, it returns the original output type\n */\nexport type ToolReturnType<TInput, TConfig, TOutput> =\n  TOutput extends DirectToolOutput\n    ? TOutput\n    : TConfig extends { toolCall: { id: string } }\n      ? ToolMessage\n      : TConfig extends { toolCall: { id: undefined } }\n        ? TOutput\n        : TConfig extends { toolCall: { id?: string } }\n          ? TOutput | ToolMessage\n          : TInput extends ToolCall\n            ? ToolMessage\n            : TOutput;\n\n/**\n * Base type that establishes the types of input schemas that can be used for LangChain tool\n * definitions.\n */\nexport type ToolInputSchemaBase = z3.ZodTypeAny | JSONSchema;\n\n/**\n * Parameters for the Tool classes.\n */\nexport interface ToolParams extends BaseLangChainParams {\n  /**\n   * The tool response format.\n   *\n   * If \"content\" then the output of the tool is interpreted as the contents of a\n   * ToolMessage. If \"content_and_artifact\" then the output is expected to be a\n   * two-tuple corresponding to the (content, artifact) of a ToolMessage.\n   *\n   * @default \"content\"\n   */\n  responseFormat?: ResponseFormat;\n  /**\n   * Default config object for the tool runnable.\n   */\n  defaultConfig?: ToolRunnableConfig;\n  /**\n   * Whether to show full details in the thrown parsing errors.\n   *\n   * @default false\n   */\n  verboseParsingErrors?: boolean;\n  /**\n   * Metadata for the tool.\n   */\n  metadata?: Record<string, unknown>;\n  /**\n   * Optional provider-specific extra fields for the tool.\n   *\n   * This is used to pass provider-specific configuration that doesn't fit into\n   * standard tool fields.\n   */\n  extras?: Record<string, unknown>;\n}\n\nexport type ToolRunnableConfig<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  ConfigurableFieldType extends Record<string, any> = Record<string, any>,\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  ContextSchema = any,\n> = RunnableConfig<ConfigurableFieldType> & {\n  toolCall?: ToolCall;\n  context?: ContextSchema;\n};\n\n/**\n * Schema for defining tools.\n *\n * @version 0.2.19\n */\nexport interface StructuredToolParams extends Pick<\n  StructuredToolInterface,\n  \"name\" | \"schema\" | \"extras\"\n> {\n  /**\n   * An optional description of the tool to pass to the model.\n   */\n  description?: string;\n}\n\n/**\n * Utility type that resolves the output type of a tool input schema.\n *\n * Input & Output types are a concept used with Zod schema, as Zod allows for transforms to occur\n * during parsing. When using JSONSchema, input and output types are the same.\n *\n * The input type for a given schema should match the structure of the arguments that the LLM\n * generates as part of its {@link ToolCall}. The output type will be the type that results from\n * applying any transforms defined in your schema. If there are no transforms, the input and output\n * types will be the same.\n */\nexport type ToolInputSchemaOutputType<T> = T extends InteropZodType\n  ? InferInteropZodOutput<T>\n  : T extends JSONSchema\n    ? unknown\n    : never;\n\n/**\n * Utility type that resolves the input type of a tool input schema.\n *\n * Input & Output types are a concept used with Zod schema, as Zod allows for transforms to occur\n * during parsing. When using JSONSchema, input and output types are the same.\n *\n * The input type for a given schema should match the structure of the arguments that the LLM\n * generates as part of its {@link ToolCall}. The output type will be the type that results from\n * applying any transforms defined in your schema. If there are no transforms, the input and output\n * types will be the same.\n */\nexport type ToolInputSchemaInputType<T> = T extends InteropZodType\n  ? InferInteropZodInput<T>\n  : T extends JSONSchema\n    ? unknown\n    : never;\n\n/**\n * Defines the type that will be passed into a tool handler function as a result of a tool call.\n *\n * @param SchemaT - The type of the tool input schema. Usually you don't need to specify this.\n * @param SchemaInputT - The TypeScript type representing the structure of the tool arguments generated by the LLM. Useful for type checking tool handler functions when using JSONSchema.\n */\nexport type StructuredToolCallInput<\n  SchemaT = ToolInputSchemaBase,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n> =\n  | (ToolInputSchemaOutputType<SchemaT> extends string ? string : never)\n  | SchemaInputT\n  | ToolCall;\n\n/**\n * An input schema type for tools that accept a single string input.\n *\n * This schema defines a tool that takes an optional string parameter named \"input\".\n * It uses Zod's effects to transform the input and strip any extra properties.\n *\n * This is primarily used for creating simple string-based tools where the LLM\n * only needs to provide a single text value as input to the tool.\n */\nexport type StringInputToolSchema = z3.ZodType<\n  string | undefined,\n  z3.ZodTypeDef,\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  any\n>;\n\n/**\n * Defines the type for input to a tool's call method.\n *\n * This type is a convenience alias for StructuredToolCallInput with the input type\n * derived from the schema. It represents the possible inputs that can be passed to a tool,\n * which can be either:\n * - A string (if the tool accepts string input)\n * - A structured input matching the tool's schema\n * - A ToolCall object (typically from an LLM)\n *\n * @param SchemaT - The schema type for the tool input, defaults to StringInputToolSchema\n */\nexport type ToolCallInput<SchemaT = StringInputToolSchema> =\n  StructuredToolCallInput<SchemaT, ToolInputSchemaInputType<SchemaT>>;\n\n/**\n * Interface that defines the shape of a LangChain structured tool.\n *\n * A structured tool is a tool that uses a schema to define the structure of the arguments that the\n * LLM generates as part of its {@link ToolCall}.\n *\n * @param SchemaT - The type of the tool input schema. Usually you don't need to specify this.\n * @param SchemaInputT - The TypeScript type representing the structure of the tool arguments generated by the LLM. Useful for type checking tool handler functions when using JSONSchema.\n */\nexport interface StructuredToolInterface<\n  SchemaT = ToolInputSchemaBase,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n> extends RunnableInterface<\n  StructuredToolCallInput<SchemaT, SchemaInputT>,\n  ToolOutputT | ToolMessage\n> {\n  lc_namespace: string[];\n\n  /**\n   * A Zod schema representing the parameters of the tool.\n   */\n  schema: SchemaT;\n\n  /**\n   * Invokes the tool with the provided argument and configuration.\n   * @param arg The input argument for the tool.\n   * @param configArg Optional configuration for the tool call.\n   * @returns A Promise that resolves with the tool's output.\n   */\n  invoke<\n    TArg extends StructuredToolCallInput<SchemaT, SchemaInputT>,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    arg: TArg,\n    configArg?: TConfig\n  ): Promise<ToolReturnType<TArg, TConfig, ToolOutputT>>;\n\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.3.0.\n   *\n   * Calls the tool with the provided argument, configuration, and tags. It\n   * parses the input according to the schema, handles any errors, and\n   * manages callbacks.\n   * @param arg The input argument for the tool.\n   * @param configArg Optional configuration or callbacks for the tool.\n   * @param tags Optional tags for the tool.\n   * @returns A Promise that resolves with a string.\n   */\n  call<\n    TArg extends StructuredToolCallInput<SchemaT, SchemaInputT>,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    arg: TArg,\n    configArg?: TConfig,\n    /** @deprecated */\n    tags?: string[]\n  ): Promise<ToolReturnType<TArg, TConfig, ToolOutputT>>;\n\n  /**\n   * The name of the tool.\n   */\n  name: string;\n\n  /**\n   * A description of the tool.\n   */\n  description: string;\n\n  /**\n   * Whether to return the tool's output directly.\n   *\n   * Setting this to true means that after the tool is called,\n   * an agent should stop looping.\n   */\n  returnDirect: boolean;\n\n  /**\n   * Optional provider-specific extra fields for the tool.\n   *\n   * This is used to pass provider-specific configuration that doesn't fit into\n   * standard tool fields.\n   */\n  extras?: Record<string, unknown>;\n}\n\n/**\n * A special interface for tools that accept a string input, usually defined with the {@link Tool} class.\n *\n * @param SchemaT - The type of the tool input schema. Usually you don't need to specify this.\n * @param SchemaInputT - The TypeScript type representing the structure of the tool arguments generated by the LLM. Useful for type checking tool handler functions when using JSONSchema.\n */\nexport interface ToolInterface<\n  SchemaT = StringInputToolSchema,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n> extends StructuredToolInterface<SchemaT, SchemaInputT, ToolOutputT> {\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.3.0.\n   *\n   * Calls the tool with the provided argument and callbacks. It handles\n   * string inputs specifically.\n   * @param arg The input argument for the tool, which can be a string, undefined, or an input of the tool's schema.\n   * @param callbacks Optional callbacks for the tool.\n   * @returns A Promise that resolves with a string.\n   */\n  call<\n    TArg extends StructuredToolCallInput<SchemaT, SchemaInputT>,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    // TODO: shouldn't this be narrowed based on SchemaT?\n    arg: TArg,\n    callbacks?: TConfig\n  ): Promise<ToolReturnType<NonNullable<TArg>, TConfig, ToolOutputT>>;\n}\n\n/**\n * Base interface for the input parameters of the {@link DynamicTool} and\n * {@link DynamicStructuredTool} classes.\n */\nexport interface BaseDynamicToolInput extends ToolParams {\n  name: string;\n  description: string;\n  /**\n   * Whether to return the tool's output directly.\n   *\n   * Setting this to true means that after the tool is called,\n   * an agent should stop looping.\n   */\n  returnDirect?: boolean;\n}\n\n/**\n * Interface for the input parameters of the DynamicTool class.\n */\nexport interface DynamicToolInput<\n  ToolOutputT = ToolOutputType,\n> extends BaseDynamicToolInput {\n  func: (\n    input: string,\n    runManager?: CallbackManagerForToolRun,\n    config?: ToolRunnableConfig\n  ) => Promise<ToolOutputT> | AsyncGenerator<unknown, ToolOutputT>;\n}\n\n/**\n * Interface for the input parameters of the DynamicStructuredTool class.\n *\n * @param SchemaT - The type of the tool input schema. Usually you don't need to specify this.\n * @param SchemaOutputT - The TypeScript type representing the result of applying the schema to the tool arguments. Useful for type checking tool handler functions when using JSONSchema.\n */\nexport interface DynamicStructuredToolInput<\n  SchemaT = ToolInputSchemaBase,\n  SchemaOutputT = ToolInputSchemaOutputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n> extends BaseDynamicToolInput {\n  /**\n   * Tool handler function - the function that will be called when the tool is invoked.\n   *\n   * @param input - The input to the tool.\n   * @param runManager - The run manager for the tool.\n   * @param config - The configuration for the tool.\n   * @returns The result of the tool.\n   */\n  func: (\n    input: SchemaOutputT,\n    runManager?: CallbackManagerForToolRun,\n    config?: RunnableConfig\n  ) => Promise<ToolOutputT> | AsyncGenerator<unknown, ToolOutputT>;\n  schema: SchemaT;\n}\n\n/**\n * Confirm whether the inputted tool is an instance of `StructuredToolInterface`.\n *\n * @param {StructuredToolInterface | JSONSchema | undefined} tool The tool to check if it is an instance of `StructuredToolInterface`.\n * @returns {tool is StructuredToolInterface} Whether the inputted tool is an instance of `StructuredToolInterface`.\n */\nexport function isStructuredTool(\n  tool?: StructuredToolInterface | ToolDefinition | JSONSchema\n): tool is StructuredToolInterface {\n  return (\n    tool !== undefined &&\n    Array.isArray((tool as StructuredToolInterface).lc_namespace)\n  );\n}\n\n/**\n * Confirm whether the inputted tool is an instance of `RunnableToolLike`.\n *\n * @param {unknown | undefined} tool The tool to check if it is an instance of `RunnableToolLike`.\n * @returns {tool is RunnableToolLike} Whether the inputted tool is an instance of `RunnableToolLike`.\n */\nexport function isRunnableToolLike(tool?: unknown): tool is RunnableToolLike {\n  return (\n    tool !== undefined &&\n    Runnable.isRunnable(tool) &&\n    \"lc_name\" in tool.constructor &&\n    typeof tool.constructor.lc_name === \"function\" &&\n    tool.constructor.lc_name() === \"RunnableToolLike\"\n  );\n}\n\n/**\n * Confirm whether or not the tool contains the necessary properties to be considered a `StructuredToolParams`.\n *\n * @param {unknown | undefined} tool The object to check if it is a `StructuredToolParams`.\n * @returns {tool is StructuredToolParams} Whether the inputted object is a `StructuredToolParams`.\n */\nexport function isStructuredToolParams(\n  tool?: unknown\n): tool is StructuredToolParams {\n  return (\n    !!tool &&\n    typeof tool === \"object\" &&\n    \"name\" in tool &&\n    \"schema\" in tool &&\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    (isInteropZodSchema(tool.schema as Record<string, any>) ||\n      (tool.schema != null &&\n        typeof tool.schema === \"object\" &&\n        \"type\" in tool.schema &&\n        typeof tool.schema.type === \"string\" &&\n        [\"null\", \"boolean\", \"object\", \"array\", \"number\", \"string\"].includes(\n          tool.schema.type\n        )))\n  );\n}\n\n/**\n * Whether or not the tool is one of StructuredTool, RunnableTool or StructuredToolParams.\n * It returns `is StructuredToolParams` since that is the most minimal interface of the three,\n * while still containing the necessary properties to be passed to a LLM for tool calling.\n *\n * @param {unknown | undefined} tool The tool to check if it is a LangChain tool.\n * @returns {tool is StructuredToolParams} Whether the inputted tool is a LangChain tool.\n */\nexport function isLangChainTool(tool?: unknown): tool is StructuredToolParams {\n  return (\n    isStructuredToolParams(tool) ||\n    isRunnableToolLike(tool) ||\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    isStructuredTool(tool as any)\n  );\n}\n\n/**\n * Runtime context automatically injected into tools.\n *\n * When a tool function has a parameter named `tool_runtime` with type hint\n * `ToolRuntime`, the tool execution system will automatically inject an instance\n * containing:\n *\n * - `state`: The current graph state\n * - `toolCallId`: The ID of the current tool call\n * - `config`: `RunnableConfig` for the current execution\n * - `context`: Runtime context\n * - `store`: `BaseStore` instance for persistent storage\n * - `writer`: Stream writer for streaming output\n *\n * No `Annotated` wrapper is needed - just use `runtime: ToolRuntime`\n * as a parameter.\n *\n * @example\n * ```typescript\n * import { tool, type ToolRuntime } from \"@langchain/core/tools\";\n * import { z } from \"zod\";\n *\n * const stateSchema = z.object({\n *   messages: z.array(z.any()),\n *   userId: z.string().optional(),\n * });\n *\n * const greet = tool(\n *   async ({ name }, runtime: ToolRuntime<typeof stateSchema>) => {\n *     // Access state\n *     const messages = runtime.state.messages;\n *\n *     // Access tool_call_id\n *     console.log(`Tool call ID: ${runtime.toolCallId}`);\n *\n *     // Access config\n *     console.log(`Run ID: ${runtime.config.runId}`);\n *\n *     // Access runtime context\n *     const userId = runtime.context?.userId;\n *\n *     // Access store\n *     await runtime.store?.mset([[\"key\", \"value\"]]);\n *\n *     // Stream output\n *     runtime.writer?.(\"Processing...\");\n *\n *     return `Hello! User ID: ${runtime.state.userId || \"unknown\"} ${name}`;\n *   },\n *   {\n *     name: \"greet\",\n *     description: \"Use this to greet the user once you found their info.\",\n *     schema: z.object({ name: z.string() }),\n *     stateSchema,\n *   }\n * );\n *\n * const agent = createAgent({\n *   model,\n *   tools: [greet],\n *   stateSchema,\n *   contextSchema,\n * });\n * ```\n *\n * @template StateT - The type of the state schema (inferred from stateSchema)\n * @template ContextT - The type of the context schema (inferred from contextSchema)\n */\nexport type ToolRuntime<\n  TState = unknown,\n  TContext = unknown,\n> = RunnableConfig & {\n  /**\n   * The current graph state.\n   */\n  state: TState extends InteropZodObject\n    ? InferInteropZodOutput<TState>\n    : TState extends Record<string, unknown>\n      ? TState\n      : unknown;\n  /**\n   * The ID of the current tool call.\n   */\n  toolCallId: string;\n  /**\n   * The current tool call.\n   */\n  toolCall?: ToolCall;\n  /**\n   * RunnableConfig for the current execution.\n   */\n  config: ToolRunnableConfig;\n  /**\n   * Runtime context (from langgraph `Runtime`).\n   */\n  context: TContext extends InteropZodObject\n    ? InferInteropZodOutput<TContext>\n    : TContext extends Record<string, unknown>\n      ? TContext\n      : unknown;\n  /**\n   * BaseStore instance for persistent storage (from langgraph `Runtime`).\n   */\n  store: BaseStore<string, unknown> | null;\n  /**\n   * Stream writer for streaming output (from langgraph `Runtime`).\n   */\n  writer: ((chunk: unknown) => void) | null;\n};\n", "import {\n  StructuredToolInterface,\n  StructuredToolParams,\n  isLangChainTool,\n} from \"../tools/types.js\";\nimport { FunctionDefinition, ToolDefinition } from \"../language_models/base.js\";\nimport { RunnableToolLike } from \"../runnables/base.js\";\nimport { toJsonSchema } from \"./json_schema.js\";\n\n// These utility functions were moved to a more appropriate location,\n// but we still export them here for backwards compatibility.\nexport {\n  isStructuredTool,\n  isStructuredToolParams,\n  isRunnableToolLike,\n  isLangChainTool,\n} from \"../tools/types.js\";\n\n/**\n * Formats a `StructuredTool` or `RunnableToolLike` instance into a format\n * that is compatible with OpenAI function calling. If `StructuredTool` or\n * `RunnableToolLike` has a zod schema, the output will be converted into a\n * JSON schema, which is then used as the parameters for the OpenAI tool.\n *\n * @param {StructuredToolInterface | RunnableToolLike} tool The tool to convert to an OpenAI function.\n * @returns {FunctionDefinition} The inputted tool in OpenAI function format.\n */\nexport function convertToOpenAIFunction(\n  tool: StructuredToolInterface | RunnableToolLike | StructuredToolParams,\n  fields?:\n    | {\n        /**\n         * If `true`, model output is guaranteed to exactly match the JSON Schema\n         * provided in the function definition.\n         */\n        strict?: boolean;\n      }\n    | number\n): FunctionDefinition {\n  // @TODO 0.3.0 Remove the `number` typing\n  const fieldsCopy = typeof fields === \"number\" ? undefined : fields;\n\n  return {\n    name: tool.name,\n    description: tool.description,\n    parameters: toJsonSchema(tool.schema),\n    // Do not include the `strict` field if it is `undefined`.\n    ...(fieldsCopy?.strict !== undefined ? { strict: fieldsCopy.strict } : {}),\n  };\n}\n\n/**\n * Formats a `StructuredTool` or `RunnableToolLike` instance into a\n * format that is compatible with OpenAI tool calling. If `StructuredTool` or\n * `RunnableToolLike` has a zod schema, the output will be converted into a\n * JSON schema, which is then used as the parameters for the OpenAI tool.\n *\n * @param {StructuredToolInterface | Record<string, any> | RunnableToolLike} tool The tool to convert to an OpenAI tool.\n * @returns {ToolDefinition} The inputted tool in OpenAI tool format.\n */\nexport function convertToOpenAITool(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  tool: StructuredToolInterface | Record<string, any> | RunnableToolLike,\n  fields?:\n    | {\n        /**\n         * If `true`, model output is guaranteed to exactly match the JSON Schema\n         * provided in the function definition.\n         */\n        strict?: boolean;\n      }\n    | number\n): ToolDefinition {\n  // @TODO 0.3.0 Remove the `number` typing\n  const fieldsCopy = typeof fields === \"number\" ? undefined : fields;\n\n  let toolDef: ToolDefinition | undefined;\n  if (isLangChainTool(tool)) {\n    toolDef = {\n      type: \"function\",\n      function: convertToOpenAIFunction(tool),\n    };\n  } else {\n    toolDef = tool as ToolDefinition;\n  }\n\n  if (fieldsCopy?.strict !== undefined) {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    (toolDef.function as any).strict = fieldsCopy.strict;\n  }\n\n  return toolDef;\n}\n", "import { __exportAll } from \"../../_virtual/_rolldown/runtime.js\";\nimport { extendInteropZodObject, getInteropZodDefaultGetter, getInteropZodObjectShape, getSchemaDescription, interopParse, interopParseAsync, interopSafeParse, interopSafeParseAsync, interopZodObjectMakeFieldsOptional, interopZodObjectPartial, interopZodObjectPassthrough, interopZodObjectStrict, interopZodTransformInputSchema, isInteropZodError, isInteropZodLiteral, isInteropZodObject, isInteropZodSchema, isShapelessZodSchema, isSimpleStringZodSchema, isZodArrayV4, isZodLiteralV3, isZodLiteralV4, isZodNullableV4, isZodObjectV3, isZodObjectV4, isZodOptionalV4, isZodSchema, isZodSchemaV3, isZodSchemaV4 } from \"./zod.js\";\n\n//#region src/utils/types/index.ts\nvar types_exports = /* @__PURE__ */ __exportAll({\n\textendInteropZodObject: () => extendInteropZodObject,\n\tgetInteropZodDefaultGetter: () => getInteropZodDefaultGetter,\n\tgetInteropZodObjectShape: () => getInteropZodObjectShape,\n\tgetSchemaDescription: () => getSchemaDescription,\n\tinteropParse: () => interopParse,\n\tinteropParseAsync: () => interopParseAsync,\n\tinteropSafeParse: () => interopSafeParse,\n\tinteropSafeParseAsync: () => interopSafeParseAsync,\n\tinteropZodObjectMakeFieldsOptional: () => interopZodObjectMakeFieldsOptional,\n\tinteropZodObjectPartial: () => interopZodObjectPartial,\n\tinteropZodObjectPassthrough: () => interopZodObjectPassthrough,\n\tinteropZodObjectStrict: () => interopZodObjectStrict,\n\tinteropZodTransformInputSchema: () => interopZodTransformInputSchema,\n\tisInteropZodError: () => isInteropZodError,\n\tisInteropZodLiteral: () => isInteropZodLiteral,\n\tisInteropZodObject: () => isInteropZodObject,\n\tisInteropZodSchema: () => isInteropZodSchema,\n\tisShapelessZodSchema: () => isShapelessZodSchema,\n\tisSimpleStringZodSchema: () => isSimpleStringZodSchema,\n\tisZodArrayV4: () => isZodArrayV4,\n\tisZodLiteralV3: () => isZodLiteralV3,\n\tisZodLiteralV4: () => isZodLiteralV4,\n\tisZodNullableV4: () => isZodNullableV4,\n\tisZodObjectV3: () => isZodObjectV3,\n\tisZodObjectV4: () => isZodObjectV4,\n\tisZodOptionalV4: () => isZodOptionalV4,\n\tisZodSchema: () => isZodSchema,\n\tisZodSchemaV3: () => isZodSchemaV3,\n\tisZodSchemaV4: () => isZodSchemaV4\n});\n\n//#endregion\nexport { extendInteropZodObject, getInteropZodDefaultGetter, getInteropZodObjectShape, getSchemaDescription, interopParse, interopParseAsync, interopSafeParse, interopSafeParseAsync, interopZodObjectMakeFieldsOptional, interopZodObjectPartial, interopZodObjectPassthrough, interopZodObjectStrict, interopZodTransformInputSchema, isInteropZodError, isInteropZodLiteral, isInteropZodObject, isInteropZodSchema, isShapelessZodSchema, isSimpleStringZodSchema, isZodArrayV4, isZodLiteralV3, isZodLiteralV4, isZodNullableV4, isZodObjectV3, isZodObjectV4, isZodOptionalV4, isZodSchema, isZodSchemaV3, isZodSchemaV4, types_exports };\n//# sourceMappingURL=index.js.map", "import { OpenAI as OpenAIClient } from \"openai\";\n\nimport { ToolDefinition } from \"@langchain/core/language_models/base\";\nimport { BindToolsInput } from \"@langchain/core/language_models/chat_models\";\nimport {\n  convertToOpenAITool as formatToOpenAITool,\n  isLangChainTool,\n} from \"@langchain/core/utils/function_calling\";\nimport { DynamicTool, StructuredToolInterface } from \"@langchain/core/tools\";\nimport { isInteropZodSchema } from \"@langchain/core/utils/types\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\nimport { ToolCall } from \"@langchain/core/messages/tool\";\n\n/**\n * Formats a tool in either OpenAI format, or LangChain structured tool format\n * into an OpenAI tool format. If the tool is already in OpenAI format, return without\n * any changes. If it is in LangChain structured tool format, convert it to OpenAI tool format\n * using OpenAI's `zodFunction` util, falling back to `convertToOpenAIFunction` if the parameters\n * returned from the `zodFunction` util are not defined.\n *\n * @param {BindToolsInput} tool The tool to convert to an OpenAI tool.\n * @param {Object} [fields] Additional fields to add to the OpenAI tool.\n * @returns {ToolDefinition} The inputted tool in OpenAI tool format.\n */\nexport function _convertToOpenAITool(\n  tool: BindToolsInput,\n  fields?: {\n    /**\n     * If `true`, model output is guaranteed to exactly match the JSON Schema\n     * provided in the function definition.\n     */\n    strict?: boolean;\n  }\n): OpenAIClient.ChatCompletionTool {\n  let toolDef: OpenAIClient.ChatCompletionTool | undefined;\n\n  if (isLangChainTool(tool)) {\n    toolDef = formatToOpenAITool(tool);\n  } else {\n    toolDef = tool as ToolDefinition;\n  }\n\n  if (fields?.strict !== undefined) {\n    toolDef.function.strict = fields.strict;\n  }\n\n  return toolDef;\n}\n\ntype OpenAIFunction = OpenAIClient.Chat.ChatCompletionCreateParams.Function;\n\n// Types representing the OpenAI function definitions. While the OpenAI client library\n// does have types for function definitions, the properties are just Record<string, unknown>,\n// which isn't very useful for type checking this formatting code.\nexport interface FunctionDef extends Omit<OpenAIFunction, \"parameters\"> {\n  name: string;\n  description?: string;\n  parameters: ObjectProp;\n}\n\ninterface ObjectProp {\n  type: \"object\";\n  properties?: {\n    [key: string]: Prop;\n  };\n  required?: string[];\n}\n\ninterface AnyOfProp {\n  anyOf: Prop[];\n}\n\ntype Prop = {\n  description?: string;\n} & (\n  | AnyOfProp\n  | ObjectProp\n  | {\n      type: \"string\";\n      enum?: string[];\n    }\n  | {\n      type: \"number\" | \"integer\";\n      minimum?: number;\n      maximum?: number;\n      enum?: number[];\n    }\n  | { type: \"boolean\" }\n  | { type: \"null\" }\n  | {\n      type: \"array\";\n      items?: Prop;\n    }\n);\n\nfunction isAnyOfProp(prop: Prop): prop is AnyOfProp {\n  return (\n    (prop as AnyOfProp).anyOf !== undefined &&\n    Array.isArray((prop as AnyOfProp).anyOf)\n  );\n}\n\n// When OpenAI use functions in the prompt, they format them as TypeScript definitions rather than OpenAPI JSON schemas.\n// This function converts the JSON schemas into TypeScript definitions.\nexport function formatFunctionDefinitions(functions: FunctionDef[]) {\n  const lines = [\"namespace functions {\", \"\"];\n  for (const f of functions) {\n    if (f.description) {\n      lines.push(`// ${f.description}`);\n    }\n    if (Object.keys(f.parameters.properties ?? {}).length > 0) {\n      lines.push(`type ${f.name} = (_: {`);\n      lines.push(formatObjectProperties(f.parameters, 0));\n      lines.push(\"}) => any;\");\n    } else {\n      lines.push(`type ${f.name} = () => any;`);\n    }\n    lines.push(\"\");\n  }\n  lines.push(\"} // namespace functions\");\n  return lines.join(\"\\n\");\n}\n\n// Format just the properties of an object (not including the surrounding braces)\nfunction formatObjectProperties(obj: ObjectProp, indent: number): string {\n  const lines: string[] = [];\n  for (const [name, param] of Object.entries(obj.properties ?? {})) {\n    if (param.description && indent < 2) {\n      lines.push(`// ${param.description}`);\n    }\n    if (obj.required?.includes(name)) {\n      lines.push(`${name}: ${formatType(param, indent)},`);\n    } else {\n      lines.push(`${name}?: ${formatType(param, indent)},`);\n    }\n  }\n  return lines.map((line) => \" \".repeat(indent) + line).join(\"\\n\");\n}\n\n// Format a single property type\nfunction formatType(param: Prop, indent: number): string {\n  if (isAnyOfProp(param)) {\n    return param.anyOf.map((v) => formatType(v, indent)).join(\" | \");\n  }\n  switch (param.type) {\n    case \"string\":\n      if (param.enum) {\n        return param.enum.map((v) => `\"${v}\"`).join(\" | \");\n      }\n      return \"string\";\n    case \"number\":\n      if (param.enum) {\n        return param.enum.map((v) => `${v}`).join(\" | \");\n      }\n      return \"number\";\n    case \"integer\":\n      if (param.enum) {\n        return param.enum.map((v) => `${v}`).join(\" | \");\n      }\n      return \"number\";\n    case \"boolean\":\n      return \"boolean\";\n    case \"null\":\n      return \"null\";\n    case \"object\":\n      return [\"{\", formatObjectProperties(param, indent + 2), \"}\"].join(\"\\n\");\n    case \"array\":\n      if (param.items) {\n        return `${formatType(param.items, indent)}[]`;\n      }\n      return \"any[]\";\n    default:\n      return \"\";\n  }\n}\n\nexport function formatToOpenAIAssistantTool(\n  tool: StructuredToolInterface\n): ToolDefinition {\n  return {\n    type: \"function\",\n    function: {\n      name: tool.name,\n      description: tool.description,\n      parameters: isInteropZodSchema(tool.schema)\n        ? toJsonSchema(tool.schema)\n        : tool.schema,\n    },\n  };\n}\n\nexport type OpenAIToolChoice =\n  | OpenAIClient.ChatCompletionToolChoiceOption\n  | \"any\"\n  | string;\n\nexport type ResponsesToolChoice = NonNullable<\n  OpenAIClient.Responses.ResponseCreateParams[\"tool_choice\"]\n>;\n\nexport type ChatOpenAIToolType =\n  | BindToolsInput\n  | OpenAIClient.Chat.ChatCompletionTool\n  | ResponsesTool;\n\nexport type ResponsesTool = NonNullable<\n  OpenAIClient.Responses.ResponseCreateParams[\"tools\"]\n>[number];\n\nexport function formatToOpenAIToolChoice(\n  toolChoice?: OpenAIToolChoice\n): OpenAIClient.ChatCompletionToolChoiceOption | undefined {\n  if (!toolChoice) {\n    return undefined;\n  } else if (toolChoice === \"any\" || toolChoice === \"required\") {\n    return \"required\";\n  } else if (toolChoice === \"auto\") {\n    return \"auto\";\n  } else if (toolChoice === \"none\") {\n    return \"none\";\n  } else if (typeof toolChoice === \"string\") {\n    return {\n      type: \"function\",\n      function: {\n        name: toolChoice,\n      },\n    };\n  } else {\n    return toolChoice;\n  }\n}\n\nexport function isBuiltInTool(tool: ChatOpenAIToolType): tool is ResponsesTool {\n  return \"type\" in tool && tool.type !== \"function\";\n}\n\n/**\n * Type for LangChain tools that have a provider-specific tool definition\n * stored in extras.providerToolDefinition.\n */\ntype LangchainToolWithProviderDefinition = StructuredToolInterface & {\n  extras: {\n    providerToolDefinition: ResponsesTool;\n  };\n};\n\n/**\n * Checks if a tool has a provider-specific tool definition in extras.providerToolDefinition.\n * This is used for tools like localShell, shell, computerUse, and applyPatch\n * that need to be sent as built-in tool types to the OpenAI API.\n */\nexport function hasProviderToolDefinition(\n  tool: unknown\n): tool is LangchainToolWithProviderDefinition {\n  return (\n    typeof tool === \"object\" &&\n    tool !== null &&\n    \"extras\" in tool &&\n    typeof (tool as LangchainToolWithProviderDefinition).extras === \"object\" &&\n    (tool as LangchainToolWithProviderDefinition).extras !== null &&\n    \"providerToolDefinition\" in\n      (tool as LangchainToolWithProviderDefinition).extras &&\n    typeof (tool as LangchainToolWithProviderDefinition).extras\n      .providerToolDefinition === \"object\" &&\n    (tool as LangchainToolWithProviderDefinition).extras\n      .providerToolDefinition !== null\n  );\n}\n\nexport function isBuiltInToolChoice(\n  tool_choice: OpenAIToolChoice | ResponsesToolChoice | undefined\n): tool_choice is ResponsesToolChoice {\n  return (\n    tool_choice != null &&\n    typeof tool_choice === \"object\" &&\n    \"type\" in tool_choice &&\n    tool_choice.type !== \"function\"\n  );\n}\n\nexport type CustomToolCall = ToolCall & {\n  call_id: string;\n  isCustomTool: true;\n};\n\ntype LangchainCustomTool = DynamicTool<string> & {\n  metadata: {\n    customTool: OpenAIClient.Responses.CustomTool;\n  };\n};\n\nexport function isCustomTool(tool: unknown): tool is LangchainCustomTool {\n  return (\n    typeof tool === \"object\" &&\n    tool !== null &&\n    \"metadata\" in tool &&\n    typeof tool.metadata === \"object\" &&\n    tool.metadata !== null &&\n    \"customTool\" in tool.metadata &&\n    typeof tool.metadata.customTool === \"object\" &&\n    tool.metadata.customTool !== null\n  );\n}\n\nexport function isOpenAICustomTool(\n  tool: ChatOpenAIToolType\n): tool is OpenAIClient.Chat.ChatCompletionCustomTool {\n  return (\n    \"type\" in tool &&\n    tool.type === \"custom\" &&\n    \"custom\" in tool &&\n    typeof tool.custom === \"object\" &&\n    tool.custom !== null\n  );\n}\n\nexport function parseCustomToolCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>\n): CustomToolCall | undefined {\n  if (rawToolCall.type !== \"custom_tool_call\") {\n    return undefined;\n  }\n  return {\n    ...rawToolCall,\n    type: \"tool_call\",\n    call_id: rawToolCall.id,\n    id: rawToolCall.call_id,\n    name: rawToolCall.name,\n    isCustomTool: true,\n    args: {\n      input: rawToolCall.input,\n    },\n  };\n}\n\nexport type ComputerToolCall = ToolCall & {\n  call_id: string;\n  /**\n   * marker to indicate that the tool call is a computer tool call\n   */\n  isComputerTool: true;\n};\n\n/**\n * Parses a computer_call output item from the OpenAI Responses API\n * into a ToolCall format that can be processed by the ToolNode.\n *\n * @param rawToolCall - The raw computer_call output item from the API\n * @returns A ComputerToolCall object if valid, undefined otherwise\n */\nexport function parseComputerCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>\n): ComputerToolCall | undefined {\n  if (rawToolCall.type !== \"computer_call\") {\n    return undefined;\n  }\n  return {\n    ...rawToolCall,\n    type: \"tool_call\",\n    call_id: rawToolCall.id,\n    id: rawToolCall.call_id,\n    name: \"computer_use\",\n    isComputerTool: true,\n    args: {\n      action: rawToolCall.action,\n    },\n  };\n}\n\n/**\n * Checks if a tool call is a computer tool call.\n * @param toolCall - The tool call to check.\n * @returns True if the tool call is a computer tool call, false otherwise.\n */\nexport function isComputerToolCall(\n  toolCall: unknown\n): toolCall is ComputerToolCall {\n  return (\n    typeof toolCall === \"object\" &&\n    toolCall !== null &&\n    \"type\" in toolCall &&\n    toolCall.type === \"tool_call\" &&\n    \"isComputerTool\" in toolCall &&\n    toolCall.isComputerTool === true\n  );\n}\n\nexport function isCustomToolCall(\n  toolCall: unknown\n): toolCall is CustomToolCall {\n  return (\n    typeof toolCall === \"object\" &&\n    toolCall !== null &&\n    \"type\" in toolCall &&\n    toolCall.type === \"tool_call\" &&\n    \"isCustomTool\" in toolCall &&\n    toolCall.isCustomTool === true\n  );\n}\n\nexport function convertCompletionsCustomTool(\n  tool: OpenAIClient.Chat.ChatCompletionCustomTool\n): OpenAIClient.Responses.CustomTool {\n  const getFormat = () => {\n    if (!tool.custom.format) {\n      return undefined;\n    }\n    if (tool.custom.format.type === \"grammar\") {\n      return {\n        type: \"grammar\" as const,\n        definition: tool.custom.format.grammar.definition,\n        syntax: tool.custom.format.grammar.syntax,\n      };\n    }\n    if (tool.custom.format.type === \"text\") {\n      return {\n        type: \"text\" as const,\n      };\n    }\n    return undefined;\n  };\n  return {\n    type: \"custom\",\n    name: tool.custom.name,\n    description: tool.custom.description,\n    format: getFormat(),\n  };\n}\n\nexport function convertResponsesCustomTool(\n  tool: OpenAIClient.Responses.CustomTool\n): OpenAIClient.Chat.ChatCompletionCustomTool {\n  const getFormat = () => {\n    if (!tool.format) {\n      return undefined;\n    }\n    if (tool.format.type === \"grammar\") {\n      return {\n        type: \"grammar\" as const,\n        grammar: {\n          definition: tool.format.definition,\n          syntax: tool.format.syntax,\n        },\n      };\n    }\n    if (tool.format.type === \"text\") {\n      return {\n        type: \"text\" as const,\n      };\n    }\n    return undefined;\n  };\n  return {\n    type: \"custom\",\n    custom: {\n      name: tool.name,\n      description: tool.description,\n      format: getFormat(),\n    },\n  };\n}\n", "export * as core from \"../core/index.js\";\nexport * from \"./schemas.js\";\nexport * from \"./checks.js\";\nexport * from \"./errors.js\";\nexport * from \"./parse.js\";\nexport * from \"./compat.js\";\n// zod-specified\nimport { config } from \"../core/index.js\";\nimport en from \"../locales/en.js\";\nconfig(en());\nexport { globalRegistry, registry, config, $output, $input, $brand, clone, regexes, treeifyError, prettifyError, formatError, flattenError, TimePrecision, util, NEVER, } from \"../core/index.js\";\nexport { toJSONSchema } from \"../core/json-schema-processors.js\";\nexport { fromJSONSchema } from \"./from-json-schema.js\";\nexport * as locales from \"../locales/index.js\";\n// iso\n// must be exported from top-level\n// https://github.com/colinhacks/zod/issues/4491\nexport { ZodISODateTime, ZodISODate, ZodISOTime, ZodISODuration } from \"./iso.js\";\nexport * as iso from \"./iso.js\";\nexport * as coerce from \"./coerce.js\";\n", "import * as core from \"../core/index.js\";\nimport { util } from \"../core/index.js\";\nimport * as processors from \"../core/json-schema-processors.js\";\nimport { createStandardJSONSchemaMethod, createToJSONSchemaMethod } from \"../core/to-json-schema.js\";\nimport * as checks from \"./checks.js\";\nimport * as iso from \"./iso.js\";\nimport * as parse from \"./parse.js\";\nexport const ZodType = /*@__PURE__*/ core.$constructor(\"ZodType\", (inst, def) => {\n    core.$ZodType.init(inst, def);\n    Object.assign(inst[\"~standard\"], {\n        jsonSchema: {\n            input: createStandardJSONSchemaMethod(inst, \"input\"),\n            output: createStandardJSONSchemaMethod(inst, \"output\"),\n        },\n    });\n    inst.toJSONSchema = createToJSONSchemaMethod(inst, {});\n    inst.def = def;\n    inst.type = def.type;\n    Object.defineProperty(inst, \"_def\", { value: def });\n    // base methods\n    inst.check = (...checks) => {\n        return inst.clone(util.mergeDefs(def, {\n            checks: [\n                ...(def.checks ?? []),\n                ...checks.map((ch) => typeof ch === \"function\" ? { _zod: { check: ch, def: { check: \"custom\" }, onattach: [] } } : ch),\n            ],\n        }), {\n            parent: true,\n        });\n    };\n    inst.with = inst.check;\n    inst.clone = (def, params) => core.clone(inst, def, params);\n    inst.brand = () => inst;\n    inst.register = ((reg, meta) => {\n        reg.add(inst, meta);\n        return inst;\n    });\n    // parsing\n    inst.parse = (data, params) => parse.parse(inst, data, params, { callee: inst.parse });\n    inst.safeParse = (data, params) => parse.safeParse(inst, data, params);\n    inst.parseAsync = async (data, params) => parse.parseAsync(inst, data, params, { callee: inst.parseAsync });\n    inst.safeParseAsync = async (data, params) => parse.safeParseAsync(inst, data, params);\n    inst.spa = inst.safeParseAsync;\n    // encoding/decoding\n    inst.encode = (data, params) => parse.encode(inst, data, params);\n    inst.decode = (data, params) => parse.decode(inst, data, params);\n    inst.encodeAsync = async (data, params) => parse.encodeAsync(inst, data, params);\n    inst.decodeAsync = async (data, params) => parse.decodeAsync(inst, data, params);\n    inst.safeEncode = (data, params) => parse.safeEncode(inst, data, params);\n    inst.safeDecode = (data, params) => parse.safeDecode(inst, data, params);\n    inst.safeEncodeAsync = async (data, params) => parse.safeEncodeAsync(inst, data, params);\n    inst.safeDecodeAsync = async (data, params) => parse.safeDecodeAsync(inst, data, params);\n    // refinements\n    inst.refine = (check, params) => inst.check(refine(check, params));\n    inst.superRefine = (refinement) => inst.check(superRefine(refinement));\n    inst.overwrite = (fn) => inst.check(checks.overwrite(fn));\n    // wrappers\n    inst.optional = () => optional(inst);\n    inst.exactOptional = () => exactOptional(inst);\n    inst.nullable = () => nullable(inst);\n    inst.nullish = () => optional(nullable(inst));\n    inst.nonoptional = (params) => nonoptional(inst, params);\n    inst.array = () => array(inst);\n    inst.or = (arg) => union([inst, arg]);\n    inst.and = (arg) => intersection(inst, arg);\n    inst.transform = (tx) => pipe(inst, transform(tx));\n    inst.default = (def) => _default(inst, def);\n    inst.prefault = (def) => prefault(inst, def);\n    // inst.coalesce = (def, params) => coalesce(inst, def, params);\n    inst.catch = (params) => _catch(inst, params);\n    inst.pipe = (target) => pipe(inst, target);\n    inst.readonly = () => readonly(inst);\n    // meta\n    inst.describe = (description) => {\n        const cl = inst.clone();\n        core.globalRegistry.add(cl, { description });\n        return cl;\n    };\n    Object.defineProperty(inst, \"description\", {\n        get() {\n            return core.globalRegistry.get(inst)?.description;\n        },\n        configurable: true,\n    });\n    inst.meta = (...args) => {\n        if (args.length === 0) {\n            return core.globalRegistry.get(inst);\n        }\n        const cl = inst.clone();\n        core.globalRegistry.add(cl, args[0]);\n        return cl;\n    };\n    // helpers\n    inst.isOptional = () => inst.safeParse(undefined).success;\n    inst.isNullable = () => inst.safeParse(null).success;\n    inst.apply = (fn) => fn(inst);\n    return inst;\n});\n/** @internal */\nexport const _ZodString = /*@__PURE__*/ core.$constructor(\"_ZodString\", (inst, def) => {\n    core.$ZodString.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.stringProcessor(inst, ctx, json, params);\n    const bag = inst._zod.bag;\n    inst.format = bag.format ?? null;\n    inst.minLength = bag.minimum ?? null;\n    inst.maxLength = bag.maximum ?? null;\n    // validations\n    inst.regex = (...args) => inst.check(checks.regex(...args));\n    inst.includes = (...args) => inst.check(checks.includes(...args));\n    inst.startsWith = (...args) => inst.check(checks.startsWith(...args));\n    inst.endsWith = (...args) => inst.check(checks.endsWith(...args));\n    inst.min = (...args) => inst.check(checks.minLength(...args));\n    inst.max = (...args) => inst.check(checks.maxLength(...args));\n    inst.length = (...args) => inst.check(checks.length(...args));\n    inst.nonempty = (...args) => inst.check(checks.minLength(1, ...args));\n    inst.lowercase = (params) => inst.check(checks.lowercase(params));\n    inst.uppercase = (params) => inst.check(checks.uppercase(params));\n    // transforms\n    inst.trim = () => inst.check(checks.trim());\n    inst.normalize = (...args) => inst.check(checks.normalize(...args));\n    inst.toLowerCase = () => inst.check(checks.toLowerCase());\n    inst.toUpperCase = () => inst.check(checks.toUpperCase());\n    inst.slugify = () => inst.check(checks.slugify());\n});\nexport const ZodString = /*@__PURE__*/ core.$constructor(\"ZodString\", (inst, def) => {\n    core.$ZodString.init(inst, def);\n    _ZodString.init(inst, def);\n    inst.email = (params) => inst.check(core._email(ZodEmail, params));\n    inst.url = (params) => inst.check(core._url(ZodURL, params));\n    inst.jwt = (params) => inst.check(core._jwt(ZodJWT, params));\n    inst.emoji = (params) => inst.check(core._emoji(ZodEmoji, params));\n    inst.guid = (params) => inst.check(core._guid(ZodGUID, params));\n    inst.uuid = (params) => inst.check(core._uuid(ZodUUID, params));\n    inst.uuidv4 = (params) => inst.check(core._uuidv4(ZodUUID, params));\n    inst.uuidv6 = (params) => inst.check(core._uuidv6(ZodUUID, params));\n    inst.uuidv7 = (params) => inst.check(core._uuidv7(ZodUUID, params));\n    inst.nanoid = (params) => inst.check(core._nanoid(ZodNanoID, params));\n    inst.guid = (params) => inst.check(core._guid(ZodGUID, params));\n    inst.cuid = (params) => inst.check(core._cuid(ZodCUID, params));\n    inst.cuid2 = (params) => inst.check(core._cuid2(ZodCUID2, params));\n    inst.ulid = (params) => inst.check(core._ulid(ZodULID, params));\n    inst.base64 = (params) => inst.check(core._base64(ZodBase64, params));\n    inst.base64url = (params) => inst.check(core._base64url(ZodBase64URL, params));\n    inst.xid = (params) => inst.check(core._xid(ZodXID, params));\n    inst.ksuid = (params) => inst.check(core._ksuid(ZodKSUID, params));\n    inst.ipv4 = (params) => inst.check(core._ipv4(ZodIPv4, params));\n    inst.ipv6 = (params) => inst.check(core._ipv6(ZodIPv6, params));\n    inst.cidrv4 = (params) => inst.check(core._cidrv4(ZodCIDRv4, params));\n    inst.cidrv6 = (params) => inst.check(core._cidrv6(ZodCIDRv6, params));\n    inst.e164 = (params) => inst.check(core._e164(ZodE164, params));\n    // iso\n    inst.datetime = (params) => inst.check(iso.datetime(params));\n    inst.date = (params) => inst.check(iso.date(params));\n    inst.time = (params) => inst.check(iso.time(params));\n    inst.duration = (params) => inst.check(iso.duration(params));\n});\nexport function string(params) {\n    return core._string(ZodString, params);\n}\nexport const ZodStringFormat = /*@__PURE__*/ core.$constructor(\"ZodStringFormat\", (inst, def) => {\n    core.$ZodStringFormat.init(inst, def);\n    _ZodString.init(inst, def);\n});\nexport const ZodEmail = /*@__PURE__*/ core.$constructor(\"ZodEmail\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodEmail.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function email(params) {\n    return core._email(ZodEmail, params);\n}\nexport const ZodGUID = /*@__PURE__*/ core.$constructor(\"ZodGUID\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodGUID.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function guid(params) {\n    return core._guid(ZodGUID, params);\n}\nexport const ZodUUID = /*@__PURE__*/ core.$constructor(\"ZodUUID\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodUUID.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function uuid(params) {\n    return core._uuid(ZodUUID, params);\n}\nexport function uuidv4(params) {\n    return core._uuidv4(ZodUUID, params);\n}\n// ZodUUIDv6\nexport function uuidv6(params) {\n    return core._uuidv6(ZodUUID, params);\n}\n// ZodUUIDv7\nexport function uuidv7(params) {\n    return core._uuidv7(ZodUUID, params);\n}\nexport const ZodURL = /*@__PURE__*/ core.$constructor(\"ZodURL\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodURL.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function url(params) {\n    return core._url(ZodURL, params);\n}\nexport function httpUrl(params) {\n    return core._url(ZodURL, {\n        protocol: /^https?$/,\n        hostname: core.regexes.domain,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodEmoji = /*@__PURE__*/ core.$constructor(\"ZodEmoji\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodEmoji.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function emoji(params) {\n    return core._emoji(ZodEmoji, params);\n}\nexport const ZodNanoID = /*@__PURE__*/ core.$constructor(\"ZodNanoID\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodNanoID.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function nanoid(params) {\n    return core._nanoid(ZodNanoID, params);\n}\nexport const ZodCUID = /*@__PURE__*/ core.$constructor(\"ZodCUID\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodCUID.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function cuid(params) {\n    return core._cuid(ZodCUID, params);\n}\nexport const ZodCUID2 = /*@__PURE__*/ core.$constructor(\"ZodCUID2\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodCUID2.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function cuid2(params) {\n    return core._cuid2(ZodCUID2, params);\n}\nexport const ZodULID = /*@__PURE__*/ core.$constructor(\"ZodULID\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodULID.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function ulid(params) {\n    return core._ulid(ZodULID, params);\n}\nexport const ZodXID = /*@__PURE__*/ core.$constructor(\"ZodXID\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodXID.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function xid(params) {\n    return core._xid(ZodXID, params);\n}\nexport const ZodKSUID = /*@__PURE__*/ core.$constructor(\"ZodKSUID\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodKSUID.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function ksuid(params) {\n    return core._ksuid(ZodKSUID, params);\n}\nexport const ZodIPv4 = /*@__PURE__*/ core.$constructor(\"ZodIPv4\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodIPv4.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function ipv4(params) {\n    return core._ipv4(ZodIPv4, params);\n}\nexport const ZodMAC = /*@__PURE__*/ core.$constructor(\"ZodMAC\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodMAC.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function mac(params) {\n    return core._mac(ZodMAC, params);\n}\nexport const ZodIPv6 = /*@__PURE__*/ core.$constructor(\"ZodIPv6\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodIPv6.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function ipv6(params) {\n    return core._ipv6(ZodIPv6, params);\n}\nexport const ZodCIDRv4 = /*@__PURE__*/ core.$constructor(\"ZodCIDRv4\", (inst, def) => {\n    core.$ZodCIDRv4.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function cidrv4(params) {\n    return core._cidrv4(ZodCIDRv4, params);\n}\nexport const ZodCIDRv6 = /*@__PURE__*/ core.$constructor(\"ZodCIDRv6\", (inst, def) => {\n    core.$ZodCIDRv6.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function cidrv6(params) {\n    return core._cidrv6(ZodCIDRv6, params);\n}\nexport const ZodBase64 = /*@__PURE__*/ core.$constructor(\"ZodBase64\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodBase64.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function base64(params) {\n    return core._base64(ZodBase64, params);\n}\nexport const ZodBase64URL = /*@__PURE__*/ core.$constructor(\"ZodBase64URL\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodBase64URL.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function base64url(params) {\n    return core._base64url(ZodBase64URL, params);\n}\nexport const ZodE164 = /*@__PURE__*/ core.$constructor(\"ZodE164\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodE164.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function e164(params) {\n    return core._e164(ZodE164, params);\n}\nexport const ZodJWT = /*@__PURE__*/ core.$constructor(\"ZodJWT\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodJWT.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function jwt(params) {\n    return core._jwt(ZodJWT, params);\n}\nexport const ZodCustomStringFormat = /*@__PURE__*/ core.$constructor(\"ZodCustomStringFormat\", (inst, def) => {\n    // ZodStringFormat.init(inst, def);\n    core.$ZodCustomStringFormat.init(inst, def);\n    ZodStringFormat.init(inst, def);\n});\nexport function stringFormat(format, fnOrRegex, _params = {}) {\n    return core._stringFormat(ZodCustomStringFormat, format, fnOrRegex, _params);\n}\nexport function hostname(_params) {\n    return core._stringFormat(ZodCustomStringFormat, \"hostname\", core.regexes.hostname, _params);\n}\nexport function hex(_params) {\n    return core._stringFormat(ZodCustomStringFormat, \"hex\", core.regexes.hex, _params);\n}\nexport function hash(alg, params) {\n    const enc = params?.enc ?? \"hex\";\n    const format = `${alg}_${enc}`;\n    const regex = core.regexes[format];\n    if (!regex)\n        throw new Error(`Unrecognized hash format: ${format}`);\n    return core._stringFormat(ZodCustomStringFormat, format, regex, params);\n}\nexport const ZodNumber = /*@__PURE__*/ core.$constructor(\"ZodNumber\", (inst, def) => {\n    core.$ZodNumber.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.numberProcessor(inst, ctx, json, params);\n    inst.gt = (value, params) => inst.check(checks.gt(value, params));\n    inst.gte = (value, params) => inst.check(checks.gte(value, params));\n    inst.min = (value, params) => inst.check(checks.gte(value, params));\n    inst.lt = (value, params) => inst.check(checks.lt(value, params));\n    inst.lte = (value, params) => inst.check(checks.lte(value, params));\n    inst.max = (value, params) => inst.check(checks.lte(value, params));\n    inst.int = (params) => inst.check(int(params));\n    inst.safe = (params) => inst.check(int(params));\n    inst.positive = (params) => inst.check(checks.gt(0, params));\n    inst.nonnegative = (params) => inst.check(checks.gte(0, params));\n    inst.negative = (params) => inst.check(checks.lt(0, params));\n    inst.nonpositive = (params) => inst.check(checks.lte(0, params));\n    inst.multipleOf = (value, params) => inst.check(checks.multipleOf(value, params));\n    inst.step = (value, params) => inst.check(checks.multipleOf(value, params));\n    // inst.finite = (params) => inst.check(core.finite(params));\n    inst.finite = () => inst;\n    const bag = inst._zod.bag;\n    inst.minValue =\n        Math.max(bag.minimum ?? Number.NEGATIVE_INFINITY, bag.exclusiveMinimum ?? Number.NEGATIVE_INFINITY) ?? null;\n    inst.maxValue =\n        Math.min(bag.maximum ?? Number.POSITIVE_INFINITY, bag.exclusiveMaximum ?? Number.POSITIVE_INFINITY) ?? null;\n    inst.isInt = (bag.format ?? \"\").includes(\"int\") || Number.isSafeInteger(bag.multipleOf ?? 0.5);\n    inst.isFinite = true;\n    inst.format = bag.format ?? null;\n});\nexport function number(params) {\n    return core._number(ZodNumber, params);\n}\nexport const ZodNumberFormat = /*@__PURE__*/ core.$constructor(\"ZodNumberFormat\", (inst, def) => {\n    core.$ZodNumberFormat.init(inst, def);\n    ZodNumber.init(inst, def);\n});\nexport function int(params) {\n    return core._int(ZodNumberFormat, params);\n}\nexport function float32(params) {\n    return core._float32(ZodNumberFormat, params);\n}\nexport function float64(params) {\n    return core._float64(ZodNumberFormat, params);\n}\nexport function int32(params) {\n    return core._int32(ZodNumberFormat, params);\n}\nexport function uint32(params) {\n    return core._uint32(ZodNumberFormat, params);\n}\nexport const ZodBoolean = /*@__PURE__*/ core.$constructor(\"ZodBoolean\", (inst, def) => {\n    core.$ZodBoolean.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.booleanProcessor(inst, ctx, json, params);\n});\nexport function boolean(params) {\n    return core._boolean(ZodBoolean, params);\n}\nexport const ZodBigInt = /*@__PURE__*/ core.$constructor(\"ZodBigInt\", (inst, def) => {\n    core.$ZodBigInt.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.bigintProcessor(inst, ctx, json, params);\n    inst.gte = (value, params) => inst.check(checks.gte(value, params));\n    inst.min = (value, params) => inst.check(checks.gte(value, params));\n    inst.gt = (value, params) => inst.check(checks.gt(value, params));\n    inst.gte = (value, params) => inst.check(checks.gte(value, params));\n    inst.min = (value, params) => inst.check(checks.gte(value, params));\n    inst.lt = (value, params) => inst.check(checks.lt(value, params));\n    inst.lte = (value, params) => inst.check(checks.lte(value, params));\n    inst.max = (value, params) => inst.check(checks.lte(value, params));\n    inst.positive = (params) => inst.check(checks.gt(BigInt(0), params));\n    inst.negative = (params) => inst.check(checks.lt(BigInt(0), params));\n    inst.nonpositive = (params) => inst.check(checks.lte(BigInt(0), params));\n    inst.nonnegative = (params) => inst.check(checks.gte(BigInt(0), params));\n    inst.multipleOf = (value, params) => inst.check(checks.multipleOf(value, params));\n    const bag = inst._zod.bag;\n    inst.minValue = bag.minimum ?? null;\n    inst.maxValue = bag.maximum ?? null;\n    inst.format = bag.format ?? null;\n});\nexport function bigint(params) {\n    return core._bigint(ZodBigInt, params);\n}\nexport const ZodBigIntFormat = /*@__PURE__*/ core.$constructor(\"ZodBigIntFormat\", (inst, def) => {\n    core.$ZodBigIntFormat.init(inst, def);\n    ZodBigInt.init(inst, def);\n});\n// int64\nexport function int64(params) {\n    return core._int64(ZodBigIntFormat, params);\n}\n// uint64\nexport function uint64(params) {\n    return core._uint64(ZodBigIntFormat, params);\n}\nexport const ZodSymbol = /*@__PURE__*/ core.$constructor(\"ZodSymbol\", (inst, def) => {\n    core.$ZodSymbol.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.symbolProcessor(inst, ctx, json, params);\n});\nexport function symbol(params) {\n    return core._symbol(ZodSymbol, params);\n}\nexport const ZodUndefined = /*@__PURE__*/ core.$constructor(\"ZodUndefined\", (inst, def) => {\n    core.$ZodUndefined.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.undefinedProcessor(inst, ctx, json, params);\n});\nfunction _undefined(params) {\n    return core._undefined(ZodUndefined, params);\n}\nexport { _undefined as undefined };\nexport const ZodNull = /*@__PURE__*/ core.$constructor(\"ZodNull\", (inst, def) => {\n    core.$ZodNull.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.nullProcessor(inst, ctx, json, params);\n});\nfunction _null(params) {\n    return core._null(ZodNull, params);\n}\nexport { _null as null };\nexport const ZodAny = /*@__PURE__*/ core.$constructor(\"ZodAny\", (inst, def) => {\n    core.$ZodAny.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.anyProcessor(inst, ctx, json, params);\n});\nexport function any() {\n    return core._any(ZodAny);\n}\nexport const ZodUnknown = /*@__PURE__*/ core.$constructor(\"ZodUnknown\", (inst, def) => {\n    core.$ZodUnknown.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.unknownProcessor(inst, ctx, json, params);\n});\nexport function unknown() {\n    return core._unknown(ZodUnknown);\n}\nexport const ZodNever = /*@__PURE__*/ core.$constructor(\"ZodNever\", (inst, def) => {\n    core.$ZodNever.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.neverProcessor(inst, ctx, json, params);\n});\nexport function never(params) {\n    return core._never(ZodNever, params);\n}\nexport const ZodVoid = /*@__PURE__*/ core.$constructor(\"ZodVoid\", (inst, def) => {\n    core.$ZodVoid.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.voidProcessor(inst, ctx, json, params);\n});\nfunction _void(params) {\n    return core._void(ZodVoid, params);\n}\nexport { _void as void };\nexport const ZodDate = /*@__PURE__*/ core.$constructor(\"ZodDate\", (inst, def) => {\n    core.$ZodDate.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.dateProcessor(inst, ctx, json, params);\n    inst.min = (value, params) => inst.check(checks.gte(value, params));\n    inst.max = (value, params) => inst.check(checks.lte(value, params));\n    const c = inst._zod.bag;\n    inst.minDate = c.minimum ? new Date(c.minimum) : null;\n    inst.maxDate = c.maximum ? new Date(c.maximum) : null;\n});\nexport function date(params) {\n    return core._date(ZodDate, params);\n}\nexport const ZodArray = /*@__PURE__*/ core.$constructor(\"ZodArray\", (inst, def) => {\n    core.$ZodArray.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.arrayProcessor(inst, ctx, json, params);\n    inst.element = def.element;\n    inst.min = (minLength, params) => inst.check(checks.minLength(minLength, params));\n    inst.nonempty = (params) => inst.check(checks.minLength(1, params));\n    inst.max = (maxLength, params) => inst.check(checks.maxLength(maxLength, params));\n    inst.length = (len, params) => inst.check(checks.length(len, params));\n    inst.unwrap = () => inst.element;\n});\nexport function array(element, params) {\n    return core._array(ZodArray, element, params);\n}\n// .keyof\nexport function keyof(schema) {\n    const shape = schema._zod.def.shape;\n    return _enum(Object.keys(shape));\n}\nexport const ZodObject = /*@__PURE__*/ core.$constructor(\"ZodObject\", (inst, def) => {\n    core.$ZodObjectJIT.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.objectProcessor(inst, ctx, json, params);\n    util.defineLazy(inst, \"shape\", () => {\n        return def.shape;\n    });\n    inst.keyof = () => _enum(Object.keys(inst._zod.def.shape));\n    inst.catchall = (catchall) => inst.clone({ ...inst._zod.def, catchall: catchall });\n    inst.passthrough = () => inst.clone({ ...inst._zod.def, catchall: unknown() });\n    inst.loose = () => inst.clone({ ...inst._zod.def, catchall: unknown() });\n    inst.strict = () => inst.clone({ ...inst._zod.def, catchall: never() });\n    inst.strip = () => inst.clone({ ...inst._zod.def, catchall: undefined });\n    inst.extend = (incoming) => {\n        return util.extend(inst, incoming);\n    };\n    inst.safeExtend = (incoming) => {\n        return util.safeExtend(inst, incoming);\n    };\n    inst.merge = (other) => util.merge(inst, other);\n    inst.pick = (mask) => util.pick(inst, mask);\n    inst.omit = (mask) => util.omit(inst, mask);\n    inst.partial = (...args) => util.partial(ZodOptional, inst, args[0]);\n    inst.required = (...args) => util.required(ZodNonOptional, inst, args[0]);\n});\nexport function object(shape, params) {\n    const def = {\n        type: \"object\",\n        shape: shape ?? {},\n        ...util.normalizeParams(params),\n    };\n    return new ZodObject(def);\n}\n// strictObject\nexport function strictObject(shape, params) {\n    return new ZodObject({\n        type: \"object\",\n        shape,\n        catchall: never(),\n        ...util.normalizeParams(params),\n    });\n}\n// looseObject\nexport function looseObject(shape, params) {\n    return new ZodObject({\n        type: \"object\",\n        shape,\n        catchall: unknown(),\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodUnion = /*@__PURE__*/ core.$constructor(\"ZodUnion\", (inst, def) => {\n    core.$ZodUnion.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.unionProcessor(inst, ctx, json, params);\n    inst.options = def.options;\n});\nexport function union(options, params) {\n    return new ZodUnion({\n        type: \"union\",\n        options: options,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodXor = /*@__PURE__*/ core.$constructor(\"ZodXor\", (inst, def) => {\n    ZodUnion.init(inst, def);\n    core.$ZodXor.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.unionProcessor(inst, ctx, json, params);\n    inst.options = def.options;\n});\n/** Creates an exclusive union (XOR) where exactly one option must match.\n * Unlike regular unions that succeed when any option matches, xor fails if\n * zero or more than one option matches the input. */\nexport function xor(options, params) {\n    return new ZodXor({\n        type: \"union\",\n        options: options,\n        inclusive: false,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodDiscriminatedUnion = /*@__PURE__*/ core.$constructor(\"ZodDiscriminatedUnion\", (inst, def) => {\n    ZodUnion.init(inst, def);\n    core.$ZodDiscriminatedUnion.init(inst, def);\n});\nexport function discriminatedUnion(discriminator, options, params) {\n    // const [options, params] = args;\n    return new ZodDiscriminatedUnion({\n        type: \"union\",\n        options,\n        discriminator,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodIntersection = /*@__PURE__*/ core.$constructor(\"ZodIntersection\", (inst, def) => {\n    core.$ZodIntersection.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.intersectionProcessor(inst, ctx, json, params);\n});\nexport function intersection(left, right) {\n    return new ZodIntersection({\n        type: \"intersection\",\n        left: left,\n        right: right,\n    });\n}\nexport const ZodTuple = /*@__PURE__*/ core.$constructor(\"ZodTuple\", (inst, def) => {\n    core.$ZodTuple.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.tupleProcessor(inst, ctx, json, params);\n    inst.rest = (rest) => inst.clone({\n        ...inst._zod.def,\n        rest: rest,\n    });\n});\nexport function tuple(items, _paramsOrRest, _params) {\n    const hasRest = _paramsOrRest instanceof core.$ZodType;\n    const params = hasRest ? _params : _paramsOrRest;\n    const rest = hasRest ? _paramsOrRest : null;\n    return new ZodTuple({\n        type: \"tuple\",\n        items: items,\n        rest,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodRecord = /*@__PURE__*/ core.$constructor(\"ZodRecord\", (inst, def) => {\n    core.$ZodRecord.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.recordProcessor(inst, ctx, json, params);\n    inst.keyType = def.keyType;\n    inst.valueType = def.valueType;\n});\nexport function record(keyType, valueType, params) {\n    return new ZodRecord({\n        type: \"record\",\n        keyType,\n        valueType: valueType,\n        ...util.normalizeParams(params),\n    });\n}\n// type alksjf = core.output<core.$ZodRecordKey>;\nexport function partialRecord(keyType, valueType, params) {\n    const k = core.clone(keyType);\n    k._zod.values = undefined;\n    return new ZodRecord({\n        type: \"record\",\n        keyType: k,\n        valueType: valueType,\n        ...util.normalizeParams(params),\n    });\n}\nexport function looseRecord(keyType, valueType, params) {\n    return new ZodRecord({\n        type: \"record\",\n        keyType,\n        valueType: valueType,\n        mode: \"loose\",\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodMap = /*@__PURE__*/ core.$constructor(\"ZodMap\", (inst, def) => {\n    core.$ZodMap.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.mapProcessor(inst, ctx, json, params);\n    inst.keyType = def.keyType;\n    inst.valueType = def.valueType;\n    inst.min = (...args) => inst.check(core._minSize(...args));\n    inst.nonempty = (params) => inst.check(core._minSize(1, params));\n    inst.max = (...args) => inst.check(core._maxSize(...args));\n    inst.size = (...args) => inst.check(core._size(...args));\n});\nexport function map(keyType, valueType, params) {\n    return new ZodMap({\n        type: \"map\",\n        keyType: keyType,\n        valueType: valueType,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodSet = /*@__PURE__*/ core.$constructor(\"ZodSet\", (inst, def) => {\n    core.$ZodSet.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.setProcessor(inst, ctx, json, params);\n    inst.min = (...args) => inst.check(core._minSize(...args));\n    inst.nonempty = (params) => inst.check(core._minSize(1, params));\n    inst.max = (...args) => inst.check(core._maxSize(...args));\n    inst.size = (...args) => inst.check(core._size(...args));\n});\nexport function set(valueType, params) {\n    return new ZodSet({\n        type: \"set\",\n        valueType: valueType,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodEnum = /*@__PURE__*/ core.$constructor(\"ZodEnum\", (inst, def) => {\n    core.$ZodEnum.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.enumProcessor(inst, ctx, json, params);\n    inst.enum = def.entries;\n    inst.options = Object.values(def.entries);\n    const keys = new Set(Object.keys(def.entries));\n    inst.extract = (values, params) => {\n        const newEntries = {};\n        for (const value of values) {\n            if (keys.has(value)) {\n                newEntries[value] = def.entries[value];\n            }\n            else\n                throw new Error(`Key ${value} not found in enum`);\n        }\n        return new ZodEnum({\n            ...def,\n            checks: [],\n            ...util.normalizeParams(params),\n            entries: newEntries,\n        });\n    };\n    inst.exclude = (values, params) => {\n        const newEntries = { ...def.entries };\n        for (const value of values) {\n            if (keys.has(value)) {\n                delete newEntries[value];\n            }\n            else\n                throw new Error(`Key ${value} not found in enum`);\n        }\n        return new ZodEnum({\n            ...def,\n            checks: [],\n            ...util.normalizeParams(params),\n            entries: newEntries,\n        });\n    };\n});\nfunction _enum(values, params) {\n    const entries = Array.isArray(values) ? Object.fromEntries(values.map((v) => [v, v])) : values;\n    return new ZodEnum({\n        type: \"enum\",\n        entries,\n        ...util.normalizeParams(params),\n    });\n}\nexport { _enum as enum };\n/** @deprecated This API has been merged into `z.enum()`. Use `z.enum()` instead.\n *\n * ```ts\n * enum Colors { red, green, blue }\n * z.enum(Colors);\n * ```\n */\nexport function nativeEnum(entries, params) {\n    return new ZodEnum({\n        type: \"enum\",\n        entries,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodLiteral = /*@__PURE__*/ core.$constructor(\"ZodLiteral\", (inst, def) => {\n    core.$ZodLiteral.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.literalProcessor(inst, ctx, json, params);\n    inst.values = new Set(def.values);\n    Object.defineProperty(inst, \"value\", {\n        get() {\n            if (def.values.length > 1) {\n                throw new Error(\"This schema contains multiple valid literal values. Use `.values` instead.\");\n            }\n            return def.values[0];\n        },\n    });\n});\nexport function literal(value, params) {\n    return new ZodLiteral({\n        type: \"literal\",\n        values: Array.isArray(value) ? value : [value],\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodFile = /*@__PURE__*/ core.$constructor(\"ZodFile\", (inst, def) => {\n    core.$ZodFile.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.fileProcessor(inst, ctx, json, params);\n    inst.min = (size, params) => inst.check(core._minSize(size, params));\n    inst.max = (size, params) => inst.check(core._maxSize(size, params));\n    inst.mime = (types, params) => inst.check(core._mime(Array.isArray(types) ? types : [types], params));\n});\nexport function file(params) {\n    return core._file(ZodFile, params);\n}\nexport const ZodTransform = /*@__PURE__*/ core.$constructor(\"ZodTransform\", (inst, def) => {\n    core.$ZodTransform.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.transformProcessor(inst, ctx, json, params);\n    inst._zod.parse = (payload, _ctx) => {\n        if (_ctx.direction === \"backward\") {\n            throw new core.$ZodEncodeError(inst.constructor.name);\n        }\n        payload.addIssue = (issue) => {\n            if (typeof issue === \"string\") {\n                payload.issues.push(util.issue(issue, payload.value, def));\n            }\n            else {\n                // for Zod 3 backwards compatibility\n                const _issue = issue;\n                if (_issue.fatal)\n                    _issue.continue = false;\n                _issue.code ?? (_issue.code = \"custom\");\n                _issue.input ?? (_issue.input = payload.value);\n                _issue.inst ?? (_issue.inst = inst);\n                // _issue.continue ??= true;\n                payload.issues.push(util.issue(_issue));\n            }\n        };\n        const output = def.transform(payload.value, payload);\n        if (output instanceof Promise) {\n            return output.then((output) => {\n                payload.value = output;\n                return payload;\n            });\n        }\n        payload.value = output;\n        return payload;\n    };\n});\nexport function transform(fn) {\n    return new ZodTransform({\n        type: \"transform\",\n        transform: fn,\n    });\n}\nexport const ZodOptional = /*@__PURE__*/ core.$constructor(\"ZodOptional\", (inst, def) => {\n    core.$ZodOptional.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.optionalProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function optional(innerType) {\n    return new ZodOptional({\n        type: \"optional\",\n        innerType: innerType,\n    });\n}\nexport const ZodExactOptional = /*@__PURE__*/ core.$constructor(\"ZodExactOptional\", (inst, def) => {\n    core.$ZodExactOptional.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.optionalProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function exactOptional(innerType) {\n    return new ZodExactOptional({\n        type: \"optional\",\n        innerType: innerType,\n    });\n}\nexport const ZodNullable = /*@__PURE__*/ core.$constructor(\"ZodNullable\", (inst, def) => {\n    core.$ZodNullable.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.nullableProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function nullable(innerType) {\n    return new ZodNullable({\n        type: \"nullable\",\n        innerType: innerType,\n    });\n}\n// nullish\nexport function nullish(innerType) {\n    return optional(nullable(innerType));\n}\nexport const ZodDefault = /*@__PURE__*/ core.$constructor(\"ZodDefault\", (inst, def) => {\n    core.$ZodDefault.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.defaultProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n    inst.removeDefault = inst.unwrap;\n});\nexport function _default(innerType, defaultValue) {\n    return new ZodDefault({\n        type: \"default\",\n        innerType: innerType,\n        get defaultValue() {\n            return typeof defaultValue === \"function\" ? defaultValue() : util.shallowClone(defaultValue);\n        },\n    });\n}\nexport const ZodPrefault = /*@__PURE__*/ core.$constructor(\"ZodPrefault\", (inst, def) => {\n    core.$ZodPrefault.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.prefaultProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function prefault(innerType, defaultValue) {\n    return new ZodPrefault({\n        type: \"prefault\",\n        innerType: innerType,\n        get defaultValue() {\n            return typeof defaultValue === \"function\" ? defaultValue() : util.shallowClone(defaultValue);\n        },\n    });\n}\nexport const ZodNonOptional = /*@__PURE__*/ core.$constructor(\"ZodNonOptional\", (inst, def) => {\n    core.$ZodNonOptional.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.nonoptionalProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function nonoptional(innerType, params) {\n    return new ZodNonOptional({\n        type: \"nonoptional\",\n        innerType: innerType,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodSuccess = /*@__PURE__*/ core.$constructor(\"ZodSuccess\", (inst, def) => {\n    core.$ZodSuccess.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.successProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function success(innerType) {\n    return new ZodSuccess({\n        type: \"success\",\n        innerType: innerType,\n    });\n}\nexport const ZodCatch = /*@__PURE__*/ core.$constructor(\"ZodCatch\", (inst, def) => {\n    core.$ZodCatch.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.catchProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n    inst.removeCatch = inst.unwrap;\n});\nfunction _catch(innerType, catchValue) {\n    return new ZodCatch({\n        type: \"catch\",\n        innerType: innerType,\n        catchValue: (typeof catchValue === \"function\" ? catchValue : () => catchValue),\n    });\n}\nexport { _catch as catch };\nexport const ZodNaN = /*@__PURE__*/ core.$constructor(\"ZodNaN\", (inst, def) => {\n    core.$ZodNaN.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.nanProcessor(inst, ctx, json, params);\n});\nexport function nan(params) {\n    return core._nan(ZodNaN, params);\n}\nexport const ZodPipe = /*@__PURE__*/ core.$constructor(\"ZodPipe\", (inst, def) => {\n    core.$ZodPipe.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.pipeProcessor(inst, ctx, json, params);\n    inst.in = def.in;\n    inst.out = def.out;\n});\nexport function pipe(in_, out) {\n    return new ZodPipe({\n        type: \"pipe\",\n        in: in_,\n        out: out,\n        // ...util.normalizeParams(params),\n    });\n}\nexport const ZodCodec = /*@__PURE__*/ core.$constructor(\"ZodCodec\", (inst, def) => {\n    ZodPipe.init(inst, def);\n    core.$ZodCodec.init(inst, def);\n});\nexport function codec(in_, out, params) {\n    return new ZodCodec({\n        type: \"pipe\",\n        in: in_,\n        out: out,\n        transform: params.decode,\n        reverseTransform: params.encode,\n    });\n}\nexport const ZodReadonly = /*@__PURE__*/ core.$constructor(\"ZodReadonly\", (inst, def) => {\n    core.$ZodReadonly.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.readonlyProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function readonly(innerType) {\n    return new ZodReadonly({\n        type: \"readonly\",\n        innerType: innerType,\n    });\n}\nexport const ZodTemplateLiteral = /*@__PURE__*/ core.$constructor(\"ZodTemplateLiteral\", (inst, def) => {\n    core.$ZodTemplateLiteral.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.templateLiteralProcessor(inst, ctx, json, params);\n});\nexport function templateLiteral(parts, params) {\n    return new ZodTemplateLiteral({\n        type: \"template_literal\",\n        parts,\n        ...util.normalizeParams(params),\n    });\n}\nexport const ZodLazy = /*@__PURE__*/ core.$constructor(\"ZodLazy\", (inst, def) => {\n    core.$ZodLazy.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.lazyProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.getter();\n});\nexport function lazy(getter) {\n    return new ZodLazy({\n        type: \"lazy\",\n        getter: getter,\n    });\n}\nexport const ZodPromise = /*@__PURE__*/ core.$constructor(\"ZodPromise\", (inst, def) => {\n    core.$ZodPromise.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.promiseProcessor(inst, ctx, json, params);\n    inst.unwrap = () => inst._zod.def.innerType;\n});\nexport function promise(innerType) {\n    return new ZodPromise({\n        type: \"promise\",\n        innerType: innerType,\n    });\n}\nexport const ZodFunction = /*@__PURE__*/ core.$constructor(\"ZodFunction\", (inst, def) => {\n    core.$ZodFunction.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.functionProcessor(inst, ctx, json, params);\n});\nexport function _function(params) {\n    return new ZodFunction({\n        type: \"function\",\n        input: Array.isArray(params?.input) ? tuple(params?.input) : (params?.input ?? array(unknown())),\n        output: params?.output ?? unknown(),\n    });\n}\nexport { _function as function };\nexport const ZodCustom = /*@__PURE__*/ core.$constructor(\"ZodCustom\", (inst, def) => {\n    core.$ZodCustom.init(inst, def);\n    ZodType.init(inst, def);\n    inst._zod.processJSONSchema = (ctx, json, params) => processors.customProcessor(inst, ctx, json, params);\n});\n// custom checks\nexport function check(fn) {\n    const ch = new core.$ZodCheck({\n        check: \"custom\",\n        // ...util.normalizeParams(params),\n    });\n    ch._zod.check = fn;\n    return ch;\n}\nexport function custom(fn, _params) {\n    return core._custom(ZodCustom, fn ?? (() => true), _params);\n}\nexport function refine(fn, _params = {}) {\n    return core._refine(ZodCustom, fn, _params);\n}\n// superRefine\nexport function superRefine(fn) {\n    return core._superRefine(fn);\n}\n// Re-export describe and meta from core\nexport const describe = core.describe;\nexport const meta = core.meta;\nfunction _instanceof(cls, params = {}) {\n    const inst = new ZodCustom({\n        type: \"custom\",\n        check: \"custom\",\n        fn: (data) => data instanceof cls,\n        abort: true,\n        ...util.normalizeParams(params),\n    });\n    inst._zod.bag.Class = cls;\n    // Override check to emit invalid_type instead of custom\n    inst._zod.check = (payload) => {\n        if (!(payload.value instanceof cls)) {\n            payload.issues.push({\n                code: \"invalid_type\",\n                expected: cls.name,\n                input: payload.value,\n                inst,\n                path: [...(inst._zod.def.path ?? [])],\n            });\n        }\n    };\n    return inst;\n}\nexport { _instanceof as instanceof };\n// stringbool\nexport const stringbool = (...args) => core._stringbool({\n    Codec: ZodCodec,\n    Boolean: ZodBoolean,\n    String: ZodString,\n}, ...args);\nexport function json(params) {\n    const jsonSchema = lazy(() => {\n        return union([string(params), number(), boolean(), _null(), array(jsonSchema), record(string(), jsonSchema)]);\n    });\n    return jsonSchema;\n}\n// preprocess\n// /** @deprecated Use `z.pipe()` and `z.transform()` instead. */\nexport function preprocess(fn, schema) {\n    return pipe(transform(fn), schema);\n}\n", "export { _lt as lt, _lte as lte, _gt as gt, _gte as gte, _positive as positive, _negative as negative, _nonpositive as nonpositive, _nonnegative as nonnegative, _multipleOf as multipleOf, _maxSize as maxSize, _minSize as minSize, _size as size, _maxLength as maxLength, _minLength as minLength, _length as length, _regex as regex, _lowercase as lowercase, _uppercase as uppercase, _includes as includes, _startsWith as startsWith, _endsWith as endsWith, _property as property, _mime as mime, _overwrite as overwrite, _normalize as normalize, _trim as trim, _toLowerCase as toLowerCase, _toUpperCase as toUpperCase, _slugify as slugify, } from \"../core/index.js\";\n", "import * as core from \"../core/index.js\";\nimport * as schemas from \"./schemas.js\";\nexport const ZodISODateTime = /*@__PURE__*/ core.$constructor(\"ZodISODateTime\", (inst, def) => {\n    core.$ZodISODateTime.init(inst, def);\n    schemas.ZodStringFormat.init(inst, def);\n});\nexport function datetime(params) {\n    return core._isoDateTime(ZodISODateTime, params);\n}\nexport const ZodISODate = /*@__PURE__*/ core.$constructor(\"ZodISODate\", (inst, def) => {\n    core.$ZodISODate.init(inst, def);\n    schemas.ZodStringFormat.init(inst, def);\n});\nexport function date(params) {\n    return core._isoDate(ZodISODate, params);\n}\nexport const ZodISOTime = /*@__PURE__*/ core.$constructor(\"ZodISOTime\", (inst, def) => {\n    core.$ZodISOTime.init(inst, def);\n    schemas.ZodStringFormat.init(inst, def);\n});\nexport function time(params) {\n    return core._isoTime(ZodISOTime, params);\n}\nexport const ZodISODuration = /*@__PURE__*/ core.$constructor(\"ZodISODuration\", (inst, def) => {\n    core.$ZodISODuration.init(inst, def);\n    schemas.ZodStringFormat.init(inst, def);\n});\nexport function duration(params) {\n    return core._isoDuration(ZodISODuration, params);\n}\n", "import * as core from \"../core/index.js\";\nimport { $ZodError } from \"../core/index.js\";\nimport * as util from \"../core/util.js\";\nconst initializer = (inst, issues) => {\n    $ZodError.init(inst, issues);\n    inst.name = \"ZodError\";\n    Object.defineProperties(inst, {\n        format: {\n            value: (mapper) => core.formatError(inst, mapper),\n            // enumerable: false,\n        },\n        flatten: {\n            value: (mapper) => core.flattenError(inst, mapper),\n            // enumerable: false,\n        },\n        addIssue: {\n            value: (issue) => {\n                inst.issues.push(issue);\n                inst.message = JSON.stringify(inst.issues, util.jsonStringifyReplacer, 2);\n            },\n            // enumerable: false,\n        },\n        addIssues: {\n            value: (issues) => {\n                inst.issues.push(...issues);\n                inst.message = JSON.stringify(inst.issues, util.jsonStringifyReplacer, 2);\n            },\n            // enumerable: false,\n        },\n        isEmpty: {\n            get() {\n                return inst.issues.length === 0;\n            },\n            // enumerable: false,\n        },\n    });\n    // Object.defineProperty(inst, \"isEmpty\", {\n    //   get() {\n    //     return inst.issues.length === 0;\n    //   },\n    // });\n};\nexport const ZodError = core.$constructor(\"ZodError\", initializer);\nexport const ZodRealError = core.$constructor(\"ZodError\", initializer, {\n    Parent: Error,\n});\n// /** @deprecated Use `z.core.$ZodErrorMapCtx` instead. */\n// export type ErrorMapCtx = core.$ZodErrorMapCtx;\n", "import * as core from \"../core/index.js\";\nimport { ZodRealError } from \"./errors.js\";\nexport const parse = /* @__PURE__ */ core._parse(ZodRealError);\nexport const parseAsync = /* @__PURE__ */ core._parseAsync(ZodRealError);\nexport const safeParse = /* @__PURE__ */ core._safeParse(ZodRealError);\nexport const safeParseAsync = /* @__PURE__ */ core._safeParseAsync(ZodRealError);\n// Codec functions\nexport const encode = /* @__PURE__ */ core._encode(ZodRealError);\nexport const decode = /* @__PURE__ */ core._decode(ZodRealError);\nexport const encodeAsync = /* @__PURE__ */ core._encodeAsync(ZodRealError);\nexport const decodeAsync = /* @__PURE__ */ core._decodeAsync(ZodRealError);\nexport const safeEncode = /* @__PURE__ */ core._safeEncode(ZodRealError);\nexport const safeDecode = /* @__PURE__ */ core._safeDecode(ZodRealError);\nexport const safeEncodeAsync = /* @__PURE__ */ core._safeEncodeAsync(ZodRealError);\nexport const safeDecodeAsync = /* @__PURE__ */ core._safeDecodeAsync(ZodRealError);\n", "// Zod 3 compat layer\nimport * as core from \"../core/index.js\";\n/** @deprecated Use the raw string literal codes instead, e.g. \"invalid_type\". */\nexport const ZodIssueCode = {\n    invalid_type: \"invalid_type\",\n    too_big: \"too_big\",\n    too_small: \"too_small\",\n    invalid_format: \"invalid_format\",\n    not_multiple_of: \"not_multiple_of\",\n    unrecognized_keys: \"unrecognized_keys\",\n    invalid_union: \"invalid_union\",\n    invalid_key: \"invalid_key\",\n    invalid_element: \"invalid_element\",\n    invalid_value: \"invalid_value\",\n    custom: \"custom\",\n};\nexport { $brand, config } from \"../core/index.js\";\n/** @deprecated Use `z.config(params)` instead. */\nexport function setErrorMap(map) {\n    core.config({\n        customError: map,\n    });\n}\n/** @deprecated Use `z.config()` instead. */\nexport function getErrorMap() {\n    return core.config().customError;\n}\n/** @deprecated Do not use. Stub definition, only included for zod-to-json-schema compatibility. */\nexport var ZodFirstPartyTypeKind;\n(function (ZodFirstPartyTypeKind) {\n})(ZodFirstPartyTypeKind || (ZodFirstPartyTypeKind = {}));\n", "import { globalRegistry } from \"../core/registries.js\";\nimport * as _checks from \"./checks.js\";\nimport * as _iso from \"./iso.js\";\nimport * as _schemas from \"./schemas.js\";\n// Local z object to avoid circular dependency with ../index.js\nconst z = {\n    ..._schemas,\n    ..._checks,\n    iso: _iso,\n};\n// Keys that are recognized and handled by the conversion logic\nconst RECOGNIZED_KEYS = new Set([\n    // Schema identification\n    \"$schema\",\n    \"$ref\",\n    \"$defs\",\n    \"definitions\",\n    // Core schema keywords\n    \"$id\",\n    \"id\",\n    \"$comment\",\n    \"$anchor\",\n    \"$vocabulary\",\n    \"$dynamicRef\",\n    \"$dynamicAnchor\",\n    // Type\n    \"type\",\n    \"enum\",\n    \"const\",\n    // Composition\n    \"anyOf\",\n    \"oneOf\",\n    \"allOf\",\n    \"not\",\n    // Object\n    \"properties\",\n    \"required\",\n    \"additionalProperties\",\n    \"patternProperties\",\n    \"propertyNames\",\n    \"minProperties\",\n    \"maxProperties\",\n    // Array\n    \"items\",\n    \"prefixItems\",\n    \"additionalItems\",\n    \"minItems\",\n    \"maxItems\",\n    \"uniqueItems\",\n    \"contains\",\n    \"minContains\",\n    \"maxContains\",\n    // String\n    \"minLength\",\n    \"maxLength\",\n    \"pattern\",\n    \"format\",\n    // Number\n    \"minimum\",\n    \"maximum\",\n    \"exclusiveMinimum\",\n    \"exclusiveMaximum\",\n    \"multipleOf\",\n    // Already handled metadata\n    \"description\",\n    \"default\",\n    // Content\n    \"contentEncoding\",\n    \"contentMediaType\",\n    \"contentSchema\",\n    // Unsupported (error-throwing)\n    \"unevaluatedItems\",\n    \"unevaluatedProperties\",\n    \"if\",\n    \"then\",\n    \"else\",\n    \"dependentSchemas\",\n    \"dependentRequired\",\n    // OpenAPI\n    \"nullable\",\n    \"readOnly\",\n]);\nfunction detectVersion(schema, defaultTarget) {\n    const $schema = schema.$schema;\n    if ($schema === \"https://json-schema.org/draft/2020-12/schema\") {\n        return \"draft-2020-12\";\n    }\n    if ($schema === \"http://json-schema.org/draft-07/schema#\") {\n        return \"draft-7\";\n    }\n    if ($schema === \"http://json-schema.org/draft-04/schema#\") {\n        return \"draft-4\";\n    }\n    // Use defaultTarget if provided, otherwise default to draft-2020-12\n    return defaultTarget ?? \"draft-2020-12\";\n}\nfunction resolveRef(ref, ctx) {\n    if (!ref.startsWith(\"#\")) {\n        throw new Error(\"External $ref is not supported, only local refs (#/...) are allowed\");\n    }\n    const path = ref.slice(1).split(\"/\").filter(Boolean);\n    // Handle root reference \"#\"\n    if (path.length === 0) {\n        return ctx.rootSchema;\n    }\n    const defsKey = ctx.version === \"draft-2020-12\" ? \"$defs\" : \"definitions\";\n    if (path[0] === defsKey) {\n        const key = path[1];\n        if (!key || !ctx.defs[key]) {\n            throw new Error(`Reference not found: ${ref}`);\n        }\n        return ctx.defs[key];\n    }\n    throw new Error(`Reference not found: ${ref}`);\n}\nfunction convertBaseSchema(schema, ctx) {\n    // Handle unsupported features\n    if (schema.not !== undefined) {\n        // Special case: { not: {} } represents never\n        if (typeof schema.not === \"object\" && Object.keys(schema.not).length === 0) {\n            return z.never();\n        }\n        throw new Error(\"not is not supported in Zod (except { not: {} } for never)\");\n    }\n    if (schema.unevaluatedItems !== undefined) {\n        throw new Error(\"unevaluatedItems is not supported\");\n    }\n    if (schema.unevaluatedProperties !== undefined) {\n        throw new Error(\"unevaluatedProperties is not supported\");\n    }\n    if (schema.if !== undefined || schema.then !== undefined || schema.else !== undefined) {\n        throw new Error(\"Conditional schemas (if/then/else) are not supported\");\n    }\n    if (schema.dependentSchemas !== undefined || schema.dependentRequired !== undefined) {\n        throw new Error(\"dependentSchemas and dependentRequired are not supported\");\n    }\n    // Handle $ref\n    if (schema.$ref) {\n        const refPath = schema.$ref;\n        if (ctx.refs.has(refPath)) {\n            return ctx.refs.get(refPath);\n        }\n        if (ctx.processing.has(refPath)) {\n            // Circular reference - use lazy\n            return z.lazy(() => {\n                if (!ctx.refs.has(refPath)) {\n                    throw new Error(`Circular reference not resolved: ${refPath}`);\n                }\n                return ctx.refs.get(refPath);\n            });\n        }\n        ctx.processing.add(refPath);\n        const resolved = resolveRef(refPath, ctx);\n        const zodSchema = convertSchema(resolved, ctx);\n        ctx.refs.set(refPath, zodSchema);\n        ctx.processing.delete(refPath);\n        return zodSchema;\n    }\n    // Handle enum\n    if (schema.enum !== undefined) {\n        const enumValues = schema.enum;\n        // Special case: OpenAPI 3.0 null representation { type: \"string\", nullable: true, enum: [null] }\n        if (ctx.version === \"openapi-3.0\" &&\n            schema.nullable === true &&\n            enumValues.length === 1 &&\n            enumValues[0] === null) {\n            return z.null();\n        }\n        if (enumValues.length === 0) {\n            return z.never();\n        }\n        if (enumValues.length === 1) {\n            return z.literal(enumValues[0]);\n        }\n        // Check if all values are strings\n        if (enumValues.every((v) => typeof v === \"string\")) {\n            return z.enum(enumValues);\n        }\n        // Mixed types - use union of literals\n        const literalSchemas = enumValues.map((v) => z.literal(v));\n        if (literalSchemas.length < 2) {\n            return literalSchemas[0];\n        }\n        return z.union([literalSchemas[0], literalSchemas[1], ...literalSchemas.slice(2)]);\n    }\n    // Handle const\n    if (schema.const !== undefined) {\n        return z.literal(schema.const);\n    }\n    // Handle type\n    const type = schema.type;\n    if (Array.isArray(type)) {\n        // Expand type array into anyOf union\n        const typeSchemas = type.map((t) => {\n            const typeSchema = { ...schema, type: t };\n            return convertBaseSchema(typeSchema, ctx);\n        });\n        if (typeSchemas.length === 0) {\n            return z.never();\n        }\n        if (typeSchemas.length === 1) {\n            return typeSchemas[0];\n        }\n        return z.union(typeSchemas);\n    }\n    if (!type) {\n        // No type specified - empty schema (any)\n        return z.any();\n    }\n    let zodSchema;\n    switch (type) {\n        case \"string\": {\n            let stringSchema = z.string();\n            // Apply format using .check() with Zod format functions\n            if (schema.format) {\n                const format = schema.format;\n                // Map common formats to Zod check functions\n                if (format === \"email\") {\n                    stringSchema = stringSchema.check(z.email());\n                }\n                else if (format === \"uri\" || format === \"uri-reference\") {\n                    stringSchema = stringSchema.check(z.url());\n                }\n                else if (format === \"uuid\" || format === \"guid\") {\n                    stringSchema = stringSchema.check(z.uuid());\n                }\n                else if (format === \"date-time\") {\n                    stringSchema = stringSchema.check(z.iso.datetime());\n                }\n                else if (format === \"date\") {\n                    stringSchema = stringSchema.check(z.iso.date());\n                }\n                else if (format === \"time\") {\n                    stringSchema = stringSchema.check(z.iso.time());\n                }\n                else if (format === \"duration\") {\n                    stringSchema = stringSchema.check(z.iso.duration());\n                }\n                else if (format === \"ipv4\") {\n                    stringSchema = stringSchema.check(z.ipv4());\n                }\n                else if (format === \"ipv6\") {\n                    stringSchema = stringSchema.check(z.ipv6());\n                }\n                else if (format === \"mac\") {\n                    stringSchema = stringSchema.check(z.mac());\n                }\n                else if (format === \"cidr\") {\n                    stringSchema = stringSchema.check(z.cidrv4());\n                }\n                else if (format === \"cidr-v6\") {\n                    stringSchema = stringSchema.check(z.cidrv6());\n                }\n                else if (format === \"base64\") {\n                    stringSchema = stringSchema.check(z.base64());\n                }\n                else if (format === \"base64url\") {\n                    stringSchema = stringSchema.check(z.base64url());\n                }\n                else if (format === \"e164\") {\n                    stringSchema = stringSchema.check(z.e164());\n                }\n                else if (format === \"jwt\") {\n                    stringSchema = stringSchema.check(z.jwt());\n                }\n                else if (format === \"emoji\") {\n                    stringSchema = stringSchema.check(z.emoji());\n                }\n                else if (format === \"nanoid\") {\n                    stringSchema = stringSchema.check(z.nanoid());\n                }\n                else if (format === \"cuid\") {\n                    stringSchema = stringSchema.check(z.cuid());\n                }\n                else if (format === \"cuid2\") {\n                    stringSchema = stringSchema.check(z.cuid2());\n                }\n                else if (format === \"ulid\") {\n                    stringSchema = stringSchema.check(z.ulid());\n                }\n                else if (format === \"xid\") {\n                    stringSchema = stringSchema.check(z.xid());\n                }\n                else if (format === \"ksuid\") {\n                    stringSchema = stringSchema.check(z.ksuid());\n                }\n                // Note: json-string format is not currently supported by Zod\n                // Custom formats are ignored - keep as plain string\n            }\n            // Apply constraints\n            if (typeof schema.minLength === \"number\") {\n                stringSchema = stringSchema.min(schema.minLength);\n            }\n            if (typeof schema.maxLength === \"number\") {\n                stringSchema = stringSchema.max(schema.maxLength);\n            }\n            if (schema.pattern) {\n                // JSON Schema patterns are not implicitly anchored (match anywhere in string)\n                stringSchema = stringSchema.regex(new RegExp(schema.pattern));\n            }\n            zodSchema = stringSchema;\n            break;\n        }\n        case \"number\":\n        case \"integer\": {\n            let numberSchema = type === \"integer\" ? z.number().int() : z.number();\n            // Apply constraints\n            if (typeof schema.minimum === \"number\") {\n                numberSchema = numberSchema.min(schema.minimum);\n            }\n            if (typeof schema.maximum === \"number\") {\n                numberSchema = numberSchema.max(schema.maximum);\n            }\n            if (typeof schema.exclusiveMinimum === \"number\") {\n                numberSchema = numberSchema.gt(schema.exclusiveMinimum);\n            }\n            else if (schema.exclusiveMinimum === true && typeof schema.minimum === \"number\") {\n                numberSchema = numberSchema.gt(schema.minimum);\n            }\n            if (typeof schema.exclusiveMaximum === \"number\") {\n                numberSchema = numberSchema.lt(schema.exclusiveMaximum);\n            }\n            else if (schema.exclusiveMaximum === true && typeof schema.maximum === \"number\") {\n                numberSchema = numberSchema.lt(schema.maximum);\n            }\n            if (typeof schema.multipleOf === \"number\") {\n                numberSchema = numberSchema.multipleOf(schema.multipleOf);\n            }\n            zodSchema = numberSchema;\n            break;\n        }\n        case \"boolean\": {\n            zodSchema = z.boolean();\n            break;\n        }\n        case \"null\": {\n            zodSchema = z.null();\n            break;\n        }\n        case \"object\": {\n            const shape = {};\n            const properties = schema.properties || {};\n            const requiredSet = new Set(schema.required || []);\n            // Convert properties - mark optional ones\n            for (const [key, propSchema] of Object.entries(properties)) {\n                const propZodSchema = convertSchema(propSchema, ctx);\n                // If not in required array, make it optional\n                shape[key] = requiredSet.has(key) ? propZodSchema : propZodSchema.optional();\n            }\n            // Handle propertyNames\n            if (schema.propertyNames) {\n                const keySchema = convertSchema(schema.propertyNames, ctx);\n                const valueSchema = schema.additionalProperties && typeof schema.additionalProperties === \"object\"\n                    ? convertSchema(schema.additionalProperties, ctx)\n                    : z.any();\n                // Case A: No properties (pure record)\n                if (Object.keys(shape).length === 0) {\n                    zodSchema = z.record(keySchema, valueSchema);\n                    break;\n                }\n                // Case B: With properties (intersection of object and looseRecord)\n                const objectSchema = z.object(shape).passthrough();\n                const recordSchema = z.looseRecord(keySchema, valueSchema);\n                zodSchema = z.intersection(objectSchema, recordSchema);\n                break;\n            }\n            // Handle patternProperties\n            if (schema.patternProperties) {\n                // patternProperties: keys matching pattern must satisfy corresponding schema\n                // Use loose records so non-matching keys pass through\n                const patternProps = schema.patternProperties;\n                const patternKeys = Object.keys(patternProps);\n                const looseRecords = [];\n                for (const pattern of patternKeys) {\n                    const patternValue = convertSchema(patternProps[pattern], ctx);\n                    const keySchema = z.string().regex(new RegExp(pattern));\n                    looseRecords.push(z.looseRecord(keySchema, patternValue));\n                }\n                // Build intersection: object schema + all pattern property records\n                const schemasToIntersect = [];\n                if (Object.keys(shape).length > 0) {\n                    // Use passthrough so patternProperties can validate additional keys\n                    schemasToIntersect.push(z.object(shape).passthrough());\n                }\n                schemasToIntersect.push(...looseRecords);\n                if (schemasToIntersect.length === 0) {\n                    zodSchema = z.object({}).passthrough();\n                }\n                else if (schemasToIntersect.length === 1) {\n                    zodSchema = schemasToIntersect[0];\n                }\n                else {\n                    // Chain intersections: (A & B) & C & D ...\n                    let result = z.intersection(schemasToIntersect[0], schemasToIntersect[1]);\n                    for (let i = 2; i < schemasToIntersect.length; i++) {\n                        result = z.intersection(result, schemasToIntersect[i]);\n                    }\n                    zodSchema = result;\n                }\n                break;\n            }\n            // Handle additionalProperties\n            // In JSON Schema, additionalProperties defaults to true (allow any extra properties)\n            // In Zod, objects strip unknown keys by default, so we need to handle this explicitly\n            const objectSchema = z.object(shape);\n            if (schema.additionalProperties === false) {\n                // Strict mode - no extra properties allowed\n                zodSchema = objectSchema.strict();\n            }\n            else if (typeof schema.additionalProperties === \"object\") {\n                // Extra properties must match the specified schema\n                zodSchema = objectSchema.catchall(convertSchema(schema.additionalProperties, ctx));\n            }\n            else {\n                // additionalProperties is true or undefined - allow any extra properties (passthrough)\n                zodSchema = objectSchema.passthrough();\n            }\n            break;\n        }\n        case \"array\": {\n            // TODO: uniqueItems is not supported\n            // TODO: contains/minContains/maxContains are not supported\n            // Check if this is a tuple (prefixItems or items as array)\n            const prefixItems = schema.prefixItems;\n            const items = schema.items;\n            if (prefixItems && Array.isArray(prefixItems)) {\n                // Tuple with prefixItems (draft-2020-12)\n                const tupleItems = prefixItems.map((item) => convertSchema(item, ctx));\n                const rest = items && typeof items === \"object\" && !Array.isArray(items)\n                    ? convertSchema(items, ctx)\n                    : undefined;\n                if (rest) {\n                    zodSchema = z.tuple(tupleItems).rest(rest);\n                }\n                else {\n                    zodSchema = z.tuple(tupleItems);\n                }\n                // Apply minItems/maxItems constraints to tuples\n                if (typeof schema.minItems === \"number\") {\n                    zodSchema = zodSchema.check(z.minLength(schema.minItems));\n                }\n                if (typeof schema.maxItems === \"number\") {\n                    zodSchema = zodSchema.check(z.maxLength(schema.maxItems));\n                }\n            }\n            else if (Array.isArray(items)) {\n                // Tuple with items array (draft-7)\n                const tupleItems = items.map((item) => convertSchema(item, ctx));\n                const rest = schema.additionalItems && typeof schema.additionalItems === \"object\"\n                    ? convertSchema(schema.additionalItems, ctx)\n                    : undefined; // additionalItems: false means no rest, handled by default tuple behavior\n                if (rest) {\n                    zodSchema = z.tuple(tupleItems).rest(rest);\n                }\n                else {\n                    zodSchema = z.tuple(tupleItems);\n                }\n                // Apply minItems/maxItems constraints to tuples\n                if (typeof schema.minItems === \"number\") {\n                    zodSchema = zodSchema.check(z.minLength(schema.minItems));\n                }\n                if (typeof schema.maxItems === \"number\") {\n                    zodSchema = zodSchema.check(z.maxLength(schema.maxItems));\n                }\n            }\n            else if (items !== undefined) {\n                // Regular array\n                const element = convertSchema(items, ctx);\n                let arraySchema = z.array(element);\n                // Apply constraints\n                if (typeof schema.minItems === \"number\") {\n                    arraySchema = arraySchema.min(schema.minItems);\n                }\n                if (typeof schema.maxItems === \"number\") {\n                    arraySchema = arraySchema.max(schema.maxItems);\n                }\n                zodSchema = arraySchema;\n            }\n            else {\n                // No items specified - array of any\n                zodSchema = z.array(z.any());\n            }\n            break;\n        }\n        default:\n            throw new Error(`Unsupported type: ${type}`);\n    }\n    // Apply metadata\n    if (schema.description) {\n        zodSchema = zodSchema.describe(schema.description);\n    }\n    if (schema.default !== undefined) {\n        zodSchema = zodSchema.default(schema.default);\n    }\n    return zodSchema;\n}\nfunction convertSchema(schema, ctx) {\n    if (typeof schema === \"boolean\") {\n        return schema ? z.any() : z.never();\n    }\n    // Convert base schema first (ignoring composition keywords)\n    let baseSchema = convertBaseSchema(schema, ctx);\n    const hasExplicitType = schema.type || schema.enum !== undefined || schema.const !== undefined;\n    // Process composition keywords LAST (they can appear together)\n    // Handle anyOf - wrap base schema with union\n    if (schema.anyOf && Array.isArray(schema.anyOf)) {\n        const options = schema.anyOf.map((s) => convertSchema(s, ctx));\n        const anyOfUnion = z.union(options);\n        baseSchema = hasExplicitType ? z.intersection(baseSchema, anyOfUnion) : anyOfUnion;\n    }\n    // Handle oneOf - exclusive union (exactly one must match)\n    if (schema.oneOf && Array.isArray(schema.oneOf)) {\n        const options = schema.oneOf.map((s) => convertSchema(s, ctx));\n        const oneOfUnion = z.xor(options);\n        baseSchema = hasExplicitType ? z.intersection(baseSchema, oneOfUnion) : oneOfUnion;\n    }\n    // Handle allOf - wrap base schema with intersection\n    if (schema.allOf && Array.isArray(schema.allOf)) {\n        if (schema.allOf.length === 0) {\n            baseSchema = hasExplicitType ? baseSchema : z.any();\n        }\n        else {\n            let result = hasExplicitType ? baseSchema : convertSchema(schema.allOf[0], ctx);\n            const startIdx = hasExplicitType ? 0 : 1;\n            for (let i = startIdx; i < schema.allOf.length; i++) {\n                result = z.intersection(result, convertSchema(schema.allOf[i], ctx));\n            }\n            baseSchema = result;\n        }\n    }\n    // Handle nullable (OpenAPI 3.0)\n    if (schema.nullable === true && ctx.version === \"openapi-3.0\") {\n        baseSchema = z.nullable(baseSchema);\n    }\n    // Handle readOnly\n    if (schema.readOnly === true) {\n        baseSchema = z.readonly(baseSchema);\n    }\n    // Collect metadata: core schema keywords and unrecognized keys\n    const extraMeta = {};\n    // Core schema keywords that should be captured as metadata\n    const coreMetadataKeys = [\"$id\", \"id\", \"$comment\", \"$anchor\", \"$vocabulary\", \"$dynamicRef\", \"$dynamicAnchor\"];\n    for (const key of coreMetadataKeys) {\n        if (key in schema) {\n            extraMeta[key] = schema[key];\n        }\n    }\n    // Content keywords - store as metadata\n    const contentMetadataKeys = [\"contentEncoding\", \"contentMediaType\", \"contentSchema\"];\n    for (const key of contentMetadataKeys) {\n        if (key in schema) {\n            extraMeta[key] = schema[key];\n        }\n    }\n    // Unrecognized keys (custom metadata)\n    for (const key of Object.keys(schema)) {\n        if (!RECOGNIZED_KEYS.has(key)) {\n            extraMeta[key] = schema[key];\n        }\n    }\n    if (Object.keys(extraMeta).length > 0) {\n        ctx.registry.add(baseSchema, extraMeta);\n    }\n    return baseSchema;\n}\n/**\n * Converts a JSON Schema to a Zod schema. This function should be considered semi-experimental. It's behavior is liable to change. */\nexport function fromJSONSchema(schema, params) {\n    // Handle boolean schemas\n    if (typeof schema === \"boolean\") {\n        return schema ? z.any() : z.never();\n    }\n    const version = detectVersion(schema, params?.defaultTarget);\n    const defs = (schema.$defs || schema.definitions || {});\n    const ctx = {\n        version,\n        defs,\n        refs: new Map(),\n        processing: new Set(),\n        rootSchema: schema,\n        registry: params?.registry ?? globalRegistry,\n    };\n    return convertSchema(schema, ctx);\n}\n", "import * as core from \"../core/index.js\";\nimport * as schemas from \"./schemas.js\";\nexport function string(params) {\n    return core._coercedString(schemas.ZodString, params);\n}\nexport function number(params) {\n    return core._coercedNumber(schemas.ZodNumber, params);\n}\nexport function boolean(params) {\n    return core._coercedBoolean(schemas.ZodBoolean, params);\n}\nexport function bigint(params) {\n    return core._coercedBigint(schemas.ZodBigInt, params);\n}\nexport function date(params) {\n    return core._coercedDate(schemas.ZodDate, params);\n}\n", "import { ZodSchema, ZodTypeDef } from 'zod/v3';\nimport { Refs, Seen } from './Refs';\nimport { JsonSchema7Type } from './parseDef';\n\nexport type Targets = 'jsonSchema7' | 'jsonSchema2019-09' | 'openApi3';\n\nexport type DateStrategy = 'format:date-time' | 'format:date' | 'string' | 'integer';\n\nexport const ignoreOverride = Symbol('Let zodToJsonSchema decide on which parser to use');\n\nexport type Options<Target extends Targets = 'jsonSchema7'> = {\n  name: string | undefined;\n  $refStrategy: 'root' | 'relative' | 'none' | 'seen' | 'extract-to-root';\n  basePath: string[];\n  effectStrategy: 'input' | 'any';\n  pipeStrategy: 'input' | 'output' | 'all';\n  dateStrategy: DateStrategy | DateStrategy[];\n  mapStrategy: 'entries' | 'record';\n  removeAdditionalStrategy: 'passthrough' | 'strict';\n  nullableStrategy: 'from-target' | 'property';\n  target: Target;\n  strictUnions: boolean;\n  definitionPath: string;\n  definitions: Record<string, ZodSchema | ZodTypeDef>;\n  errorMessages: boolean;\n  markdownDescription: boolean;\n  patternStrategy: 'escape' | 'preserve';\n  applyRegexFlags: boolean;\n  emailStrategy: 'format:email' | 'format:idn-email' | 'pattern:zod';\n  base64Strategy: 'format:binary' | 'contentEncoding:base64' | 'pattern:zod';\n  nameStrategy: 'ref' | 'duplicate-ref' | 'title';\n  override?: (\n    def: ZodTypeDef,\n    refs: Refs,\n    seen: Seen | undefined,\n    forceResolution?: boolean,\n  ) => JsonSchema7Type | undefined | typeof ignoreOverride;\n  openaiStrictMode?: boolean;\n};\n\nconst defaultOptions: Omit<Options, 'definitions' | 'basePath'> = {\n  name: undefined,\n  $refStrategy: 'root',\n  effectStrategy: 'input',\n  pipeStrategy: 'all',\n  dateStrategy: 'format:date-time',\n  mapStrategy: 'entries',\n  nullableStrategy: 'from-target',\n  removeAdditionalStrategy: 'passthrough',\n  definitionPath: 'definitions',\n  target: 'jsonSchema7',\n  strictUnions: false,\n  errorMessages: false,\n  markdownDescription: false,\n  patternStrategy: 'escape',\n  applyRegexFlags: false,\n  emailStrategy: 'format:email',\n  base64Strategy: 'contentEncoding:base64',\n  nameStrategy: 'ref',\n};\n\nexport const getDefaultOptions = <Target extends Targets>(\n  options: Partial<Options<Target>> | string | undefined,\n) => {\n  // We need to add `definitions` here as we may mutate it\n  return (\n    typeof options === 'string' ?\n      {\n        ...defaultOptions,\n        basePath: ['#'],\n        definitions: {},\n        name: options,\n      }\n    : {\n        ...defaultOptions,\n        basePath: ['#'],\n        definitions: {},\n        ...options,\n      }) as Options<Target>;\n};\n", "import type { ZodSchema, ZodTypeDef } from 'zod/v3';\n\nexport const zodDef = (zodSchema: ZodSchema | ZodTypeDef): ZodTypeDef => {\n  return '_def' in zodSchema ? zodSchema._def : zodSchema;\n};\n\nexport function isEmptyObj(obj: Object | null | undefined): boolean {\n  if (!obj) return true;\n  for (const _k in obj) return false;\n  return true;\n}\n", "import type { ZodTypeDef } from 'zod/v3';\nimport { getDefaultOptions, Options, Targets } from './Options';\nimport { JsonSchema7Type } from './parseDef';\nimport { zodDef } from './util';\n\nexport type Refs = {\n  seen: Map<ZodTypeDef, Seen>;\n  /**\n   * Set of all the `$ref`s we created, e.g. `Set(['#/$defs/ui'])`\n   * this notable does not include any `definitions` that were\n   * explicitly given as an option.\n   */\n  seenRefs: Set<string>;\n  currentPath: string[];\n  propertyPath: string[] | undefined;\n} & Options<Targets>;\n\nexport type Seen = {\n  def: ZodTypeDef;\n  path: string[];\n  jsonSchema: JsonSchema7Type | undefined;\n};\n\nexport const getRefs = (options?: string | Partial<Options<Targets>>): Refs => {\n  const _options = getDefaultOptions(options);\n  const currentPath =\n    _options.name !== undefined ?\n      [..._options.basePath, _options.definitionPath, _options.name]\n    : _options.basePath;\n  return {\n    ..._options,\n    currentPath: currentPath,\n    propertyPath: undefined,\n    seenRefs: new Set(),\n    seen: new Map(\n      Object.entries(_options.definitions).map(([name, def]) => [\n        zodDef(def),\n        {\n          def: zodDef(def),\n          path: [..._options.basePath, _options.definitionPath, name],\n          // Resolution of references will be forced even though seen, so it's ok that the schema is undefined here for now.\n          jsonSchema: undefined,\n        },\n      ]),\n    ),\n  };\n};\n", "import { JsonSchema7TypeUnion } from './parseDef';\nimport { Refs } from './Refs';\n\nexport type ErrorMessages<T extends JsonSchema7TypeUnion, OmitProperties extends string = ''> = Partial<\n  Omit<{ [key in keyof T]: string }, OmitProperties | 'type' | 'errorMessages'>\n>;\n\nexport function addErrorMessage<T extends { errorMessage?: ErrorMessages<any> }>(\n  res: T,\n  key: keyof T,\n  errorMessage: string | undefined,\n  refs: Refs,\n) {\n  if (!refs?.errorMessages) return;\n  if (errorMessage) {\n    res.errorMessage = {\n      ...res.errorMessage,\n      [key]: errorMessage,\n    };\n  }\n}\n\nexport function setResponseValueAndErrors<\n  Json7Type extends JsonSchema7TypeUnion & {\n    errorMessage?: ErrorMessages<Json7Type>;\n  },\n  Key extends keyof Omit<Json7Type, 'errorMessage'>,\n>(res: Json7Type, key: Key, value: Json7Type[Key], errorMessage: string | undefined, refs: Refs) {\n  res[key] = value;\n  addErrorMessage(res, key, errorMessage, refs);\n}\n", "export type JsonSchema7AnyType = {};\n\nexport function parseAnyDef(): JsonSchema7AnyType {\n  return {};\n}\n", "import { ZodArrayDef, ZodFirstPartyTypeKind } from 'zod/v3';\nimport { ErrorMessages, setResponseValueAndErrors } from '../errorMessages';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport type JsonSchema7ArrayType = {\n  type: 'array';\n  items?: JsonSchema7Type | undefined;\n  minItems?: number;\n  maxItems?: number;\n  errorMessages?: ErrorMessages<JsonSchema7ArrayType, 'items'>;\n};\n\nexport function parseArrayDef(def: ZodArrayDef, refs: Refs) {\n  const res: JsonSchema7ArrayType = {\n    type: 'array',\n  };\n  if (def.type?._def?.typeName !== ZodFirstPartyTypeKind.ZodAny) {\n    res.items = parseDef(def.type._def, {\n      ...refs,\n      currentPath: [...refs.currentPath, 'items'],\n    });\n  }\n\n  if (def.minLength) {\n    setResponseValueAndErrors(res, 'minItems', def.minLength.value, def.minLength.message, refs);\n  }\n  if (def.maxLength) {\n    setResponseValueAndErrors(res, 'maxItems', def.maxLength.value, def.maxLength.message, refs);\n  }\n  if (def.exactLength) {\n    setResponseValueAndErrors(res, 'minItems', def.exactLength.value, def.exactLength.message, refs);\n    setResponseValueAndErrors(res, 'maxItems', def.exactLength.value, def.exactLength.message, refs);\n  }\n  return res;\n}\n", "import { ZodBigIntDef } from 'zod/v3';\nimport { Refs } from '../Refs';\nimport { ErrorMessages, setResponseValueAndErrors } from '../errorMessages';\n\nexport type JsonSchema7BigintType = {\n  type: 'integer';\n  format: 'int64';\n  minimum?: BigInt;\n  exclusiveMinimum?: BigInt;\n  maximum?: BigInt;\n  exclusiveMaximum?: BigInt;\n  multipleOf?: BigInt;\n  errorMessage?: ErrorMessages<JsonSchema7BigintType>;\n};\n\nexport function parseBigintDef(def: ZodBigIntDef, refs: Refs): JsonSchema7BigintType {\n  const res: JsonSchema7BigintType = {\n    type: 'integer',\n    format: 'int64',\n  };\n\n  if (!def.checks) return res;\n\n  for (const check of def.checks) {\n    switch (check.kind) {\n      case 'min':\n        if (refs.target === 'jsonSchema7') {\n          if (check.inclusive) {\n            setResponseValueAndErrors(res, 'minimum', check.value, check.message, refs);\n          } else {\n            setResponseValueAndErrors(res, 'exclusiveMinimum', check.value, check.message, refs);\n          }\n        } else {\n          if (!check.inclusive) {\n            res.exclusiveMinimum = true as any;\n          }\n          setResponseValueAndErrors(res, 'minimum', check.value, check.message, refs);\n        }\n        break;\n      case 'max':\n        if (refs.target === 'jsonSchema7') {\n          if (check.inclusive) {\n            setResponseValueAndErrors(res, 'maximum', check.value, check.message, refs);\n          } else {\n            setResponseValueAndErrors(res, 'exclusiveMaximum', check.value, check.message, refs);\n          }\n        } else {\n          if (!check.inclusive) {\n            res.exclusiveMaximum = true as any;\n          }\n          setResponseValueAndErrors(res, 'maximum', check.value, check.message, refs);\n        }\n        break;\n      case 'multipleOf':\n        setResponseValueAndErrors(res, 'multipleOf', check.value, check.message, refs);\n        break;\n    }\n  }\n  return res;\n}\n", "export type JsonSchema7BooleanType = {\n  type: 'boolean';\n};\n\nexport function parseBooleanDef(): JsonSchema7BooleanType {\n  return {\n    type: 'boolean',\n  };\n}\n", "import { ZodBrandedDef } from 'zod/v3';\nimport { parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport function parseBrandedDef(_def: ZodBrandedDef<any>, refs: Refs) {\n  return parseDef(_def.type._def, refs);\n}\n", "import { ZodCatchDef } from 'zod/v3';\nimport { parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport const parseCatchDef = (def: ZodCatchDef<any>, refs: Refs) => {\n  return parseDef(def.innerType._def, refs);\n};\n", "import { ZodDateDef } from 'zod/v3';\nimport { Refs } from '../Refs';\nimport { ErrorMessages, setResponseValueAndErrors } from '../errorMessages';\nimport { JsonSchema7NumberType } from './number';\nimport { DateStrategy } from '../Options';\n\nexport type JsonSchema7DateType =\n  | {\n      type: 'integer' | 'string';\n      format: 'unix-time' | 'date-time' | 'date';\n      minimum?: number;\n      maximum?: number;\n      errorMessage?: ErrorMessages<JsonSchema7NumberType>;\n    }\n  | {\n      anyOf: JsonSchema7DateType[];\n    };\n\nexport function parseDateDef(\n  def: ZodDateDef,\n  refs: Refs,\n  overrideDateStrategy?: DateStrategy,\n): JsonSchema7DateType {\n  const strategy = overrideDateStrategy ?? refs.dateStrategy;\n\n  if (Array.isArray(strategy)) {\n    return {\n      anyOf: strategy.map((item, i) => parseDateDef(def, refs, item)),\n    };\n  }\n\n  switch (strategy) {\n    case 'string':\n    case 'format:date-time':\n      return {\n        type: 'string',\n        format: 'date-time',\n      };\n    case 'format:date':\n      return {\n        type: 'string',\n        format: 'date',\n      };\n    case 'integer':\n      return integerDateParser(def, refs);\n  }\n}\n\nconst integerDateParser = (def: ZodDateDef, refs: Refs) => {\n  const res: JsonSchema7DateType = {\n    type: 'integer',\n    format: 'unix-time',\n  };\n\n  if (refs.target === 'openApi3') {\n    return res;\n  }\n\n  for (const check of def.checks) {\n    switch (check.kind) {\n      case 'min':\n        setResponseValueAndErrors(\n          res,\n          'minimum',\n          check.value, // This is in milliseconds\n          check.message,\n          refs,\n        );\n        break;\n      case 'max':\n        setResponseValueAndErrors(\n          res,\n          'maximum',\n          check.value, // This is in milliseconds\n          check.message,\n          refs,\n        );\n        break;\n    }\n  }\n\n  return res;\n};\n", "import { ZodDefaultDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport function parseDefaultDef(_def: ZodDefaultDef, refs: Refs): JsonSchema7Type & { default: any } {\n  return {\n    ...parseDef(_def.innerType._def, refs),\n    default: _def.defaultValue(),\n  };\n}\n", "import { ZodEffectsDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport function parseEffectsDef(\n  _def: ZodEffectsDef,\n  refs: Refs,\n  forceResolution: boolean,\n): JsonSchema7Type | undefined {\n  return refs.effectStrategy === 'input' ? parseDef(_def.schema._def, refs, forceResolution) : {};\n}\n", "import { ZodEnumDef } from 'zod/v3';\n\nexport type JsonSchema7EnumType = {\n  type: 'string';\n  enum: string[];\n};\n\nexport function parseEnumDef(def: ZodEnumDef): JsonSchema7EnumType {\n  return {\n    type: 'string',\n    enum: [...def.values],\n  };\n}\n", "import { ZodIntersectionDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\nimport { JsonSchema7StringType } from './string';\n\nexport type JsonSchema7AllOfType = {\n  allOf: JsonSchema7Type[];\n  unevaluatedProperties?: boolean;\n};\n\nconst isJsonSchema7AllOfType = (\n  type: JsonSchema7Type | JsonSchema7StringType,\n): type is JsonSchema7AllOfType => {\n  if ('type' in type && type.type === 'string') return false;\n  return 'allOf' in type;\n};\n\nexport function parseIntersectionDef(\n  def: ZodIntersectionDef,\n  refs: Refs,\n): JsonSchema7AllOfType | JsonSchema7Type | undefined {\n  const allOf = [\n    parseDef(def.left._def, {\n      ...refs,\n      currentPath: [...refs.currentPath, 'allOf', '0'],\n    }),\n    parseDef(def.right._def, {\n      ...refs,\n      currentPath: [...refs.currentPath, 'allOf', '1'],\n    }),\n  ].filter((x): x is JsonSchema7Type => !!x);\n\n  let unevaluatedProperties: Pick<JsonSchema7AllOfType, 'unevaluatedProperties'> | undefined =\n    refs.target === 'jsonSchema2019-09' ? { unevaluatedProperties: false } : undefined;\n\n  const mergedAllOf: JsonSchema7Type[] = [];\n  // If either of the schemas is an allOf, merge them into a single allOf\n  allOf.forEach((schema) => {\n    if (isJsonSchema7AllOfType(schema)) {\n      mergedAllOf.push(...schema.allOf);\n      if (schema.unevaluatedProperties === undefined) {\n        // If one of the schemas has no unevaluatedProperties set,\n        // the merged schema should also have no unevaluatedProperties set\n        unevaluatedProperties = undefined;\n      }\n    } else {\n      let nestedSchema: JsonSchema7Type = schema;\n      if ('additionalProperties' in schema && schema.additionalProperties === false) {\n        const { additionalProperties, ...rest } = schema;\n        nestedSchema = rest;\n      } else {\n        // As soon as one of the schemas has additionalProperties set not to false, we allow unevaluatedProperties\n        unevaluatedProperties = undefined;\n      }\n      mergedAllOf.push(nestedSchema);\n    }\n  });\n  return mergedAllOf.length ?\n      {\n        allOf: mergedAllOf,\n        ...unevaluatedProperties,\n      }\n    : undefined;\n}\n", "import { ZodLiteralDef } from 'zod/v3';\nimport { Refs } from '../Refs';\n\nexport type JsonSchema7LiteralType =\n  | {\n      type: 'string' | 'number' | 'integer' | 'boolean';\n      const: string | number | boolean;\n    }\n  | {\n      type: 'object' | 'array';\n    };\n\nexport function parseLiteralDef(def: ZodLiteralDef, refs: Refs): JsonSchema7LiteralType {\n  const parsedType = typeof def.value;\n  if (\n    parsedType !== 'bigint' &&\n    parsedType !== 'number' &&\n    parsedType !== 'boolean' &&\n    parsedType !== 'string'\n  ) {\n    return {\n      type: Array.isArray(def.value) ? 'array' : 'object',\n    };\n  }\n\n  if (refs.target === 'openApi3') {\n    return {\n      type: parsedType === 'bigint' ? 'integer' : parsedType,\n      enum: [def.value],\n    } as any;\n  }\n\n  return {\n    type: parsedType === 'bigint' ? 'integer' : parsedType,\n    const: def.value,\n  };\n}\n", "// @ts-nocheck\nimport { ZodStringDef } from 'zod/v3';\nimport { ErrorMessages, setResponseValueAndErrors } from '../errorMessages';\nimport { Refs } from '../Refs';\n\nlet emojiRegex: RegExp | undefined;\n\n/**\n * Generated from the regular expressions found here as of 2024-05-22:\n * https://github.com/colinhacks/zod/blob/master/src/types.ts.\n *\n * Expressions with /i flag have been changed accordingly.\n */\nexport const zodPatterns = {\n  /**\n   * `c` was changed to `[cC]` to replicate /i flag\n   */\n  cuid: /^[cC][^\\s-]{8,}$/,\n  cuid2: /^[0-9a-z]+$/,\n  ulid: /^[0-9A-HJKMNP-TV-Z]{26}$/,\n  /**\n   * `a-z` was added to replicate /i flag\n   */\n  email: /^(?!\\.)(?!.*\\.\\.)([a-zA-Z0-9_'+\\-\\.]*)[a-zA-Z0-9_+-]@([a-zA-Z0-9][a-zA-Z0-9\\-]*\\.)+[a-zA-Z]{2,}$/,\n  /**\n   * Constructed a valid Unicode RegExp\n   *\n   * Lazily instantiate since this type of regex isn't supported\n   * in all envs (e.g. React Native).\n   *\n   * See:\n   * https://github.com/colinhacks/zod/issues/2433\n   * Fix in Zod:\n   * https://github.com/colinhacks/zod/commit/9340fd51e48576a75adc919bff65dbc4a5d4c99b\n   */\n  emoji: () => {\n    if (emojiRegex === undefined) {\n      emojiRegex = RegExp('^(\\\\p{Extended_Pictographic}|\\\\p{Emoji_Component})+$', 'u');\n    }\n    return emojiRegex;\n  },\n  /**\n   * Unused\n   */\n  uuid: /^[0-9a-fA-F]{8}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{12}$/,\n  /**\n   * Unused\n   */\n  ipv4: /^(?:(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])\\.){3}(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])$/,\n  /**\n   * Unused\n   */\n  ipv6: /^(([a-f0-9]{1,4}:){7}|::([a-f0-9]{1,4}:){0,6}|([a-f0-9]{1,4}:){1}:([a-f0-9]{1,4}:){0,5}|([a-f0-9]{1,4}:){2}:([a-f0-9]{1,4}:){0,4}|([a-f0-9]{1,4}:){3}:([a-f0-9]{1,4}:){0,3}|([a-f0-9]{1,4}:){4}:([a-f0-9]{1,4}:){0,2}|([a-f0-9]{1,4}:){5}:([a-f0-9]{1,4}:){0,1})([a-f0-9]{1,4}|(((25[0-5])|(2[0-4][0-9])|(1[0-9]{2})|([0-9]{1,2}))\\.){3}((25[0-5])|(2[0-4][0-9])|(1[0-9]{2})|([0-9]{1,2})))$/,\n  base64: /^([0-9a-zA-Z+/]{4})*(([0-9a-zA-Z+/]{2}==)|([0-9a-zA-Z+/]{3}=))?$/,\n  nanoid: /^[a-zA-Z0-9_-]{21}$/,\n} as const;\n\nexport type JsonSchema7StringType = {\n  type: 'string';\n  minLength?: number;\n  maxLength?: number;\n  format?:\n    | 'email'\n    | 'idn-email'\n    | 'uri'\n    | 'uuid'\n    | 'date-time'\n    | 'ipv4'\n    | 'ipv6'\n    | 'date'\n    | 'time'\n    | 'duration';\n  pattern?: string;\n  allOf?: {\n    pattern: string;\n    errorMessage?: ErrorMessages<{ pattern: string }>;\n  }[];\n  anyOf?: {\n    format: string;\n    errorMessage?: ErrorMessages<{ format: string }>;\n  }[];\n  errorMessage?: ErrorMessages<JsonSchema7StringType>;\n  contentEncoding?: string;\n};\n\nexport function parseStringDef(def: ZodStringDef, refs: Refs): JsonSchema7StringType {\n  const res: JsonSchema7StringType = {\n    type: 'string',\n  };\n\n  function processPattern(value: string): string {\n    return refs.patternStrategy === 'escape' ? escapeNonAlphaNumeric(value) : value;\n  }\n\n  if (def.checks) {\n    for (const check of def.checks) {\n      switch (check.kind) {\n        case 'min':\n          setResponseValueAndErrors(\n            res,\n            'minLength',\n            typeof res.minLength === 'number' ? Math.max(res.minLength, check.value) : check.value,\n            check.message,\n            refs,\n          );\n          break;\n        case 'max':\n          setResponseValueAndErrors(\n            res,\n            'maxLength',\n            typeof res.maxLength === 'number' ? Math.min(res.maxLength, check.value) : check.value,\n            check.message,\n            refs,\n          );\n\n          break;\n        case 'email':\n          switch (refs.emailStrategy) {\n            case 'format:email':\n              addFormat(res, 'email', check.message, refs);\n              break;\n            case 'format:idn-email':\n              addFormat(res, 'idn-email', check.message, refs);\n              break;\n            case 'pattern:zod':\n              addPattern(res, zodPatterns.email, check.message, refs);\n              break;\n          }\n\n          break;\n        case 'url':\n          addFormat(res, 'uri', check.message, refs);\n          break;\n        case 'uuid':\n          addFormat(res, 'uuid', check.message, refs);\n          break;\n        case 'regex':\n          addPattern(res, check.regex, check.message, refs);\n          break;\n        case 'cuid':\n          addPattern(res, zodPatterns.cuid, check.message, refs);\n          break;\n        case 'cuid2':\n          addPattern(res, zodPatterns.cuid2, check.message, refs);\n          break;\n        case 'startsWith':\n          addPattern(res, RegExp(`^${processPattern(check.value)}`), check.message, refs);\n          break;\n        case 'endsWith':\n          addPattern(res, RegExp(`${processPattern(check.value)}$`), check.message, refs);\n          break;\n\n        case 'datetime':\n          addFormat(res, 'date-time', check.message, refs);\n          break;\n        case 'date':\n          addFormat(res, 'date', check.message, refs);\n          break;\n        case 'time':\n          addFormat(res, 'time', check.message, refs);\n          break;\n        case 'duration':\n          addFormat(res, 'duration', check.message, refs);\n          break;\n        case 'length':\n          setResponseValueAndErrors(\n            res,\n            'minLength',\n            typeof res.minLength === 'number' ? Math.max(res.minLength, check.value) : check.value,\n            check.message,\n            refs,\n          );\n          setResponseValueAndErrors(\n            res,\n            'maxLength',\n            typeof res.maxLength === 'number' ? Math.min(res.maxLength, check.value) : check.value,\n            check.message,\n            refs,\n          );\n          break;\n        case 'includes': {\n          addPattern(res, RegExp(processPattern(check.value)), check.message, refs);\n          break;\n        }\n        case 'ip': {\n          if (check.version !== 'v6') {\n            addFormat(res, 'ipv4', check.message, refs);\n          }\n          if (check.version !== 'v4') {\n            addFormat(res, 'ipv6', check.message, refs);\n          }\n          break;\n        }\n        case 'emoji':\n          addPattern(res, zodPatterns.emoji, check.message, refs);\n          break;\n        case 'ulid': {\n          addPattern(res, zodPatterns.ulid, check.message, refs);\n          break;\n        }\n        case 'base64': {\n          switch (refs.base64Strategy) {\n            case 'format:binary': {\n              addFormat(res, 'binary' as any, check.message, refs);\n              break;\n            }\n\n            case 'contentEncoding:base64': {\n              setResponseValueAndErrors(res, 'contentEncoding', 'base64', check.message, refs);\n              break;\n            }\n\n            case 'pattern:zod': {\n              addPattern(res, zodPatterns.base64, check.message, refs);\n              break;\n            }\n          }\n          break;\n        }\n        case 'nanoid': {\n          addPattern(res, zodPatterns.nanoid, check.message, refs);\n        }\n        case 'toLowerCase':\n        case 'toUpperCase':\n        case 'trim':\n          break;\n        default:\n          ((_: never) => {})(check);\n      }\n    }\n  }\n\n  return res;\n}\n\nconst escapeNonAlphaNumeric = (value: string) =>\n  Array.from(value)\n    .map((c) => (/[a-zA-Z0-9]/.test(c) ? c : `\\\\${c}`))\n    .join('');\n\nconst addFormat = (\n  schema: JsonSchema7StringType,\n  value: Required<JsonSchema7StringType>['format'],\n  message: string | undefined,\n  refs: Refs,\n) => {\n  if (schema.format || schema.anyOf?.some((x) => x.format)) {\n    if (!schema.anyOf) {\n      schema.anyOf = [];\n    }\n\n    if (schema.format) {\n      schema.anyOf!.push({\n        format: schema.format,\n        ...(schema.errorMessage &&\n          refs.errorMessages && {\n            errorMessage: { format: schema.errorMessage.format },\n          }),\n      });\n      delete schema.format;\n      if (schema.errorMessage) {\n        delete schema.errorMessage.format;\n        if (Object.keys(schema.errorMessage).length === 0) {\n          delete schema.errorMessage;\n        }\n      }\n    }\n\n    schema.anyOf!.push({\n      format: value,\n      ...(message && refs.errorMessages && { errorMessage: { format: message } }),\n    });\n  } else {\n    setResponseValueAndErrors(schema, 'format', value, message, refs);\n  }\n};\n\nconst addPattern = (\n  schema: JsonSchema7StringType,\n  regex: RegExp | (() => RegExp),\n  message: string | undefined,\n  refs: Refs,\n) => {\n  if (schema.pattern || schema.allOf?.some((x) => x.pattern)) {\n    if (!schema.allOf) {\n      schema.allOf = [];\n    }\n\n    if (schema.pattern) {\n      schema.allOf!.push({\n        pattern: schema.pattern,\n        ...(schema.errorMessage &&\n          refs.errorMessages && {\n            errorMessage: { pattern: schema.errorMessage.pattern },\n          }),\n      });\n      delete schema.pattern;\n      if (schema.errorMessage) {\n        delete schema.errorMessage.pattern;\n        if (Object.keys(schema.errorMessage).length === 0) {\n          delete schema.errorMessage;\n        }\n      }\n    }\n\n    schema.allOf!.push({\n      pattern: processRegExp(regex, refs),\n      ...(message && refs.errorMessages && { errorMessage: { pattern: message } }),\n    });\n  } else {\n    setResponseValueAndErrors(schema, 'pattern', processRegExp(regex, refs), message, refs);\n  }\n};\n\n// Mutate z.string.regex() in a best attempt to accommodate for regex flags when applyRegexFlags is true\nconst processRegExp = (regexOrFunction: RegExp | (() => RegExp), refs: Refs): string => {\n  const regex = typeof regexOrFunction === 'function' ? regexOrFunction() : regexOrFunction;\n  if (!refs.applyRegexFlags || !regex.flags) return regex.source;\n\n  // Currently handled flags\n  const flags = {\n    i: regex.flags.includes('i'), // Case-insensitive\n    m: regex.flags.includes('m'), // `^` and `$` matches adjacent to newline characters\n    s: regex.flags.includes('s'), // `.` matches newlines\n  };\n\n  // The general principle here is to step through each character, one at a time, applying mutations as flags require. We keep track when the current character is escaped, and when it's inside a group /like [this]/ or (also) a range like /[a-z]/. The following is fairly brittle imperative code; edit at your peril!\n\n  const source = flags.i ? regex.source.toLowerCase() : regex.source;\n  let pattern = '';\n  let isEscaped = false;\n  let inCharGroup = false;\n  let inCharRange = false;\n\n  for (let i = 0; i < source.length; i++) {\n    if (isEscaped) {\n      pattern += source[i];\n      isEscaped = false;\n      continue;\n    }\n\n    if (flags.i) {\n      if (inCharGroup) {\n        if (source[i].match(/[a-z]/)) {\n          if (inCharRange) {\n            pattern += source[i];\n            pattern += `${source[i - 2]}-${source[i]}`.toUpperCase();\n            inCharRange = false;\n          } else if (source[i + 1] === '-' && source[i + 2]?.match(/[a-z]/)) {\n            pattern += source[i];\n            inCharRange = true;\n          } else {\n            pattern += `${source[i]}${source[i].toUpperCase()}`;\n          }\n          continue;\n        }\n      } else if (source[i].match(/[a-z]/)) {\n        pattern += `[${source[i]}${source[i].toUpperCase()}]`;\n        continue;\n      }\n    }\n\n    if (flags.m) {\n      if (source[i] === '^') {\n        pattern += `(^|(?<=[\\r\\n]))`;\n        continue;\n      } else if (source[i] === '$') {\n        pattern += `($|(?=[\\r\\n]))`;\n        continue;\n      }\n    }\n\n    if (flags.s && source[i] === '.') {\n      pattern += inCharGroup ? `${source[i]}\\r\\n` : `[${source[i]}\\r\\n]`;\n      continue;\n    }\n\n    pattern += source[i];\n    if (source[i] === '\\\\') {\n      isEscaped = true;\n    } else if (inCharGroup && source[i] === ']') {\n      inCharGroup = false;\n    } else if (!inCharGroup && source[i] === '[') {\n      inCharGroup = true;\n    }\n  }\n\n  try {\n    const regexTest = new RegExp(pattern);\n  } catch {\n    console.warn(\n      `Could not convert regex pattern at ${refs.currentPath.join(\n        '/',\n      )} to a flag-independent form! Falling back to the flag-ignorant source`,\n    );\n    return regex.source;\n  }\n\n  return pattern;\n};\n", "import { ZodFirstPartyTypeKind, ZodMapDef, ZodRecordDef, ZodTypeAny } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\nimport { JsonSchema7EnumType } from './enum';\nimport { JsonSchema7ObjectType } from './object';\nimport { JsonSchema7StringType, parseStringDef } from './string';\n\ntype JsonSchema7RecordPropertyNamesType =\n  | Omit<JsonSchema7StringType, 'type'>\n  | Omit<JsonSchema7EnumType, 'type'>;\n\nexport type JsonSchema7RecordType = {\n  type: 'object';\n  additionalProperties: JsonSchema7Type;\n  propertyNames?: JsonSchema7RecordPropertyNamesType;\n};\n\nexport function parseRecordDef(\n  def: ZodRecordDef<ZodTypeAny, ZodTypeAny> | ZodMapDef,\n  refs: Refs,\n): JsonSchema7RecordType {\n  if (refs.target === 'openApi3' && def.keyType?._def.typeName === ZodFirstPartyTypeKind.ZodEnum) {\n    return {\n      type: 'object',\n      required: def.keyType._def.values,\n      properties: def.keyType._def.values.reduce(\n        (acc: Record<string, JsonSchema7Type>, key: string) => ({\n          ...acc,\n          [key]:\n            parseDef(def.valueType._def, {\n              ...refs,\n              currentPath: [...refs.currentPath, 'properties', key],\n            }) ?? {},\n        }),\n        {},\n      ),\n      additionalProperties: false,\n    } satisfies JsonSchema7ObjectType as any;\n  }\n\n  const schema: JsonSchema7RecordType = {\n    type: 'object',\n    additionalProperties:\n      parseDef(def.valueType._def, {\n        ...refs,\n        currentPath: [...refs.currentPath, 'additionalProperties'],\n      }) ?? {},\n  };\n\n  if (refs.target === 'openApi3') {\n    return schema;\n  }\n\n  if (def.keyType?._def.typeName === ZodFirstPartyTypeKind.ZodString && def.keyType._def.checks?.length) {\n    const keyType: JsonSchema7RecordPropertyNamesType = Object.entries(\n      parseStringDef(def.keyType._def, refs),\n    ).reduce((acc, [key, value]) => (key === 'type' ? acc : { ...acc, [key]: value }), {});\n\n    return {\n      ...schema,\n      propertyNames: keyType,\n    };\n  } else if (def.keyType?._def.typeName === ZodFirstPartyTypeKind.ZodEnum) {\n    return {\n      ...schema,\n      propertyNames: {\n        enum: def.keyType._def.values,\n      },\n    };\n  }\n\n  return schema;\n}\n", "import { ZodMapDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\nimport { JsonSchema7RecordType, parseRecordDef } from './record';\n\nexport type JsonSchema7MapType = {\n  type: 'array';\n  maxItems: 125;\n  items: {\n    type: 'array';\n    items: [JsonSchema7Type, JsonSchema7Type];\n    minItems: 2;\n    maxItems: 2;\n  };\n};\n\nexport function parseMapDef(def: ZodMapDef, refs: Refs): JsonSchema7MapType | JsonSchema7RecordType {\n  if (refs.mapStrategy === 'record') {\n    return parseRecordDef(def, refs);\n  }\n\n  const keys =\n    parseDef(def.keyType._def, {\n      ...refs,\n      currentPath: [...refs.currentPath, 'items', 'items', '0'],\n    }) || {};\n  const values =\n    parseDef(def.valueType._def, {\n      ...refs,\n      currentPath: [...refs.currentPath, 'items', 'items', '1'],\n    }) || {};\n  return {\n    type: 'array',\n    maxItems: 125,\n    items: {\n      type: 'array',\n      items: [keys, values],\n      minItems: 2,\n      maxItems: 2,\n    },\n  };\n}\n", "import { ZodNativeEnumDef } from 'zod/v3';\n\nexport type JsonSchema7NativeEnumType = {\n  type: 'string' | 'number' | ['string', 'number'];\n  enum: (string | number)[];\n};\n\nexport function parseNativeEnumDef(def: ZodNativeEnumDef): JsonSchema7NativeEnumType {\n  const object = def.values;\n  const actualKeys = Object.keys(def.values).filter((key: string) => {\n    return typeof object[object[key]!] !== 'number';\n  });\n\n  const actualValues = actualKeys.map((key: string) => object[key]!);\n\n  const parsedTypes = Array.from(new Set(actualValues.map((values: string | number) => typeof values)));\n\n  return {\n    type:\n      parsedTypes.length === 1 ?\n        parsedTypes[0] === 'string' ?\n          'string'\n        : 'number'\n      : ['string', 'number'],\n    enum: actualValues,\n  };\n}\n", "export type JsonSchema7NeverType = {\n  not: {};\n};\n\nexport function parseNeverDef(): JsonSchema7NeverType {\n  return {\n    not: {},\n  };\n}\n", "import { Refs } from '../Refs';\n\nexport type JsonSchema7NullType = {\n  type: 'null';\n};\n\nexport function parseNullDef(refs: Refs): JsonSchema7NullType {\n  return refs.target === 'openApi3' ?\n      ({\n        enum: ['null'],\n        nullable: true,\n      } as any)\n    : {\n        type: 'null',\n      };\n}\n", "import { ZodDiscriminatedUnionDef, ZodLiteralDef, ZodTypeAny, ZodUnionDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport const primitiveMappings = {\n  ZodString: 'string',\n  ZodNumber: 'number',\n  ZodBigInt: 'integer',\n  ZodBoolean: 'boolean',\n  ZodNull: 'null',\n} as const;\ntype ZodPrimitive = keyof typeof primitiveMappings;\ntype JsonSchema7Primitive = (typeof primitiveMappings)[keyof typeof primitiveMappings];\n\nexport type JsonSchema7UnionType = JsonSchema7PrimitiveUnionType | JsonSchema7AnyOfType;\n\ntype JsonSchema7PrimitiveUnionType =\n  | {\n      type: JsonSchema7Primitive | JsonSchema7Primitive[];\n    }\n  | {\n      type: JsonSchema7Primitive | JsonSchema7Primitive[];\n      enum: (string | number | bigint | boolean | null)[];\n    };\n\ntype JsonSchema7AnyOfType = {\n  anyOf: JsonSchema7Type[];\n};\n\nexport function parseUnionDef(\n  def: ZodUnionDef | ZodDiscriminatedUnionDef<any, any>,\n  refs: Refs,\n): JsonSchema7PrimitiveUnionType | JsonSchema7AnyOfType | undefined {\n  if (refs.target === 'openApi3') return asAnyOf(def, refs);\n\n  const options: readonly ZodTypeAny[] =\n    def.options instanceof Map ? Array.from(def.options.values()) : def.options;\n\n  // This blocks tries to look ahead a bit to produce nicer looking schemas with type array instead of anyOf.\n  if (\n    options.every((x) => x._def.typeName in primitiveMappings && (!x._def.checks || !x._def.checks.length))\n  ) {\n    // all types in union are primitive and lack checks, so might as well squash into {type: [...]}\n\n    const types = options.reduce((types: JsonSchema7Primitive[], x) => {\n      const type = primitiveMappings[x._def.typeName as ZodPrimitive]; //Can be safely casted due to row 43\n      return type && !types.includes(type) ? [...types, type] : types;\n    }, []);\n\n    return {\n      type: types.length > 1 ? types : types[0]!,\n    };\n  } else if (options.every((x) => x._def.typeName === 'ZodLiteral' && !x.description)) {\n    // all options literals\n\n    const types = options.reduce((acc: JsonSchema7Primitive[], x: { _def: ZodLiteralDef }) => {\n      const type = typeof x._def.value;\n      switch (type) {\n        case 'string':\n        case 'number':\n        case 'boolean':\n          return [...acc, type];\n        case 'bigint':\n          return [...acc, 'integer' as const];\n        case 'object':\n          if (x._def.value === null) return [...acc, 'null' as const];\n        case 'symbol':\n        case 'undefined':\n        case 'function':\n        default:\n          return acc;\n      }\n    }, []);\n\n    if (types.length === options.length) {\n      // all the literals are primitive, as far as null can be considered primitive\n\n      const uniqueTypes = types.filter((x, i, a) => a.indexOf(x) === i);\n      return {\n        type: uniqueTypes.length > 1 ? uniqueTypes : uniqueTypes[0]!,\n        enum: options.reduce(\n          (acc, x) => {\n            return acc.includes(x._def.value) ? acc : [...acc, x._def.value];\n          },\n          [] as (string | number | bigint | boolean | null)[],\n        ),\n      };\n    }\n  } else if (options.every((x) => x._def.typeName === 'ZodEnum')) {\n    return {\n      type: 'string',\n      enum: options.reduce(\n        (acc: string[], x) => [...acc, ...x._def.values.filter((x: string) => !acc.includes(x))],\n        [],\n      ),\n    };\n  }\n\n  return asAnyOf(def, refs);\n}\n\nconst asAnyOf = (\n  def: ZodUnionDef | ZodDiscriminatedUnionDef<any, any>,\n  refs: Refs,\n): JsonSchema7PrimitiveUnionType | JsonSchema7AnyOfType | undefined => {\n  const anyOf = ((def.options instanceof Map ? Array.from(def.options.values()) : def.options) as any[])\n    .map((x, i) =>\n      parseDef(x._def, {\n        ...refs,\n        currentPath: [...refs.currentPath, 'anyOf', `${i}`],\n      }),\n    )\n    .filter(\n      (x): x is JsonSchema7Type =>\n        !!x && (!refs.strictUnions || (typeof x === 'object' && Object.keys(x).length > 0)),\n    );\n\n  return anyOf.length ? { anyOf } : undefined;\n};\n", "import { ZodNullableDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\nimport { JsonSchema7NullType } from './null';\nimport { primitiveMappings } from './union';\n\nexport type JsonSchema7NullableType =\n  | {\n      anyOf: [JsonSchema7Type, JsonSchema7NullType];\n    }\n  | {\n      type: [string, 'null'];\n    };\n\nexport function parseNullableDef(def: ZodNullableDef, refs: Refs): JsonSchema7NullableType | undefined {\n  if (\n    ['ZodString', 'ZodNumber', 'ZodBigInt', 'ZodBoolean', 'ZodNull'].includes(def.innerType._def.typeName) &&\n    (!def.innerType._def.checks || !def.innerType._def.checks.length)\n  ) {\n    if (refs.target === 'openApi3' || refs.nullableStrategy === 'property') {\n      return {\n        type: primitiveMappings[def.innerType._def.typeName as keyof typeof primitiveMappings],\n        nullable: true,\n      } as any;\n    }\n\n    return {\n      type: [primitiveMappings[def.innerType._def.typeName as keyof typeof primitiveMappings], 'null'],\n    };\n  }\n\n  if (refs.target === 'openApi3') {\n    const base = parseDef(def.innerType._def, {\n      ...refs,\n      currentPath: [...refs.currentPath],\n    });\n\n    if (base && '$ref' in base) return { allOf: [base], nullable: true } as any;\n\n    return base && ({ ...base, nullable: true } as any);\n  }\n\n  const base = parseDef(def.innerType._def, {\n    ...refs,\n    currentPath: [...refs.currentPath, 'anyOf', '0'],\n  });\n\n  return base && { anyOf: [base, { type: 'null' }] };\n}\n", "import { ZodNumberDef } from 'zod/v3';\nimport { addErrorMessage, ErrorMessages, setResponseValueAndErrors } from '../errorMessages';\nimport { Refs } from '../Refs';\n\nexport type JsonSchema7NumberType = {\n  type: 'number' | 'integer';\n  minimum?: number;\n  exclusiveMinimum?: number;\n  maximum?: number;\n  exclusiveMaximum?: number;\n  multipleOf?: number;\n  errorMessage?: ErrorMessages<JsonSchema7NumberType>;\n};\n\nexport function parseNumberDef(def: ZodNumberDef, refs: Refs): JsonSchema7NumberType {\n  const res: JsonSchema7NumberType = {\n    type: 'number',\n  };\n\n  if (!def.checks) return res;\n\n  for (const check of def.checks) {\n    switch (check.kind) {\n      case 'int':\n        res.type = 'integer';\n        addErrorMessage(res, 'type', check.message, refs);\n        break;\n      case 'min':\n        if (refs.target === 'jsonSchema7') {\n          if (check.inclusive) {\n            setResponseValueAndErrors(res, 'minimum', check.value, check.message, refs);\n          } else {\n            setResponseValueAndErrors(res, 'exclusiveMinimum', check.value, check.message, refs);\n          }\n        } else {\n          if (!check.inclusive) {\n            res.exclusiveMinimum = true as any;\n          }\n          setResponseValueAndErrors(res, 'minimum', check.value, check.message, refs);\n        }\n        break;\n      case 'max':\n        if (refs.target === 'jsonSchema7') {\n          if (check.inclusive) {\n            setResponseValueAndErrors(res, 'maximum', check.value, check.message, refs);\n          } else {\n            setResponseValueAndErrors(res, 'exclusiveMaximum', check.value, check.message, refs);\n          }\n        } else {\n          if (!check.inclusive) {\n            res.exclusiveMaximum = true as any;\n          }\n          setResponseValueAndErrors(res, 'maximum', check.value, check.message, refs);\n        }\n        break;\n      case 'multipleOf':\n        setResponseValueAndErrors(res, 'multipleOf', check.value, check.message, refs);\n        break;\n    }\n  }\n  return res;\n}\n", "import { ZodObjectDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nfunction decideAdditionalProperties(def: ZodObjectDef, refs: Refs) {\n  if (refs.removeAdditionalStrategy === 'strict') {\n    return def.catchall._def.typeName === 'ZodNever' ?\n        def.unknownKeys !== 'strict'\n      : parseDef(def.catchall._def, {\n          ...refs,\n          currentPath: [...refs.currentPath, 'additionalProperties'],\n        }) ?? true;\n  } else {\n    return def.catchall._def.typeName === 'ZodNever' ?\n        def.unknownKeys === 'passthrough'\n      : parseDef(def.catchall._def, {\n          ...refs,\n          currentPath: [...refs.currentPath, 'additionalProperties'],\n        }) ?? true;\n  }\n}\n\nexport type JsonSchema7ObjectType = {\n  type: 'object';\n  properties: Record<string, JsonSchema7Type>;\n  additionalProperties: boolean | JsonSchema7Type;\n  required?: string[];\n};\n\nexport function parseObjectDef(def: ZodObjectDef, refs: Refs) {\n  const result: JsonSchema7ObjectType = {\n    type: 'object',\n    ...Object.entries(def.shape()).reduce(\n      (\n        acc: {\n          properties: Record<string, JsonSchema7Type>;\n          required: string[];\n        },\n        [propName, propDef],\n      ) => {\n        if (propDef === undefined || propDef._def === undefined) return acc;\n        const propertyPath = [...refs.currentPath, 'properties', propName];\n        const parsedDef = parseDef(propDef._def, {\n          ...refs,\n          currentPath: propertyPath,\n          propertyPath,\n        });\n        if (parsedDef === undefined) return acc;\n        if (\n          refs.openaiStrictMode &&\n          propDef.isOptional() &&\n          !propDef.isNullable() &&\n          typeof propDef._def?.defaultValue === 'undefined'\n        ) {\n          throw new Error(\n            `Zod field at \\`${propertyPath.join(\n              '/',\n            )}\\` uses \\`.optional()\\` without \\`.nullable()\\` which is not supported by the API. See: https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#all-fields-must-be-required`,\n          );\n        }\n        return {\n          properties: {\n            ...acc.properties,\n            [propName]: parsedDef,\n          },\n          required:\n            propDef.isOptional() && !refs.openaiStrictMode ? acc.required : [...acc.required, propName],\n        };\n      },\n      { properties: {}, required: [] },\n    ),\n    additionalProperties: decideAdditionalProperties(def, refs),\n  };\n  if (!result.required!.length) delete result.required;\n  return result;\n}\n", "import { ZodOptionalDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport const parseOptionalDef = (def: ZodOptionalDef, refs: Refs): JsonSchema7Type | undefined => {\n  if (\n    refs.propertyPath &&\n    refs.currentPath.slice(0, refs.propertyPath.length).toString() === refs.propertyPath.toString()\n  ) {\n    return parseDef(def.innerType._def, { ...refs, currentPath: refs.currentPath });\n  }\n\n  const innerSchema = parseDef(def.innerType._def, {\n    ...refs,\n    currentPath: [...refs.currentPath, 'anyOf', '1'],\n  });\n\n  return innerSchema ?\n      {\n        anyOf: [\n          {\n            not: {},\n          },\n          innerSchema,\n        ],\n      }\n    : {};\n};\n", "import { ZodPipelineDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\nimport { JsonSchema7AllOfType } from './intersection';\n\nexport const parsePipelineDef = (\n  def: ZodPipelineDef<any, any>,\n  refs: Refs,\n): JsonSchema7AllOfType | JsonSchema7Type | undefined => {\n  if (refs.pipeStrategy === 'input') {\n    return parseDef(def.in._def, refs);\n  } else if (refs.pipeStrategy === 'output') {\n    return parseDef(def.out._def, refs);\n  }\n\n  const a = parseDef(def.in._def, {\n    ...refs,\n    currentPath: [...refs.currentPath, 'allOf', '0'],\n  });\n  const b = parseDef(def.out._def, {\n    ...refs,\n    currentPath: [...refs.currentPath, 'allOf', a ? '1' : '0'],\n  });\n\n  return {\n    allOf: [a, b].filter((x): x is JsonSchema7Type => x !== undefined),\n  };\n};\n", "import { ZodPromiseDef } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport function parsePromiseDef(def: ZodPromiseDef, refs: Refs): JsonSchema7Type | undefined {\n  return parseDef(def.type._def, refs);\n}\n", "import { ZodSetDef } from 'zod/v3';\nimport { ErrorMessages, setResponseValueAndErrors } from '../errorMessages';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport type JsonSchema7SetType = {\n  type: 'array';\n  uniqueItems: true;\n  items?: JsonSchema7Type | undefined;\n  minItems?: number;\n  maxItems?: number;\n  errorMessage?: ErrorMessages<JsonSchema7SetType>;\n};\n\nexport function parseSetDef(def: ZodSetDef, refs: Refs): JsonSchema7SetType {\n  const items = parseDef(def.valueType._def, {\n    ...refs,\n    currentPath: [...refs.currentPath, 'items'],\n  });\n\n  const schema: JsonSchema7SetType = {\n    type: 'array',\n    uniqueItems: true,\n    items,\n  };\n\n  if (def.minSize) {\n    setResponseValueAndErrors(schema, 'minItems', def.minSize.value, def.minSize.message, refs);\n  }\n\n  if (def.maxSize) {\n    setResponseValueAndErrors(schema, 'maxItems', def.maxSize.value, def.maxSize.message, refs);\n  }\n\n  return schema;\n}\n", "import { ZodTupleDef, ZodTupleItems, ZodTypeAny } from 'zod/v3';\nimport { JsonSchema7Type, parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport type JsonSchema7TupleType = {\n  type: 'array';\n  minItems: number;\n  items: JsonSchema7Type[];\n} & (\n  | {\n      maxItems: number;\n    }\n  | {\n      additionalItems?: JsonSchema7Type | undefined;\n    }\n);\n\nexport function parseTupleDef(\n  def: ZodTupleDef<ZodTupleItems | [], ZodTypeAny | null>,\n  refs: Refs,\n): JsonSchema7TupleType {\n  if (def.rest) {\n    return {\n      type: 'array',\n      minItems: def.items.length,\n      items: def.items\n        .map((x, i) =>\n          parseDef(x._def, {\n            ...refs,\n            currentPath: [...refs.currentPath, 'items', `${i}`],\n          }),\n        )\n        .reduce((acc: JsonSchema7Type[], x) => (x === undefined ? acc : [...acc, x]), []),\n      additionalItems: parseDef(def.rest._def, {\n        ...refs,\n        currentPath: [...refs.currentPath, 'additionalItems'],\n      }),\n    };\n  } else {\n    return {\n      type: 'array',\n      minItems: def.items.length,\n      maxItems: def.items.length,\n      items: def.items\n        .map((x, i) =>\n          parseDef(x._def, {\n            ...refs,\n            currentPath: [...refs.currentPath, 'items', `${i}`],\n          }),\n        )\n        .reduce((acc: JsonSchema7Type[], x) => (x === undefined ? acc : [...acc, x]), []),\n    };\n  }\n}\n", "export type JsonSchema7UndefinedType = {\n  not: {};\n};\n\nexport function parseUndefinedDef(): JsonSchema7UndefinedType {\n  return {\n    not: {},\n  };\n}\n", "export type JsonSchema7UnknownType = {};\n\nexport function parseUnknownDef(): JsonSchema7UnknownType {\n  return {};\n}\n", "import { ZodReadonlyDef } from 'zod/v3';\nimport { parseDef } from '../parseDef';\nimport { Refs } from '../Refs';\n\nexport const parseReadonlyDef = (def: ZodReadonlyDef<any>, refs: Refs) => {\n  return parseDef(def.innerType._def, refs);\n};\n", "import { ZodFirstPartyTypeKind, ZodTypeDef } from 'zod/v3';\nimport { JsonSchema7AnyType, parseAnyDef } from './parsers/any';\nimport { JsonSchema7ArrayType, parseArrayDef } from './parsers/array';\nimport { JsonSchema7BigintType, parseBigintDef } from './parsers/bigint';\nimport { JsonSchema7BooleanType, parseBooleanDef } from './parsers/boolean';\nimport { parseBrandedDef } from './parsers/branded';\nimport { parseCatchDef } from './parsers/catch';\nimport { JsonSchema7DateType, parseDateDef } from './parsers/date';\nimport { parseDefaultDef } from './parsers/default';\nimport { parseEffectsDef } from './parsers/effects';\nimport { JsonSchema7EnumType, parseEnumDef } from './parsers/enum';\nimport { JsonSchema7AllOfType, parseIntersectionDef } from './parsers/intersection';\nimport { JsonSchema7LiteralType, parseLiteralDef } from './parsers/literal';\nimport { JsonSchema7MapType, parseMapDef } from './parsers/map';\nimport { JsonSchema7NativeEnumType, parseNativeEnumDef } from './parsers/nativeEnum';\nimport { JsonSchema7NeverType, parseNeverDef } from './parsers/never';\nimport { JsonSchema7NullType, parseNullDef } from './parsers/null';\nimport { JsonSchema7NullableType, parseNullableDef } from './parsers/nullable';\nimport { JsonSchema7NumberType, parseNumberDef } from './parsers/number';\nimport { JsonSchema7ObjectType, parseObjectDef } from './parsers/object';\nimport { parseOptionalDef } from './parsers/optional';\nimport { parsePipelineDef } from './parsers/pipeline';\nimport { parsePromiseDef } from './parsers/promise';\nimport { JsonSchema7RecordType, parseRecordDef } from './parsers/record';\nimport { JsonSchema7SetType, parseSetDef } from './parsers/set';\nimport { JsonSchema7StringType, parseStringDef } from './parsers/string';\nimport { JsonSchema7TupleType, parseTupleDef } from './parsers/tuple';\nimport { JsonSchema7UndefinedType, parseUndefinedDef } from './parsers/undefined';\nimport { JsonSchema7UnionType, parseUnionDef } from './parsers/union';\nimport { JsonSchema7UnknownType, parseUnknownDef } from './parsers/unknown';\nimport { Refs, Seen } from './Refs';\nimport { parseReadonlyDef } from './parsers/readonly';\nimport { ignoreOverride } from './Options';\n\ntype JsonSchema7RefType = { $ref: string };\ntype JsonSchema7Meta = {\n  title?: string;\n  default?: any;\n  description?: string;\n  markdownDescription?: string;\n};\n\nexport type JsonSchema7TypeUnion =\n  | JsonSchema7StringType\n  | JsonSchema7ArrayType\n  | JsonSchema7NumberType\n  | JsonSchema7BigintType\n  | JsonSchema7BooleanType\n  | JsonSchema7DateType\n  | JsonSchema7EnumType\n  | JsonSchema7LiteralType\n  | JsonSchema7NativeEnumType\n  | JsonSchema7NullType\n  | JsonSchema7NumberType\n  | JsonSchema7ObjectType\n  | JsonSchema7RecordType\n  | JsonSchema7TupleType\n  | JsonSchema7UnionType\n  | JsonSchema7UndefinedType\n  | JsonSchema7RefType\n  | JsonSchema7NeverType\n  | JsonSchema7MapType\n  | JsonSchema7AnyType\n  | JsonSchema7NullableType\n  | JsonSchema7AllOfType\n  | JsonSchema7UnknownType\n  | JsonSchema7SetType;\n\nexport type JsonSchema7Type = JsonSchema7TypeUnion & JsonSchema7Meta;\n\nexport function parseDef(\n  def: ZodTypeDef,\n  refs: Refs,\n  forceResolution = false, // Forces a new schema to be instantiated even though its def has been seen. Used for improving refs in definitions. See https://github.com/StefanTerdell/zod-to-json-schema/pull/61.\n): JsonSchema7Type | undefined {\n  const seenItem = refs.seen.get(def);\n\n  if (refs.override) {\n    const overrideResult = refs.override?.(def, refs, seenItem, forceResolution);\n\n    if (overrideResult !== ignoreOverride) {\n      return overrideResult;\n    }\n  }\n\n  if (seenItem && !forceResolution) {\n    const seenSchema = get$ref(seenItem, refs);\n\n    if (seenSchema !== undefined) {\n      if ('$ref' in seenSchema) {\n        refs.seenRefs.add(seenSchema.$ref);\n      }\n\n      return seenSchema;\n    }\n  }\n\n  const newItem: Seen = { def, path: refs.currentPath, jsonSchema: undefined };\n\n  refs.seen.set(def, newItem);\n\n  const jsonSchema = selectParser(def, (def as any).typeName, refs, forceResolution);\n\n  if (jsonSchema) {\n    addMeta(def, refs, jsonSchema);\n  }\n\n  newItem.jsonSchema = jsonSchema;\n\n  return jsonSchema;\n}\n\nconst get$ref = (\n  item: Seen,\n  refs: Refs,\n):\n  | {\n      $ref: string;\n    }\n  | {}\n  | undefined => {\n  switch (refs.$refStrategy) {\n    case 'root':\n      return { $ref: item.path.join('/') };\n    // this case is needed as OpenAI strict mode doesn't support top-level `$ref`s, i.e.\n    // the top-level schema *must* be `{\"type\": \"object\", \"properties\": {...}}` but if we ever\n    // need to define a `$ref`, relative `$ref`s aren't supported, so we need to extract\n    // the schema to `#/definitions/` and reference that.\n    //\n    // e.g. if we need to reference a schema at\n    // `[\"#\",\"definitions\",\"contactPerson\",\"properties\",\"person1\",\"properties\",\"name\"]`\n    // then we'll extract it out to `contactPerson_properties_person1_properties_name`\n    case 'extract-to-root':\n      const name = item.path.slice(refs.basePath.length + 1).join('_');\n\n      // we don't need to extract the root schema in this case, as it's already\n      // been added to the definitions\n      if (name !== refs.name && refs.nameStrategy === 'duplicate-ref') {\n        refs.definitions[name] = item.def;\n      }\n\n      return { $ref: [...refs.basePath, refs.definitionPath, name].join('/') };\n    case 'relative':\n      return { $ref: getRelativePath(refs.currentPath, item.path) };\n    case 'none':\n    case 'seen': {\n      if (\n        item.path.length < refs.currentPath.length &&\n        item.path.every((value, index) => refs.currentPath[index] === value)\n      ) {\n        console.warn(`Recursive reference detected at ${refs.currentPath.join('/')}! Defaulting to any`);\n\n        return {};\n      }\n\n      return refs.$refStrategy === 'seen' ? {} : undefined;\n    }\n  }\n};\n\nconst getRelativePath = (pathA: string[], pathB: string[]) => {\n  let i = 0;\n  for (; i < pathA.length && i < pathB.length; i++) {\n    if (pathA[i] !== pathB[i]) break;\n  }\n  return [(pathA.length - i).toString(), ...pathB.slice(i)].join('/');\n};\n\nconst selectParser = (\n  def: any,\n  typeName: ZodFirstPartyTypeKind,\n  refs: Refs,\n  forceResolution: boolean,\n): JsonSchema7Type | undefined => {\n  switch (typeName) {\n    case ZodFirstPartyTypeKind.ZodString:\n      return parseStringDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodNumber:\n      return parseNumberDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodObject:\n      return parseObjectDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodBigInt:\n      return parseBigintDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodBoolean:\n      return parseBooleanDef();\n    case ZodFirstPartyTypeKind.ZodDate:\n      return parseDateDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodUndefined:\n      return parseUndefinedDef();\n    case ZodFirstPartyTypeKind.ZodNull:\n      return parseNullDef(refs);\n    case ZodFirstPartyTypeKind.ZodArray:\n      return parseArrayDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodUnion:\n    case ZodFirstPartyTypeKind.ZodDiscriminatedUnion:\n      return parseUnionDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodIntersection:\n      return parseIntersectionDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodTuple:\n      return parseTupleDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodRecord:\n      return parseRecordDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodLiteral:\n      return parseLiteralDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodEnum:\n      return parseEnumDef(def);\n    case ZodFirstPartyTypeKind.ZodNativeEnum:\n      return parseNativeEnumDef(def);\n    case ZodFirstPartyTypeKind.ZodNullable:\n      return parseNullableDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodOptional:\n      return parseOptionalDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodMap:\n      return parseMapDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodSet:\n      return parseSetDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodLazy:\n      return parseDef(def.getter()._def, refs);\n    case ZodFirstPartyTypeKind.ZodPromise:\n      return parsePromiseDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodNaN:\n    case ZodFirstPartyTypeKind.ZodNever:\n      return parseNeverDef();\n    case ZodFirstPartyTypeKind.ZodEffects:\n      return parseEffectsDef(def, refs, forceResolution);\n    case ZodFirstPartyTypeKind.ZodAny:\n      return parseAnyDef();\n    case ZodFirstPartyTypeKind.ZodUnknown:\n      return parseUnknownDef();\n    case ZodFirstPartyTypeKind.ZodDefault:\n      return parseDefaultDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodBranded:\n      return parseBrandedDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodReadonly:\n      return parseReadonlyDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodCatch:\n      return parseCatchDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodPipeline:\n      return parsePipelineDef(def, refs);\n    case ZodFirstPartyTypeKind.ZodFunction:\n    case ZodFirstPartyTypeKind.ZodVoid:\n    case ZodFirstPartyTypeKind.ZodSymbol:\n      return undefined;\n    default:\n      return ((_: never) => undefined)(typeName);\n  }\n};\n\nconst addMeta = (def: ZodTypeDef, refs: Refs, jsonSchema: JsonSchema7Type): JsonSchema7Type => {\n  if (def.description) {\n    jsonSchema.description = def.description;\n\n    if (refs.markdownDescription) {\n      jsonSchema.markdownDescription = def.description;\n    }\n  }\n  return jsonSchema;\n};\n", "import { ZodSchema } from 'zod/v3';\nimport { Options, Targets } from './Options';\nimport { JsonSchema7Type, parseDef } from './parseDef';\nimport { getRefs } from './Refs';\nimport { zodDef, isEmptyObj } from './util';\n\nconst zodToJsonSchema = <Target extends Targets = 'jsonSchema7'>(\n  schema: ZodSchema<any>,\n  options?: Partial<Options<Target>> | string,\n): (Target extends 'jsonSchema7' ? JsonSchema7Type : object) & {\n  $schema?: string;\n  definitions?: {\n    [key: string]: Target extends 'jsonSchema7' ? JsonSchema7Type\n    : Target extends 'jsonSchema2019-09' ? JsonSchema7Type\n    : object;\n  };\n} => {\n  const refs = getRefs(options);\n\n  const name =\n    typeof options === 'string' ? options\n    : options?.nameStrategy === 'title' ? undefined\n    : options?.name;\n\n  const main =\n    parseDef(\n      schema._def,\n      name === undefined ? refs : (\n        {\n          ...refs,\n          currentPath: [...refs.basePath, refs.definitionPath, name],\n        }\n      ),\n      false,\n    ) ?? {};\n\n  const title =\n    typeof options === 'object' && options.name !== undefined && options.nameStrategy === 'title' ?\n      options.name\n    : undefined;\n\n  if (title !== undefined) {\n    main.title = title;\n  }\n\n  const definitions = (() => {\n    if (isEmptyObj(refs.definitions)) {\n      return undefined;\n    }\n\n    const definitions: Record<string, any> = {};\n    const processedDefinitions = new Set();\n\n    // the call to `parseDef()` here might itself add more entries to `.definitions`\n    // so we need to continually evaluate definitions until we've resolved all of them\n    //\n    // we have a generous iteration limit here to avoid blowing up the stack if there\n    // are any bugs that would otherwise result in us iterating indefinitely\n    for (let i = 0; i < 500; i++) {\n      const newDefinitions = Object.entries(refs.definitions).filter(\n        ([key]) => !processedDefinitions.has(key),\n      );\n      if (newDefinitions.length === 0) break;\n\n      for (const [key, schema] of newDefinitions) {\n        definitions[key] =\n          parseDef(\n            zodDef(schema),\n            { ...refs, currentPath: [...refs.basePath, refs.definitionPath, key] },\n            true,\n          ) ?? {};\n        processedDefinitions.add(key);\n      }\n    }\n\n    return definitions;\n  })();\n\n  const combined: ReturnType<typeof zodToJsonSchema<Target>> =\n    name === undefined ?\n      definitions ?\n        {\n          ...main,\n          [refs.definitionPath]: definitions,\n        }\n      : main\n    : refs.nameStrategy === 'duplicate-ref' ?\n      {\n        ...main,\n        ...(definitions || refs.seenRefs.size ?\n          {\n            [refs.definitionPath]: {\n              ...definitions,\n              // only actually duplicate the schema definition if it was ever referenced\n              // otherwise the duplication is completely pointless\n              ...(refs.seenRefs.size ? { [name]: main } : undefined),\n            },\n          }\n        : undefined),\n      }\n    : {\n        $ref: [...(refs.$refStrategy === 'relative' ? [] : refs.basePath), refs.definitionPath, name].join(\n          '/',\n        ),\n        [refs.definitionPath]: {\n          ...definitions,\n          [name]: main,\n        },\n      };\n\n  if (refs.target === 'jsonSchema7') {\n    combined.$schema = 'http://json-schema.org/draft-07/schema#';\n  } else if (refs.target === 'jsonSchema2019-09') {\n    combined.$schema = 'https://json-schema.org/draft/2019-09/schema#';\n  }\n\n  return combined;\n};\n\nexport { zodToJsonSchema };\n", "import type { JSONSchema, JSONSchemaDefinition } from './jsonschema';\n\nexport function toStrictJsonSchema(schema: JSONSchema): JSONSchema {\n  if (schema.type !== 'object') {\n    throw new Error(\n      `Root schema must have type: 'object' but got type: ${schema.type ? `'${schema.type}'` : 'undefined'}`,\n    );\n  }\n\n  const schemaCopy = structuredClone(schema);\n  return ensureStrictJsonSchema(schemaCopy, [], schemaCopy);\n}\n\nfunction isNullable(schema: JSONSchemaDefinition): boolean {\n  if (typeof schema === 'boolean') {\n    return false;\n  }\n  if (schema.type === 'null') {\n    return true;\n  }\n  for (const oneOfVariant of schema.oneOf ?? []) {\n    if (isNullable(oneOfVariant)) {\n      return true;\n    }\n  }\n  for (const allOfVariant of schema.anyOf ?? []) {\n    if (isNullable(allOfVariant)) {\n      return true;\n    }\n  }\n  return false;\n}\n\n/**\n * Mutates the given JSON schema to ensure it conforms to the `strict` standard\n * that the API expects.\n */\nfunction ensureStrictJsonSchema(\n  jsonSchema: JSONSchemaDefinition,\n  path: string[],\n  root: JSONSchema,\n): JSONSchema {\n  if (typeof jsonSchema === 'boolean') {\n    throw new TypeError(`Expected object schema but got boolean; path=${path.join('/')}`);\n  }\n\n  if (!isObject(jsonSchema)) {\n    throw new TypeError(`Expected ${JSON.stringify(jsonSchema)} to be an object; path=${path.join('/')}`);\n  }\n\n  // Handle $defs (non-standard but sometimes used)\n  const defs = (jsonSchema as any).$defs;\n  if (isObject(defs)) {\n    for (const [defName, defSchema] of Object.entries(defs)) {\n      ensureStrictJsonSchema(defSchema as JSONSchema, [...path, '$defs', defName], root);\n    }\n  }\n\n  // Handle definitions (draft-04 style, deprecated in draft-07 but still used)\n  const definitions = (jsonSchema as any).definitions;\n  if (isObject(definitions)) {\n    for (const [definitionName, definitionSchema] of Object.entries(definitions)) {\n      ensureStrictJsonSchema(definitionSchema as JSONSchema, [...path, 'definitions', definitionName], root);\n    }\n  }\n\n  // Add additionalProperties: false to object types\n  const typ = jsonSchema.type;\n  if (typ === 'object' && !('additionalProperties' in jsonSchema)) {\n    jsonSchema.additionalProperties = false;\n  }\n\n  const required = jsonSchema.required ?? [];\n\n  // Handle object properties\n  const properties = jsonSchema.properties;\n  if (isObject(properties)) {\n    for (const [key, value] of Object.entries(properties)) {\n      if (!isNullable(value) && !required.includes(key)) {\n        throw new Error(\n          `Zod field at \\`${[...path, 'properties', key].join(\n            '/',\n          )}\\` uses \\`.optional()\\` without \\`.nullable()\\` which is not supported by the API. See: https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#all-fields-must-be-required`,\n        );\n      }\n    }\n    jsonSchema.required = Object.keys(properties);\n    jsonSchema.properties = Object.fromEntries(\n      Object.entries(properties).map(([key, propSchema]) => [\n        key,\n        ensureStrictJsonSchema(propSchema, [...path, 'properties', key], root),\n      ]),\n    );\n  }\n\n  // Handle arrays\n  const items = jsonSchema.items;\n  if (isObject(items)) {\n    jsonSchema.items = ensureStrictJsonSchema(items, [...path, 'items'], root);\n  }\n\n  // Handle unions (anyOf)\n  const anyOf = jsonSchema.anyOf;\n  if (Array.isArray(anyOf)) {\n    jsonSchema.anyOf = anyOf.map((variant, i) =>\n      ensureStrictJsonSchema(variant, [...path, 'anyOf', String(i)], root),\n    );\n  }\n\n  // Handle intersections (allOf)\n  const allOf = jsonSchema.allOf;\n  if (Array.isArray(allOf)) {\n    if (allOf.length === 1) {\n      const resolved = ensureStrictJsonSchema(allOf[0]!, [...path, 'allOf', '0'], root);\n      Object.assign(jsonSchema, resolved);\n      delete jsonSchema.allOf;\n    } else {\n      jsonSchema.allOf = allOf.map((entry, i) =>\n        ensureStrictJsonSchema(entry, [...path, 'allOf', String(i)], root),\n      );\n    }\n  }\n\n  // Strip `null` defaults as there's no meaningful distinction\n  if (jsonSchema.default === null) {\n    delete jsonSchema.default;\n  }\n\n  // Handle $ref with additional properties\n  const ref = (jsonSchema as any).$ref;\n  if (ref && hasMoreThanNKeys(jsonSchema, 1)) {\n    if (typeof ref !== 'string') {\n      throw new TypeError(`Received non-string $ref - ${ref}; path=${path.join('/')}`);\n    }\n\n    const resolved = resolveRef(root, ref);\n    if (typeof resolved === 'boolean') {\n      throw new Error(`Expected \\`$ref: ${ref}\\` to resolve to an object schema but got boolean`);\n    }\n    if (!isObject(resolved)) {\n      throw new Error(\n        `Expected \\`$ref: ${ref}\\` to resolve to an object but got ${JSON.stringify(resolved)}`,\n      );\n    }\n\n    // Properties from the json schema take priority over the ones on the `$ref`\n    Object.assign(jsonSchema, { ...resolved, ...jsonSchema });\n    delete (jsonSchema as any).$ref;\n\n    // Since the schema expanded from `$ref` might not have `additionalProperties: false` applied,\n    // we call `ensureStrictJsonSchema` again to fix the inlined schema and ensure it's valid.\n    return ensureStrictJsonSchema(jsonSchema, path, root);\n  }\n\n  return jsonSchema;\n}\n\nfunction resolveRef(root: JSONSchema, ref: string): JSONSchemaDefinition {\n  if (!ref.startsWith('#/')) {\n    throw new Error(`Unexpected $ref format ${JSON.stringify(ref)}; Does not start with #/`);\n  }\n\n  const pathParts = ref.slice(2).split('/');\n  let resolved: any = root;\n\n  for (const key of pathParts) {\n    if (!isObject(resolved)) {\n      throw new Error(`encountered non-object entry while resolving ${ref} - ${JSON.stringify(resolved)}`);\n    }\n    const value = resolved[key];\n    if (value === undefined) {\n      throw new Error(`Key ${key} not found while resolving ${ref}`);\n    }\n    resolved = value;\n  }\n\n  return resolved;\n}\n\nfunction isObject<T>(obj: T | Array<any>): obj is Extract<T, Record<string, any>> {\n  return typeof obj === 'object' && obj !== null && !Array.isArray(obj);\n}\n\nfunction hasMoreThanNKeys(obj: Record<string, any>, n: number): boolean {\n  let i = 0;\n  for (const _ in obj) {\n    i++;\n    if (i > n) {\n      return true;\n    }\n  }\n  return false;\n}\n", "import { ResponseFormatJSONSchema } from '../resources/index';\nimport * as z3 from 'zod/v3';\nimport * as z4 from 'zod/v4';\nimport {\n  AutoParseableResponseFormat,\n  AutoParseableTextFormat,\n  AutoParseableTool,\n  makeParseableResponseFormat,\n  makeParseableTextFormat,\n  makeParseableTool,\n} from '../lib/parser';\nimport { zodToJsonSchema as _zodToJsonSchema } from '../_vendor/zod-to-json-schema';\nimport { AutoParseableResponseTool, makeParseableResponseTool } from '../lib/ResponsesParser';\nimport { type ResponseFormatTextJSONSchemaConfig } from '../resources/responses/responses';\nimport { toStrictJsonSchema } from '../lib/transform';\nimport { JSONSchema } from '../lib/jsonschema';\n\ntype InferZodType<T> =\n  T extends z4.ZodType ? z4.infer<T>\n  : T extends z3.ZodType ? z3.infer<T>\n  : never;\n\nfunction zodV3ToJsonSchema(schema: z3.ZodType, options: { name: string }): Record<string, unknown> {\n  return _zodToJsonSchema(schema, {\n    openaiStrictMode: true,\n    name: options.name,\n    nameStrategy: 'duplicate-ref',\n    $refStrategy: 'extract-to-root',\n    nullableStrategy: 'property',\n  });\n}\n\nfunction zodV4ToJsonSchema(schema: z4.ZodType): Record<string, unknown> {\n  return toStrictJsonSchema(\n    z4.toJSONSchema(schema, {\n      target: 'draft-7',\n    }) as JSONSchema,\n  ) as Record<string, unknown>;\n}\n\nfunction isZodV4(zodObject: z3.ZodType | z4.ZodType): zodObject is z4.ZodType {\n  return '_zod' in zodObject;\n}\n\n/**\n * Creates a chat completion `JSONSchema` response format object from\n * the given Zod schema.\n *\n * If this is passed to the `.parse()`, `.stream()` or `.runTools()`\n * chat completion methods then the response message will contain a\n * `.parsed` property that is the result of parsing the content with\n * the given Zod object.\n *\n * ```ts\n * const completion = await client.chat.completions.parse({\n *    model: 'gpt-4o-2024-08-06',\n *    messages: [\n *      { role: 'system', content: 'You are a helpful math tutor.' },\n *      { role: 'user', content: 'solve 8x + 31 = 2' },\n *    ],\n *    response_format: zodResponseFormat(\n *      z.object({\n *        steps: z.array(z.object({\n *          explanation: z.string(),\n *          answer: z.string(),\n *        })),\n *        final_answer: z.string(),\n *      }),\n *      'math_answer',\n *    ),\n *  });\n *  const message = completion.choices[0]?.message;\n *  if (message?.parsed) {\n *    console.log(message.parsed);\n *    console.log(message.parsed.final_answer);\n * }\n * ```\n *\n * This can be passed directly to the `.create()` method but will not\n * result in any automatic parsing, you'll have to parse the response yourself.\n */\nexport function zodResponseFormat<ZodInput extends z3.ZodType | z4.ZodType>(\n  zodObject: ZodInput,\n  name: string,\n  props?: Omit<ResponseFormatJSONSchema.JSONSchema, 'schema' | 'strict' | 'name'>,\n): AutoParseableResponseFormat<InferZodType<ZodInput>> {\n  return makeParseableResponseFormat(\n    {\n      type: 'json_schema',\n      json_schema: {\n        ...props,\n        name,\n        strict: true,\n        schema: isZodV4(zodObject) ? zodV4ToJsonSchema(zodObject) : zodV3ToJsonSchema(zodObject, { name }),\n      },\n    },\n    (content) => zodObject.parse(JSON.parse(content)),\n  );\n}\n\nexport function zodTextFormat<ZodInput extends z3.ZodType | z4.ZodType>(\n  zodObject: ZodInput,\n  name: string,\n  props?: Omit<ResponseFormatTextJSONSchemaConfig, 'schema' | 'type' | 'strict' | 'name'>,\n): AutoParseableTextFormat<InferZodType<ZodInput>> {\n  return makeParseableTextFormat(\n    {\n      type: 'json_schema',\n      ...props,\n      name,\n      strict: true,\n      schema: isZodV4(zodObject) ? zodV4ToJsonSchema(zodObject) : zodV3ToJsonSchema(zodObject, { name }),\n    },\n    (content) => zodObject.parse(JSON.parse(content)),\n  );\n}\n\n/**\n * Creates a chat completion `function` tool that can be invoked\n * automatically by the chat completion `.runTools()` method or automatically\n * parsed by `.parse()` / `.stream()`.\n */\nexport function zodFunction<Parameters extends z3.ZodType | z4.ZodType>(options: {\n  name: string;\n  parameters: Parameters;\n  function?: ((args: InferZodType<Parameters>) => unknown | Promise<unknown>) | undefined;\n  description?: string | undefined;\n}): AutoParseableTool<{\n  arguments: Parameters;\n  name: string;\n  function: (args: InferZodType<Parameters>) => unknown;\n}> {\n  // @ts-expect-error TODO\n  return makeParseableTool<any>(\n    {\n      type: 'function',\n      function: {\n        name: options.name,\n        parameters:\n          isZodV4(options.parameters) ?\n            zodV4ToJsonSchema(options.parameters)\n          : zodV3ToJsonSchema(options.parameters, { name: options.name }),\n        strict: true,\n        ...(options.description ? { description: options.description } : undefined),\n      },\n    },\n    {\n      callback: options.function,\n      parser: (args) => options.parameters.parse(JSON.parse(args)),\n    },\n  );\n}\n\nexport function zodResponsesFunction<Parameters extends z3.ZodType | z4.ZodType>(options: {\n  name: string;\n  parameters: Parameters;\n  function?: ((args: InferZodType<Parameters>) => unknown | Promise<unknown>) | undefined;\n  description?: string | undefined;\n}): AutoParseableResponseTool<{\n  arguments: Parameters;\n  name: string;\n  function: (args: InferZodType<Parameters>) => unknown;\n}> {\n  return makeParseableResponseTool<any>(\n    {\n      type: 'function',\n      name: options.name,\n      parameters:\n        isZodV4(options.parameters) ?\n          zodV4ToJsonSchema(options.parameters)\n        : zodV3ToJsonSchema(options.parameters, { name: options.name }),\n      strict: true,\n      ...(options.description ? { description: options.description } : undefined),\n    },\n    {\n      callback: options.function,\n      parser: (args) => options.parameters.parse(JSON.parse(args)),\n    },\n  );\n}\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport {\n  InteropZodType,\n  isZodSchemaV3,\n  isZodSchemaV4,\n} from \"@langchain/core/utils/types\";\nimport { parse as parseV4 } from \"zod/v4/core\";\nimport { ResponseFormatJSONSchema } from \"openai/resources\";\nimport { zodResponseFormat } from \"openai/helpers/zod\";\nimport { ContentBlock, UsageMetadata } from \"@langchain/core/messages\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\n\nconst SUPPORTED_METHODS = [\n  \"jsonSchema\",\n  \"functionCalling\",\n  \"jsonMode\",\n] as const;\ntype SupportedMethod = (typeof SUPPORTED_METHODS)[number];\n\n/**\n * Get the structured output method for a given model. By default, it uses\n * `jsonSchema` if the model supports it, otherwise it uses `functionCalling`.\n *\n * @throws if the method is invalid, e.g. is not a string or invalid method is provided.\n * @param model - The model name.\n * @param config - The structured output method options.\n * @returns The structured output method.\n */\nexport function getStructuredOutputMethod(\n  model: string,\n  method: unknown\n): SupportedMethod {\n  /**\n   * If a method is provided, validate it.\n   */\n  if (\n    typeof method !== \"undefined\" &&\n    !SUPPORTED_METHODS.includes(method as SupportedMethod)\n  ) {\n    throw new Error(\n      `Invalid method: ${method}. Supported methods are: ${SUPPORTED_METHODS.join(\n        \", \"\n      )}`\n    );\n  }\n\n  const hasSupportForJsonSchema =\n    !model.startsWith(\"gpt-3\") &&\n    !model.startsWith(\"gpt-4-\") &&\n    model !== \"gpt-4\";\n\n  /**\n   * If the model supports JSON Schema, use it by default.\n   */\n  if (hasSupportForJsonSchema && !method) {\n    return \"jsonSchema\";\n  }\n\n  if (!hasSupportForJsonSchema && method === \"jsonSchema\") {\n    throw new Error(\n      `JSON Schema is not supported for model \"${model}\". Please use a different method, e.g. \"functionCalling\" or \"jsonMode\".`\n    );\n  }\n\n  /**\n   * If the model does not support JSON Schema, use function calling by default.\n   */\n  return (method as SupportedMethod) ?? \"functionCalling\";\n}\n\n// inlined from openai/lib/parser.ts\nfunction makeParseableResponseFormat<ParsedT>(\n  response_format: ResponseFormatJSONSchema,\n  parser: (content: string) => ParsedT\n) {\n  const obj = { ...response_format };\n\n  Object.defineProperties(obj, {\n    $brand: {\n      value: \"auto-parseable-response-format\",\n      enumerable: false,\n    },\n    $parseRaw: {\n      value: parser,\n      enumerable: false,\n    },\n  });\n\n  return obj;\n}\n\nexport function interopZodResponseFormat(\n  zodSchema: InteropZodType,\n  name: string,\n  props: Omit<ResponseFormatJSONSchema.JSONSchema, \"schema\" | \"strict\" | \"name\">\n) {\n  if (isZodSchemaV3(zodSchema)) {\n    return zodResponseFormat(zodSchema, name, props);\n  }\n  if (isZodSchemaV4(zodSchema)) {\n    return makeParseableResponseFormat(\n      {\n        type: \"json_schema\",\n        json_schema: {\n          ...props,\n          name,\n          strict: true,\n          schema: toJsonSchema(zodSchema, {\n            cycles: \"ref\", // equivalent to nameStrategy: 'duplicate-ref'\n            reused: \"ref\", // equivalent to $refStrategy: 'extract-to-root'\n            override(ctx) {\n              ctx.jsonSchema.title = name; // equivalent to `name` property\n              // TODO: implement `nullableStrategy` patch-fix (zod doesn't support openApi3 json schema target)\n              // TODO: implement `openaiStrictMode` patch-fix (where optional properties without `nullable` are not supported)\n            },\n            /// property equivalents from native `zodResponseFormat` fn\n            // openaiStrictMode: true,\n            // name,\n            // nameStrategy: 'duplicate-ref',\n            // $refStrategy: 'extract-to-root',\n            // nullableStrategy: 'property',\n          }),\n        },\n      },\n      (content) => parseV4(zodSchema, JSON.parse(content))\n    );\n  }\n  throw new Error(\"Unsupported schema response format\");\n}\n\n/**\n * Handle multi modal response content.\n *\n * @param content The content of the message.\n * @param messages The messages of the response.\n * @returns The new content of the message.\n */\nexport function handleMultiModalOutput(\n  content: string,\n  messages: unknown\n): ContentBlock[] | string {\n  /**\n   * Handle OpenRouter image responses\n   * @see https://openrouter.ai/docs/features/multimodal/image-generation#api-usage\n   */\n  if (\n    messages &&\n    typeof messages === \"object\" &&\n    \"images\" in messages &&\n    Array.isArray(messages.images)\n  ) {\n    const images = messages.images\n      .filter((image) => typeof image?.image_url?.url === \"string\")\n      .map(\n        (image) =>\n          ({\n            type: \"image\",\n            url: image.image_url.url as string,\n          }) as const\n      );\n    return [{ type: \"text\", text: content }, ...images];\n  }\n\n  return content;\n}\n\n// TODO: make this a converter\nexport function _convertOpenAIResponsesUsageToLangChainUsage(\n  usage?: OpenAIClient.Responses.ResponseUsage\n): UsageMetadata {\n  const inputTokenDetails = {\n    ...(usage?.input_tokens_details?.cached_tokens != null && {\n      cache_read: usage?.input_tokens_details?.cached_tokens,\n    }),\n  };\n  const outputTokenDetails = {\n    ...(usage?.output_tokens_details?.reasoning_tokens != null && {\n      reasoning: usage?.output_tokens_details?.reasoning_tokens,\n    }),\n  };\n  return {\n    input_tokens: usage?.input_tokens ?? 0,\n    output_tokens: usage?.output_tokens ?? 0,\n    total_tokens: usage?.total_tokens ?? 0,\n    input_token_details: inputTokenDetails,\n    output_token_details: outputTokenDetails,\n  };\n}\n", "/**\n * This file was automatically generated by an automated script. Do not edit manually.\n */\nimport type { ModelProfile } from \"@langchain/core/language_models/profile\";\nconst PROFILES: Record<string, ModelProfile> = {\n  \"gpt-4o-2024-11-20\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.3-codex\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-codex\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-pro\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 272000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o-mini\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"text-embedding-ada-002\": {\n    maxInputTokens: 8192,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 1536,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-chat-latest\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"codex-mini-latest\": {\n    maxInputTokens: 200000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.1-codex-max\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o-2024-05-13\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 4096,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.2-chat-latest\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.2-codex\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o3-deep-research\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  o1: {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.1\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o4-mini-deep-research\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.3-codex-spark\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  o3: {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"text-embedding-3-small\": {\n    maxInputTokens: 8191,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 1536,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4.1-nano\": {\n    maxInputTokens: 1047576,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"text-embedding-3-large\": {\n    maxInputTokens: 8191,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 3072,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-3.5-turbo\": {\n    maxInputTokens: 16385,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: false,\n    videoInputs: false,\n    maxOutputTokens: 4096,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: false,\n    imageUrlInputs: false,\n    pdfToolMessage: false,\n    imageToolMessage: false,\n    toolChoice: true,\n  },\n  \"gpt-5.1-codex-mini\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.2\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4.1\": {\n    maxInputTokens: 1047576,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o3-pro\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4-turbo\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 4096,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o4-mini\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4.1-mini\": {\n    maxInputTokens: 1047576,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o1-preview\": {\n    maxInputTokens: 128000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o1-pro\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.1-codex\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.2-pro\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o3-mini\": {\n    maxInputTokens: 200000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o-2024-08-06\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-mini\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5.1-chat-latest\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4\": {\n    maxInputTokens: 8192,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 8192,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-nano\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o1-mini\": {\n    maxInputTokens: 128000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 65536,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n};\nexport default PROFILES;\n", "// @ts-nocheck\n\n// Inlined to deal with portability issues with importing crypto module\n\n/**\n * [js-sha256]{@link https://github.com/emn178/js-sha256}\n *\n * @version 0.11.1\n * @author Chen, Yi-Cyuan [emn178@gmail.com]\n * @copyright Chen, Yi-Cyuan 2014-2025\n * @license MIT\n */\n/*jslint bitwise: true */\n\"use strict\";\n\nvar HEX_CHARS = \"0123456789abcdef\".split(\"\");\nvar EXTRA = [-2147483648, 8388608, 32768, 128];\nvar SHIFT = [24, 16, 8, 0];\nvar K = [\n  0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b, 0x59f111f1,\n  0x923f82a4, 0xab1c5ed5, 0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n  0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174, 0xe49b69c1, 0xefbe4786,\n  0x0fc19dc6, 0x240ca1cc, 0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n  0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7, 0xc6e00bf3, 0xd5a79147,\n  0x06ca6351, 0x14292967, 0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n  0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85, 0xa2bfe8a1, 0xa81a664b,\n  0xc24b8b70, 0xc76c51a3, 0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n  0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5, 0x391c0cb3, 0x4ed8aa4a,\n  0x5b9cca4f, 0x682e6ff3, 0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n  0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,\n];\nvar OUTPUT_TYPES = [\"hex\", \"array\", \"digest\", \"arrayBuffer\"];\n\nvar blocks = [];\n\nfunction Sha256(is224, sharedMemory) {\n  if (sharedMemory) {\n    blocks[0] =\n      blocks[16] =\n      blocks[1] =\n      blocks[2] =\n      blocks[3] =\n      blocks[4] =\n      blocks[5] =\n      blocks[6] =\n      blocks[7] =\n      blocks[8] =\n      blocks[9] =\n      blocks[10] =\n      blocks[11] =\n      blocks[12] =\n      blocks[13] =\n      blocks[14] =\n      blocks[15] =\n        0;\n    this.blocks = blocks;\n  } else {\n    this.blocks = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];\n  }\n\n  if (is224) {\n    this.h0 = 0xc1059ed8;\n    this.h1 = 0x367cd507;\n    this.h2 = 0x3070dd17;\n    this.h3 = 0xf70e5939;\n    this.h4 = 0xffc00b31;\n    this.h5 = 0x68581511;\n    this.h6 = 0x64f98fa7;\n    this.h7 = 0xbefa4fa4;\n  } else {\n    // 256\n    this.h0 = 0x6a09e667;\n    this.h1 = 0xbb67ae85;\n    this.h2 = 0x3c6ef372;\n    this.h3 = 0xa54ff53a;\n    this.h4 = 0x510e527f;\n    this.h5 = 0x9b05688c;\n    this.h6 = 0x1f83d9ab;\n    this.h7 = 0x5be0cd19;\n  }\n\n  this.block = this.start = this.bytes = this.hBytes = 0;\n  this.finalized = this.hashed = false;\n  this.first = true;\n  this.is224 = is224;\n}\n\nSha256.prototype.update = function (message) {\n  if (this.finalized) {\n    return;\n  }\n  var notString,\n    type = typeof message;\n  if (type !== \"string\") {\n    if (type === \"object\") {\n      if (message === null) {\n        throw new Error(ERROR);\n      } else if (ARRAY_BUFFER && message.constructor === ArrayBuffer) {\n        message = new Uint8Array(message);\n      } else if (!Array.isArray(message)) {\n        if (!ARRAY_BUFFER || !ArrayBuffer.isView(message)) {\n          throw new Error(ERROR);\n        }\n      }\n    } else {\n      throw new Error(ERROR);\n    }\n    notString = true;\n  }\n  var code,\n    index = 0,\n    i,\n    length = message.length,\n    blocks = this.blocks;\n  while (index < length) {\n    if (this.hashed) {\n      this.hashed = false;\n      blocks[0] = this.block;\n      this.block =\n        blocks[16] =\n        blocks[1] =\n        blocks[2] =\n        blocks[3] =\n        blocks[4] =\n        blocks[5] =\n        blocks[6] =\n        blocks[7] =\n        blocks[8] =\n        blocks[9] =\n        blocks[10] =\n        blocks[11] =\n        blocks[12] =\n        blocks[13] =\n        blocks[14] =\n        blocks[15] =\n          0;\n    }\n\n    if (notString) {\n      for (i = this.start; index < length && i < 64; ++index) {\n        blocks[i >>> 2] |= message[index] << SHIFT[i++ & 3];\n      }\n    } else {\n      for (i = this.start; index < length && i < 64; ++index) {\n        code = message.charCodeAt(index);\n        if (code < 0x80) {\n          blocks[i >>> 2] |= code << SHIFT[i++ & 3];\n        } else if (code < 0x800) {\n          blocks[i >>> 2] |= (0xc0 | (code >>> 6)) << SHIFT[i++ & 3];\n          blocks[i >>> 2] |= (0x80 | (code & 0x3f)) << SHIFT[i++ & 3];\n        } else if (code < 0xd800 || code >= 0xe000) {\n          blocks[i >>> 2] |= (0xe0 | (code >>> 12)) << SHIFT[i++ & 3];\n          blocks[i >>> 2] |= (0x80 | ((code >>> 6) & 0x3f)) << SHIFT[i++ & 3];\n          blocks[i >>> 2] |= (0x80 | (code & 0x3f)) << SHIFT[i++ & 3];\n        } else {\n          code =\n            0x10000 +\n            (((code & 0x3ff) << 10) | (message.charCodeAt(++index) & 0x3ff));\n          blocks[i >>> 2] |= (0xf0 | (code >>> 18)) << SHIFT[i++ & 3];\n          blocks[i >>> 2] |= (0x80 | ((code >>> 12) & 0x3f)) << SHIFT[i++ & 3];\n          blocks[i >>> 2] |= (0x80 | ((code >>> 6) & 0x3f)) << SHIFT[i++ & 3];\n          blocks[i >>> 2] |= (0x80 | (code & 0x3f)) << SHIFT[i++ & 3];\n        }\n      }\n    }\n\n    this.lastByteIndex = i;\n    this.bytes += i - this.start;\n    if (i >= 64) {\n      this.block = blocks[16];\n      this.start = i - 64;\n      this.hash();\n      this.hashed = true;\n    } else {\n      this.start = i;\n    }\n  }\n  if (this.bytes > 4294967295) {\n    this.hBytes += (this.bytes / 4294967296) << 0;\n    this.bytes = this.bytes % 4294967296;\n  }\n  return this;\n};\n\nSha256.prototype.finalize = function () {\n  if (this.finalized) {\n    return;\n  }\n  this.finalized = true;\n  var blocks = this.blocks,\n    i = this.lastByteIndex;\n  blocks[16] = this.block;\n  blocks[i >>> 2] |= EXTRA[i & 3];\n  this.block = blocks[16];\n  if (i >= 56) {\n    if (!this.hashed) {\n      this.hash();\n    }\n    blocks[0] = this.block;\n    blocks[16] =\n      blocks[1] =\n      blocks[2] =\n      blocks[3] =\n      blocks[4] =\n      blocks[5] =\n      blocks[6] =\n      blocks[7] =\n      blocks[8] =\n      blocks[9] =\n      blocks[10] =\n      blocks[11] =\n      blocks[12] =\n      blocks[13] =\n      blocks[14] =\n      blocks[15] =\n        0;\n  }\n  blocks[14] = (this.hBytes << 3) | (this.bytes >>> 29);\n  blocks[15] = this.bytes << 3;\n  this.hash();\n};\n\nSha256.prototype.hash = function () {\n  var a = this.h0,\n    b = this.h1,\n    c = this.h2,\n    d = this.h3,\n    e = this.h4,\n    f = this.h5,\n    g = this.h6,\n    h = this.h7,\n    blocks = this.blocks,\n    j,\n    s0,\n    s1,\n    maj,\n    t1,\n    t2,\n    ch,\n    ab,\n    da,\n    cd,\n    bc;\n\n  for (j = 16; j < 64; ++j) {\n    // rightrotate\n    t1 = blocks[j - 15];\n    s0 = ((t1 >>> 7) | (t1 << 25)) ^ ((t1 >>> 18) | (t1 << 14)) ^ (t1 >>> 3);\n    t1 = blocks[j - 2];\n    s1 = ((t1 >>> 17) | (t1 << 15)) ^ ((t1 >>> 19) | (t1 << 13)) ^ (t1 >>> 10);\n    blocks[j] = (blocks[j - 16] + s0 + blocks[j - 7] + s1) << 0;\n  }\n\n  bc = b & c;\n  for (j = 0; j < 64; j += 4) {\n    if (this.first) {\n      if (this.is224) {\n        ab = 300032;\n        t1 = blocks[0] - 1413257819;\n        h = (t1 - 150054599) << 0;\n        d = (t1 + 24177077) << 0;\n      } else {\n        ab = 704751109;\n        t1 = blocks[0] - 210244248;\n        h = (t1 - 1521486534) << 0;\n        d = (t1 + 143694565) << 0;\n      }\n      this.first = false;\n    } else {\n      s0 =\n        ((a >>> 2) | (a << 30)) ^\n        ((a >>> 13) | (a << 19)) ^\n        ((a >>> 22) | (a << 10));\n      s1 =\n        ((e >>> 6) | (e << 26)) ^\n        ((e >>> 11) | (e << 21)) ^\n        ((e >>> 25) | (e << 7));\n      ab = a & b;\n      maj = ab ^ (a & c) ^ bc;\n      ch = (e & f) ^ (~e & g);\n      t1 = h + s1 + ch + K[j] + blocks[j];\n      t2 = s0 + maj;\n      h = (d + t1) << 0;\n      d = (t1 + t2) << 0;\n    }\n    s0 =\n      ((d >>> 2) | (d << 30)) ^\n      ((d >>> 13) | (d << 19)) ^\n      ((d >>> 22) | (d << 10));\n    s1 =\n      ((h >>> 6) | (h << 26)) ^\n      ((h >>> 11) | (h << 21)) ^\n      ((h >>> 25) | (h << 7));\n    da = d & a;\n    maj = da ^ (d & b) ^ ab;\n    ch = (g & h) ^ (~g & e);\n    t1 = f + s1 + ch + K[j + 1] + blocks[j + 1];\n    t2 = s0 + maj;\n    g = (c + t1) << 0;\n    c = (t1 + t2) << 0;\n    s0 =\n      ((c >>> 2) | (c << 30)) ^\n      ((c >>> 13) | (c << 19)) ^\n      ((c >>> 22) | (c << 10));\n    s1 =\n      ((g >>> 6) | (g << 26)) ^\n      ((g >>> 11) | (g << 21)) ^\n      ((g >>> 25) | (g << 7));\n    cd = c & d;\n    maj = cd ^ (c & a) ^ da;\n    ch = (f & g) ^ (~f & h);\n    t1 = e + s1 + ch + K[j + 2] + blocks[j + 2];\n    t2 = s0 + maj;\n    f = (b + t1) << 0;\n    b = (t1 + t2) << 0;\n    s0 =\n      ((b >>> 2) | (b << 30)) ^\n      ((b >>> 13) | (b << 19)) ^\n      ((b >>> 22) | (b << 10));\n    s1 =\n      ((f >>> 6) | (f << 26)) ^\n      ((f >>> 11) | (f << 21)) ^\n      ((f >>> 25) | (f << 7));\n    bc = b & c;\n    maj = bc ^ (b & d) ^ cd;\n    ch = (f & g) ^ (~f & h);\n    t1 = e + s1 + ch + K[j + 3] + blocks[j + 3];\n    t2 = s0 + maj;\n    e = (a + t1) << 0;\n    a = (t1 + t2) << 0;\n    this.chromeBugWorkAround = true;\n  }\n\n  this.h0 = (this.h0 + a) << 0;\n  this.h1 = (this.h1 + b) << 0;\n  this.h2 = (this.h2 + c) << 0;\n  this.h3 = (this.h3 + d) << 0;\n  this.h4 = (this.h4 + e) << 0;\n  this.h5 = (this.h5 + f) << 0;\n  this.h6 = (this.h6 + g) << 0;\n  this.h7 = (this.h7 + h) << 0;\n};\n\nSha256.prototype.hex = function () {\n  this.finalize();\n\n  var h0 = this.h0,\n    h1 = this.h1,\n    h2 = this.h2,\n    h3 = this.h3,\n    h4 = this.h4,\n    h5 = this.h5,\n    h6 = this.h6,\n    h7 = this.h7;\n\n  var hex =\n    HEX_CHARS[(h0 >>> 28) & 0x0f] +\n    HEX_CHARS[(h0 >>> 24) & 0x0f] +\n    HEX_CHARS[(h0 >>> 20) & 0x0f] +\n    HEX_CHARS[(h0 >>> 16) & 0x0f] +\n    HEX_CHARS[(h0 >>> 12) & 0x0f] +\n    HEX_CHARS[(h0 >>> 8) & 0x0f] +\n    HEX_CHARS[(h0 >>> 4) & 0x0f] +\n    HEX_CHARS[h0 & 0x0f] +\n    HEX_CHARS[(h1 >>> 28) & 0x0f] +\n    HEX_CHARS[(h1 >>> 24) & 0x0f] +\n    HEX_CHARS[(h1 >>> 20) & 0x0f] +\n    HEX_CHARS[(h1 >>> 16) & 0x0f] +\n    HEX_CHARS[(h1 >>> 12) & 0x0f] +\n    HEX_CHARS[(h1 >>> 8) & 0x0f] +\n    HEX_CHARS[(h1 >>> 4) & 0x0f] +\n    HEX_CHARS[h1 & 0x0f] +\n    HEX_CHARS[(h2 >>> 28) & 0x0f] +\n    HEX_CHARS[(h2 >>> 24) & 0x0f] +\n    HEX_CHARS[(h2 >>> 20) & 0x0f] +\n    HEX_CHARS[(h2 >>> 16) & 0x0f] +\n    HEX_CHARS[(h2 >>> 12) & 0x0f] +\n    HEX_CHARS[(h2 >>> 8) & 0x0f] +\n    HEX_CHARS[(h2 >>> 4) & 0x0f] +\n    HEX_CHARS[h2 & 0x0f] +\n    HEX_CHARS[(h3 >>> 28) & 0x0f] +\n    HEX_CHARS[(h3 >>> 24) & 0x0f] +\n    HEX_CHARS[(h3 >>> 20) & 0x0f] +\n    HEX_CHARS[(h3 >>> 16) & 0x0f] +\n    HEX_CHARS[(h3 >>> 12) & 0x0f] +\n    HEX_CHARS[(h3 >>> 8) & 0x0f] +\n    HEX_CHARS[(h3 >>> 4) & 0x0f] +\n    HEX_CHARS[h3 & 0x0f] +\n    HEX_CHARS[(h4 >>> 28) & 0x0f] +\n    HEX_CHARS[(h4 >>> 24) & 0x0f] +\n    HEX_CHARS[(h4 >>> 20) & 0x0f] +\n    HEX_CHARS[(h4 >>> 16) & 0x0f] +\n    HEX_CHARS[(h4 >>> 12) & 0x0f] +\n    HEX_CHARS[(h4 >>> 8) & 0x0f] +\n    HEX_CHARS[(h4 >>> 4) & 0x0f] +\n    HEX_CHARS[h4 & 0x0f] +\n    HEX_CHARS[(h5 >>> 28) & 0x0f] +\n    HEX_CHARS[(h5 >>> 24) & 0x0f] +\n    HEX_CHARS[(h5 >>> 20) & 0x0f] +\n    HEX_CHARS[(h5 >>> 16) & 0x0f] +\n    HEX_CHARS[(h5 >>> 12) & 0x0f] +\n    HEX_CHARS[(h5 >>> 8) & 0x0f] +\n    HEX_CHARS[(h5 >>> 4) & 0x0f] +\n    HEX_CHARS[h5 & 0x0f] +\n    HEX_CHARS[(h6 >>> 28) & 0x0f] +\n    HEX_CHARS[(h6 >>> 24) & 0x0f] +\n    HEX_CHARS[(h6 >>> 20) & 0x0f] +\n    HEX_CHARS[(h6 >>> 16) & 0x0f] +\n    HEX_CHARS[(h6 >>> 12) & 0x0f] +\n    HEX_CHARS[(h6 >>> 8) & 0x0f] +\n    HEX_CHARS[(h6 >>> 4) & 0x0f] +\n    HEX_CHARS[h6 & 0x0f];\n  if (!this.is224) {\n    hex +=\n      HEX_CHARS[(h7 >>> 28) & 0x0f] +\n      HEX_CHARS[(h7 >>> 24) & 0x0f] +\n      HEX_CHARS[(h7 >>> 20) & 0x0f] +\n      HEX_CHARS[(h7 >>> 16) & 0x0f] +\n      HEX_CHARS[(h7 >>> 12) & 0x0f] +\n      HEX_CHARS[(h7 >>> 8) & 0x0f] +\n      HEX_CHARS[(h7 >>> 4) & 0x0f] +\n      HEX_CHARS[h7 & 0x0f];\n  }\n  return hex;\n};\n\nSha256.prototype.toString = Sha256.prototype.hex;\n\nSha256.prototype.digest = function () {\n  this.finalize();\n\n  var h0 = this.h0,\n    h1 = this.h1,\n    h2 = this.h2,\n    h3 = this.h3,\n    h4 = this.h4,\n    h5 = this.h5,\n    h6 = this.h6,\n    h7 = this.h7;\n\n  var arr = [\n    (h0 >>> 24) & 0xff,\n    (h0 >>> 16) & 0xff,\n    (h0 >>> 8) & 0xff,\n    h0 & 0xff,\n    (h1 >>> 24) & 0xff,\n    (h1 >>> 16) & 0xff,\n    (h1 >>> 8) & 0xff,\n    h1 & 0xff,\n    (h2 >>> 24) & 0xff,\n    (h2 >>> 16) & 0xff,\n    (h2 >>> 8) & 0xff,\n    h2 & 0xff,\n    (h3 >>> 24) & 0xff,\n    (h3 >>> 16) & 0xff,\n    (h3 >>> 8) & 0xff,\n    h3 & 0xff,\n    (h4 >>> 24) & 0xff,\n    (h4 >>> 16) & 0xff,\n    (h4 >>> 8) & 0xff,\n    h4 & 0xff,\n    (h5 >>> 24) & 0xff,\n    (h5 >>> 16) & 0xff,\n    (h5 >>> 8) & 0xff,\n    h5 & 0xff,\n    (h6 >>> 24) & 0xff,\n    (h6 >>> 16) & 0xff,\n    (h6 >>> 8) & 0xff,\n    h6 & 0xff,\n  ];\n  if (!this.is224) {\n    arr.push(\n      (h7 >>> 24) & 0xff,\n      (h7 >>> 16) & 0xff,\n      (h7 >>> 8) & 0xff,\n      h7 & 0xff\n    );\n  }\n  return arr;\n};\n\nSha256.prototype.array = Sha256.prototype.digest;\n\nSha256.prototype.arrayBuffer = function () {\n  this.finalize();\n\n  var buffer = new ArrayBuffer(this.is224 ? 28 : 32);\n  var dataView = new DataView(buffer);\n  dataView.setUint32(0, this.h0);\n  dataView.setUint32(4, this.h1);\n  dataView.setUint32(8, this.h2);\n  dataView.setUint32(12, this.h3);\n  dataView.setUint32(16, this.h4);\n  dataView.setUint32(20, this.h5);\n  dataView.setUint32(24, this.h6);\n  if (!this.is224) {\n    dataView.setUint32(28, this.h7);\n  }\n  return buffer;\n};\n\nexport const sha256 = (...strings: string[]) => {\n  return new Sha256(false, true).update(strings.join(\"\")).hex();\n};\n", "import { __exportAll } from \"../_virtual/_rolldown/runtime.js\";\nimport { sha256 } from \"./js-sha256/hash.js\";\n\n//#region src/utils/hash.ts\nvar hash_exports = /* @__PURE__ */ __exportAll({ sha256: () => sha256 });\n\n//#endregion\nexport { hash_exports, sha256 };\n//# sourceMappingURL=hash.js.map", "import { sha256, type HashKeyEncoder } from \"../utils/hash.js\";\nimport type { Generation, ChatGeneration } from \"../outputs.js\";\nimport { mapStoredMessageToChatMessage } from \"../messages/utils.js\";\nimport { type StoredGeneration } from \"../messages/base.js\";\n\nexport const defaultHashKeyEncoder: HashKeyEncoder = (...strings) =>\n  sha256(strings.join(\"_\"));\n\nexport function deserializeStoredGeneration(\n  storedGeneration: StoredGeneration\n) {\n  if (storedGeneration.message !== undefined) {\n    return {\n      text: storedGeneration.text,\n      message: mapStoredMessageToChatMessage(storedGeneration.message),\n    };\n  } else {\n    return { text: storedGeneration.text };\n  }\n}\n\nexport function serializeGeneration(generation: Generation) {\n  const serializedValue: StoredGeneration = {\n    text: generation.text,\n  };\n  if ((generation as ChatGeneration).message !== undefined) {\n    serializedValue.message = (generation as ChatGeneration).message.toDict();\n  }\n  return serializedValue;\n}\n\n/**\n * Base class for all caches. All caches should extend this class.\n */\nexport abstract class BaseCache<T = Generation[]> {\n  protected keyEncoder: HashKeyEncoder = defaultHashKeyEncoder;\n\n  /**\n   * Sets a custom key encoder function for the cache.\n   * This function should take a prompt and an LLM key and return a string\n   * that will be used as the cache key.\n   * @param keyEncoderFn The custom key encoder function.\n   */\n  makeDefaultKeyEncoder(keyEncoderFn: HashKeyEncoder): void {\n    this.keyEncoder = keyEncoderFn;\n  }\n\n  abstract lookup(prompt: string, llmKey: string): Promise<T | null>;\n\n  abstract update(prompt: string, llmKey: string, value: T): Promise<void>;\n}\n\nconst GLOBAL_MAP = new Map();\n\n/**\n * A cache for storing LLM generations that stores data in memory.\n */\nexport class InMemoryCache<T = Generation[]> extends BaseCache<T> {\n  private cache: Map<string, T>;\n\n  constructor(map?: Map<string, T>) {\n    super();\n    this.cache = map ?? new Map();\n  }\n\n  /**\n   * Retrieves data from the cache using a prompt and an LLM key. If the\n   * data is not found, it returns null.\n   * @param prompt The prompt used to find the data.\n   * @param llmKey The LLM key used to find the data.\n   * @returns The data corresponding to the prompt and LLM key, or null if not found.\n   */\n  lookup(prompt: string, llmKey: string): Promise<T | null> {\n    return Promise.resolve(\n      this.cache.get(this.keyEncoder(prompt, llmKey)) ?? null\n    );\n  }\n\n  /**\n   * Updates the cache with new data using a prompt and an LLM key.\n   * @param prompt The prompt used to store the data.\n   * @param llmKey The LLM key used to store the data.\n   * @param value The data to be stored.\n   */\n  async update(prompt: string, llmKey: string, value: T): Promise<void> {\n    this.cache.set(this.keyEncoder(prompt, llmKey), value);\n  }\n\n  /**\n   * Returns a global instance of InMemoryCache using a predefined global\n   * map as the initial cache.\n   * @returns A global instance of InMemoryCache.\n   */\n  static global(): InMemoryCache {\n    return new InMemoryCache(GLOBAL_MAP);\n  }\n}\n", "import { Serializable } from \"./load/serializable.js\";\nimport { type BaseMessage } from \"./messages/base.js\";\nimport { HumanMessage } from \"./messages/human.js\";\nimport { getBufferString } from \"./messages/utils.js\";\n\nexport interface BasePromptValueInterface extends Serializable {\n  toString(): string;\n\n  toChatMessages(): BaseMessage[];\n}\n\nexport interface StringPromptValueInterface extends BasePromptValueInterface {\n  value: string;\n}\n\nexport interface ChatPromptValueInterface extends BasePromptValueInterface {\n  messages: BaseMessage[];\n}\n\n/**\n * Base PromptValue class. All prompt values should extend this class.\n */\nexport abstract class BasePromptValue\n  extends Serializable\n  implements BasePromptValueInterface\n{\n  abstract toString(): string;\n\n  abstract toChatMessages(): BaseMessage[];\n}\n\n/**\n * Represents a prompt value as a string. It extends the BasePromptValue\n * class and overrides the toString and toChatMessages methods.\n */\nexport class StringPromptValue\n  extends BasePromptValue\n  implements StringPromptValueInterface\n{\n  static lc_name(): string {\n    return \"StringPromptValue\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"prompt_values\"];\n\n  lc_serializable = true;\n\n  value: string;\n\n  constructor(value: string) {\n    super({ value });\n    this.value = value;\n  }\n\n  toString() {\n    return this.value;\n  }\n\n  toChatMessages() {\n    return [new HumanMessage(this.value)];\n  }\n}\n\n/**\n * Interface for the fields of a ChatPromptValue.\n */\nexport interface ChatPromptValueFields {\n  messages: BaseMessage[];\n}\n\n/**\n * Class that represents a chat prompt value. It extends the\n * BasePromptValue and includes an array of BaseMessage instances.\n */\nexport class ChatPromptValue\n  extends BasePromptValue\n  implements ChatPromptValueInterface\n{\n  lc_namespace = [\"langchain_core\", \"prompt_values\"];\n\n  lc_serializable = true;\n\n  static lc_name() {\n    return \"ChatPromptValue\";\n  }\n\n  messages: BaseMessage[];\n\n  constructor(messages: BaseMessage[]);\n\n  constructor(fields: ChatPromptValueFields);\n\n  constructor(fields: BaseMessage[] | ChatPromptValueFields) {\n    if (Array.isArray(fields)) {\n      // eslint-disable-next-line no-param-reassign\n      fields = { messages: fields };\n    }\n\n    super(fields);\n    this.messages = fields.messages;\n  }\n\n  toString() {\n    return getBufferString(this.messages);\n  }\n\n  toChatMessages() {\n    return this.messages;\n  }\n}\n\nexport type ImageContent = {\n  /** Specifies the detail level of the image. */\n  detail?: \"auto\" | \"low\" | \"high\";\n\n  /** Either a URL of the image or the base64 encoded image data. */\n  url: string;\n};\n\nexport interface ImagePromptValueFields {\n  imageUrl: ImageContent;\n}\n\n/**\n * Class that represents an image prompt value. It extends the\n * BasePromptValue and includes an ImageURL instance.\n */\nexport class ImagePromptValue extends BasePromptValue {\n  lc_namespace = [\"langchain_core\", \"prompt_values\"];\n\n  lc_serializable = true;\n\n  static lc_name() {\n    return \"ImagePromptValue\";\n  }\n\n  imageUrl: ImageContent;\n\n  /** @ignore */\n  value: string;\n\n  constructor(fields: ImagePromptValueFields);\n\n  constructor(fields: ImageContent);\n\n  constructor(fields: ImageContent | ImagePromptValueFields) {\n    if (!(\"imageUrl\" in fields)) {\n      // eslint-disable-next-line no-param-reassign\n      fields = { imageUrl: fields };\n    }\n\n    super(fields);\n    this.imageUrl = fields.imageUrl;\n  }\n\n  toString() {\n    return this.imageUrl.url;\n  }\n\n  toChatMessages() {\n    return [\n      new HumanMessage({\n        content: [\n          {\n            type: \"image_url\",\n            image_url: {\n              detail: this.imageUrl.detail,\n              url: this.imageUrl.url,\n            },\n          },\n        ],\n      }),\n    ];\n  }\n}\n", "import base64 from 'base64-js';\n\nvar __defProp = Object.defineProperty;\nvar __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;\nvar __publicField = (obj, key, value) => {\n  __defNormalProp(obj, typeof key !== \"symbol\" ? key + \"\" : key, value);\n  return value;\n};\n\n// src/utils.ts\nfunction never(_) {\n}\nfunction bytePairMerge(piece, ranks) {\n  let parts = Array.from(\n    { length: piece.length },\n    (_, i) => ({ start: i, end: i + 1 })\n  );\n  while (parts.length > 1) {\n    let minRank = null;\n    for (let i = 0; i < parts.length - 1; i++) {\n      const slice = piece.slice(parts[i].start, parts[i + 1].end);\n      const rank = ranks.get(slice.join(\",\"));\n      if (rank == null)\n        continue;\n      if (minRank == null || rank < minRank[0]) {\n        minRank = [rank, i];\n      }\n    }\n    if (minRank != null) {\n      const i = minRank[1];\n      parts[i] = { start: parts[i].start, end: parts[i + 1].end };\n      parts.splice(i + 1, 1);\n    } else {\n      break;\n    }\n  }\n  return parts;\n}\nfunction bytePairEncode(piece, ranks) {\n  if (piece.length === 1)\n    return [ranks.get(piece.join(\",\"))];\n  return bytePairMerge(piece, ranks).map((p) => ranks.get(piece.slice(p.start, p.end).join(\",\"))).filter((x) => x != null);\n}\nfunction escapeRegex(str) {\n  return str.replace(/[\\\\^$*+?.()|[\\]{}]/g, \"\\\\$&\");\n}\nvar _Tiktoken = class {\n  /** @internal */\n  specialTokens;\n  /** @internal */\n  inverseSpecialTokens;\n  /** @internal */\n  patStr;\n  /** @internal */\n  textEncoder = new TextEncoder();\n  /** @internal */\n  textDecoder = new TextDecoder(\"utf-8\");\n  /** @internal */\n  rankMap = /* @__PURE__ */ new Map();\n  /** @internal */\n  textMap = /* @__PURE__ */ new Map();\n  constructor(ranks, extendedSpecialTokens) {\n    this.patStr = ranks.pat_str;\n    const uncompressed = ranks.bpe_ranks.split(\"\\n\").filter(Boolean).reduce((memo, x) => {\n      const [_, offsetStr, ...tokens] = x.split(\" \");\n      const offset = Number.parseInt(offsetStr, 10);\n      tokens.forEach((token, i) => memo[token] = offset + i);\n      return memo;\n    }, {});\n    for (const [token, rank] of Object.entries(uncompressed)) {\n      const bytes = base64.toByteArray(token);\n      this.rankMap.set(bytes.join(\",\"), rank);\n      this.textMap.set(rank, bytes);\n    }\n    this.specialTokens = { ...ranks.special_tokens, ...extendedSpecialTokens };\n    this.inverseSpecialTokens = Object.entries(this.specialTokens).reduce((memo, [text, rank]) => {\n      memo[rank] = this.textEncoder.encode(text);\n      return memo;\n    }, {});\n  }\n  encode(text, allowedSpecial = [], disallowedSpecial = \"all\") {\n    const regexes = new RegExp(this.patStr, \"ug\");\n    const specialRegex = _Tiktoken.specialTokenRegex(\n      Object.keys(this.specialTokens)\n    );\n    const ret = [];\n    const allowedSpecialSet = new Set(\n      allowedSpecial === \"all\" ? Object.keys(this.specialTokens) : allowedSpecial\n    );\n    const disallowedSpecialSet = new Set(\n      disallowedSpecial === \"all\" ? Object.keys(this.specialTokens).filter(\n        (x) => !allowedSpecialSet.has(x)\n      ) : disallowedSpecial\n    );\n    if (disallowedSpecialSet.size > 0) {\n      const disallowedSpecialRegex = _Tiktoken.specialTokenRegex([\n        ...disallowedSpecialSet\n      ]);\n      const specialMatch = text.match(disallowedSpecialRegex);\n      if (specialMatch != null) {\n        throw new Error(\n          `The text contains a special token that is not allowed: ${specialMatch[0]}`\n        );\n      }\n    }\n    let start = 0;\n    while (true) {\n      let nextSpecial = null;\n      let startFind = start;\n      while (true) {\n        specialRegex.lastIndex = startFind;\n        nextSpecial = specialRegex.exec(text);\n        if (nextSpecial == null || allowedSpecialSet.has(nextSpecial[0]))\n          break;\n        startFind = nextSpecial.index + 1;\n      }\n      const end = nextSpecial?.index ?? text.length;\n      for (const match of text.substring(start, end).matchAll(regexes)) {\n        const piece = this.textEncoder.encode(match[0]);\n        const token2 = this.rankMap.get(piece.join(\",\"));\n        if (token2 != null) {\n          ret.push(token2);\n          continue;\n        }\n        ret.push(...bytePairEncode(piece, this.rankMap));\n      }\n      if (nextSpecial == null)\n        break;\n      let token = this.specialTokens[nextSpecial[0]];\n      ret.push(token);\n      start = nextSpecial.index + nextSpecial[0].length;\n    }\n    return ret;\n  }\n  decode(tokens) {\n    const res = [];\n    let length = 0;\n    for (let i2 = 0; i2 < tokens.length; ++i2) {\n      const token = tokens[i2];\n      const bytes = this.textMap.get(token) ?? this.inverseSpecialTokens[token];\n      if (bytes != null) {\n        res.push(bytes);\n        length += bytes.length;\n      }\n    }\n    const mergedArray = new Uint8Array(length);\n    let i = 0;\n    for (const bytes of res) {\n      mergedArray.set(bytes, i);\n      i += bytes.length;\n    }\n    return this.textDecoder.decode(mergedArray);\n  }\n};\nvar Tiktoken = _Tiktoken;\n__publicField(Tiktoken, \"specialTokenRegex\", (tokens) => {\n  return new RegExp(tokens.map((i) => escapeRegex(i)).join(\"|\"), \"g\");\n});\nfunction getEncodingNameForModel(model) {\n  switch (model) {\n    case \"gpt2\": {\n      return \"gpt2\";\n    }\n    case \"code-cushman-001\":\n    case \"code-cushman-002\":\n    case \"code-davinci-001\":\n    case \"code-davinci-002\":\n    case \"cushman-codex\":\n    case \"davinci-codex\":\n    case \"davinci-002\":\n    case \"text-davinci-002\":\n    case \"text-davinci-003\": {\n      return \"p50k_base\";\n    }\n    case \"code-davinci-edit-001\":\n    case \"text-davinci-edit-001\": {\n      return \"p50k_edit\";\n    }\n    case \"ada\":\n    case \"babbage\":\n    case \"babbage-002\":\n    case \"code-search-ada-code-001\":\n    case \"code-search-babbage-code-001\":\n    case \"curie\":\n    case \"davinci\":\n    case \"text-ada-001\":\n    case \"text-babbage-001\":\n    case \"text-curie-001\":\n    case \"text-davinci-001\":\n    case \"text-search-ada-doc-001\":\n    case \"text-search-babbage-doc-001\":\n    case \"text-search-curie-doc-001\":\n    case \"text-search-davinci-doc-001\":\n    case \"text-similarity-ada-001\":\n    case \"text-similarity-babbage-001\":\n    case \"text-similarity-curie-001\":\n    case \"text-similarity-davinci-001\": {\n      return \"r50k_base\";\n    }\n    case \"gpt-3.5-turbo-instruct-0914\":\n    case \"gpt-3.5-turbo-instruct\":\n    case \"gpt-3.5-turbo-16k-0613\":\n    case \"gpt-3.5-turbo-16k\":\n    case \"gpt-3.5-turbo-0613\":\n    case \"gpt-3.5-turbo-0301\":\n    case \"gpt-3.5-turbo\":\n    case \"gpt-4-32k-0613\":\n    case \"gpt-4-32k-0314\":\n    case \"gpt-4-32k\":\n    case \"gpt-4-0613\":\n    case \"gpt-4-0314\":\n    case \"gpt-4\":\n    case \"gpt-3.5-turbo-1106\":\n    case \"gpt-35-turbo\":\n    case \"gpt-4-1106-preview\":\n    case \"gpt-4-vision-preview\":\n    case \"gpt-3.5-turbo-0125\":\n    case \"gpt-4-turbo\":\n    case \"gpt-4-turbo-2024-04-09\":\n    case \"gpt-4-turbo-preview\":\n    case \"gpt-4-0125-preview\":\n    case \"text-embedding-ada-002\":\n    case \"text-embedding-3-small\":\n    case \"text-embedding-3-large\": {\n      return \"cl100k_base\";\n    }\n    case \"gpt-4o\":\n    case \"gpt-4o-2024-05-13\":\n    case \"gpt-4o-2024-08-06\":\n    case \"gpt-4o-2024-11-20\":\n    case \"gpt-4o-mini-2024-07-18\":\n    case \"gpt-4o-mini\":\n    case \"gpt-4o-search-preview\":\n    case \"gpt-4o-search-preview-2025-03-11\":\n    case \"gpt-4o-mini-search-preview\":\n    case \"gpt-4o-mini-search-preview-2025-03-11\":\n    case \"gpt-4o-audio-preview\":\n    case \"gpt-4o-audio-preview-2024-12-17\":\n    case \"gpt-4o-audio-preview-2024-10-01\":\n    case \"gpt-4o-mini-audio-preview\":\n    case \"gpt-4o-mini-audio-preview-2024-12-17\":\n    case \"o1\":\n    case \"o1-2024-12-17\":\n    case \"o1-mini\":\n    case \"o1-mini-2024-09-12\":\n    case \"o1-preview\":\n    case \"o1-preview-2024-09-12\":\n    case \"o1-pro\":\n    case \"o1-pro-2025-03-19\":\n    case \"o3\":\n    case \"o3-2025-04-16\":\n    case \"o3-mini\":\n    case \"o3-mini-2025-01-31\":\n    case \"o4-mini\":\n    case \"o4-mini-2025-04-16\":\n    case \"chatgpt-4o-latest\":\n    case \"gpt-4o-realtime\":\n    case \"gpt-4o-realtime-preview-2024-10-01\":\n    case \"gpt-4o-realtime-preview-2024-12-17\":\n    case \"gpt-4o-mini-realtime-preview\":\n    case \"gpt-4o-mini-realtime-preview-2024-12-17\":\n    case \"gpt-4.1\":\n    case \"gpt-4.1-2025-04-14\":\n    case \"gpt-4.1-mini\":\n    case \"gpt-4.1-mini-2025-04-14\":\n    case \"gpt-4.1-nano\":\n    case \"gpt-4.1-nano-2025-04-14\":\n    case \"gpt-4.5-preview\":\n    case \"gpt-4.5-preview-2025-02-27\":\n    case \"gpt-5\":\n    case \"gpt-5-2025-08-07\":\n    case \"gpt-5-nano\":\n    case \"gpt-5-nano-2025-08-07\":\n    case \"gpt-5-mini\":\n    case \"gpt-5-mini-2025-08-07\":\n    case \"gpt-5-chat-latest\": {\n      return \"o200k_base\";\n    }\n    default:\n      throw new Error(\"Unknown model\");\n  }\n}\n\nexport { Tiktoken, getEncodingNameForModel, never };\n", "import {\n  Tiktoken,\n  TiktokenEncoding,\n  TiktokenModel,\n  getEncodingNameForModel,\n} from \"js-tiktoken/lite\";\nimport { AsyncCaller } from \"./async_caller.js\";\n\nconst cache: Record<string, Promise<Tiktoken>> = {};\n\nconst caller = /* #__PURE__ */ new AsyncCaller({});\n\nexport async function getEncoding(encoding: TiktokenEncoding) {\n  if (!(encoding in cache)) {\n    cache[encoding] = caller\n      .fetch(`https://tiktoken.pages.dev/js/${encoding}.json`)\n      .then((res) => res.json())\n      .then((data) => new Tiktoken(data))\n      .catch((e) => {\n        delete cache[encoding];\n        throw e;\n      });\n  }\n\n  return await cache[encoding];\n}\n\nexport async function encodingForModel(model: TiktokenModel) {\n  return getEncoding(getEncodingNameForModel(model));\n}\n", "import type { Tiktoken, TiktokenModel } from \"js-tiktoken/lite\";\nimport type { ZodType as ZodTypeV3 } from \"zod/v3\";\nimport type { $ZodType as ZodTypeV4 } from \"zod/v4/core\";\n\nimport { type BaseCache, InMemoryCache } from \"../caches/index.js\";\nimport {\n  type BasePromptValueInterface,\n  StringPromptValue,\n  ChatPromptValue,\n} from \"../prompt_values.js\";\nimport {\n  type BaseMessage,\n  type BaseMessageLike,\n  type MessageContent,\n} from \"../messages/base.js\";\nimport { coerceMessageLikeToMessage } from \"../messages/utils.js\";\nimport { type LLMResult } from \"../outputs.js\";\nimport { CallbackManager, Callbacks } from \"../callbacks/manager.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../utils/async_caller.js\";\nimport { encodingForModel } from \"../utils/tiktoken.js\";\nimport { Runnable, type RunnableInterface } from \"../runnables/base.js\";\nimport { RunnableConfig } from \"../runnables/config.js\";\nimport { JSONSchema } from \"../utils/json_schema.js\";\nimport {\n  InferInteropZodOutput,\n  InteropZodObject,\n  InteropZodType,\n} from \"../utils/types/zod.js\";\nimport { ModelProfile } from \"./profile.js\";\n\n// https://www.npmjs.com/package/js-tiktoken\n\nexport const getModelNameForTiktoken = (modelName: string): TiktokenModel => {\n  if (modelName.startsWith(\"gpt-5\")) {\n    return \"gpt-5\" as TiktokenModel;\n  }\n\n  if (modelName.startsWith(\"gpt-3.5-turbo-16k\")) {\n    return \"gpt-3.5-turbo-16k\";\n  }\n\n  if (modelName.startsWith(\"gpt-3.5-turbo-\")) {\n    return \"gpt-3.5-turbo\";\n  }\n\n  if (modelName.startsWith(\"gpt-4-32k\")) {\n    return \"gpt-4-32k\";\n  }\n\n  if (modelName.startsWith(\"gpt-4-\")) {\n    return \"gpt-4\";\n  }\n\n  if (modelName.startsWith(\"gpt-4o\")) {\n    return \"gpt-4o\";\n  }\n\n  return modelName as TiktokenModel;\n};\n\nexport const getEmbeddingContextSize = (modelName?: string): number => {\n  switch (modelName) {\n    case \"text-embedding-ada-002\":\n      return 8191;\n    default:\n      return 2046;\n  }\n};\n\n/**\n * Get the context window size (max input tokens) for a given model.\n *\n * Context window sizes are sourced from official model documentation:\n * - OpenAI: https://platform.openai.com/docs/models\n * - Anthropic: https://docs.anthropic.com/claude/docs/models-overview\n * - Google: https://ai.google.dev/gemini/docs/models/gemini\n *\n * @param modelName - The name of the model\n * @returns The context window size in tokens\n */\nexport const getModelContextSize = (modelName: string): number => {\n  const normalizedName = getModelNameForTiktoken(modelName) as string;\n\n  switch (normalizedName) {\n    // GPT-5 series\n    case \"gpt-5\":\n    case \"gpt-5-turbo\":\n    case \"gpt-5-turbo-preview\":\n      return 400000;\n\n    // GPT-4o series\n    case \"gpt-4o\":\n    case \"gpt-4o-mini\":\n    case \"gpt-4o-2024-05-13\":\n    case \"gpt-4o-2024-08-06\":\n      return 128000;\n\n    // GPT-4 Turbo series\n    case \"gpt-4-turbo\":\n    case \"gpt-4-turbo-preview\":\n    case \"gpt-4-turbo-2024-04-09\":\n    case \"gpt-4-0125-preview\":\n    case \"gpt-4-1106-preview\":\n      return 128000;\n\n    // GPT-4 series\n    case \"gpt-4-32k\":\n    case \"gpt-4-32k-0314\":\n    case \"gpt-4-32k-0613\":\n      return 32768;\n    case \"gpt-4\":\n    case \"gpt-4-0314\":\n    case \"gpt-4-0613\":\n      return 8192;\n\n    // GPT-3.5 Turbo series\n    case \"gpt-3.5-turbo-16k\":\n    case \"gpt-3.5-turbo-16k-0613\":\n      return 16384;\n    case \"gpt-3.5-turbo\":\n    case \"gpt-3.5-turbo-0301\":\n    case \"gpt-3.5-turbo-0613\":\n    case \"gpt-3.5-turbo-1106\":\n    case \"gpt-3.5-turbo-0125\":\n      return 4096;\n\n    // Legacy GPT-3 models\n    case \"text-davinci-003\":\n    case \"text-davinci-002\":\n      return 4097;\n    case \"text-davinci-001\":\n      return 2049;\n    case \"text-curie-001\":\n    case \"text-babbage-001\":\n    case \"text-ada-001\":\n      return 2048;\n\n    // Code models\n    case \"code-davinci-002\":\n    case \"code-davinci-001\":\n      return 8000;\n    case \"code-cushman-001\":\n      return 2048;\n\n    // Claude models (Anthropic)\n    case \"claude-3-5-sonnet-20241022\":\n    case \"claude-3-5-sonnet-20240620\":\n    case \"claude-3-opus-20240229\":\n    case \"claude-3-sonnet-20240229\":\n    case \"claude-3-haiku-20240307\":\n    case \"claude-2.1\":\n      return 200000;\n    case \"claude-2.0\":\n    case \"claude-instant-1.2\":\n      return 100000;\n\n    // Gemini models (Google)\n    case \"gemini-1.5-pro\":\n    case \"gemini-1.5-pro-latest\":\n    case \"gemini-1.5-flash\":\n    case \"gemini-1.5-flash-latest\":\n      return 1000000; // 1M tokens\n    case \"gemini-pro\":\n    case \"gemini-pro-vision\":\n      return 32768;\n\n    default:\n      return 4097;\n  }\n};\n\n/**\n * Whether or not the input matches the OpenAI tool definition.\n * @param {unknown} tool The input to check.\n * @returns {boolean} Whether the input is an OpenAI tool definition.\n */\nexport function isOpenAITool(tool: unknown): tool is ToolDefinition {\n  if (typeof tool !== \"object\" || !tool) return false;\n  if (\n    \"type\" in tool &&\n    tool.type === \"function\" &&\n    \"function\" in tool &&\n    typeof tool.function === \"object\" &&\n    tool.function &&\n    \"name\" in tool.function &&\n    \"parameters\" in tool.function\n  ) {\n    return true;\n  }\n  return false;\n}\n\ninterface CalculateMaxTokenProps {\n  prompt: string;\n  modelName: TiktokenModel;\n}\n\nexport const calculateMaxTokens = async ({\n  prompt,\n  modelName,\n}: CalculateMaxTokenProps) => {\n  let numTokens;\n\n  try {\n    numTokens = (\n      await encodingForModel(getModelNameForTiktoken(modelName))\n    ).encode(prompt).length;\n  } catch {\n    console.warn(\n      \"Failed to calculate number of tokens, falling back to approximate count\"\n    );\n\n    // fallback to approximate calculation if tiktoken is not available\n    // each token is ~4 characters: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them#\n    numTokens = Math.ceil(prompt.length / 4);\n  }\n\n  const maxTokens = getModelContextSize(modelName);\n  return maxTokens - numTokens;\n};\n\nconst getVerbosity = () => false;\n\nexport type SerializedLLM = {\n  _model: string;\n  _type: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport interface BaseLangChainParams {\n  verbose?: boolean;\n  callbacks?: Callbacks;\n  tags?: string[];\n  metadata?: Record<string, unknown>;\n}\n\n/**\n * Base class for language models, chains, tools.\n */\nexport abstract class BaseLangChain<\n  RunInput,\n  RunOutput,\n  CallOptions extends RunnableConfig = RunnableConfig,\n>\n  extends Runnable<RunInput, RunOutput, CallOptions>\n  implements BaseLangChainParams\n{\n  /**\n   * Whether to print out response text.\n   */\n  verbose: boolean;\n\n  callbacks?: Callbacks;\n\n  tags?: string[];\n\n  metadata?: Record<string, unknown>;\n\n  get lc_attributes(): { [key: string]: undefined } | undefined {\n    return {\n      callbacks: undefined,\n      verbose: undefined,\n    };\n  }\n\n  constructor(params: BaseLangChainParams) {\n    super(params);\n    this.verbose = params.verbose ?? getVerbosity();\n    this.callbacks = params.callbacks;\n    this.tags = params.tags ?? [];\n    this.metadata = params.metadata ?? {};\n  }\n}\n\n/**\n * Base interface for language model parameters.\n * A subclass of {@link BaseLanguageModel} should have a constructor that\n * takes in a parameter that extends this interface.\n */\nexport interface BaseLanguageModelParams\n  extends AsyncCallerParams, BaseLangChainParams {\n  /**\n   * @deprecated Use `callbacks` instead\n   */\n  callbackManager?: CallbackManager;\n\n  cache?: BaseCache | boolean;\n}\n\nexport interface BaseLanguageModelTracingCallOptions {\n  /**\n   * Describes the format of structured outputs.\n   * This should be provided if an output is considered to be structured\n   */\n  ls_structured_output_format?: {\n    /**\n     * An object containing the method used for structured output (e.g., \"jsonMode\").\n     */\n    kwargs: { method: string };\n    /**\n     * The JSON schema describing the expected output structure.\n     */\n    schema?: JSONSchema;\n  };\n}\n\nexport interface BaseLanguageModelCallOptions\n  extends RunnableConfig, BaseLanguageModelTracingCallOptions {\n  /**\n   * Stop tokens to use for this call.\n   * If not provided, the default stop tokens for the model will be used.\n   */\n  stop?: string[];\n}\n\nexport interface FunctionDefinition {\n  /**\n   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain\n   * underscores and dashes, with a maximum length of 64.\n   */\n  name: string;\n\n  /**\n   * The parameters the functions accepts, described as a JSON Schema object. See the\n   * [guide](https://platform.openai.com/docs/guides/gpt/function-calling) for\n   * examples, and the\n   * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n   * documentation about the format.\n   *\n   * To describe a function that accepts no parameters, provide the value\n   * `{\"type\": \"object\", \"properties\": {}}`.\n   */\n  parameters: Record<string, unknown> | JSONSchema;\n\n  /**\n   * A description of what the function does, used by the model to choose when and\n   * how to call the function.\n   */\n  description?: string;\n}\n\nexport interface ToolDefinition {\n  type: \"function\";\n  function: FunctionDefinition;\n}\n\nexport type FunctionCallOption = {\n  name: string;\n};\n\nexport interface BaseFunctionCallOptions extends BaseLanguageModelCallOptions {\n  function_call?: FunctionCallOption;\n  functions?: FunctionDefinition[];\n}\n\nexport type BaseLanguageModelInput =\n  | BasePromptValueInterface\n  | string\n  | BaseMessageLike[];\n\nexport type StructuredOutputType = InferInteropZodOutput<InteropZodObject>;\n\nexport type StructuredOutputMethodOptions<IncludeRaw extends boolean = false> =\n  {\n    name?: string;\n    method?: \"functionCalling\" | \"jsonMode\" | \"jsonSchema\" | string;\n    includeRaw?: IncludeRaw;\n    /** Whether to use strict mode. Currently only supported by OpenAI models. */\n    strict?: boolean;\n  };\n\n/** @deprecated Use StructuredOutputMethodOptions instead */\nexport type StructuredOutputMethodParams<\n  RunOutput,\n  IncludeRaw extends boolean = false,\n> = {\n  /** @deprecated Pass schema in as the first argument */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  schema: InteropZodType<RunOutput> | Record<string, any>;\n  name?: string;\n  method?: \"functionCalling\" | \"jsonMode\";\n  includeRaw?: IncludeRaw;\n};\n\nexport interface BaseLanguageModelInterface<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput = any,\n  CallOptions extends BaseLanguageModelCallOptions =\n    BaseLanguageModelCallOptions,\n> extends RunnableInterface<BaseLanguageModelInput, RunOutput, CallOptions> {\n  get callKeys(): string[];\n\n  generatePrompt(\n    promptValues: BasePromptValueInterface[],\n    options?: string[] | Partial<CallOptions>,\n    callbacks?: Callbacks\n  ): Promise<LLMResult>;\n\n  _modelType(): string;\n\n  _llmType(): string;\n\n  getNumTokens(content: MessageContent): Promise<number>;\n\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams(): Record<string, any>;\n\n  serialize(): SerializedLLM;\n}\n\nexport type LanguageModelOutput = BaseMessage | string;\n\nexport type LanguageModelLike = Runnable<\n  BaseLanguageModelInput,\n  LanguageModelOutput\n>;\n\n/**\n * Base class for language models.\n */\nexport abstract class BaseLanguageModel<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput = any,\n  CallOptions extends BaseLanguageModelCallOptions =\n    BaseLanguageModelCallOptions,\n>\n  extends BaseLangChain<BaseLanguageModelInput, RunOutput, CallOptions>\n  implements\n    BaseLanguageModelParams,\n    BaseLanguageModelInterface<RunOutput, CallOptions>\n{\n  /**\n   * Keys that the language model accepts as call options.\n   */\n  get callKeys(): string[] {\n    return [\"stop\", \"timeout\", \"signal\", \"tags\", \"metadata\", \"callbacks\"];\n  }\n\n  /**\n   * The async caller should be used by subclasses to make any async calls,\n   * which will thus benefit from the concurrency and retry logic.\n   */\n  caller: AsyncCaller;\n\n  cache?: BaseCache;\n\n  constructor({\n    callbacks,\n    callbackManager,\n    ...params\n  }: BaseLanguageModelParams) {\n    const { cache, ...rest } = params;\n    super({\n      callbacks: callbacks ?? callbackManager,\n      ...rest,\n    });\n    if (typeof cache === \"object\") {\n      this.cache = cache;\n    } else if (cache) {\n      this.cache = InMemoryCache.global();\n    } else {\n      this.cache = undefined;\n    }\n    this.caller = new AsyncCaller(params ?? {});\n  }\n\n  abstract generatePrompt(\n    promptValues: BasePromptValueInterface[],\n    options?: string[] | CallOptions,\n    callbacks?: Callbacks\n  ): Promise<LLMResult>;\n\n  abstract _modelType(): string;\n\n  abstract _llmType(): string;\n\n  private _encoding?: Tiktoken;\n\n  /**\n   * Get the number of tokens in the content.\n   * @param content The content to get the number of tokens for.\n   * @returns The number of tokens in the content.\n   */\n  async getNumTokens(content: MessageContent) {\n    // Extract text content from MessageContent\n    let textContent: string;\n    if (typeof content === \"string\") {\n      textContent = content;\n    } else {\n      /**\n       * Content is an array of ContentBlock\n       *\n       * ToDo(@christian-bromann): This is a temporary fix to get the number of tokens for the content.\n       * We need to find a better way to do this.\n       * @see https://github.com/langchain-ai/langchainjs/pull/8341#pullrequestreview-2933713116\n       */\n      textContent = content\n        .map((item) => {\n          if (typeof item === \"string\") return item;\n          if (item.type === \"text\" && \"text\" in item) return item.text;\n          return \"\";\n        })\n        .join(\"\");\n    }\n\n    // fallback to approximate calculation if tiktoken is not available\n    let numTokens = Math.ceil(textContent.length / 4);\n\n    if (!this._encoding) {\n      try {\n        this._encoding = await encodingForModel(\n          \"modelName\" in this\n            ? getModelNameForTiktoken(this.modelName as string)\n            : \"gpt2\"\n        );\n      } catch (error) {\n        console.warn(\n          \"Failed to calculate number of tokens, falling back to approximate count\",\n          error\n        );\n      }\n    }\n\n    if (this._encoding) {\n      try {\n        numTokens = this._encoding.encode(textContent).length;\n      } catch (error) {\n        console.warn(\n          \"Failed to calculate number of tokens, falling back to approximate count\",\n          error\n        );\n      }\n    }\n\n    return numTokens;\n  }\n\n  protected static _convertInputToPromptValue(\n    input: BaseLanguageModelInput\n  ): BasePromptValueInterface {\n    if (typeof input === \"string\") {\n      return new StringPromptValue(input);\n    } else if (Array.isArray(input)) {\n      return new ChatPromptValue(input.map(coerceMessageLikeToMessage));\n    } else {\n      return input;\n    }\n  }\n\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams(): Record<string, any> {\n    return {};\n  }\n\n  /**\n   * Create a unique cache key for a specific call to a specific language model.\n   * @param callOptions Call options for the model\n   * @returns A unique cache key.\n   */\n  _getSerializedCacheKeyParametersForCall(\n    // TODO: Fix when we remove the RunnableLambda backwards compatibility shim.\n    { config, ...callOptions }: CallOptions & { config?: RunnableConfig }\n  ): string {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const params: Record<string, any> = {\n      ...this._identifyingParams(),\n      ...callOptions,\n      _type: this._llmType(),\n      _model: this._modelType(),\n    };\n    const filteredEntries = Object.entries(params).filter(\n      ([_, value]) => value !== undefined\n    );\n    const serializedEntries = filteredEntries\n      .map(([key, value]) => `${key}:${JSON.stringify(value)}`)\n      .sort()\n      .join(\",\");\n    return serializedEntries;\n  }\n\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize(): SerializedLLM {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType(),\n    };\n  }\n\n  /**\n   * @deprecated\n   * Load an LLM from a json-like object describing it.\n   */\n  static async deserialize(_data: SerializedLLM): Promise<BaseLanguageModel> {\n    throw new Error(\"Use .toJSON() instead\");\n  }\n\n  /**\n   * Return profiling information for the model.\n   *\n   * @returns {ModelProfile} An object describing the model's capabilities and constraints\n   */\n  get profile(): ModelProfile {\n    return {};\n  }\n\n  withStructuredOutput?<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    schema:\n      | ZodTypeV3<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput?<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    schema:\n      | ZodTypeV3<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput?<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    schema:\n      | ZodTypeV4<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput?<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    schema:\n      | ZodTypeV4<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  /**\n   * Model wrapper that returns outputs formatted to match the given schema.\n   *\n   * @template {BaseLanguageModelInput} RunInput The input type for the Runnable, expected to be the same input for the LLM.\n   * @template {Record<string, any>} RunOutput The output type for the Runnable, expected to be a Zod schema object for structured output validation.\n   *\n   * @param {InteropZodType<RunOutput>} schema The schema for the structured output. Either as a Zod schema or a valid JSON schema object.\n   *   If a Zod schema is passed, the returned attributes will be validated, whereas with JSON schema they will not be.\n   * @param {string} name The name of the function to call.\n   * @param {\"functionCalling\" | \"jsonMode\"} [method=functionCalling] The method to use for getting the structured output. Defaults to \"functionCalling\".\n   * @param {boolean | undefined} [includeRaw=false] Whether to include the raw output in the result. Defaults to false.\n   * @returns {Runnable<RunInput, RunOutput> | Runnable<RunInput, { raw: BaseMessage; parsed: RunOutput }>} A new runnable that calls the LLM with structured output.\n   */\n  withStructuredOutput?<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    schema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<\n        BaseLanguageModelInput,\n        {\n          raw: BaseMessage;\n          parsed: RunOutput;\n        }\n      >;\n}\n\n/**\n * Shared interface for token usage\n * return type from LLM calls.\n */\nexport interface TokenUsage {\n  completionTokens?: number;\n  promptTokens?: number;\n  totalTokens?: number;\n}\n", "import { concat } from \"../utils/stream.js\";\nimport {\n  Runnable,\n  RunnableAssign,\n  RunnableMap,\n  RunnableMapLike,\n} from \"./base.js\";\nimport { ensureConfig, type RunnableConfig } from \"./config.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype RunnablePassthroughFunc<RunInput = any> =\n  | ((input: RunInput) => void)\n  | ((input: RunInput, config?: RunnableConfig) => void)\n  | ((input: RunInput) => Promise<void>)\n  | ((input: RunInput, config?: RunnableConfig) => Promise<void>);\n\n/**\n * A runnable to passthrough inputs unchanged or with additional keys.\n *\n * This runnable behaves almost like the identity function, except that it\n * can be configured to add additional keys to the output, if the input is\n * an object.\n *\n * The example below demonstrates how to use `RunnablePassthrough to\n * passthrough the input from the `.invoke()`\n *\n * @example\n * ```typescript\n * const chain = RunnableSequence.from([\n *   {\n *     question: new RunnablePassthrough(),\n *     context: async () => loadContextFromStore(),\n *   },\n *   prompt,\n *   llm,\n *   outputParser,\n * ]);\n * const response = await chain.invoke(\n *   \"I can pass a single string instead of an object since I'm using `RunnablePassthrough`.\"\n * );\n * ```\n */\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport class RunnablePassthrough<RunInput = any> extends Runnable<\n  RunInput,\n  RunInput\n> {\n  static lc_name() {\n    return \"RunnablePassthrough\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"runnables\"];\n\n  lc_serializable = true;\n\n  func?: RunnablePassthroughFunc<RunInput>;\n\n  constructor(fields?: { func?: RunnablePassthroughFunc<RunInput> }) {\n    super(fields);\n    if (fields) {\n      this.func = fields.func;\n    }\n  }\n\n  async invoke(\n    input: RunInput,\n    options?: Partial<RunnableConfig>\n  ): Promise<RunInput> {\n    const config = ensureConfig(options);\n    if (this.func) {\n      await this.func(input, config);\n    }\n\n    return this._callWithConfig(\n      (input: RunInput) => Promise.resolve(input),\n      input,\n      config\n    );\n  }\n\n  async *transform(\n    generator: AsyncGenerator<RunInput>,\n    options: Partial<RunnableConfig>\n  ): AsyncGenerator<RunInput> {\n    const config = ensureConfig(options);\n    let finalOutput: RunInput | undefined;\n    let finalOutputSupported = true;\n\n    for await (const chunk of this._transformStreamWithConfig(\n      generator,\n      (input: AsyncGenerator<RunInput>) => input,\n      config\n    )) {\n      yield chunk;\n      if (finalOutputSupported) {\n        if (finalOutput === undefined) {\n          finalOutput = chunk;\n        } else {\n          try {\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            finalOutput = concat(finalOutput, chunk as any);\n          } catch {\n            finalOutput = undefined;\n            finalOutputSupported = false;\n          }\n        }\n      }\n    }\n\n    if (this.func && finalOutput !== undefined) {\n      await this.func(finalOutput, config);\n    }\n  }\n\n  /**\n   * A runnable that assigns key-value pairs to the input.\n   *\n   * The example below shows how you could use it with an inline function.\n   *\n   * @example\n   * ```typescript\n   * const prompt =\n   *   PromptTemplate.fromTemplate(`Write a SQL query to answer the question using the following schema: {schema}\n   * Question: {question}\n   * SQL Query:`);\n   *\n   * // The `RunnablePassthrough.assign()` is used here to passthrough the input from the `.invoke()`\n   * // call (in this example it's the question), along with any inputs passed to the `.assign()` method.\n   * // In this case, we're passing the schema.\n   * const sqlQueryGeneratorChain = RunnableSequence.from([\n   *   RunnablePassthrough.assign({\n   *     schema: async () => db.getTableInfo(),\n   *   }),\n   *   prompt,\n   *   new ChatOpenAI({ model: \"gpt-4o-mini\" }).withConfig({ stop: [\"\\nSQLResult:\"] }),\n   *   new StringOutputParser(),\n   * ]);\n   * const result = await sqlQueryGeneratorChain.invoke({\n   *   question: \"How many employees are there?\",\n   * });\n   * ```\n   */\n  static assign<\n    RunInput extends Record<string, unknown> = Record<string, unknown>,\n    RunOutput extends Record<string, unknown> = Record<string, unknown>,\n  >(\n    mapping: RunnableMapLike<RunInput, RunOutput>\n  ): RunnableAssign<RunInput, RunInput & RunOutput> {\n    return new RunnableAssign(new RunnableMap({ steps: mapping }));\n  }\n}\n", "import { BaseMessage } from \"../messages/base.js\";\n\ntype Constructor<T> = new (...args: unknown[]) => T;\n\nexport const iife = <T>(fn: () => T): T => fn();\n\nfunction castStandardMessageContent<T extends BaseMessage>(message: T) {\n  const Cls = message.constructor as Constructor<T>;\n  return new Cls({\n    ...message,\n    content: message.contentBlocks,\n    response_metadata: {\n      ...message.response_metadata,\n      output_version: \"v1\",\n    },\n  });\n}\n\nexport { castStandardMessageContent };\n", "import type { ZodType as ZodTypeV3 } from \"zod/v3\";\nimport type { $ZodType as ZodTypeV4 } from \"zod/v4/core\";\nimport {\n  AIMessage,\n  type BaseMessage,\n  BaseMessageChunk,\n  type BaseMessageLike,\n  coerceMessageLikeToMessage,\n  AIMessageChunk,\n  isAIMessageChunk,\n  isBaseMessage,\n  isAIMessage,\n  MessageOutputVersion,\n} from \"../messages/index.js\";\nimport {\n  convertToOpenAIImageBlock,\n  isURLContentBlock,\n  isBase64ContentBlock,\n} from \"../messages/content/data.js\";\nimport type { BasePromptValueInterface } from \"../prompt_values.js\";\nimport {\n  LLMResult,\n  RUN_KEY,\n  type ChatGeneration,\n  ChatGenerationChunk,\n  type ChatResult,\n  type Generation,\n} from \"../outputs.js\";\nimport {\n  BaseLanguageModel,\n  type StructuredOutputMethodOptions,\n  type ToolDefinition,\n  type BaseLanguageModelCallOptions,\n  type BaseLanguageModelInput,\n  type BaseLanguageModelParams,\n} from \"./base.js\";\nimport {\n  CallbackManager,\n  type CallbackManagerForLLMRun,\n  type Callbacks,\n} from \"../callbacks/manager.js\";\nimport type { RunnableConfig } from \"../runnables/config.js\";\nimport type { BaseCache } from \"../caches/index.js\";\nimport {\n  StructuredToolInterface,\n  StructuredToolParams,\n} from \"../tools/index.js\";\nimport {\n  Runnable,\n  RunnableLambda,\n  RunnableSequence,\n  RunnableToolLike,\n} from \"../runnables/base.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { RunnablePassthrough } from \"../runnables/passthrough.js\";\nimport {\n  getSchemaDescription,\n  InteropZodType,\n  isInteropZodSchema,\n} from \"../utils/types/zod.js\";\nimport { ModelAbortError } from \"../errors/index.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\nimport { toJsonSchema } from \"../utils/json_schema.js\";\nimport { getEnvironmentVariable } from \"../utils/env.js\";\nimport { castStandardMessageContent, iife } from \"./utils.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type ToolChoice = string | Record<string, any> | \"auto\" | \"any\";\n\n/**\n * Represents a serialized chat model.\n */\nexport type SerializedChatModel = {\n  _model: string;\n  _type: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\n// todo?\n/**\n * Represents a serialized large language model.\n */\nexport type SerializedLLM = {\n  _model: string;\n  _type: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\n/**\n * Represents the parameters for a base chat model.\n */\nexport type BaseChatModelParams = BaseLanguageModelParams & {\n  /**\n   * Whether to disable streaming.\n   *\n   * If streaming is bypassed, then `stream()` will defer to\n   * `invoke()`.\n   *\n   * - If true, will always bypass streaming case.\n   * - If false (default), will always use streaming case if available.\n   */\n  disableStreaming?: boolean;\n  /**\n   * Version of `AIMessage` output format to store in message content.\n   *\n   * `AIMessage.contentBlocks` will lazily parse the contents of `content` into a\n   * standard format. This flag can be used to additionally store the standard format\n   * as the message content, e.g., for serialization purposes.\n   *\n   * - \"v0\": provider-specific format in content (can lazily parse with `.contentBlocks`)\n   * - \"v1\": standardized format in content (consistent with `.contentBlocks`)\n   *\n   * You can also set `LC_OUTPUT_VERSION` as an environment variable to \"v1\" to\n   * enable this by default.\n   *\n   * @default \"v0\"\n   */\n  outputVersion?: MessageOutputVersion;\n};\n\n/**\n * Represents the call options for a base chat model.\n */\nexport type BaseChatModelCallOptions = BaseLanguageModelCallOptions & {\n  /**\n   * Specifies how the chat model should use tools.\n   * @default undefined\n   *\n   * Possible values:\n   * - \"auto\": The model may choose to use any of the provided tools, or none.\n   * - \"any\": The model must use one of the provided tools.\n   * - \"none\": The model must not use any tools.\n   * - A string (not \"auto\", \"any\", or \"none\"): The name of a specific tool the model must use.\n   * - An object: A custom schema specifying tool choice parameters. Specific to the provider.\n   *\n   * Note: Not all providers support tool_choice. An error will be thrown\n   * if used with an unsupported model.\n   */\n  tool_choice?: ToolChoice;\n  /**\n   * Version of `AIMessage` output format to store in message content.\n   *\n   * `AIMessage.contentBlocks` will lazily parse the contents of `content` into a\n   * standard format. This flag can be used to additionally store the standard format\n   * as the message content, e.g., for serialization purposes.\n   *\n   * - \"v0\": provider-specific format in content (can lazily parse with `.contentBlocks`)\n   * - \"v1\": standardized format in content (consistent with `.contentBlocks`)\n   *\n   * You can also set `LC_OUTPUT_VERSION` as an environment variable to \"v1\" to\n   * enable this by default.\n   *\n   * @default \"v0\"\n   */\n  outputVersion?: MessageOutputVersion;\n};\n\nfunction _formatForTracing(messages: BaseMessage[]): BaseMessage[] {\n  const messagesToTrace: BaseMessage[] = [];\n  for (const message of messages) {\n    let messageToTrace = message;\n    if (Array.isArray(message.content)) {\n      for (let idx = 0; idx < message.content.length; idx++) {\n        const block = message.content[idx];\n        if (isURLContentBlock(block) || isBase64ContentBlock(block)) {\n          if (messageToTrace === message) {\n            // Also shallow-copy content\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            messageToTrace = new (message.constructor as any)({\n              ...messageToTrace,\n              content: [\n                ...message.content.slice(0, idx),\n                convertToOpenAIImageBlock(block),\n                ...message.content.slice(idx + 1),\n              ],\n            });\n          }\n        }\n      }\n    }\n    messagesToTrace.push(messageToTrace);\n  }\n  return messagesToTrace;\n}\n\nexport type LangSmithParams = {\n  ls_provider?: string;\n  ls_model_name?: string;\n  ls_model_type: \"chat\";\n  ls_temperature?: number;\n  ls_max_tokens?: number;\n  ls_stop?: Array<string>;\n};\n\nexport type BindToolsInput =\n  | StructuredToolInterface\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  | Record<string, any>\n  | ToolDefinition\n  | RunnableToolLike\n  | StructuredToolParams;\n\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport abstract class BaseChatModel<\n  CallOptions extends BaseChatModelCallOptions = BaseChatModelCallOptions,\n  // TODO: Fix the parameter order on the next minor version.\n  OutputMessageType extends BaseMessageChunk = AIMessageChunk,\n> extends BaseLanguageModel<OutputMessageType, CallOptions> {\n  // Backwards compatibility since fields have been moved to RunnableConfig\n  declare ParsedCallOptions: Omit<\n    CallOptions,\n    Exclude<keyof RunnableConfig, \"signal\" | \"timeout\" | \"maxConcurrency\">\n  >;\n\n  // Only ever instantiated in main LangChain\n  lc_namespace = [\"langchain\", \"chat_models\", this._llmType()];\n\n  disableStreaming = false;\n\n  outputVersion?: MessageOutputVersion;\n\n  get callKeys(): string[] {\n    return [...super.callKeys, \"outputVersion\"];\n  }\n\n  constructor(fields: BaseChatModelParams) {\n    super(fields);\n    this.outputVersion = iife(() => {\n      const outputVersion =\n        fields.outputVersion ?? getEnvironmentVariable(\"LC_OUTPUT_VERSION\");\n      if (outputVersion && [\"v0\", \"v1\"].includes(outputVersion)) {\n        return outputVersion as \"v0\" | \"v1\";\n      }\n      return \"v0\";\n    });\n  }\n\n  _combineLLMOutput?(\n    ...llmOutputs: LLMResult[\"llmOutput\"][]\n  ): LLMResult[\"llmOutput\"];\n\n  protected _separateRunnableConfigFromCallOptionsCompat(\n    options?: Partial<CallOptions>\n  ): [RunnableConfig, this[\"ParsedCallOptions\"]] {\n    // For backwards compat, keep `signal` in both runnableConfig and callOptions\n    const [runnableConfig, callOptions] =\n      super._separateRunnableConfigFromCallOptions(options);\n    (callOptions as this[\"ParsedCallOptions\"]).signal = runnableConfig.signal;\n    return [runnableConfig, callOptions as this[\"ParsedCallOptions\"]];\n  }\n\n  /**\n   * Bind tool-like objects to this chat model.\n   *\n   * @param tools A list of tool definitions to bind to this chat model.\n   * Can be a structured tool, an OpenAI formatted tool, or an object\n   * matching the provider's specific tool schema.\n   * @param kwargs Any additional parameters to bind.\n   */\n  bindTools?(\n    tools: BindToolsInput[],\n    kwargs?: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, OutputMessageType, CallOptions>;\n\n  /**\n   * Invokes the chat model with a single input.\n   * @param input The input for the language model.\n   * @param options The call options.\n   * @returns A Promise that resolves to a BaseMessageChunk.\n   */\n  async invoke(\n    input: BaseLanguageModelInput,\n    options?: Partial<CallOptions>\n  ): Promise<OutputMessageType> {\n    const promptValue = BaseChatModel._convertInputToPromptValue(input);\n    const result = await this.generatePrompt(\n      [promptValue],\n      options,\n      options?.callbacks\n    );\n    const chatGeneration = result.generations[0][0] as ChatGeneration;\n    // TODO: Remove cast after figuring out inheritance\n    return chatGeneration.message as OutputMessageType;\n  }\n\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(\n    _messages: BaseMessage[],\n    _options: this[\"ParsedCallOptions\"],\n    _runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    throw new Error(\"Not implemented.\");\n  }\n\n  async *_streamIterator(\n    input: BaseLanguageModelInput,\n    options?: Partial<CallOptions>\n  ): AsyncGenerator<OutputMessageType> {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (\n      this._streamResponseChunks ===\n        BaseChatModel.prototype._streamResponseChunks ||\n      this.disableStreaming\n    ) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseChatModel._convertInputToPromptValue(input);\n      const messages = prompt.toChatMessages();\n      const [runnableConfig, callOptions] =\n        this._separateRunnableConfigFromCallOptionsCompat(options);\n\n      const inheritableMetadata = {\n        ...runnableConfig.metadata,\n        ...this.getLsParams(callOptions),\n      };\n      const callbackManager_ = await CallbackManager.configure(\n        runnableConfig.callbacks,\n        this.callbacks,\n        runnableConfig.tags,\n        this.tags,\n        inheritableMetadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions),\n        batch_size: 1,\n      };\n      const outputVersion = callOptions.outputVersion ?? this.outputVersion;\n      const runManagers = await callbackManager_?.handleChatModelStart(\n        this.toJSON(),\n        [_formatForTracing(messages)],\n        runnableConfig.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        runnableConfig.runName\n      );\n      let generationChunk: ChatGenerationChunk | undefined;\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      let llmOutput: Record<string, any> | undefined;\n      try {\n        for await (const chunk of this._streamResponseChunks(\n          messages,\n          callOptions,\n          runManagers?.[0]\n        )) {\n          callOptions.signal?.throwIfAborted();\n          if (chunk.message.id == null) {\n            const runId = runManagers?.at(0)?.runId;\n            if (runId != null) chunk.message._updateId(`run-${runId}`);\n          }\n          chunk.message.response_metadata = {\n            ...chunk.generationInfo,\n            ...chunk.message.response_metadata,\n          };\n          if (outputVersion === \"v1\") {\n            yield castStandardMessageContent(\n              chunk.message\n            ) as OutputMessageType;\n          } else {\n            yield chunk.message as OutputMessageType;\n          }\n          if (!generationChunk) {\n            generationChunk = chunk;\n          } else {\n            generationChunk = generationChunk.concat(chunk);\n          }\n          if (\n            isAIMessageChunk(chunk.message) &&\n            chunk.message.usage_metadata !== undefined\n          ) {\n            llmOutput = {\n              tokenUsage: {\n                promptTokens: chunk.message.usage_metadata.input_tokens,\n                completionTokens: chunk.message.usage_metadata.output_tokens,\n                totalTokens: chunk.message.usage_metadata.total_tokens,\n              },\n            };\n          }\n        }\n        // Throw error if stream ended due to abort (provider returned early)\n        callOptions.signal?.throwIfAborted();\n      } catch (err) {\n        await Promise.all(\n          (runManagers ?? []).map((runManager) =>\n            runManager?.handleLLMError(err)\n          )\n        );\n        throw err;\n      }\n      await Promise.all(\n        (runManagers ?? []).map((runManager) =>\n          runManager?.handleLLMEnd({\n            // TODO: Remove cast after figuring out inheritance\n            generations: [[generationChunk as ChatGeneration]],\n            llmOutput,\n          })\n        )\n      );\n    }\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const providerName = this.getName().startsWith(\"Chat\")\n      ? this.getName().replace(\"Chat\", \"\")\n      : this.getName();\n\n    return {\n      ls_model_type: \"chat\",\n      ls_stop: options.stop,\n      ls_provider: providerName,\n    };\n  }\n\n  /** @ignore */\n  async _generateUncached(\n    messages: BaseMessageLike[][],\n    parsedOptions: this[\"ParsedCallOptions\"],\n    handledOptions: RunnableConfig,\n    startedRunManagers?: CallbackManagerForLLMRun[]\n  ): Promise<LLMResult> {\n    const baseMessages = messages.map((messageList) =>\n      messageList.map(coerceMessageLikeToMessage)\n    );\n\n    let runManagers: CallbackManagerForLLMRun[] | undefined;\n    if (\n      startedRunManagers !== undefined &&\n      startedRunManagers.length === baseMessages.length\n    ) {\n      runManagers = startedRunManagers;\n    } else {\n      const inheritableMetadata = {\n        ...handledOptions.metadata,\n        ...this.getLsParams(parsedOptions),\n      };\n      // create callback manager and start run\n      const callbackManager_ = await CallbackManager.configure(\n        handledOptions.callbacks,\n        this.callbacks,\n        handledOptions.tags,\n        this.tags,\n        inheritableMetadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: parsedOptions,\n        invocation_params: this?.invocationParams(parsedOptions),\n        batch_size: 1,\n      };\n      runManagers = await callbackManager_?.handleChatModelStart(\n        this.toJSON(),\n        baseMessages.map(_formatForTracing),\n        handledOptions.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        handledOptions.runName\n      );\n    }\n    const outputVersion = parsedOptions.outputVersion ?? this.outputVersion;\n    const generations: ChatGeneration[][] = [];\n    const llmOutputs: LLMResult[\"llmOutput\"][] = [];\n    // Even if stream is not explicitly called, check if model is implicitly\n    // called from streamEvents() or streamLog() to get all streamed events.\n    // Bail out if _streamResponseChunks not overridden\n    const hasStreamingHandler = !!runManagers?.[0].handlers.find(\n      callbackHandlerPrefersStreaming\n    );\n    if (\n      hasStreamingHandler &&\n      !this.disableStreaming &&\n      baseMessages.length === 1 &&\n      this._streamResponseChunks !==\n        BaseChatModel.prototype._streamResponseChunks\n    ) {\n      try {\n        const stream = await this._streamResponseChunks(\n          baseMessages[0],\n          parsedOptions,\n          runManagers?.[0]\n        );\n        let aggregated: ChatGenerationChunk | undefined;\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        let llmOutput: Record<string, any> | undefined;\n        for await (const chunk of stream) {\n          // Check for abort signal - throw ModelAbortError with partial output\n          if (parsedOptions.signal?.aborted) {\n            const partialMessage = aggregated?.message as\n              | AIMessageChunk\n              | undefined;\n            throw new ModelAbortError(\n              \"Model invocation was aborted.\",\n              partialMessage\n            );\n          }\n          if (chunk.message.id == null) {\n            const runId = runManagers?.at(0)?.runId;\n            if (runId != null) chunk.message._updateId(`run-${runId}`);\n          }\n          if (aggregated === undefined) {\n            aggregated = chunk;\n          } else {\n            aggregated = concat(aggregated, chunk);\n          }\n          if (\n            isAIMessageChunk(chunk.message) &&\n            chunk.message.usage_metadata !== undefined\n          ) {\n            llmOutput = {\n              tokenUsage: {\n                promptTokens: chunk.message.usage_metadata.input_tokens,\n                completionTokens: chunk.message.usage_metadata.output_tokens,\n                totalTokens: chunk.message.usage_metadata.total_tokens,\n              },\n            };\n          }\n        }\n        // Check if stream ended due to abort (provider returned early)\n        if (parsedOptions.signal?.aborted) {\n          const partialMessage = aggregated?.message as\n            | AIMessageChunk\n            | undefined;\n          throw new ModelAbortError(\n            \"Model invocation was aborted.\",\n            partialMessage\n          );\n        }\n        if (aggregated === undefined) {\n          throw new Error(\"Received empty response from chat model call.\");\n        }\n        generations.push([aggregated]);\n        await runManagers?.[0].handleLLMEnd({\n          generations,\n          llmOutput,\n        });\n      } catch (e) {\n        await runManagers?.[0].handleLLMError(e);\n        throw e;\n      }\n    } else {\n      // generate results\n      const results = await Promise.allSettled(\n        baseMessages.map(async (messageList, i) => {\n          const generateResults = await this._generate(\n            messageList,\n            { ...parsedOptions, promptIndex: i },\n            runManagers?.[i]\n          );\n          if (outputVersion === \"v1\") {\n            for (const generation of generateResults.generations) {\n              generation.message = castStandardMessageContent(\n                generation.message\n              );\n            }\n          }\n          return generateResults;\n        })\n      );\n      // handle results\n      await Promise.all(\n        results.map(async (pResult, i) => {\n          if (pResult.status === \"fulfilled\") {\n            const result = pResult.value;\n            for (const generation of result.generations) {\n              if (generation.message.id == null) {\n                const runId = runManagers?.at(0)?.runId;\n                if (runId != null) generation.message._updateId(`run-${runId}`);\n              }\n              generation.message.response_metadata = {\n                ...generation.generationInfo,\n                ...generation.message.response_metadata,\n              };\n            }\n            if (result.generations.length === 1) {\n              result.generations[0].message.response_metadata = {\n                ...result.llmOutput,\n                ...result.generations[0].message.response_metadata,\n              };\n            }\n            generations[i] = result.generations;\n            llmOutputs[i] = result.llmOutput;\n            return runManagers?.[i]?.handleLLMEnd({\n              generations: [result.generations],\n              llmOutput: result.llmOutput,\n            });\n          } else {\n            // status === \"rejected\"\n            await runManagers?.[i]?.handleLLMError(pResult.reason);\n            return Promise.reject(pResult.reason);\n          }\n        })\n      );\n    }\n    // create combined output\n    const output: LLMResult = {\n      generations,\n      llmOutput: llmOutputs.length\n        ? this._combineLLMOutput?.(...llmOutputs)\n        : undefined,\n    };\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers\n        ? { runIds: runManagers?.map((manager) => manager.runId) }\n        : undefined,\n      configurable: true,\n    });\n    return output;\n  }\n\n  async _generateCached({\n    messages,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions,\n  }: {\n    messages: BaseMessageLike[][];\n    cache: BaseCache<Generation[]>;\n    llmStringKey: string;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    parsedOptions: any;\n    handledOptions: RunnableConfig;\n  }): Promise<\n    LLMResult & {\n      missingPromptIndices: number[];\n      startedRunManagers?: CallbackManagerForLLMRun[];\n    }\n  > {\n    const baseMessages = messages.map((messageList) =>\n      messageList.map(coerceMessageLikeToMessage)\n    );\n\n    const inheritableMetadata = {\n      ...handledOptions.metadata,\n      ...this.getLsParams(parsedOptions),\n    };\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(\n      handledOptions.callbacks,\n      this.callbacks,\n      handledOptions.tags,\n      this.tags,\n      inheritableMetadata,\n      this.metadata,\n      { verbose: this.verbose }\n    );\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: 1,\n    };\n    const runManagers = await callbackManager_?.handleChatModelStart(\n      this.toJSON(),\n      baseMessages.map(_formatForTracing),\n      handledOptions.runId,\n      undefined,\n      extra,\n      undefined,\n      undefined,\n      handledOptions.runName\n    );\n\n    // generate results\n    const missingPromptIndices: number[] = [];\n    const results = await Promise.allSettled(\n      baseMessages.map(async (baseMessage, index) => {\n        // Join all content into one string for the prompt index\n        const prompt =\n          BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n        const result = await cache.lookup(prompt, llmStringKey);\n\n        if (result == null) {\n          missingPromptIndices.push(index);\n        }\n\n        return result;\n      })\n    );\n\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results\n      .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n      .filter(\n        ({ result }) =>\n          (result.status === \"fulfilled\" && result.value != null) ||\n          result.status === \"rejected\"\n      );\n\n    // Handle results and call run managers\n    const outputVersion = parsedOptions.outputVersion ?? this.outputVersion;\n    const generations: Generation[][] = [];\n    await Promise.all(\n      cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n        if (promiseResult.status === \"fulfilled\") {\n          const result = promiseResult.value as Generation[];\n          generations[i] = result.map((result) => {\n            if (\n              \"message\" in result &&\n              isBaseMessage(result.message) &&\n              isAIMessage(result.message)\n            ) {\n              result.message.usage_metadata = {\n                input_tokens: 0,\n                output_tokens: 0,\n                total_tokens: 0,\n              };\n              if (outputVersion === \"v1\") {\n                result.message = castStandardMessageContent(result.message);\n              }\n            }\n            result.generationInfo = {\n              ...result.generationInfo,\n              tokenUsage: {},\n            };\n            return result;\n          });\n          if (result.length) {\n            await runManager?.handleLLMNewToken(result[0].text);\n          }\n          return runManager?.handleLLMEnd(\n            {\n              generations: [result],\n            },\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n        } else {\n          // status === \"rejected\"\n          await runManager?.handleLLMError(\n            promiseResult.reason,\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n          return Promise.reject(promiseResult.reason);\n        }\n      })\n    );\n\n    const output = {\n      generations,\n      missingPromptIndices,\n      startedRunManagers: runManagers,\n    };\n\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers\n        ? { runIds: runManagers?.map((manager) => manager.runId) }\n        : undefined,\n      configurable: true,\n    });\n\n    return output;\n  }\n\n  /**\n   * Generates chat based on the input messages.\n   * @param messages An array of arrays of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generate(\n    messages: BaseMessageLike[][],\n    options?: string[] | Partial<CallOptions>,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    // parse call options\n    let parsedOptions: Partial<CallOptions> | undefined;\n    if (Array.isArray(options)) {\n      parsedOptions = { stop: options } as Partial<CallOptions>;\n    } else {\n      parsedOptions = options;\n    }\n\n    const baseMessages = messages.map((messageList) =>\n      messageList.map(coerceMessageLikeToMessage)\n    );\n\n    const [runnableConfig, callOptions] =\n      this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n\n    if (!this.cache) {\n      return this._generateUncached(baseMessages, callOptions, runnableConfig);\n    }\n\n    const { cache } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(\n      callOptions as CallOptions\n    );\n\n    const { generations, missingPromptIndices, startedRunManagers } =\n      await this._generateCached({\n        messages: baseMessages,\n        cache,\n        llmStringKey,\n        parsedOptions: callOptions,\n        handledOptions: runnableConfig,\n      });\n\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(\n        missingPromptIndices.map((i) => baseMessages[i]),\n        callOptions,\n        runnableConfig,\n        startedRunManagers !== undefined\n          ? missingPromptIndices.map((i) => startedRunManagers?.[i])\n          : undefined\n      );\n      await Promise.all(\n        results.generations.map(async (generation, index) => {\n          const promptIndex = missingPromptIndices[index];\n          generations[promptIndex] = generation;\n          // Join all content into one string for the prompt index\n          const prompt = BaseChatModel._convertInputToPromptValue(\n            baseMessages[promptIndex]\n          ).toString();\n          return cache.update(prompt, llmStringKey, generation);\n        })\n      );\n      llmOutput = results.llmOutput ?? {};\n    }\n\n    return { generations, llmOutput } as LLMResult;\n  }\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options?: this[\"ParsedCallOptions\"]): any {\n    return {};\n  }\n\n  _modelType(): string {\n    return \"base_chat_model\" as const;\n  }\n\n  abstract _llmType(): string;\n\n  /**\n   * Generates a prompt based on the input prompt values.\n   * @param promptValues An array of BasePromptValue instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generatePrompt(\n    promptValues: BasePromptValueInterface[],\n    options?: string[] | Partial<CallOptions>,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    const promptMessages: BaseMessage[][] = promptValues.map((promptValue) =>\n      promptValue.toChatMessages()\n    );\n    return this.generate(promptMessages, options, callbacks);\n  }\n\n  abstract _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV4<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV4<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV3<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | ZodTypeV3<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<\n        BaseLanguageModelInput,\n        {\n          raw: BaseMessage;\n          parsed: RunOutput;\n        }\n      > {\n    if (typeof this.bindTools !== \"function\") {\n      throw new Error(\n        `Chat model must implement \".bindTools()\" to use withStructuredOutput.`\n      );\n    }\n    if (config?.strict) {\n      throw new Error(\n        `\"strict\" mode is not supported for this model by default.`\n      );\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const schema: Record<string, any> | InteropZodType<RunOutput> =\n      outputSchema;\n    const name = config?.name;\n    const description =\n      getSchemaDescription(schema) ?? \"A function available to call.\";\n    const method = config?.method;\n    const includeRaw = config?.includeRaw;\n    if (method === \"jsonMode\") {\n      throw new Error(\n        `Base withStructuredOutput implementation only supports \"functionCalling\" as a method.`\n      );\n    }\n\n    let functionName = name ?? \"extract\";\n    let tools: ToolDefinition[];\n    if (isInteropZodSchema(schema)) {\n      tools = [\n        {\n          type: \"function\",\n          function: {\n            name: functionName,\n            description,\n            parameters: toJsonSchema(schema),\n          },\n        },\n      ];\n    } else {\n      if (\"name\" in schema) {\n        functionName = schema.name;\n      }\n      tools = [\n        {\n          type: \"function\",\n          function: {\n            name: functionName,\n            description,\n            parameters: schema,\n          },\n        },\n      ];\n    }\n\n    const llm = this.bindTools(tools);\n    const outputParser = RunnableLambda.from<OutputMessageType, RunOutput>(\n      (input: BaseMessageChunk): RunOutput => {\n        if (!AIMessageChunk.isInstance(input)) {\n          throw new Error(\"Input is not an AIMessageChunk.\");\n        }\n        if (!input.tool_calls || input.tool_calls.length === 0) {\n          throw new Error(\"No tool calls found in the response.\");\n        }\n        const toolCall = input.tool_calls.find(\n          (tc) => tc.name === functionName\n        );\n        if (!toolCall) {\n          throw new Error(`No tool call found with name ${functionName}.`);\n        }\n        return toolCall.args as RunOutput;\n      }\n    );\n\n    if (!includeRaw) {\n      return llm.pipe(outputParser).withConfig({\n        runName: \"StructuredOutput\",\n      }) as Runnable<BaseLanguageModelInput, RunOutput>;\n    }\n\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input: any, config) => outputParser.invoke(input.raw, config),\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null,\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone],\n    });\n    return RunnableSequence.from<\n      BaseLanguageModelInput,\n      { raw: BaseMessage; parsed: RunOutput }\n    >([\n      {\n        raw: llm,\n      },\n      parsedWithFallback,\n    ]).withConfig({\n      runName: \"StructuredOutputRunnable\",\n    });\n  }\n}\n\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport abstract class SimpleChatModel<\n  CallOptions extends BaseChatModelCallOptions = BaseChatModelCallOptions,\n> extends BaseChatModel<CallOptions> {\n  abstract _call(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<string>;\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const text = await this._call(messages, options, runManager);\n    const message = new AIMessage(text);\n    if (typeof message.content !== \"string\") {\n      throw new Error(\n        \"Cannot generate with a simple chat model when output is not a string.\"\n      );\n    }\n    return {\n      generations: [\n        {\n          text: message.content,\n          message,\n        },\n      ],\n    };\n  }\n}\n", "import { Runnable, type RunnableBatchOptions } from \"./base.js\";\nimport { IterableReadableStream } from \"../utils/stream.js\";\nimport { ensureConfig, type RunnableConfig } from \"./config.js\";\n\nexport type RouterInput = {\n  key: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  input: any;\n};\n\n/**\n * A runnable that routes to a set of runnables based on Input['key'].\n * Returns the output of the selected runnable.\n * @example\n * ```typescript\n * import { RouterRunnable, RunnableLambda } from \"@langchain/core/runnables\";\n *\n * const router = new RouterRunnable({\n *   runnables: {\n *     toUpperCase: RunnableLambda.from((text: string) => text.toUpperCase()),\n *     reverseText: RunnableLambda.from((text: string) =>\n *       text.split(\"\").reverse().join(\"\")\n *     ),\n *   },\n * });\n *\n * // Invoke the 'reverseText' runnable\n * const result1 = router.invoke({ key: \"reverseText\", input: \"Hello World\" });\n *\n * // \"dlroW olleH\"\n *\n * // Invoke the 'toUpperCase' runnable\n * const result2 = router.invoke({ key: \"toUpperCase\", input: \"Hello World\" });\n *\n * // \"HELLO WORLD\"\n * ```\n */\nexport class RouterRunnable<\n  RunInput extends RouterInput,\n  RunnableInput,\n  RunOutput,\n> extends Runnable<RunInput, RunOutput> {\n  static lc_name() {\n    return \"RouterRunnable\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"runnables\"];\n\n  lc_serializable = true;\n\n  runnables: Record<string, Runnable<RunnableInput, RunOutput>>;\n\n  constructor(fields: {\n    runnables: Record<string, Runnable<RunnableInput, RunOutput>>;\n  }) {\n    super(fields);\n    this.runnables = fields.runnables;\n  }\n\n  async invoke(\n    input: RunInput,\n    options?: Partial<RunnableConfig>\n  ): Promise<RunOutput> {\n    const { key, input: actualInput } = input;\n    const runnable = this.runnables[key];\n    if (runnable === undefined) {\n      throw new Error(`No runnable associated with key \"${key}\".`);\n    }\n    return runnable.invoke(actualInput, ensureConfig(options));\n  }\n\n  async batch(\n    inputs: RunInput[],\n    options?: Partial<RunnableConfig> | Partial<RunnableConfig>[],\n    batchOptions?: RunnableBatchOptions & { returnExceptions?: false }\n  ): Promise<RunOutput[]>;\n\n  async batch(\n    inputs: RunInput[],\n    options?: Partial<RunnableConfig> | Partial<RunnableConfig>[],\n    batchOptions?: RunnableBatchOptions & { returnExceptions: true }\n  ): Promise<(RunOutput | Error)[]>;\n\n  async batch(\n    inputs: RunInput[],\n    options?: Partial<RunnableConfig> | Partial<RunnableConfig>[],\n    batchOptions?: RunnableBatchOptions\n  ): Promise<(RunOutput | Error)[]>;\n\n  async batch(\n    inputs: RunInput[],\n    options?: Partial<RunnableConfig> | Partial<RunnableConfig>[],\n    batchOptions?: RunnableBatchOptions\n  ): Promise<(RunOutput | Error)[]> {\n    const keys = inputs.map((input) => input.key);\n    const actualInputs = inputs.map((input) => input.input);\n    const missingKey = keys.find((key) => this.runnables[key] === undefined);\n    if (missingKey !== undefined) {\n      throw new Error(`One or more keys do not have a corresponding runnable.`);\n    }\n    const runnables = keys.map((key) => this.runnables[key]);\n    const optionsList = this._getOptionsList(options ?? {}, inputs.length);\n    const maxConcurrency =\n      optionsList[0]?.maxConcurrency ?? batchOptions?.maxConcurrency;\n    const batchSize =\n      maxConcurrency && maxConcurrency > 0 ? maxConcurrency : inputs.length;\n    const batchResults = [];\n    for (let i = 0; i < actualInputs.length; i += batchSize) {\n      const batchPromises = actualInputs\n        .slice(i, i + batchSize)\n        .map((actualInput, i) =>\n          runnables[i].invoke(actualInput, optionsList[i])\n        );\n      const batchResult = await Promise.all(batchPromises);\n      batchResults.push(batchResult);\n    }\n    return batchResults.flat();\n  }\n\n  async stream(\n    input: RunInput,\n    options?: Partial<RunnableConfig>\n  ): Promise<IterableReadableStream<RunOutput>> {\n    const { key, input: actualInput } = input;\n    const runnable = this.runnables[key];\n    if (runnable === undefined) {\n      throw new Error(`No runnable associated with key \"${key}\".`);\n    }\n    return runnable.stream(actualInput, options);\n  }\n}\n", "import {\n  Runnable,\n  RunnableLike,\n  _coerceToDict,\n  _coerceToRunnable,\n} from \"./base.js\";\nimport {\n  RunnableConfig,\n  getCallbackManagerForConfig,\n  patchConfig,\n} from \"./config.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\nimport { concat } from \"../utils/stream.js\";\n\n/**\n * Type for a branch in the RunnableBranch. It consists of a condition\n * runnable and a branch runnable. The condition runnable is used to\n * determine whether the branch should be executed, and the branch runnable\n * is executed if the condition is true.\n */\nexport type Branch<RunInput, RunOutput> = [\n  Runnable<RunInput, boolean>,\n  Runnable<RunInput, RunOutput>,\n];\n\nexport type BranchLike<RunInput, RunOutput> = [\n  RunnableLike<RunInput, boolean>,\n  RunnableLike<RunInput, RunOutput>,\n];\n\n/**\n * Class that represents a runnable branch. The RunnableBranch is\n * initialized with an array of branches and a default branch. When invoked,\n * it evaluates the condition of each branch in order and executes the\n * corresponding branch if the condition is true. If none of the conditions\n * are true, it executes the default branch.\n * @example\n * ```typescript\n * const branch = RunnableBranch.from([\n *   [\n *     (x: { topic: string; question: string }) =>\n *       x.topic.toLowerCase().includes(\"anthropic\"),\n *     anthropicChain,\n *   ],\n *   [\n *     (x: { topic: string; question: string }) =>\n *       x.topic.toLowerCase().includes(\"langchain\"),\n *     langChainChain,\n *   ],\n *   generalChain,\n * ]);\n *\n * const fullChain = RunnableSequence.from([\n *   {\n *     topic: classificationChain,\n *     question: (input: { question: string }) => input.question,\n *   },\n *   branch,\n * ]);\n *\n * const result = await fullChain.invoke({\n *   question: \"how do I use LangChain?\",\n * });\n * ```\n */\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport class RunnableBranch<RunInput = any, RunOutput = any> extends Runnable<\n  RunInput,\n  RunOutput\n> {\n  static lc_name() {\n    return \"RunnableBranch\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"runnables\"];\n\n  lc_serializable = true;\n\n  default: Runnable<RunInput, RunOutput>;\n\n  branches: Branch<RunInput, RunOutput>[];\n\n  constructor(fields: {\n    branches: Branch<RunInput, RunOutput>[];\n    default: Runnable<RunInput, RunOutput>;\n  }) {\n    super(fields);\n    this.branches = fields.branches;\n    this.default = fields.default;\n  }\n\n  /**\n   * Convenience method for instantiating a RunnableBranch from\n   * RunnableLikes (objects, functions, or Runnables).\n   *\n   * Each item in the input except for the last one should be a\n   * tuple with two items. The first is a \"condition\" RunnableLike that\n   * returns \"true\" if the second RunnableLike in the tuple should run.\n   *\n   * The final item in the input should be a RunnableLike that acts as a\n   * default branch if no other branches match.\n   *\n   * @example\n   * ```ts\n   * import { RunnableBranch } from \"@langchain/core/runnables\";\n   *\n   * const branch = RunnableBranch.from([\n   *   [(x: number) => x > 0, (x: number) => x + 1],\n   *   [(x: number) => x < 0, (x: number) => x - 1],\n   *   (x: number) => x\n   * ]);\n   * ```\n   * @param branches An array where the every item except the last is a tuple of [condition, runnable]\n   *   pairs. The last item is a default runnable which is invoked if no other condition matches.\n   * @returns A new RunnableBranch.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  static from<RunInput = any, RunOutput = any>(\n    branches: [\n      ...BranchLike<RunInput, RunOutput>[],\n      RunnableLike<RunInput, RunOutput>,\n    ]\n  ) {\n    if (branches.length < 1) {\n      throw new Error(\"RunnableBranch requires at least one branch\");\n    }\n    const branchLikes = branches.slice(0, -1) as BranchLike<\n      RunInput,\n      RunOutput\n    >[];\n    const coercedBranches: Branch<RunInput, RunOutput>[] = branchLikes.map(\n      ([condition, runnable]) => [\n        _coerceToRunnable(condition),\n        _coerceToRunnable(runnable),\n      ]\n    );\n    const defaultBranch = _coerceToRunnable(\n      branches[branches.length - 1] as RunnableLike<RunInput, RunOutput>\n    );\n    return new this({\n      branches: coercedBranches,\n      default: defaultBranch,\n    });\n  }\n\n  async _invoke(\n    input: RunInput,\n    config?: Partial<RunnableConfig>,\n    runManager?: CallbackManagerForChainRun\n  ): Promise<RunOutput> {\n    let result;\n    for (let i = 0; i < this.branches.length; i += 1) {\n      const [condition, branchRunnable] = this.branches[i];\n      const conditionValue = await condition.invoke(\n        input,\n        patchConfig(config, {\n          callbacks: runManager?.getChild(`condition:${i + 1}`),\n        })\n      );\n      if (conditionValue) {\n        result = await branchRunnable.invoke(\n          input,\n          patchConfig(config, {\n            callbacks: runManager?.getChild(`branch:${i + 1}`),\n          })\n        );\n        break;\n      }\n    }\n    if (!result) {\n      result = await this.default.invoke(\n        input,\n        patchConfig(config, {\n          callbacks: runManager?.getChild(\"branch:default\"),\n        })\n      );\n    }\n    return result;\n  }\n\n  async invoke(\n    input: RunInput,\n    config: RunnableConfig = {}\n  ): Promise<RunOutput> {\n    return this._callWithConfig(this._invoke, input, config);\n  }\n\n  async *_streamIterator(input: RunInput, config?: Partial<RunnableConfig>) {\n    const callbackManager_ = await getCallbackManagerForConfig(config);\n    const runManager = await callbackManager_?.handleChainStart(\n      this.toJSON(),\n      _coerceToDict(input, \"input\"),\n      config?.runId,\n      undefined,\n      undefined,\n      undefined,\n      config?.runName\n    );\n    let finalOutput;\n    let finalOutputSupported = true;\n    let stream;\n    try {\n      for (let i = 0; i < this.branches.length; i += 1) {\n        const [condition, branchRunnable] = this.branches[i];\n        const conditionValue = await condition.invoke(\n          input,\n          patchConfig(config, {\n            callbacks: runManager?.getChild(`condition:${i + 1}`),\n          })\n        );\n        if (conditionValue) {\n          stream = await branchRunnable.stream(\n            input,\n            patchConfig(config, {\n              callbacks: runManager?.getChild(`branch:${i + 1}`),\n            })\n          );\n          for await (const chunk of stream) {\n            yield chunk;\n            if (finalOutputSupported) {\n              if (finalOutput === undefined) {\n                finalOutput = chunk;\n              } else {\n                try {\n                  finalOutput = concat(finalOutput, chunk);\n                } catch {\n                  finalOutput = undefined;\n                  finalOutputSupported = false;\n                }\n              }\n            }\n          }\n          break;\n        }\n      }\n      if (stream === undefined) {\n        stream = await this.default.stream(\n          input,\n          patchConfig(config, {\n            callbacks: runManager?.getChild(\"branch:default\"),\n          })\n        );\n        for await (const chunk of stream) {\n          yield chunk;\n          if (finalOutputSupported) {\n            if (finalOutput === undefined) {\n              finalOutput = chunk;\n            } else {\n              try {\n                finalOutput = concat(finalOutput, chunk as RunOutput);\n              } catch {\n                finalOutput = undefined;\n                finalOutputSupported = false;\n              }\n            }\n          }\n        }\n      }\n    } catch (e) {\n      await runManager?.handleChainError(e);\n      throw e;\n    }\n    await runManager?.handleChainEnd(finalOutput ?? {});\n  }\n}\n", "import {\n  BaseChatMessageHistory,\n  BaseListChatMessageHistory,\n} from \"../chat_history.js\";\nimport {\n  AIMessage,\n  BaseMessage,\n  HumanMessage,\n  isBaseMessage,\n} from \"../messages/index.js\";\nimport { Run } from \"../tracers/base.js\";\nimport {\n  Runnable,\n  RunnableBinding,\n  type RunnableBindingArgs,\n  RunnableLambda,\n} from \"./base.js\";\nimport { RunnableConfig } from \"./config.js\";\nimport { RunnablePassthrough } from \"./passthrough.js\";\n\ntype GetSessionHistoryCallable = (\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  ...args: Array<any>\n) =>\n  | Promise<BaseChatMessageHistory | BaseListChatMessageHistory>\n  | BaseChatMessageHistory\n  | BaseListChatMessageHistory;\n\nexport interface RunnableWithMessageHistoryInputs<\n  RunInput,\n  RunOutput,\n> extends Omit<RunnableBindingArgs<RunInput, RunOutput>, \"bound\" | \"config\"> {\n  runnable: Runnable<RunInput, RunOutput>;\n  getMessageHistory: GetSessionHistoryCallable;\n  inputMessagesKey?: string;\n  outputMessagesKey?: string;\n  historyMessagesKey?: string;\n  config?: RunnableConfig;\n}\n\n/**\n * Wraps a LCEL chain and manages history. It appends input messages\n * and chain outputs as history, and adds the current history messages to\n * the chain input.\n * @example\n * ```typescript\n * // pnpm install @langchain/anthropic @langchain/community @upstash/redis\n *\n * import {\n *   ChatPromptTemplate,\n *   MessagesPlaceholder,\n * } from \"@langchain/core/prompts\";\n * import { ChatAnthropic } from \"@langchain/anthropic\";\n * import { UpstashRedisChatMessageHistory } from \"@langchain/community/stores/message/upstash_redis\";\n * // For demos, you can also use an in-memory store:\n * // import { ChatMessageHistory } from \"@langchain/classic/stores/message/in_memory\";\n *\n * const prompt = ChatPromptTemplate.fromMessages([\n *   [\"system\", \"You're an assistant who's good at {ability}\"],\n *   new MessagesPlaceholder(\"history\"),\n *   [\"human\", \"{question}\"],\n * ]);\n *\n * const chain = prompt.pipe(new ChatAnthropic({}));\n *\n * const chainWithHistory = new RunnableWithMessageHistory({\n *   runnable: chain,\n *   getMessageHistory: (sessionId) =>\n *     new UpstashRedisChatMessageHistory({\n *       sessionId,\n *       config: {\n *         url: process.env.UPSTASH_REDIS_REST_URL!,\n *         token: process.env.UPSTASH_REDIS_REST_TOKEN!,\n *       },\n *     }),\n *   inputMessagesKey: \"question\",\n *   historyMessagesKey: \"history\",\n * });\n *\n * const result = await chainWithHistory.invoke(\n *   {\n *     ability: \"math\",\n *     question: \"What does cosine mean?\",\n *   },\n *   {\n *     configurable: {\n *       sessionId: \"some_string_identifying_a_user\",\n *     },\n *   }\n * );\n *\n * const result2 = await chainWithHistory.invoke(\n *   {\n *     ability: \"math\",\n *     question: \"What's its inverse?\",\n *   },\n *   {\n *     configurable: {\n *       sessionId: \"some_string_identifying_a_user\",\n *     },\n *   }\n * );\n * ```\n */\nexport class RunnableWithMessageHistory<\n  RunInput,\n  RunOutput,\n> extends RunnableBinding<RunInput, RunOutput> {\n  runnable: Runnable<RunInput, RunOutput>;\n\n  inputMessagesKey?: string;\n\n  outputMessagesKey?: string;\n\n  historyMessagesKey?: string;\n\n  getMessageHistory: GetSessionHistoryCallable;\n\n  constructor(fields: RunnableWithMessageHistoryInputs<RunInput, RunOutput>) {\n    let historyChain: Runnable = RunnableLambda.from((input, options) =>\n      this._enterHistory(input, options ?? {})\n    ).withConfig({ runName: \"loadHistory\" });\n\n    const messagesKey = fields.historyMessagesKey ?? fields.inputMessagesKey;\n    if (messagesKey) {\n      historyChain = RunnablePassthrough.assign({\n        [messagesKey]: historyChain,\n      }).withConfig({ runName: \"insertHistory\" });\n    }\n\n    const bound = historyChain\n      .pipe(\n        fields.runnable.withListeners({\n          onEnd: (run, config) => this._exitHistory(run, config ?? {}),\n        })\n      )\n      .withConfig({ runName: \"RunnableWithMessageHistory\" });\n\n    const config = fields.config ?? {};\n\n    super({\n      ...fields,\n      config,\n      bound,\n    });\n    this.runnable = fields.runnable;\n    this.getMessageHistory = fields.getMessageHistory;\n    this.inputMessagesKey = fields.inputMessagesKey;\n    this.outputMessagesKey = fields.outputMessagesKey;\n    this.historyMessagesKey = fields.historyMessagesKey;\n  }\n\n  _getInputMessages(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    inputValue: string | BaseMessage | Array<BaseMessage> | Record<string, any>\n  ): Array<BaseMessage> {\n    let parsedInputValue;\n    if (\n      typeof inputValue === \"object\" &&\n      !Array.isArray(inputValue) &&\n      !isBaseMessage(inputValue)\n    ) {\n      let key;\n      if (this.inputMessagesKey) {\n        key = this.inputMessagesKey;\n      } else if (Object.keys(inputValue).length === 1) {\n        key = Object.keys(inputValue)[0];\n      } else {\n        key = \"input\";\n      }\n      if (Array.isArray(inputValue[key]) && Array.isArray(inputValue[key][0])) {\n        parsedInputValue = inputValue[key][0];\n      } else {\n        parsedInputValue = inputValue[key];\n      }\n    } else {\n      parsedInputValue = inputValue;\n    }\n    if (typeof parsedInputValue === \"string\") {\n      return [new HumanMessage(parsedInputValue)];\n    } else if (Array.isArray(parsedInputValue)) {\n      return parsedInputValue;\n    } else if (isBaseMessage(parsedInputValue)) {\n      return [parsedInputValue];\n    } else {\n      throw new Error(\n        `Expected a string, BaseMessage, or array of BaseMessages.\\nGot ${JSON.stringify(\n          parsedInputValue,\n          null,\n          2\n        )}`\n      );\n    }\n  }\n\n  _getOutputMessages(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    outputValue: string | BaseMessage | Array<BaseMessage> | Record<string, any>\n  ): Array<BaseMessage> {\n    let parsedOutputValue;\n    if (\n      !Array.isArray(outputValue) &&\n      !isBaseMessage(outputValue) &&\n      typeof outputValue !== \"string\"\n    ) {\n      let key;\n      if (this.outputMessagesKey !== undefined) {\n        key = this.outputMessagesKey;\n      } else if (Object.keys(outputValue).length === 1) {\n        key = Object.keys(outputValue)[0];\n      } else {\n        key = \"output\";\n      }\n      // If you are wrapping a chat model directly\n      // The output is actually this weird generations object\n      if (outputValue.generations !== undefined) {\n        parsedOutputValue = outputValue.generations[0][0].message;\n      } else {\n        parsedOutputValue = outputValue[key];\n      }\n    } else {\n      parsedOutputValue = outputValue;\n    }\n\n    if (typeof parsedOutputValue === \"string\") {\n      return [new AIMessage(parsedOutputValue)];\n    } else if (Array.isArray(parsedOutputValue)) {\n      return parsedOutputValue;\n    } else if (isBaseMessage(parsedOutputValue)) {\n      return [parsedOutputValue];\n    } else {\n      throw new Error(\n        `Expected a string, BaseMessage, or array of BaseMessages. Received: ${JSON.stringify(\n          parsedOutputValue,\n          null,\n          2\n        )}`\n      );\n    }\n  }\n\n  async _enterHistory(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    input: any,\n    kwargs?: RunnableConfig\n  ): Promise<BaseMessage[]> {\n    const history = kwargs?.configurable?.messageHistory;\n    const messages = await history.getMessages();\n    if (this.historyMessagesKey === undefined) {\n      return messages.concat(this._getInputMessages(input));\n    }\n    return messages;\n  }\n\n  async _exitHistory(run: Run, config: RunnableConfig): Promise<void> {\n    const history = config.configurable?.messageHistory;\n\n    // Get input messages\n    let inputs;\n    // Chat model inputs are nested arrays\n    if (Array.isArray(run.inputs) && Array.isArray(run.inputs[0])) {\n      inputs = run.inputs[0];\n    } else {\n      inputs = run.inputs;\n    }\n    let inputMessages = this._getInputMessages(inputs);\n    // If historic messages were prepended to the input messages, remove them to\n    // avoid adding duplicate messages to history.\n    if (this.historyMessagesKey === undefined) {\n      const existingMessages = await history.getMessages();\n      inputMessages = inputMessages.slice(existingMessages.length);\n    }\n    // Get output messages\n    const outputValue = run.outputs;\n    if (!outputValue) {\n      throw new Error(\n        `Output values from 'Run' undefined. Run: ${JSON.stringify(\n          run,\n          null,\n          2\n        )}`\n      );\n    }\n    const outputMessages = this._getOutputMessages(outputValue);\n    await history.addMessages([...inputMessages, ...outputMessages]);\n  }\n\n  async _mergeConfig(...configs: Array<RunnableConfig | undefined>) {\n    const config = await super._mergeConfig(...configs);\n    // Extract sessionId\n    if (!config.configurable || !config.configurable.sessionId) {\n      const exampleInput = {\n        [this.inputMessagesKey ?? \"input\"]: \"foo\",\n      };\n      const exampleConfig = { configurable: { sessionId: \"123\" } };\n      throw new Error(\n        `sessionId is required. Pass it in as part of the config argument to .invoke() or .stream()\\n` +\n          `eg. chain.invoke(${JSON.stringify(exampleInput)}, ${JSON.stringify(\n            exampleConfig\n          )})`\n      );\n    }\n    // attach messageHistory\n    const { sessionId } = config.configurable;\n    config.configurable.messageHistory =\n      await this.getMessageHistory(sessionId);\n    return config;\n  }\n}\n", "import { __exportAll } from \"../_virtual/_rolldown/runtime.js\";\nimport { ensureConfig, getCallbackManagerForConfig, mergeConfigs, patchConfig, pickRunnableConfigKeys } from \"./config.js\";\nimport { raceWithSignal } from \"../utils/signal.js\";\nimport { Runnable, RunnableAssign, RunnableBinding, RunnableEach, RunnableLambda, RunnableMap, RunnableParallel, RunnablePick, RunnableRetry, RunnableSequence, RunnableToolLike, RunnableWithFallbacks, _coerceToRunnable } from \"./base.js\";\nimport { RunnablePassthrough } from \"./passthrough.js\";\nimport { RouterRunnable } from \"./router.js\";\nimport { RunnableBranch } from \"./branch.js\";\nimport { RunnableWithMessageHistory } from \"./history.js\";\n\n//#region src/runnables/index.ts\nvar runnables_exports = /* @__PURE__ */ __exportAll({\n\tRouterRunnable: () => RouterRunnable,\n\tRunnable: () => Runnable,\n\tRunnableAssign: () => RunnableAssign,\n\tRunnableBinding: () => RunnableBinding,\n\tRunnableBranch: () => RunnableBranch,\n\tRunnableEach: () => RunnableEach,\n\tRunnableLambda: () => RunnableLambda,\n\tRunnableMap: () => RunnableMap,\n\tRunnableParallel: () => RunnableParallel,\n\tRunnablePassthrough: () => RunnablePassthrough,\n\tRunnablePick: () => RunnablePick,\n\tRunnableRetry: () => RunnableRetry,\n\tRunnableSequence: () => RunnableSequence,\n\tRunnableToolLike: () => RunnableToolLike,\n\tRunnableWithFallbacks: () => RunnableWithFallbacks,\n\tRunnableWithMessageHistory: () => RunnableWithMessageHistory,\n\t_coerceToRunnable: () => _coerceToRunnable,\n\tensureConfig: () => ensureConfig,\n\tgetCallbackManagerForConfig: () => getCallbackManagerForConfig,\n\tmergeConfigs: () => mergeConfigs,\n\tpatchConfig: () => patchConfig,\n\tpickRunnableConfigKeys: () => pickRunnableConfigKeys,\n\traceWithSignal: () => raceWithSignal\n});\n\n//#endregion\nexport { RouterRunnable, Runnable, RunnableAssign, RunnableBinding, RunnableBranch, RunnableEach, RunnableLambda, RunnableMap, RunnableParallel, RunnablePassthrough, RunnablePick, RunnableRetry, RunnableSequence, RunnableToolLike, RunnableWithFallbacks, RunnableWithMessageHistory, _coerceToRunnable, ensureConfig, getCallbackManagerForConfig, mergeConfigs, patchConfig, pickRunnableConfigKeys, raceWithSignal, runnables_exports };\n//# sourceMappingURL=index.js.map", "import { Runnable } from \"../runnables/index.js\";\nimport type { RunnableConfig } from \"../runnables/config.js\";\nimport type { BasePromptValueInterface } from \"../prompt_values.js\";\nimport type { BaseMessage, ContentBlock } from \"../messages/index.js\";\nimport type { Callbacks } from \"../callbacks/manager.js\";\nimport type { Generation, ChatGeneration } from \"../outputs.js\";\nimport { addLangChainErrorFields } from \"../errors/index.js\";\n\n/**\n * Options for formatting instructions.\n */\nexport interface FormatInstructionsOptions {}\n\n/**\n * Abstract base class for parsing the output of a Large Language Model\n * (LLM) call. It provides methods for parsing the result of an LLM call\n * and invoking the parser with a given input.\n */\nexport abstract class BaseLLMOutputParser<T = unknown> extends Runnable<\n  string | BaseMessage,\n  T\n> {\n  /**\n   * Parses the result of an LLM call. This method is meant to be\n   * implemented by subclasses to define how the output from the LLM should\n   * be parsed.\n   * @param generations The generations from an LLM call.\n   * @param callbacks Optional callbacks.\n   * @returns A promise of the parsed output.\n   */\n  abstract parseResult(\n    generations: Generation[] | ChatGeneration[],\n    callbacks?: Callbacks\n  ): Promise<T>;\n\n  /**\n   * Parses the result of an LLM call with a given prompt. By default, it\n   * simply calls `parseResult`.\n   * @param generations The generations from an LLM call.\n   * @param _prompt The prompt used in the LLM call.\n   * @param callbacks Optional callbacks.\n   * @returns A promise of the parsed output.\n   */\n  parseResultWithPrompt(\n    generations: Generation[] | ChatGeneration[],\n    _prompt: BasePromptValueInterface,\n    callbacks?: Callbacks\n  ): Promise<T> {\n    return this.parseResult(generations, callbacks);\n  }\n\n  protected _baseMessageToString(message: BaseMessage): string {\n    return typeof message.content === \"string\"\n      ? message.content\n      : this._baseMessageContentToString(message.content);\n  }\n\n  protected _baseMessageContentToString(content: ContentBlock[]): string {\n    return JSON.stringify(content);\n  }\n\n  /**\n   * Calls the parser with a given input and optional configuration options.\n   * If the input is a string, it creates a generation with the input as\n   * text and calls `parseResult`. If the input is a `BaseMessage`, it\n   * creates a generation with the input as a message and the content of the\n   * input as text, and then calls `parseResult`.\n   * @param input The input to the parser, which can be a string or a `BaseMessage`.\n   * @param options Optional configuration options.\n   * @returns A promise of the parsed output.\n   */\n  async invoke(\n    input: string | BaseMessage,\n    options?: RunnableConfig\n  ): Promise<T> {\n    if (typeof input === \"string\") {\n      return this._callWithConfig(\n        async (input: string, options): Promise<T> =>\n          this.parseResult([{ text: input }], options?.callbacks),\n        input,\n        { ...options, runType: \"parser\" }\n      );\n    } else {\n      return this._callWithConfig(\n        async (input: BaseMessage, options): Promise<T> =>\n          this.parseResult(\n            [\n              {\n                message: input,\n                text: this._baseMessageToString(input),\n              },\n            ],\n            options?.callbacks\n          ),\n        input,\n        { ...options, runType: \"parser\" }\n      );\n    }\n  }\n}\n\n/**\n * Class to parse the output of an LLM call.\n */\nexport abstract class BaseOutputParser<\n  T = unknown,\n> extends BaseLLMOutputParser<T> {\n  parseResult(\n    generations: Generation[] | ChatGeneration[],\n    callbacks?: Callbacks\n  ): Promise<T> {\n    return this.parse(generations[0].text, callbacks);\n  }\n\n  /**\n   * Parse the output of an LLM call.\n   *\n   * @param text - LLM output to parse.\n   * @returns Parsed output.\n   */\n  abstract parse(text: string, callbacks?: Callbacks): Promise<T>;\n\n  async parseWithPrompt(\n    text: string,\n    _prompt: BasePromptValueInterface,\n    callbacks?: Callbacks\n  ): Promise<T> {\n    return this.parse(text, callbacks);\n  }\n\n  /**\n   * Return a string describing the format of the output.\n   * @returns Format instructions.\n   * @param options - Options for formatting instructions.\n   * @example\n   * ```json\n   * {\n   *  \"foo\": \"bar\"\n   * }\n   * ```\n   */\n  abstract getFormatInstructions(options?: FormatInstructionsOptions): string;\n\n  /**\n   * Return the string type key uniquely identifying this class of parser\n   */\n  _type(): string {\n    throw new Error(\"_type not implemented\");\n  }\n}\n\n/**\n * Exception that output parsers should raise to signify a parsing error.\n *\n * This exists to differentiate parsing errors from other code or execution errors\n * that also may arise inside the output parser. OutputParserExceptions will be\n * available to catch and handle in ways to fix the parsing error, while other\n * errors will be raised.\n *\n * @param message - The error that's being re-raised or an error message.\n * @param llmOutput - String model output which is error-ing.\n * @param observation - String explanation of error which can be passed to a\n *     model to try and remediate the issue.\n * @param sendToLLM - Whether to send the observation and llm_output back to an Agent\n *     after an OutputParserException has been raised. This gives the underlying\n *     model driving the agent the context that the previous output was improperly\n *     structured, in the hopes that it will update the output to the correct\n *     format.\n */\nexport class OutputParserException extends Error {\n  llmOutput?: string;\n\n  observation?: string;\n\n  sendToLLM: boolean;\n\n  constructor(\n    message: string,\n    llmOutput?: string,\n    observation?: string,\n    sendToLLM = false\n  ) {\n    super(message);\n    this.llmOutput = llmOutput;\n    this.observation = observation;\n    this.sendToLLM = sendToLLM;\n\n    if (sendToLLM) {\n      if (observation === undefined || llmOutput === undefined) {\n        throw new Error(\n          \"Arguments 'observation' & 'llmOutput' are required if 'sendToLlm' is true\"\n        );\n      }\n    }\n\n    addLangChainErrorFields(this, \"OUTPUT_PARSING_FAILURE\");\n  }\n}\n", "import { deepCompareStrict } from \"@cfworker/json-schema\";\nimport { BaseOutputParser } from \"./base.js\";\nimport {\n  type BaseMessage,\n  isBaseMessage,\n  isBaseMessageChunk,\n} from \"../messages/base.js\";\nimport { convertToChunk } from \"../messages/utils.js\";\nimport type { BaseCallbackConfig } from \"../callbacks/manager.js\";\nimport {\n  type Generation,\n  type ChatGeneration,\n  GenerationChunk,\n  ChatGenerationChunk,\n} from \"../outputs.js\";\n\n/**\n * Class to parse the output of an LLM call that also allows streaming inputs.\n */\nexport abstract class BaseTransformOutputParser<\n  T = unknown,\n> extends BaseOutputParser<T> {\n  async *_transform(\n    inputGenerator: AsyncGenerator<string | BaseMessage>\n  ): AsyncGenerator<T> {\n    for await (const chunk of inputGenerator) {\n      if (typeof chunk === \"string\") {\n        yield this.parseResult([{ text: chunk }]);\n      } else {\n        yield this.parseResult([\n          {\n            message: chunk,\n            text: this._baseMessageToString(chunk),\n          },\n        ]);\n      }\n    }\n  }\n\n  /**\n   * Transforms an asynchronous generator of input into an asynchronous\n   * generator of parsed output.\n   * @param inputGenerator An asynchronous generator of input.\n   * @param options A configuration object.\n   * @returns An asynchronous generator of parsed output.\n   */\n  async *transform(\n    inputGenerator: AsyncGenerator<string | BaseMessage>,\n    options: BaseCallbackConfig\n  ): AsyncGenerator<T> {\n    yield* this._transformStreamWithConfig(\n      inputGenerator,\n      this._transform.bind(this),\n      {\n        ...options,\n        runType: \"parser\",\n      }\n    );\n  }\n}\n\nexport type BaseCumulativeTransformOutputParserInput = { diff?: boolean };\n\n/**\n * A base class for output parsers that can handle streaming input. It\n * extends the `BaseTransformOutputParser` class and provides a method for\n * converting parsed outputs into a diff format.\n */\nexport abstract class BaseCumulativeTransformOutputParser<\n  T = unknown,\n> extends BaseTransformOutputParser<T> {\n  protected diff = false;\n\n  constructor(fields?: BaseCumulativeTransformOutputParserInput) {\n    super(fields);\n    this.diff = fields?.diff ?? this.diff;\n  }\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  protected abstract _diff(prev: any | undefined, next: any): any;\n\n  abstract parsePartialResult(\n    generations: Generation[] | ChatGeneration[]\n  ): Promise<T | undefined>;\n\n  async *_transform(\n    inputGenerator: AsyncGenerator<string | BaseMessage>\n  ): AsyncGenerator<T> {\n    let prevParsed: T | undefined;\n    let accGen: GenerationChunk | undefined;\n    for await (const chunk of inputGenerator) {\n      if (typeof chunk !== \"string\" && typeof chunk.content !== \"string\") {\n        throw new Error(\"Cannot handle non-string output.\");\n      }\n      let chunkGen: GenerationChunk;\n      if (isBaseMessageChunk(chunk)) {\n        if (typeof chunk.content !== \"string\") {\n          throw new Error(\"Cannot handle non-string message output.\");\n        }\n        chunkGen = new ChatGenerationChunk({\n          message: chunk,\n          text: chunk.content,\n        });\n      } else if (isBaseMessage(chunk)) {\n        if (typeof chunk.content !== \"string\") {\n          throw new Error(\"Cannot handle non-string message output.\");\n        }\n        chunkGen = new ChatGenerationChunk({\n          message: convertToChunk(chunk),\n          text: chunk.content,\n        });\n      } else {\n        chunkGen = new GenerationChunk({ text: chunk });\n      }\n\n      if (accGen === undefined) {\n        accGen = chunkGen;\n      } else {\n        accGen = accGen.concat(chunkGen);\n      }\n\n      const parsed = await this.parsePartialResult([accGen]);\n      if (\n        parsed !== undefined &&\n        parsed !== null &&\n        !deepCompareStrict(parsed, prevParsed)\n      ) {\n        if (this.diff) {\n          yield this._diff(prevParsed, parsed);\n        } else {\n          yield parsed;\n        }\n        prevParsed = parsed;\n      }\n    }\n  }\n\n  getFormatInstructions(): string {\n    return \"\";\n  }\n}\n", "import { BaseTransformOutputParser } from \"./transform.js\";\n\n/**\n * OutputParser that parses LLMResult into the top likely string and\n * encodes it into bytes.\n */\nexport class BytesOutputParser extends BaseTransformOutputParser<Uint8Array> {\n  static lc_name() {\n    return \"BytesOutputParser\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"output_parsers\", \"bytes\"];\n\n  lc_serializable = true;\n\n  protected textEncoder: InstanceType<typeof TextEncoder> = new TextEncoder();\n\n  parse(text: string): Promise<Uint8Array> {\n    return Promise.resolve(this.textEncoder.encode(text));\n  }\n\n  getFormatInstructions(): string {\n    return \"\";\n  }\n}\n", "import { BaseMessage } from \"../messages/index.js\";\nimport { OutputParserException } from \"./base.js\";\nimport { BaseTransformOutputParser } from \"./transform.js\";\n\n/**\n * Class to parse the output of an LLM call to a list.\n * @augments BaseOutputParser\n */\nexport abstract class ListOutputParser extends BaseTransformOutputParser<\n  string[]\n> {\n  re?: RegExp;\n\n  async *_transform(\n    inputGenerator: AsyncGenerator<string | BaseMessage>\n  ): AsyncGenerator<string[]> {\n    let buffer = \"\";\n    for await (const input of inputGenerator) {\n      if (typeof input === \"string\") {\n        // add current chunk to buffer\n        buffer += input;\n      } else {\n        // extract message content and add to buffer\n        buffer += input.content;\n      }\n      // get parts in buffer\n      if (!this.re) {\n        const parts = await this.parse(buffer);\n        if (parts.length > 1) {\n          // if there are multiple parts, yield all but the last one\n          for (const part of parts.slice(0, -1)) {\n            yield [part];\n          }\n          // keep the last part in the buffer\n          buffer = parts[parts.length - 1];\n        }\n      } else {\n        // if there is a regex, get all matches\n        const matches = [...buffer.matchAll(this.re)];\n        if (matches.length > 1) {\n          let doneIdx = 0;\n          // if there are multiple matches, yield all but the last one\n          for (const match of matches.slice(0, -1)) {\n            yield [match[1]];\n            doneIdx += (match.index ?? 0) + match[0].length;\n          }\n          // keep the last match in the buffer\n          buffer = buffer.slice(doneIdx);\n        }\n      }\n    }\n\n    // yield the last part\n    for (const part of await this.parse(buffer)) {\n      yield [part];\n    }\n  }\n}\n\n/**\n * Class to parse the output of an LLM call as a comma-separated list.\n * @augments ListOutputParser\n */\nexport class CommaSeparatedListOutputParser extends ListOutputParser {\n  static lc_name() {\n    return \"CommaSeparatedListOutputParser\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"output_parsers\", \"list\"];\n\n  lc_serializable = true;\n\n  /**\n   * Parses the given text into an array of strings, using a comma as the\n   * separator. If the parsing fails, throws an OutputParserException.\n   * @param text The text to parse.\n   * @returns An array of strings obtained by splitting the input text at each comma.\n   */\n  async parse(text: string): Promise<string[]> {\n    try {\n      return text\n        .trim()\n        .split(\",\")\n        .map((s) => s.trim());\n    } catch {\n      throw new OutputParserException(`Could not parse output: ${text}`, text);\n    }\n  }\n\n  /**\n   * Provides instructions on the expected format of the response for the\n   * CommaSeparatedListOutputParser.\n   * @returns A string containing instructions on the expected format of the response.\n   */\n  getFormatInstructions(): string {\n    return `Your response should be a list of comma separated values, eg: \\`foo, bar, baz\\``;\n  }\n}\n\n/**\n * Class to parse the output of an LLM call to a list with a specific length and separator.\n * @augments ListOutputParser\n */\nexport class CustomListOutputParser extends ListOutputParser {\n  lc_namespace = [\"langchain_core\", \"output_parsers\", \"list\"];\n\n  private length: number | undefined;\n\n  private separator: string;\n\n  constructor({ length, separator }: { length?: number; separator?: string }) {\n    super(...arguments);\n    this.length = length;\n    this.separator = separator || \",\";\n  }\n\n  /**\n   * Parses the given text into an array of strings, using the specified\n   * separator. If the parsing fails or the number of items in the list\n   * doesn't match the expected length, throws an OutputParserException.\n   * @param text The text to parse.\n   * @returns An array of strings obtained by splitting the input text at each occurrence of the specified separator.\n   */\n  async parse(text: string): Promise<string[]> {\n    try {\n      const items = text\n        .trim()\n        .split(this.separator)\n        .map((s) => s.trim());\n      if (this.length !== undefined && items.length !== this.length) {\n        throw new OutputParserException(\n          `Incorrect number of items. Expected ${this.length}, got ${items.length}.`\n        );\n      }\n      return items;\n    } catch (e) {\n      if (Object.getPrototypeOf(e) === OutputParserException.prototype) {\n        throw e;\n      }\n      throw new OutputParserException(`Could not parse output: ${text}`);\n    }\n  }\n\n  /**\n   * Provides instructions on the expected format of the response for the\n   * CustomListOutputParser, including the number of items and the\n   * separator.\n   * @returns A string containing instructions on the expected format of the response.\n   */\n  getFormatInstructions(): string {\n    return `Your response should be a list of ${\n      this.length === undefined ? \"\" : `${this.length} `\n    }items separated by \"${this.separator}\" (eg: \\`foo${this.separator} bar${\n      this.separator\n    } baz\\`)`;\n  }\n}\n\nexport class NumberedListOutputParser extends ListOutputParser {\n  static lc_name() {\n    return \"NumberedListOutputParser\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"output_parsers\", \"list\"];\n\n  lc_serializable = true;\n\n  getFormatInstructions(): string {\n    return `Your response should be a numbered list with each item on a new line. For example: \\n\\n1. foo\\n\\n2. bar\\n\\n3. baz`;\n  }\n\n  re = /\\d+\\.\\s([^\\n]+)/g;\n\n  async parse(text: string): Promise<string[]> {\n    return [...(text.matchAll(this.re) ?? [])].map((m) => m[1]);\n  }\n}\n\nexport class MarkdownListOutputParser extends ListOutputParser {\n  static lc_name() {\n    return \"NumberedListOutputParser\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"output_parsers\", \"list\"];\n\n  lc_serializable = true;\n\n  getFormatInstructions(): string {\n    return `Your response should be a numbered list with each item on a new line. For example: \\n\\n1. foo\\n\\n2. bar\\n\\n3. baz`;\n  }\n\n  re = /^\\s*[-*]\\s([^\\n]+)$/gm;\n\n  async parse(text: string): Promise<string[]> {\n    return [...(text.matchAll(this.re) ?? [])].map((m) => m[1]);\n  }\n}\n", "import { BaseTransformOutputParser } from \"./transform.js\";\nimport { ContentBlock } from \"../messages/index.js\";\n\n/**\n * OutputParser that parses LLMResult into the top likely string.\n * @example\n * ```typescript\n * const promptTemplate = PromptTemplate.fromTemplate(\n *   \"Tell me a joke about {topic}\",\n * );\n *\n * const chain = RunnableSequence.from([\n *   promptTemplate,\n *   new ChatOpenAI({ model: \"gpt-4o-mini\" }),\n *   new StringOutputParser(),\n * ]);\n *\n * const result = await chain.invoke({ topic: \"bears\" });\n * console.log(\"What do you call a bear with no teeth? A gummy bear!\");\n * ```\n */\nexport class StringOutputParser extends BaseTransformOutputParser<string> {\n  static lc_name() {\n    return \"StrOutputParser\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"output_parsers\", \"string\"];\n\n  lc_serializable = true;\n\n  /**\n   * Parses a string output from an LLM call. This method is meant to be\n   * implemented by subclasses to define how a string output from an LLM\n   * should be parsed.\n   * @param text The string output from an LLM call.\n   * @param callbacks Optional callbacks.\n   * @returns A promise of the parsed output.\n   */\n  parse(text: string): Promise<string> {\n    return Promise.resolve(text);\n  }\n\n  getFormatInstructions(): string {\n    return \"\";\n  }\n\n  protected _textContentToString(content: ContentBlock.Text): string {\n    return content.text;\n  }\n\n  protected _imageUrlContentToString(\n    _content: ContentBlock.Data.URLContentBlock\n  ): string {\n    throw new Error(\n      `Cannot coerce a multimodal \"image_url\" message part into a string.`\n    );\n  }\n\n  protected _messageContentToString(content: ContentBlock): string {\n    switch (content.type) {\n      case \"text\":\n      case \"text_delta\":\n        if (\"text\" in content) {\n          // Type guard for MessageContentText\n          return this._textContentToString(content as ContentBlock.Text);\n        }\n        break;\n      case \"image_url\":\n        if (\"image_url\" in content) {\n          // Type guard for MessageContentImageUrl\n          return this._imageUrlContentToString(\n            content as ContentBlock.Data.URLContentBlock\n          );\n        }\n        break;\n      case \"reasoning\":\n      case \"thinking\":\n      case \"redacted_thinking\":\n        return \"\";\n      default:\n        throw new Error(\n          `Cannot coerce \"${content.type}\" message part into a string.`\n        );\n    }\n    throw new Error(`Invalid content type: ${content.type}`);\n  }\n\n  protected _baseMessageContentToString(content: ContentBlock[]): string {\n    return content.reduce(\n      (acc: string, item: ContentBlock) =>\n        acc + this._messageContentToString(item),\n      \"\"\n    );\n  }\n}\n", "import { z } from \"zod/v3\";\nimport {\n  BaseOutputParser,\n  FormatInstructionsOptions,\n  OutputParserException,\n} from \"./base.js\";\nimport {\n  type InteropZodType,\n  type InferInteropZodOutput,\n  interopParseAsync,\n} from \"../utils/types/zod.js\";\nimport {\n  toJsonSchema,\n  type JsonSchema7Type,\n  type JsonSchema7ArrayType,\n  type JsonSchema7ObjectType,\n  type JsonSchema7StringType,\n  type JsonSchema7NumberType,\n  type JsonSchema7NullableType,\n} from \"../utils/json_schema.js\";\n\nexport type JsonMarkdownStructuredOutputParserInput = {\n  interpolationDepth?: number;\n};\n\nexport interface JsonMarkdownFormatInstructionsOptions extends FormatInstructionsOptions {\n  interpolationDepth?: number;\n}\n\nexport class StructuredOutputParser<\n  T extends InteropZodType,\n> extends BaseOutputParser<InferInteropZodOutput<T>> {\n  static lc_name() {\n    return \"StructuredOutputParser\";\n  }\n\n  lc_namespace = [\"langchain\", \"output_parsers\", \"structured\"];\n\n  toJSON() {\n    return this.toJSONNotImplemented();\n  }\n\n  constructor(public schema: T) {\n    super(schema);\n  }\n\n  /**\n   * Creates a new StructuredOutputParser from a Zod schema.\n   * @param schema The Zod schema which the output should match\n   * @returns A new instance of StructuredOutputParser.\n   */\n  static fromZodSchema<T extends InteropZodType>(schema: T) {\n    return new this(schema);\n  }\n\n  /**\n   * Creates a new StructuredOutputParser from a set of names and\n   * descriptions.\n   * @param schemas An object where each key is a name and each value is a description\n   * @returns A new instance of StructuredOutputParser.\n   */\n  static fromNamesAndDescriptions<S extends { [key: string]: string }>(\n    schemas: S\n  ) {\n    const zodSchema = z.object(\n      Object.fromEntries(\n        Object.entries(schemas).map(\n          ([name, description]) =>\n            [name, z.string().describe(description)] as const\n        )\n      )\n    );\n\n    return new this(zodSchema);\n  }\n\n  /**\n   * Returns a markdown code snippet with a JSON object formatted according\n   * to the schema.\n   * @param options Optional. The options for formatting the instructions\n   * @returns A markdown code snippet with a JSON object formatted according to the schema.\n   */\n  getFormatInstructions(): string {\n    return `You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n\nFor example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!\n\nHere is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:\n\\`\\`\\`json\n${JSON.stringify(toJsonSchema(this.schema))}\n\\`\\`\\`\n`;\n  }\n\n  /**\n   * Parses the given text according to the schema.\n   * @param text The text to parse\n   * @returns The parsed output.\n   */\n  async parse(text: string): Promise<InferInteropZodOutput<T>> {\n    try {\n      const trimmedText = text.trim();\n\n      const json =\n        // first case: if back ticks appear at the start of the text\n        trimmedText.match(/^```(?:json)?\\s*([\\s\\S]*?)```/)?.[1] ||\n        // second case: if back ticks with `json` appear anywhere in the text\n        trimmedText.match(/```json\\s*([\\s\\S]*?)```/)?.[1] ||\n        // otherwise, return the trimmed text\n        trimmedText;\n\n      const escapedJson = json\n        .replace(/\"([^\"\\\\]*(\\\\.[^\"\\\\]*)*)\"/g, (_match, capturedGroup) => {\n          const escapedInsideQuotes = capturedGroup.replace(/\\n/g, \"\\\\n\");\n          return `\"${escapedInsideQuotes}\"`;\n        })\n        .replace(/\\n/g, \"\");\n\n      return await interopParseAsync(this.schema, JSON.parse(escapedJson));\n    } catch (e) {\n      throw new OutputParserException(\n        `Failed to parse. Text: \"${text}\". Error: ${e}`,\n        text\n      );\n    }\n  }\n}\n\n/**\n * A specific type of `StructuredOutputParser` that parses JSON data\n * formatted as a markdown code snippet.\n */\nexport class JsonMarkdownStructuredOutputParser<\n  T extends InteropZodType,\n> extends StructuredOutputParser<T> {\n  static lc_name() {\n    return \"JsonMarkdownStructuredOutputParser\";\n  }\n\n  getFormatInstructions(\n    options?: JsonMarkdownFormatInstructionsOptions\n  ): string {\n    const interpolationDepth = options?.interpolationDepth ?? 1;\n    if (interpolationDepth < 1) {\n      throw new Error(\"f string interpolation depth must be at least 1\");\n    }\n\n    return `Return a markdown code snippet with a JSON object formatted to look like:\\n\\`\\`\\`json\\n${this._schemaToInstruction(\n      toJsonSchema(this.schema)\n    )\n      .replaceAll(\"{\", \"{\".repeat(interpolationDepth))\n      .replaceAll(\"}\", \"}\".repeat(interpolationDepth))}\\n\\`\\`\\``;\n  }\n\n  private _schemaToInstruction(\n    schemaInput: JsonSchema7Type,\n    indent = 2\n  ): string {\n    const schema = schemaInput as Extract<\n      JsonSchema7Type,\n      | JsonSchema7ObjectType\n      | JsonSchema7ArrayType\n      | JsonSchema7StringType\n      | JsonSchema7NumberType\n      | JsonSchema7NullableType\n    >;\n\n    if (\"type\" in schema) {\n      let nullable = false;\n      let type: string;\n      if (Array.isArray(schema.type)) {\n        const nullIdx = schema.type.findIndex((type) => type === \"null\");\n        if (nullIdx !== -1) {\n          nullable = true;\n          schema.type.splice(nullIdx, 1);\n        }\n        type = schema.type.join(\" | \") as string;\n      } else {\n        type = schema.type;\n      }\n\n      if (schema.type === \"object\" && schema.properties) {\n        const description = schema.description\n          ? ` // ${schema.description}`\n          : \"\";\n        const properties = Object.entries(schema.properties)\n          .map(([key, value]) => {\n            const isOptional = schema.required?.includes(key)\n              ? \"\"\n              : \" (optional)\";\n            return `${\" \".repeat(indent)}\"${key}\": ${this._schemaToInstruction(\n              value,\n              indent + 2\n            )}${isOptional}`;\n          })\n          .join(\"\\n\");\n        return `{\\n${properties}\\n${\" \".repeat(indent - 2)}}${description}`;\n      }\n      if (schema.type === \"array\" && schema.items) {\n        const description = schema.description\n          ? ` // ${schema.description}`\n          : \"\";\n        return `array[\\n${\" \".repeat(indent)}${this._schemaToInstruction(\n          schema.items,\n          indent + 2\n        )}\\n${\" \".repeat(indent - 2)}] ${description}`;\n      }\n      const isNullable = nullable ? \" (nullable)\" : \"\";\n      const description = schema.description ? ` // ${schema.description}` : \"\";\n      return `${type}${description}${isNullable}`;\n    }\n\n    if (\"anyOf\" in schema) {\n      return schema.anyOf\n        .map((s) => this._schemaToInstruction(s, indent))\n        .join(`\\n${\" \".repeat(indent - 2)}`);\n    }\n\n    throw new Error(\"unsupported schema type\");\n  }\n\n  static fromZodSchema<T extends InteropZodType>(schema: T) {\n    return new this<T>(schema);\n  }\n\n  static fromNamesAndDescriptions<S extends { [key: string]: string }>(\n    schemas: S\n  ) {\n    const zodSchema = z.object(\n      Object.fromEntries(\n        Object.entries(schemas).map(\n          ([name, description]) =>\n            [name, z.string().describe(description)] as const\n        )\n      )\n    );\n\n    return new this<typeof zodSchema>(zodSchema);\n  }\n}\n\nexport interface AsymmetricStructuredOutputParserFields<\n  T extends InteropZodType,\n> {\n  inputSchema: T;\n}\n\n/**\n * A type of `StructuredOutputParser` that handles asymmetric input and\n * output schemas.\n */\nexport abstract class AsymmetricStructuredOutputParser<\n  T extends InteropZodType,\n  Y = unknown,\n> extends BaseOutputParser<Y> {\n  private structuredInputParser: JsonMarkdownStructuredOutputParser<T>;\n\n  constructor({ inputSchema }: AsymmetricStructuredOutputParserFields<T>) {\n    super(...arguments);\n    this.structuredInputParser = new JsonMarkdownStructuredOutputParser(\n      inputSchema\n    );\n  }\n\n  /**\n   * Processes the parsed input into the desired output format. Must be\n   * implemented by subclasses.\n   * @param input The parsed input\n   * @returns The processed output.\n   */\n  abstract outputProcessor(input: InferInteropZodOutput<T>): Promise<Y>;\n\n  async parse(text: string): Promise<Y> {\n    let parsedInput;\n    try {\n      parsedInput = await this.structuredInputParser.parse(text);\n    } catch (e) {\n      throw new OutputParserException(\n        `Failed to parse. Text: \"${text}\". Error: ${e}`,\n        text\n      );\n    }\n\n    return this.outputProcessor(parsedInput);\n  }\n\n  getFormatInstructions(): string {\n    return this.structuredInputParser.getFormatInstructions();\n  }\n}\n", "import { __exportAll } from \"../_virtual/_rolldown/runtime.js\";\nimport { applyPatch } from \"./fast-json-patch/src/core.js\";\nimport { compare } from \"./fast-json-patch/src/duplex.js\";\nimport \"./fast-json-patch/index.js\";\n\n//#region src/utils/json_patch.ts\nvar json_patch_exports = /* @__PURE__ */ __exportAll({\n\tapplyPatch: () => applyPatch,\n\tcompare: () => compare\n});\n\n//#endregion\nexport { applyPatch, compare, json_patch_exports };\n//# sourceMappingURL=json_patch.js.map", "import { BaseCumulativeTransformOutputParser } from \"./transform.js\";\nimport { Operation, compare } from \"../utils/json_patch.js\";\nimport { ChatGeneration, Generation } from \"../outputs.js\";\nimport { parseJsonMarkdown, parsePartialJson } from \"../utils/json.js\";\nimport type { BaseMessage } from \"../messages/index.js\";\n\n/**\n * Class for parsing the output of an LLM into a JSON object.\n */\nexport class JsonOutputParser<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  T extends Record<string, any> = Record<string, any>,\n> extends BaseCumulativeTransformOutputParser<T> {\n  static lc_name() {\n    return \"JsonOutputParser\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"output_parsers\"];\n\n  lc_serializable = true;\n\n  /** @internal */\n  override _concatOutputChunks<T>(first: T, second: T): T {\n    if (this.diff) {\n      return super._concatOutputChunks(first, second);\n    }\n    return second;\n  }\n\n  protected _diff(\n    prev: unknown | undefined,\n    next: unknown\n  ): Operation[] | undefined {\n    if (!next) {\n      return undefined;\n    }\n    if (!prev) {\n      return [{ op: \"replace\", path: \"\", value: next }];\n    }\n    return compare(prev, next);\n  }\n\n  // This should actually return Partial<T>, but there's no way\n  // to specify emitted chunks as instances separate from the main output type.\n  async parsePartialResult(\n    generations: ChatGeneration[] | Generation[]\n  ): Promise<T | undefined> {\n    return parseJsonMarkdown(generations[0].text) as T | undefined;\n  }\n\n  async parse(text: string): Promise<T> {\n    return parseJsonMarkdown(text, JSON.parse) as T;\n  }\n\n  getFormatInstructions(): string {\n    return \"\";\n  }\n\n  /**\n   * Extracts text content from a message for JSON parsing.\n   * Uses the message's `.text` accessor which properly handles both\n   * string content and ContentBlock[] arrays (extracting text from text blocks).\n   * @param message The message to extract text from\n   * @returns The text content of the message\n   */\n  protected _baseMessageToString(message: BaseMessage): string {\n    return message.text;\n  }\n}\n\nexport { parsePartialJson, parseJsonMarkdown };\n", "// @ts-nocheck\n\n// Inlined to deal with portability issues\n// Originally from: https://github.com/isaacs/sax-js\n\nconst initializeSax = function () {\n  const sax: any = {};\n  sax.parser = function (strict, opt) {\n    return new SAXParser(strict, opt);\n  };\n  sax.SAXParser = SAXParser;\n  sax.SAXStream = SAXStream;\n  sax.createStream = createStream;\n\n  // When we pass the MAX_BUFFER_LENGTH position, start checking for buffer overruns.\n  // When we check, schedule the next check for MAX_BUFFER_LENGTH - (max(buffer lengths)),\n  // since that's the earliest that a buffer overrun could occur.  This way, checks are\n  // as rare as required, but as often as necessary to ensure never crossing this bound.\n  // Furthermore, buffers are only tested at most once per write(), so passing a very\n  // large string into write() might have undesirable effects, but this is manageable by\n  // the caller, so it is assumed to be safe.  Thus, a call to write() may, in the extreme\n  // edge case, result in creating at most one complete copy of the string passed in.\n  // Set to Infinity to have unlimited buffers.\n  sax.MAX_BUFFER_LENGTH = 64 * 1024;\n\n  const buffers = [\n    \"comment\",\n    \"sgmlDecl\",\n    \"textNode\",\n    \"tagName\",\n    \"doctype\",\n    \"procInstName\",\n    \"procInstBody\",\n    \"entity\",\n    \"attribName\",\n    \"attribValue\",\n    \"cdata\",\n    \"script\",\n  ];\n\n  sax.EVENTS = [\n    \"text\",\n    \"processinginstruction\",\n    \"sgmldeclaration\",\n    \"doctype\",\n    \"comment\",\n    \"opentagstart\",\n    \"attribute\",\n    \"opentag\",\n    \"closetag\",\n    \"opencdata\",\n    \"cdata\",\n    \"closecdata\",\n    \"error\",\n    \"end\",\n    \"ready\",\n    \"script\",\n    \"opennamespace\",\n    \"closenamespace\",\n  ];\n\n  function SAXParser(strict, opt) {\n    if (!(this instanceof SAXParser)) {\n      return new SAXParser(strict, opt);\n    }\n\n    var parser = this;\n    clearBuffers(parser);\n    parser.q = parser.c = \"\";\n    parser.bufferCheckPosition = sax.MAX_BUFFER_LENGTH;\n    parser.opt = opt || {};\n    parser.opt.lowercase = parser.opt.lowercase || parser.opt.lowercasetags;\n    parser.looseCase = parser.opt.lowercase ? \"toLowerCase\" : \"toUpperCase\";\n    parser.tags = [];\n    parser.closed = parser.closedRoot = parser.sawRoot = false;\n    parser.tag = parser.error = null;\n    parser.strict = !!strict;\n    parser.noscript = !!(strict || parser.opt.noscript);\n    parser.state = S.BEGIN;\n    parser.strictEntities = parser.opt.strictEntities;\n    parser.ENTITIES = parser.strictEntities\n      ? Object.create(sax.XML_ENTITIES)\n      : Object.create(sax.ENTITIES);\n    parser.attribList = [];\n\n    // namespaces form a prototype chain.\n    // it always points at the current tag,\n    // which protos to its parent tag.\n    if (parser.opt.xmlns) {\n      parser.ns = Object.create(rootNS);\n    }\n\n    // mostly just for error reporting\n    parser.trackPosition = parser.opt.position !== false;\n    if (parser.trackPosition) {\n      parser.position = parser.line = parser.column = 0;\n    }\n    emit(parser, \"onready\");\n  }\n\n  if (!Object.create) {\n    Object.create = function (o) {\n      function F() {}\n      F.prototype = o;\n      var newf = new F();\n      return newf;\n    };\n  }\n\n  if (!Object.keys) {\n    Object.keys = function (o) {\n      var a = [];\n      for (var i in o) if (o.hasOwnProperty(i)) a.push(i);\n      return a;\n    };\n  }\n\n  function checkBufferLength(parser) {\n    var maxAllowed = Math.max(sax.MAX_BUFFER_LENGTH, 10);\n    var maxActual = 0;\n    for (var i = 0, l = buffers.length; i < l; i++) {\n      var len = parser[buffers[i]].length;\n      if (len > maxAllowed) {\n        // Text/cdata nodes can get big, and since they're buffered,\n        // we can get here under normal conditions.\n        // Avoid issues by emitting the text node now,\n        // so at least it won't get any bigger.\n        switch (buffers[i]) {\n          case \"textNode\":\n            closeText(parser);\n            break;\n\n          case \"cdata\":\n            emitNode(parser, \"oncdata\", parser.cdata);\n            parser.cdata = \"\";\n            break;\n\n          case \"script\":\n            emitNode(parser, \"onscript\", parser.script);\n            parser.script = \"\";\n            break;\n\n          default:\n            error(parser, \"Max buffer length exceeded: \" + buffers[i]);\n        }\n      }\n      maxActual = Math.max(maxActual, len);\n    }\n    // schedule the next check for the earliest possible buffer overrun.\n    var m = sax.MAX_BUFFER_LENGTH - maxActual;\n    parser.bufferCheckPosition = m + parser.position;\n  }\n\n  function clearBuffers(parser) {\n    for (var i = 0, l = buffers.length; i < l; i++) {\n      parser[buffers[i]] = \"\";\n    }\n  }\n\n  function flushBuffers(parser) {\n    closeText(parser);\n    if (parser.cdata !== \"\") {\n      emitNode(parser, \"oncdata\", parser.cdata);\n      parser.cdata = \"\";\n    }\n    if (parser.script !== \"\") {\n      emitNode(parser, \"onscript\", parser.script);\n      parser.script = \"\";\n    }\n  }\n\n  SAXParser.prototype = {\n    end: function () {\n      end(this);\n    },\n    write: write,\n    resume: function () {\n      this.error = null;\n      return this;\n    },\n    close: function () {\n      return this.write(null);\n    },\n    flush: function () {\n      flushBuffers(this);\n    },\n  };\n\n  var Stream = ReadableStream;\n  if (!Stream) Stream = function () {};\n\n  var streamWraps = sax.EVENTS.filter(function (ev) {\n    return ev !== \"error\" && ev !== \"end\";\n  });\n\n  function createStream(strict, opt) {\n    return new SAXStream(strict, opt);\n  }\n\n  function SAXStream(strict, opt) {\n    if (!(this instanceof SAXStream)) {\n      return new SAXStream(strict, opt);\n    }\n\n    Stream.apply(this);\n\n    this._parser = new SAXParser(strict, opt);\n    this.writable = true;\n    this.readable = true;\n\n    var me = this;\n\n    this._parser.onend = function () {\n      me.emit(\"end\");\n    };\n\n    this._parser.onerror = function (er) {\n      me.emit(\"error\", er);\n\n      // if didn't throw, then means error was handled.\n      // go ahead and clear error, so we can write again.\n      me._parser.error = null;\n    };\n\n    this._decoder = null;\n\n    streamWraps.forEach(function (ev) {\n      Object.defineProperty(me, \"on\" + ev, {\n        get: function () {\n          return me._parser[\"on\" + ev];\n        },\n        set: function (h) {\n          if (!h) {\n            me.removeAllListeners(ev);\n            me._parser[\"on\" + ev] = h;\n            return h;\n          }\n          me.on(ev, h);\n        },\n        enumerable: true,\n        configurable: false,\n      });\n    });\n  }\n\n  SAXStream.prototype = Object.create(Stream.prototype, {\n    constructor: {\n      value: SAXStream,\n    },\n  });\n\n  SAXStream.prototype.write = function (data) {\n    this._parser.write(data.toString());\n    this.emit(\"data\", data);\n    return true;\n  };\n\n  SAXStream.prototype.end = function (chunk) {\n    if (chunk && chunk.length) {\n      this.write(chunk);\n    }\n    this._parser.end();\n    return true;\n  };\n\n  SAXStream.prototype.on = function (ev, handler) {\n    var me = this;\n    if (!me._parser[\"on\" + ev] && streamWraps.indexOf(ev) !== -1) {\n      me._parser[\"on\" + ev] = function () {\n        var args =\n          arguments.length === 1\n            ? [arguments[0]]\n            : Array.apply(null, arguments);\n        args.splice(0, 0, ev);\n        me.emit.apply(me, args);\n      };\n    }\n\n    return Stream.prototype.on.call(me, ev, handler);\n  };\n\n  // this really needs to be replaced with character classes.\n  // XML allows all manner of ridiculous numbers and digits.\n  var CDATA = \"[CDATA[\";\n  var DOCTYPE = \"DOCTYPE\";\n  var XML_NAMESPACE = \"http://www.w3.org/XML/1998/namespace\";\n  var XMLNS_NAMESPACE = \"http://www.w3.org/2000/xmlns/\";\n  var rootNS = { xml: XML_NAMESPACE, xmlns: XMLNS_NAMESPACE };\n\n  // http://www.w3.org/TR/REC-xml/#NT-NameStartChar\n  // This implementation works on strings, a single character at a time\n  // as such, it cannot ever support astral-plane characters (10000-EFFFF)\n  // without a significant breaking change to either this  parser, or the\n  // JavaScript language.  Implementation of an emoji-capable xml parser\n  // is left as an exercise for the reader.\n  var nameStart =\n    /[:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD]/;\n\n  var nameBody =\n    /[:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\u00B7\\u0300-\\u036F\\u203F-\\u2040.\\d-]/;\n\n  var entityStart =\n    /[#:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD]/;\n  var entityBody =\n    /[#:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\u00B7\\u0300-\\u036F\\u203F-\\u2040.\\d-]/;\n\n  function isWhitespace(c) {\n    return c === \" \" || c === \"\\n\" || c === \"\\r\" || c === \"\\t\";\n  }\n\n  function isQuote(c) {\n    return c === '\"' || c === \"'\";\n  }\n\n  function isAttribEnd(c) {\n    return c === \">\" || isWhitespace(c);\n  }\n\n  function isMatch(regex, c) {\n    return regex.test(c);\n  }\n\n  function notMatch(regex, c) {\n    return !isMatch(regex, c);\n  }\n\n  var S = 0;\n  sax.STATE = {\n    BEGIN: S++, // leading byte order mark or whitespace\n    BEGIN_WHITESPACE: S++, // leading whitespace\n    TEXT: S++, // general stuff\n    TEXT_ENTITY: S++, // &amp and such.\n    OPEN_WAKA: S++, // <\n    SGML_DECL: S++, // <!BLARG\n    SGML_DECL_QUOTED: S++, // <!BLARG foo \"bar\n    DOCTYPE: S++, // <!DOCTYPE\n    DOCTYPE_QUOTED: S++, // <!DOCTYPE \"//blah\n    DOCTYPE_DTD: S++, // <!DOCTYPE \"//blah\" [ ...\n    DOCTYPE_DTD_QUOTED: S++, // <!DOCTYPE \"//blah\" [ \"foo\n    COMMENT_STARTING: S++, // <!-\n    COMMENT: S++, // <!--\n    COMMENT_ENDING: S++, // <!-- blah -\n    COMMENT_ENDED: S++, // <!-- blah --\n    CDATA: S++, // <![CDATA[ something\n    CDATA_ENDING: S++, // ]\n    CDATA_ENDING_2: S++, // ]]\n    PROC_INST: S++, // <?hi\n    PROC_INST_BODY: S++, // <?hi there\n    PROC_INST_ENDING: S++, // <?hi \"there\" ?\n    OPEN_TAG: S++, // <strong\n    OPEN_TAG_SLASH: S++, // <strong /\n    ATTRIB: S++, // <a\n    ATTRIB_NAME: S++, // <a foo\n    ATTRIB_NAME_SAW_WHITE: S++, // <a foo _\n    ATTRIB_VALUE: S++, // <a foo=\n    ATTRIB_VALUE_QUOTED: S++, // <a foo=\"bar\n    ATTRIB_VALUE_CLOSED: S++, // <a foo=\"bar\"\n    ATTRIB_VALUE_UNQUOTED: S++, // <a foo=bar\n    ATTRIB_VALUE_ENTITY_Q: S++, // <foo bar=\"&quot;\"\n    ATTRIB_VALUE_ENTITY_U: S++, // <foo bar=&quot\n    CLOSE_TAG: S++, // </a\n    CLOSE_TAG_SAW_WHITE: S++, // </a   >\n    SCRIPT: S++, // <script> ...\n    SCRIPT_ENDING: S++, // <script> ... <\n  };\n\n  sax.XML_ENTITIES = {\n    amp: \"&\",\n    gt: \">\",\n    lt: \"<\",\n    quot: '\"',\n    apos: \"'\",\n  };\n\n  sax.ENTITIES = {\n    amp: \"&\",\n    gt: \">\",\n    lt: \"<\",\n    quot: '\"',\n    apos: \"'\",\n    AElig: 198,\n    Aacute: 193,\n    Acirc: 194,\n    Agrave: 192,\n    Aring: 197,\n    Atilde: 195,\n    Auml: 196,\n    Ccedil: 199,\n    ETH: 208,\n    Eacute: 201,\n    Ecirc: 202,\n    Egrave: 200,\n    Euml: 203,\n    Iacute: 205,\n    Icirc: 206,\n    Igrave: 204,\n    Iuml: 207,\n    Ntilde: 209,\n    Oacute: 211,\n    Ocirc: 212,\n    Ograve: 210,\n    Oslash: 216,\n    Otilde: 213,\n    Ouml: 214,\n    THORN: 222,\n    Uacute: 218,\n    Ucirc: 219,\n    Ugrave: 217,\n    Uuml: 220,\n    Yacute: 221,\n    aacute: 225,\n    acirc: 226,\n    aelig: 230,\n    agrave: 224,\n    aring: 229,\n    atilde: 227,\n    auml: 228,\n    ccedil: 231,\n    eacute: 233,\n    ecirc: 234,\n    egrave: 232,\n    eth: 240,\n    euml: 235,\n    iacute: 237,\n    icirc: 238,\n    igrave: 236,\n    iuml: 239,\n    ntilde: 241,\n    oacute: 243,\n    ocirc: 244,\n    ograve: 242,\n    oslash: 248,\n    otilde: 245,\n    ouml: 246,\n    szlig: 223,\n    thorn: 254,\n    uacute: 250,\n    ucirc: 251,\n    ugrave: 249,\n    uuml: 252,\n    yacute: 253,\n    yuml: 255,\n    copy: 169,\n    reg: 174,\n    nbsp: 160,\n    iexcl: 161,\n    cent: 162,\n    pound: 163,\n    curren: 164,\n    yen: 165,\n    brvbar: 166,\n    sect: 167,\n    uml: 168,\n    ordf: 170,\n    laquo: 171,\n    not: 172,\n    shy: 173,\n    macr: 175,\n    deg: 176,\n    plusmn: 177,\n    sup1: 185,\n    sup2: 178,\n    sup3: 179,\n    acute: 180,\n    micro: 181,\n    para: 182,\n    middot: 183,\n    cedil: 184,\n    ordm: 186,\n    raquo: 187,\n    frac14: 188,\n    frac12: 189,\n    frac34: 190,\n    iquest: 191,\n    times: 215,\n    divide: 247,\n    OElig: 338,\n    oelig: 339,\n    Scaron: 352,\n    scaron: 353,\n    Yuml: 376,\n    fnof: 402,\n    circ: 710,\n    tilde: 732,\n    Alpha: 913,\n    Beta: 914,\n    Gamma: 915,\n    Delta: 916,\n    Epsilon: 917,\n    Zeta: 918,\n    Eta: 919,\n    Theta: 920,\n    Iota: 921,\n    Kappa: 922,\n    Lambda: 923,\n    Mu: 924,\n    Nu: 925,\n    Xi: 926,\n    Omicron: 927,\n    Pi: 928,\n    Rho: 929,\n    Sigma: 931,\n    Tau: 932,\n    Upsilon: 933,\n    Phi: 934,\n    Chi: 935,\n    Psi: 936,\n    Omega: 937,\n    alpha: 945,\n    beta: 946,\n    gamma: 947,\n    delta: 948,\n    epsilon: 949,\n    zeta: 950,\n    eta: 951,\n    theta: 952,\n    iota: 953,\n    kappa: 954,\n    lambda: 955,\n    mu: 956,\n    nu: 957,\n    xi: 958,\n    omicron: 959,\n    pi: 960,\n    rho: 961,\n    sigmaf: 962,\n    sigma: 963,\n    tau: 964,\n    upsilon: 965,\n    phi: 966,\n    chi: 967,\n    psi: 968,\n    omega: 969,\n    thetasym: 977,\n    upsih: 978,\n    piv: 982,\n    ensp: 8194,\n    emsp: 8195,\n    thinsp: 8201,\n    zwnj: 8204,\n    zwj: 8205,\n    lrm: 8206,\n    rlm: 8207,\n    ndash: 8211,\n    mdash: 8212,\n    lsquo: 8216,\n    rsquo: 8217,\n    sbquo: 8218,\n    ldquo: 8220,\n    rdquo: 8221,\n    bdquo: 8222,\n    dagger: 8224,\n    Dagger: 8225,\n    bull: 8226,\n    hellip: 8230,\n    permil: 8240,\n    prime: 8242,\n    Prime: 8243,\n    lsaquo: 8249,\n    rsaquo: 8250,\n    oline: 8254,\n    frasl: 8260,\n    euro: 8364,\n    image: 8465,\n    weierp: 8472,\n    real: 8476,\n    trade: 8482,\n    alefsym: 8501,\n    larr: 8592,\n    uarr: 8593,\n    rarr: 8594,\n    darr: 8595,\n    harr: 8596,\n    crarr: 8629,\n    lArr: 8656,\n    uArr: 8657,\n    rArr: 8658,\n    dArr: 8659,\n    hArr: 8660,\n    forall: 8704,\n    part: 8706,\n    exist: 8707,\n    empty: 8709,\n    nabla: 8711,\n    isin: 8712,\n    notin: 8713,\n    ni: 8715,\n    prod: 8719,\n    sum: 8721,\n    minus: 8722,\n    lowast: 8727,\n    radic: 8730,\n    prop: 8733,\n    infin: 8734,\n    ang: 8736,\n    and: 8743,\n    or: 8744,\n    cap: 8745,\n    cup: 8746,\n    int: 8747,\n    there4: 8756,\n    sim: 8764,\n    cong: 8773,\n    asymp: 8776,\n    ne: 8800,\n    equiv: 8801,\n    le: 8804,\n    ge: 8805,\n    sub: 8834,\n    sup: 8835,\n    nsub: 8836,\n    sube: 8838,\n    supe: 8839,\n    oplus: 8853,\n    otimes: 8855,\n    perp: 8869,\n    sdot: 8901,\n    lceil: 8968,\n    rceil: 8969,\n    lfloor: 8970,\n    rfloor: 8971,\n    lang: 9001,\n    rang: 9002,\n    loz: 9674,\n    spades: 9824,\n    clubs: 9827,\n    hearts: 9829,\n    diams: 9830,\n  };\n\n  Object.keys(sax.ENTITIES).forEach(function (key) {\n    var e = sax.ENTITIES[key];\n    var s = typeof e === \"number\" ? String.fromCharCode(e) : e;\n    sax.ENTITIES[key] = s;\n  });\n\n  for (var s in sax.STATE) {\n    sax.STATE[sax.STATE[s]] = s;\n  }\n\n  // shorthand\n  S = sax.STATE;\n\n  function emit(parser, event, data) {\n    parser[event] && parser[event](data);\n  }\n\n  function emitNode(parser, nodeType, data) {\n    if (parser.textNode) closeText(parser);\n    emit(parser, nodeType, data);\n  }\n\n  function closeText(parser) {\n    parser.textNode = textopts(parser.opt, parser.textNode);\n    if (parser.textNode) emit(parser, \"ontext\", parser.textNode);\n    parser.textNode = \"\";\n  }\n\n  function textopts(opt, text) {\n    if (opt.trim) text = text.trim();\n    if (opt.normalize) text = text.replace(/\\s+/g, \" \");\n    return text;\n  }\n\n  function error(parser, er) {\n    closeText(parser);\n    if (parser.trackPosition) {\n      er +=\n        \"\\nLine: \" +\n        parser.line +\n        \"\\nColumn: \" +\n        parser.column +\n        \"\\nChar: \" +\n        parser.c;\n    }\n    er = new Error(er);\n    parser.error = er;\n    emit(parser, \"onerror\", er);\n    return parser;\n  }\n\n  function end(parser) {\n    if (parser.sawRoot && !parser.closedRoot)\n      strictFail(parser, \"Unclosed root tag\");\n    if (\n      parser.state !== S.BEGIN &&\n      parser.state !== S.BEGIN_WHITESPACE &&\n      parser.state !== S.TEXT\n    ) {\n      error(parser, \"Unexpected end\");\n    }\n    closeText(parser);\n    parser.c = \"\";\n    parser.closed = true;\n    emit(parser, \"onend\");\n    SAXParser.call(parser, parser.strict, parser.opt);\n    return parser;\n  }\n\n  function strictFail(parser, message) {\n    if (typeof parser !== \"object\" || !(parser instanceof SAXParser)) {\n      throw new Error(\"bad call to strictFail\");\n    }\n    if (parser.strict) {\n      error(parser, message);\n    }\n  }\n\n  function newTag(parser) {\n    if (!parser.strict) parser.tagName = parser.tagName[parser.looseCase]();\n    var parent = parser.tags[parser.tags.length - 1] || parser;\n    var tag = (parser.tag = { name: parser.tagName, attributes: {} });\n\n    // will be overridden if tag contails an xmlns=\"foo\" or xmlns:foo=\"bar\"\n    if (parser.opt.xmlns) {\n      tag.ns = parent.ns;\n    }\n    parser.attribList.length = 0;\n    emitNode(parser, \"onopentagstart\", tag);\n  }\n\n  function qname(name, attribute) {\n    var i = name.indexOf(\":\");\n    var qualName = i < 0 ? [\"\", name] : name.split(\":\");\n    var prefix = qualName[0];\n    var local = qualName[1];\n\n    // <x \"xmlns\"=\"http://foo\">\n    if (attribute && name === \"xmlns\") {\n      prefix = \"xmlns\";\n      local = \"\";\n    }\n\n    return { prefix: prefix, local: local };\n  }\n\n  function attrib(parser) {\n    if (!parser.strict) {\n      parser.attribName = parser.attribName[parser.looseCase]();\n    }\n\n    if (\n      parser.attribList.indexOf(parser.attribName) !== -1 ||\n      parser.tag.attributes.hasOwnProperty(parser.attribName)\n    ) {\n      parser.attribName = parser.attribValue = \"\";\n      return;\n    }\n\n    if (parser.opt.xmlns) {\n      var qn = qname(parser.attribName, true);\n      var prefix = qn.prefix;\n      var local = qn.local;\n\n      if (prefix === \"xmlns\") {\n        // namespace binding attribute. push the binding into scope\n        if (local === \"xml\" && parser.attribValue !== XML_NAMESPACE) {\n          strictFail(\n            parser,\n            \"xml: prefix must be bound to \" +\n              XML_NAMESPACE +\n              \"\\n\" +\n              \"Actual: \" +\n              parser.attribValue\n          );\n        } else if (\n          local === \"xmlns\" &&\n          parser.attribValue !== XMLNS_NAMESPACE\n        ) {\n          strictFail(\n            parser,\n            \"xmlns: prefix must be bound to \" +\n              XMLNS_NAMESPACE +\n              \"\\n\" +\n              \"Actual: \" +\n              parser.attribValue\n          );\n        } else {\n          var tag = parser.tag;\n          var parent = parser.tags[parser.tags.length - 1] || parser;\n          if (tag.ns === parent.ns) {\n            tag.ns = Object.create(parent.ns);\n          }\n          tag.ns[local] = parser.attribValue;\n        }\n      }\n\n      // defer onattribute events until all attributes have been seen\n      // so any new bindings can take effect. preserve attribute order\n      // so deferred events can be emitted in document order\n      parser.attribList.push([parser.attribName, parser.attribValue]);\n    } else {\n      // in non-xmlns mode, we can emit the event right away\n      parser.tag.attributes[parser.attribName] = parser.attribValue;\n      emitNode(parser, \"onattribute\", {\n        name: parser.attribName,\n        value: parser.attribValue,\n      });\n    }\n\n    parser.attribName = parser.attribValue = \"\";\n  }\n\n  function openTag(parser, selfClosing) {\n    if (parser.opt.xmlns) {\n      // emit namespace binding events\n      var tag = parser.tag;\n\n      // add namespace info to tag\n      var qn = qname(parser.tagName);\n      tag.prefix = qn.prefix;\n      tag.local = qn.local;\n      tag.uri = tag.ns[qn.prefix] || \"\";\n\n      if (tag.prefix && !tag.uri) {\n        strictFail(\n          parser,\n          \"Unbound namespace prefix: \" + JSON.stringify(parser.tagName)\n        );\n        tag.uri = qn.prefix;\n      }\n\n      var parent = parser.tags[parser.tags.length - 1] || parser;\n      if (tag.ns && parent.ns !== tag.ns) {\n        Object.keys(tag.ns).forEach(function (p) {\n          emitNode(parser, \"onopennamespace\", {\n            prefix: p,\n            uri: tag.ns[p],\n          });\n        });\n      }\n\n      // handle deferred onattribute events\n      // Note: do not apply default ns to attributes:\n      //   http://www.w3.org/TR/REC-xml-names/#defaulting\n      for (var i = 0, l = parser.attribList.length; i < l; i++) {\n        var nv = parser.attribList[i];\n        var name = nv[0];\n        var value = nv[1];\n        var qualName = qname(name, true);\n        var prefix = qualName.prefix;\n        var local = qualName.local;\n        var uri = prefix === \"\" ? \"\" : tag.ns[prefix] || \"\";\n        var a = {\n          name: name,\n          value: value,\n          prefix: prefix,\n          local: local,\n          uri: uri,\n        };\n\n        // if there's any attributes with an undefined namespace,\n        // then fail on them now.\n        if (prefix && prefix !== \"xmlns\" && !uri) {\n          strictFail(\n            parser,\n            \"Unbound namespace prefix: \" + JSON.stringify(prefix)\n          );\n          a.uri = prefix;\n        }\n        parser.tag.attributes[name] = a;\n        emitNode(parser, \"onattribute\", a);\n      }\n      parser.attribList.length = 0;\n    }\n\n    parser.tag.isSelfClosing = !!selfClosing;\n\n    // process the tag\n    parser.sawRoot = true;\n    parser.tags.push(parser.tag);\n    emitNode(parser, \"onopentag\", parser.tag);\n    if (!selfClosing) {\n      // special case for <script> in non-strict mode.\n      if (!parser.noscript && parser.tagName.toLowerCase() === \"script\") {\n        parser.state = S.SCRIPT;\n      } else {\n        parser.state = S.TEXT;\n      }\n      parser.tag = null;\n      parser.tagName = \"\";\n    }\n    parser.attribName = parser.attribValue = \"\";\n    parser.attribList.length = 0;\n  }\n\n  function closeTag(parser) {\n    if (!parser.tagName) {\n      strictFail(parser, \"Weird empty close tag.\");\n      parser.textNode += \"</>\";\n      parser.state = S.TEXT;\n      return;\n    }\n\n    if (parser.script) {\n      if (parser.tagName !== \"script\") {\n        parser.script += \"</\" + parser.tagName + \">\";\n        parser.tagName = \"\";\n        parser.state = S.SCRIPT;\n        return;\n      }\n      emitNode(parser, \"onscript\", parser.script);\n      parser.script = \"\";\n    }\n\n    // first make sure that the closing tag actually exists.\n    // <a><b></c></b></a> will close everything, otherwise.\n    var t = parser.tags.length;\n    var tagName = parser.tagName;\n    if (!parser.strict) {\n      tagName = tagName[parser.looseCase]();\n    }\n    var closeTo = tagName;\n    while (t--) {\n      var close = parser.tags[t];\n      if (close.name !== closeTo) {\n        // fail the first time in strict mode\n        strictFail(parser, \"Unexpected close tag\");\n      } else {\n        break;\n      }\n    }\n\n    // didn't find it.  we already failed for strict, so just abort.\n    if (t < 0) {\n      strictFail(parser, \"Unmatched closing tag: \" + parser.tagName);\n      parser.textNode += \"</\" + parser.tagName + \">\";\n      parser.state = S.TEXT;\n      return;\n    }\n    parser.tagName = tagName;\n    var s = parser.tags.length;\n    while (s-- > t) {\n      var tag = (parser.tag = parser.tags.pop());\n      parser.tagName = parser.tag.name;\n      emitNode(parser, \"onclosetag\", parser.tagName);\n\n      var x = {};\n      for (var i in tag.ns) {\n        x[i] = tag.ns[i];\n      }\n\n      var parent = parser.tags[parser.tags.length - 1] || parser;\n      if (parser.opt.xmlns && tag.ns !== parent.ns) {\n        // remove namespace bindings introduced by tag\n        Object.keys(tag.ns).forEach(function (p) {\n          var n = tag.ns[p];\n          emitNode(parser, \"onclosenamespace\", { prefix: p, uri: n });\n        });\n      }\n    }\n    if (t === 0) parser.closedRoot = true;\n    parser.tagName = parser.attribValue = parser.attribName = \"\";\n    parser.attribList.length = 0;\n    parser.state = S.TEXT;\n  }\n\n  function parseEntity(parser) {\n    var entity = parser.entity;\n    var entityLC = entity.toLowerCase();\n    var num;\n    var numStr = \"\";\n\n    if (parser.ENTITIES[entity]) {\n      return parser.ENTITIES[entity];\n    }\n    if (parser.ENTITIES[entityLC]) {\n      return parser.ENTITIES[entityLC];\n    }\n    entity = entityLC;\n    if (entity.charAt(0) === \"#\") {\n      if (entity.charAt(1) === \"x\") {\n        entity = entity.slice(2);\n        num = parseInt(entity, 16);\n        numStr = num.toString(16);\n      } else {\n        entity = entity.slice(1);\n        num = parseInt(entity, 10);\n        numStr = num.toString(10);\n      }\n    }\n    entity = entity.replace(/^0+/, \"\");\n    if (isNaN(num) || numStr.toLowerCase() !== entity) {\n      strictFail(parser, \"Invalid character entity\");\n      return \"&\" + parser.entity + \";\";\n    }\n\n    return String.fromCodePoint(num);\n  }\n\n  function beginWhiteSpace(parser, c) {\n    if (c === \"<\") {\n      parser.state = S.OPEN_WAKA;\n      parser.startTagPosition = parser.position;\n    } else if (!isWhitespace(c)) {\n      // have to process this as a text node.\n      // weird, but happens.\n      strictFail(parser, \"Non-whitespace before first tag.\");\n      parser.textNode = c;\n      parser.state = S.TEXT;\n    }\n  }\n\n  function charAt(chunk, i) {\n    var result = \"\";\n    if (i < chunk.length) {\n      result = chunk.charAt(i);\n    }\n    return result;\n  }\n\n  function write(chunk) {\n    var parser = this;\n    if (this.error) {\n      throw this.error;\n    }\n    if (parser.closed) {\n      return error(\n        parser,\n        \"Cannot write after close. Assign an onready handler.\"\n      );\n    }\n    if (chunk === null) {\n      return end(parser);\n    }\n    if (typeof chunk === \"object\") {\n      chunk = chunk.toString();\n    }\n    var i = 0;\n    var c = \"\";\n    while (true) {\n      c = charAt(chunk, i++);\n      parser.c = c;\n\n      if (!c) {\n        break;\n      }\n\n      if (parser.trackPosition) {\n        parser.position++;\n        if (c === \"\\n\") {\n          parser.line++;\n          parser.column = 0;\n        } else {\n          parser.column++;\n        }\n      }\n\n      switch (parser.state) {\n        case S.BEGIN:\n          parser.state = S.BEGIN_WHITESPACE;\n          if (c === \"\\uFEFF\") {\n            continue;\n          }\n          beginWhiteSpace(parser, c);\n          continue;\n\n        case S.BEGIN_WHITESPACE:\n          beginWhiteSpace(parser, c);\n          continue;\n\n        case S.TEXT:\n          if (parser.sawRoot && !parser.closedRoot) {\n            var starti = i - 1;\n            while (c && c !== \"<\" && c !== \"&\") {\n              c = charAt(chunk, i++);\n              if (c && parser.trackPosition) {\n                parser.position++;\n                if (c === \"\\n\") {\n                  parser.line++;\n                  parser.column = 0;\n                } else {\n                  parser.column++;\n                }\n              }\n            }\n            parser.textNode += chunk.substring(starti, i - 1);\n          }\n          if (\n            c === \"<\" &&\n            !(parser.sawRoot && parser.closedRoot && !parser.strict)\n          ) {\n            parser.state = S.OPEN_WAKA;\n            parser.startTagPosition = parser.position;\n          } else {\n            if (!isWhitespace(c) && (!parser.sawRoot || parser.closedRoot)) {\n              strictFail(parser, \"Text data outside of root node.\");\n            }\n            if (c === \"&\") {\n              parser.state = S.TEXT_ENTITY;\n            } else {\n              parser.textNode += c;\n            }\n          }\n          continue;\n\n        case S.SCRIPT:\n          // only non-strict\n          if (c === \"<\") {\n            parser.state = S.SCRIPT_ENDING;\n          } else {\n            parser.script += c;\n          }\n          continue;\n\n        case S.SCRIPT_ENDING:\n          if (c === \"/\") {\n            parser.state = S.CLOSE_TAG;\n          } else {\n            parser.script += \"<\" + c;\n            parser.state = S.SCRIPT;\n          }\n          continue;\n\n        case S.OPEN_WAKA:\n          // either a /, ?, !, or text is coming next.\n          if (c === \"!\") {\n            parser.state = S.SGML_DECL;\n            parser.sgmlDecl = \"\";\n          } else if (isWhitespace(c)) {\n            // wait for it...\n          } else if (isMatch(nameStart, c)) {\n            parser.state = S.OPEN_TAG;\n            parser.tagName = c;\n          } else if (c === \"/\") {\n            parser.state = S.CLOSE_TAG;\n            parser.tagName = \"\";\n          } else if (c === \"?\") {\n            parser.state = S.PROC_INST;\n            parser.procInstName = parser.procInstBody = \"\";\n          } else {\n            strictFail(parser, \"Unencoded <\");\n            // if there was some whitespace, then add that in.\n            if (parser.startTagPosition + 1 < parser.position) {\n              var pad = parser.position - parser.startTagPosition;\n              c = new Array(pad).join(\" \") + c;\n            }\n            parser.textNode += \"<\" + c;\n            parser.state = S.TEXT;\n          }\n          continue;\n\n        case S.SGML_DECL:\n          if ((parser.sgmlDecl + c).toUpperCase() === CDATA) {\n            emitNode(parser, \"onopencdata\");\n            parser.state = S.CDATA;\n            parser.sgmlDecl = \"\";\n            parser.cdata = \"\";\n          } else if (parser.sgmlDecl + c === \"--\") {\n            parser.state = S.COMMENT;\n            parser.comment = \"\";\n            parser.sgmlDecl = \"\";\n          } else if ((parser.sgmlDecl + c).toUpperCase() === DOCTYPE) {\n            parser.state = S.DOCTYPE;\n            if (parser.doctype || parser.sawRoot) {\n              strictFail(parser, \"Inappropriately located doctype declaration\");\n            }\n            parser.doctype = \"\";\n            parser.sgmlDecl = \"\";\n          } else if (c === \">\") {\n            emitNode(parser, \"onsgmldeclaration\", parser.sgmlDecl);\n            parser.sgmlDecl = \"\";\n            parser.state = S.TEXT;\n          } else if (isQuote(c)) {\n            parser.state = S.SGML_DECL_QUOTED;\n            parser.sgmlDecl += c;\n          } else {\n            parser.sgmlDecl += c;\n          }\n          continue;\n\n        case S.SGML_DECL_QUOTED:\n          if (c === parser.q) {\n            parser.state = S.SGML_DECL;\n            parser.q = \"\";\n          }\n          parser.sgmlDecl += c;\n          continue;\n\n        case S.DOCTYPE:\n          if (c === \">\") {\n            parser.state = S.TEXT;\n            emitNode(parser, \"ondoctype\", parser.doctype);\n            parser.doctype = true; // just remember that we saw it.\n          } else {\n            parser.doctype += c;\n            if (c === \"[\") {\n              parser.state = S.DOCTYPE_DTD;\n            } else if (isQuote(c)) {\n              parser.state = S.DOCTYPE_QUOTED;\n              parser.q = c;\n            }\n          }\n          continue;\n\n        case S.DOCTYPE_QUOTED:\n          parser.doctype += c;\n          if (c === parser.q) {\n            parser.q = \"\";\n            parser.state = S.DOCTYPE;\n          }\n          continue;\n\n        case S.DOCTYPE_DTD:\n          parser.doctype += c;\n          if (c === \"]\") {\n            parser.state = S.DOCTYPE;\n          } else if (isQuote(c)) {\n            parser.state = S.DOCTYPE_DTD_QUOTED;\n            parser.q = c;\n          }\n          continue;\n\n        case S.DOCTYPE_DTD_QUOTED:\n          parser.doctype += c;\n          if (c === parser.q) {\n            parser.state = S.DOCTYPE_DTD;\n            parser.q = \"\";\n          }\n          continue;\n\n        case S.COMMENT:\n          if (c === \"-\") {\n            parser.state = S.COMMENT_ENDING;\n          } else {\n            parser.comment += c;\n          }\n          continue;\n\n        case S.COMMENT_ENDING:\n          if (c === \"-\") {\n            parser.state = S.COMMENT_ENDED;\n            parser.comment = textopts(parser.opt, parser.comment);\n            if (parser.comment) {\n              emitNode(parser, \"oncomment\", parser.comment);\n            }\n            parser.comment = \"\";\n          } else {\n            parser.comment += \"-\" + c;\n            parser.state = S.COMMENT;\n          }\n          continue;\n\n        case S.COMMENT_ENDED:\n          if (c !== \">\") {\n            strictFail(parser, \"Malformed comment\");\n            // allow <!-- blah -- bloo --> in non-strict mode,\n            // which is a comment of \" blah -- bloo \"\n            parser.comment += \"--\" + c;\n            parser.state = S.COMMENT;\n          } else {\n            parser.state = S.TEXT;\n          }\n          continue;\n\n        case S.CDATA:\n          if (c === \"]\") {\n            parser.state = S.CDATA_ENDING;\n          } else {\n            parser.cdata += c;\n          }\n          continue;\n\n        case S.CDATA_ENDING:\n          if (c === \"]\") {\n            parser.state = S.CDATA_ENDING_2;\n          } else {\n            parser.cdata += \"]\" + c;\n            parser.state = S.CDATA;\n          }\n          continue;\n\n        case S.CDATA_ENDING_2:\n          if (c === \">\") {\n            if (parser.cdata) {\n              emitNode(parser, \"oncdata\", parser.cdata);\n            }\n            emitNode(parser, \"onclosecdata\");\n            parser.cdata = \"\";\n            parser.state = S.TEXT;\n          } else if (c === \"]\") {\n            parser.cdata += \"]\";\n          } else {\n            parser.cdata += \"]]\" + c;\n            parser.state = S.CDATA;\n          }\n          continue;\n\n        case S.PROC_INST:\n          if (c === \"?\") {\n            parser.state = S.PROC_INST_ENDING;\n          } else if (isWhitespace(c)) {\n            parser.state = S.PROC_INST_BODY;\n          } else {\n            parser.procInstName += c;\n          }\n          continue;\n\n        case S.PROC_INST_BODY:\n          if (!parser.procInstBody && isWhitespace(c)) {\n            continue;\n          } else if (c === \"?\") {\n            parser.state = S.PROC_INST_ENDING;\n          } else {\n            parser.procInstBody += c;\n          }\n          continue;\n\n        case S.PROC_INST_ENDING:\n          if (c === \">\") {\n            emitNode(parser, \"onprocessinginstruction\", {\n              name: parser.procInstName,\n              body: parser.procInstBody,\n            });\n            parser.procInstName = parser.procInstBody = \"\";\n            parser.state = S.TEXT;\n          } else {\n            parser.procInstBody += \"?\" + c;\n            parser.state = S.PROC_INST_BODY;\n          }\n          continue;\n\n        case S.OPEN_TAG:\n          if (isMatch(nameBody, c)) {\n            parser.tagName += c;\n          } else {\n            newTag(parser);\n            if (c === \">\") {\n              openTag(parser);\n            } else if (c === \"/\") {\n              parser.state = S.OPEN_TAG_SLASH;\n            } else {\n              if (!isWhitespace(c)) {\n                strictFail(parser, \"Invalid character in tag name\");\n              }\n              parser.state = S.ATTRIB;\n            }\n          }\n          continue;\n\n        case S.OPEN_TAG_SLASH:\n          if (c === \">\") {\n            openTag(parser, true);\n            closeTag(parser);\n          } else {\n            strictFail(\n              parser,\n              \"Forward-slash in opening tag not followed by >\"\n            );\n            parser.state = S.ATTRIB;\n          }\n          continue;\n\n        case S.ATTRIB:\n          // haven't read the attribute name yet.\n          if (isWhitespace(c)) {\n            continue;\n          } else if (c === \">\") {\n            openTag(parser);\n          } else if (c === \"/\") {\n            parser.state = S.OPEN_TAG_SLASH;\n          } else if (isMatch(nameStart, c)) {\n            parser.attribName = c;\n            parser.attribValue = \"\";\n            parser.state = S.ATTRIB_NAME;\n          } else {\n            strictFail(parser, \"Invalid attribute name\");\n          }\n          continue;\n\n        case S.ATTRIB_NAME:\n          if (c === \"=\") {\n            parser.state = S.ATTRIB_VALUE;\n          } else if (c === \">\") {\n            strictFail(parser, \"Attribute without value\");\n            parser.attribValue = parser.attribName;\n            attrib(parser);\n            openTag(parser);\n          } else if (isWhitespace(c)) {\n            parser.state = S.ATTRIB_NAME_SAW_WHITE;\n          } else if (isMatch(nameBody, c)) {\n            parser.attribName += c;\n          } else {\n            strictFail(parser, \"Invalid attribute name\");\n          }\n          continue;\n\n        case S.ATTRIB_NAME_SAW_WHITE:\n          if (c === \"=\") {\n            parser.state = S.ATTRIB_VALUE;\n          } else if (isWhitespace(c)) {\n            continue;\n          } else {\n            strictFail(parser, \"Attribute without value\");\n            parser.tag.attributes[parser.attribName] = \"\";\n            parser.attribValue = \"\";\n            emitNode(parser, \"onattribute\", {\n              name: parser.attribName,\n              value: \"\",\n            });\n            parser.attribName = \"\";\n            if (c === \">\") {\n              openTag(parser);\n            } else if (isMatch(nameStart, c)) {\n              parser.attribName = c;\n              parser.state = S.ATTRIB_NAME;\n            } else {\n              strictFail(parser, \"Invalid attribute name\");\n              parser.state = S.ATTRIB;\n            }\n          }\n          continue;\n\n        case S.ATTRIB_VALUE:\n          if (isWhitespace(c)) {\n            continue;\n          } else if (isQuote(c)) {\n            parser.q = c;\n            parser.state = S.ATTRIB_VALUE_QUOTED;\n          } else {\n            strictFail(parser, \"Unquoted attribute value\");\n            parser.state = S.ATTRIB_VALUE_UNQUOTED;\n            parser.attribValue = c;\n          }\n          continue;\n\n        case S.ATTRIB_VALUE_QUOTED:\n          if (c !== parser.q) {\n            if (c === \"&\") {\n              parser.state = S.ATTRIB_VALUE_ENTITY_Q;\n            } else {\n              parser.attribValue += c;\n            }\n            continue;\n          }\n          attrib(parser);\n          parser.q = \"\";\n          parser.state = S.ATTRIB_VALUE_CLOSED;\n          continue;\n\n        case S.ATTRIB_VALUE_CLOSED:\n          if (isWhitespace(c)) {\n            parser.state = S.ATTRIB;\n          } else if (c === \">\") {\n            openTag(parser);\n          } else if (c === \"/\") {\n            parser.state = S.OPEN_TAG_SLASH;\n          } else if (isMatch(nameStart, c)) {\n            strictFail(parser, \"No whitespace between attributes\");\n            parser.attribName = c;\n            parser.attribValue = \"\";\n            parser.state = S.ATTRIB_NAME;\n          } else {\n            strictFail(parser, \"Invalid attribute name\");\n          }\n          continue;\n\n        case S.ATTRIB_VALUE_UNQUOTED:\n          if (!isAttribEnd(c)) {\n            if (c === \"&\") {\n              parser.state = S.ATTRIB_VALUE_ENTITY_U;\n            } else {\n              parser.attribValue += c;\n            }\n            continue;\n          }\n          attrib(parser);\n          if (c === \">\") {\n            openTag(parser);\n          } else {\n            parser.state = S.ATTRIB;\n          }\n          continue;\n\n        case S.CLOSE_TAG:\n          if (!parser.tagName) {\n            if (isWhitespace(c)) {\n              continue;\n            } else if (notMatch(nameStart, c)) {\n              if (parser.script) {\n                parser.script += \"</\" + c;\n                parser.state = S.SCRIPT;\n              } else {\n                strictFail(parser, \"Invalid tagname in closing tag.\");\n              }\n            } else {\n              parser.tagName = c;\n            }\n          } else if (c === \">\") {\n            closeTag(parser);\n          } else if (isMatch(nameBody, c)) {\n            parser.tagName += c;\n          } else if (parser.script) {\n            parser.script += \"</\" + parser.tagName;\n            parser.tagName = \"\";\n            parser.state = S.SCRIPT;\n          } else {\n            if (!isWhitespace(c)) {\n              strictFail(parser, \"Invalid tagname in closing tag\");\n            }\n            parser.state = S.CLOSE_TAG_SAW_WHITE;\n          }\n          continue;\n\n        case S.CLOSE_TAG_SAW_WHITE:\n          if (isWhitespace(c)) {\n            continue;\n          }\n          if (c === \">\") {\n            closeTag(parser);\n          } else {\n            strictFail(parser, \"Invalid characters in closing tag\");\n          }\n          continue;\n\n        case S.TEXT_ENTITY:\n        case S.ATTRIB_VALUE_ENTITY_Q:\n        case S.ATTRIB_VALUE_ENTITY_U:\n          var returnState;\n          var buffer;\n          switch (parser.state) {\n            case S.TEXT_ENTITY:\n              returnState = S.TEXT;\n              buffer = \"textNode\";\n              break;\n\n            case S.ATTRIB_VALUE_ENTITY_Q:\n              returnState = S.ATTRIB_VALUE_QUOTED;\n              buffer = \"attribValue\";\n              break;\n\n            case S.ATTRIB_VALUE_ENTITY_U:\n              returnState = S.ATTRIB_VALUE_UNQUOTED;\n              buffer = \"attribValue\";\n              break;\n          }\n\n          if (c === \";\") {\n            if (parser.opt.unparsedEntities) {\n              var parsedEntity = parseEntity(parser);\n              parser.entity = \"\";\n              parser.state = returnState;\n              parser.write(parsedEntity);\n            } else {\n              parser[buffer] += parseEntity(parser);\n              parser.entity = \"\";\n              parser.state = returnState;\n            }\n          } else if (\n            isMatch(parser.entity.length ? entityBody : entityStart, c)\n          ) {\n            parser.entity += c;\n          } else {\n            strictFail(parser, \"Invalid character in entity name\");\n            parser[buffer] += \"&\" + parser.entity + c;\n            parser.entity = \"\";\n            parser.state = returnState;\n          }\n\n          continue;\n\n        default: /* istanbul ignore next */ {\n          throw new Error(parser, \"Unknown state: \" + parser.state);\n        }\n      }\n    } // while\n\n    if (parser.position >= parser.bufferCheckPosition) {\n      checkBufferLength(parser);\n    }\n    return parser;\n  }\n\n  /*! http://mths.be/fromcodepoint v0.1.0 by @mathias */\n  /* istanbul ignore next */\n  if (!String.fromCodePoint) {\n    (function () {\n      var stringFromCharCode = String.fromCharCode;\n      var floor = Math.floor;\n      var fromCodePoint = function () {\n        var MAX_SIZE = 0x4000;\n        var codeUnits = [];\n        var highSurrogate;\n        var lowSurrogate;\n        var index = -1;\n        var length = arguments.length;\n        if (!length) {\n          return \"\";\n        }\n        var result = \"\";\n        while (++index < length) {\n          var codePoint = Number(arguments[index]);\n          if (\n            !isFinite(codePoint) || // `NaN`, `+Infinity`, or `-Infinity`\n            codePoint < 0 || // not a valid Unicode code point\n            codePoint > 0x10ffff || // not a valid Unicode code point\n            floor(codePoint) !== codePoint // not an integer\n          ) {\n            throw RangeError(\"Invalid code point: \" + codePoint);\n          }\n          if (codePoint <= 0xffff) {\n            // BMP code point\n            codeUnits.push(codePoint);\n          } else {\n            // Astral code point; split in surrogate halves\n            // http://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae\n            codePoint -= 0x10000;\n            highSurrogate = (codePoint >> 10) + 0xd800;\n            lowSurrogate = (codePoint % 0x400) + 0xdc00;\n            codeUnits.push(highSurrogate, lowSurrogate);\n          }\n          if (index + 1 === length || codeUnits.length > MAX_SIZE) {\n            result += stringFromCharCode.apply(null, codeUnits);\n            codeUnits.length = 0;\n          }\n        }\n        return result;\n      };\n      /* istanbul ignore next */\n      if (Object.defineProperty) {\n        Object.defineProperty(String, \"fromCodePoint\", {\n          value: fromCodePoint,\n          configurable: true,\n          writable: true,\n        });\n      } else {\n        String.fromCodePoint = fromCodePoint;\n      }\n    })();\n  }\n  return sax;\n};\n\nconst sax = /** #__PURE__ */ initializeSax();\n\nexport { sax };\n", "import {\n  BaseCumulativeTransformOutputParser,\n  BaseCumulativeTransformOutputParserInput,\n} from \"./transform.js\";\nimport { Operation, compare } from \"../utils/json_patch.js\";\nimport { sax } from \"../utils/sax-js/sax.js\";\nimport { ChatGeneration, Generation } from \"../outputs.js\";\n\nexport const XML_FORMAT_INSTRUCTIONS = `The output should be formatted as a XML file.\n1. Output should conform to the tags below. \n2. If tags are not given, make them on your own.\n3. Remember to always open and close all the tags.\n\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema. \n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\n\nHere are the output tags:\n\\`\\`\\`\n{tags}\n\\`\\`\\``;\n\nexport interface XMLOutputParserFields extends BaseCumulativeTransformOutputParserInput {\n  /**\n   * Optional list of tags that the output should conform to.\n   * Only used in formatting of the prompt.\n   */\n  tags?: string[];\n}\n\nexport type Content = string | undefined | Array<{ [key: string]: Content }>;\n\nexport type XMLResult = {\n  [key: string]: Content;\n};\n\nexport class XMLOutputParser extends BaseCumulativeTransformOutputParser<XMLResult> {\n  tags?: string[];\n\n  constructor(fields?: XMLOutputParserFields) {\n    super(fields);\n\n    this.tags = fields?.tags;\n  }\n\n  static lc_name() {\n    return \"XMLOutputParser\";\n  }\n\n  lc_namespace = [\"langchain_core\", \"output_parsers\"];\n\n  lc_serializable = true;\n\n  protected _diff(\n    prev: unknown | undefined,\n    next: unknown\n  ): Operation[] | undefined {\n    if (!next) {\n      return undefined;\n    }\n    if (!prev) {\n      return [{ op: \"replace\", path: \"\", value: next }];\n    }\n    return compare(prev, next);\n  }\n\n  async parsePartialResult(\n    generations: ChatGeneration[] | Generation[]\n  ): Promise<XMLResult | undefined> {\n    return parseXMLMarkdown(generations[0].text);\n  }\n\n  async parse(text: string): Promise<XMLResult> {\n    return parseXMLMarkdown(text);\n  }\n\n  getFormatInstructions(): string {\n    const withTags = !!(this.tags && this.tags.length > 0);\n    return withTags\n      ? XML_FORMAT_INSTRUCTIONS.replace(\"{tags}\", this.tags?.join(\", \") ?? \"\")\n      : XML_FORMAT_INSTRUCTIONS;\n  }\n}\n\nconst strip = (text: string) =>\n  text\n    .split(\"\\n\")\n    .map((line) => line.replace(/^\\s+/, \"\"))\n    .join(\"\\n\")\n    .trim();\n\ntype ParsedResult = {\n  name: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  attributes: Record<string, any>;\n  children: Array<ParsedResult>;\n  text?: string;\n  isSelfClosing: boolean;\n};\n\nconst parseParsedResult = (input: ParsedResult): XMLResult => {\n  if (Object.keys(input).length === 0) {\n    return {};\n  }\n  const result: XMLResult = {};\n  if (input.children.length > 0) {\n    result[input.name] = input.children.map(parseParsedResult);\n    return result;\n  } else {\n    result[input.name] = input.text ?? undefined;\n    return result;\n  }\n};\n\nexport function parseXMLMarkdown(s: string): XMLResult {\n  const cleanedString = strip(s);\n  const parser = sax.parser(true);\n  let parsedResult: ParsedResult = {} as ParsedResult;\n  const elementStack: ParsedResult[] = [];\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  parser.onopentag = (node: any) => {\n    const element = {\n      name: node.name,\n      attributes: node.attributes,\n      children: [],\n      text: \"\",\n      isSelfClosing: node.isSelfClosing,\n    };\n\n    if (elementStack.length > 0) {\n      const parentElement = elementStack[elementStack.length - 1];\n      parentElement.children.push(element);\n    } else {\n      parsedResult = element as ParsedResult;\n    }\n\n    if (!node.isSelfClosing) {\n      elementStack.push(element);\n    }\n  };\n\n  parser.onclosetag = () => {\n    if (elementStack.length > 0) {\n      const lastElement = elementStack.pop();\n      if (elementStack.length === 0 && lastElement) {\n        parsedResult = lastElement as ParsedResult;\n      }\n    }\n  };\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  parser.ontext = (text: any) => {\n    if (elementStack.length > 0) {\n      const currentElement = elementStack[elementStack.length - 1];\n      currentElement.text += text;\n    }\n  };\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  parser.onattribute = (attr: any) => {\n    if (elementStack.length > 0) {\n      const currentElement = elementStack[elementStack.length - 1];\n      currentElement.attributes[attr.name] = attr.value;\n    }\n  };\n\n  // Try to find XML string within triple backticks.\n  const match = /```(xml)?(.*)```/s.exec(cleanedString);\n  const xmlString = match ? match[2] : cleanedString;\n  parser.write(xmlString).close();\n\n  // Remove the XML declaration if present\n  if (parsedResult && parsedResult.name === \"?xml\") {\n    parsedResult = parsedResult.children[0] as ParsedResult;\n  }\n\n  return parseParsedResult(parsedResult);\n}\n", "import { __exportAll } from \"../_virtual/_rolldown/runtime.js\";\nimport { parseJsonMarkdown, parsePartialJson } from \"../utils/json.js\";\nimport { BaseLLMOutputParser, BaseOutputParser, OutputParserException } from \"./base.js\";\nimport { BaseCumulativeTransformOutputParser, BaseTransformOutputParser } from \"./transform.js\";\nimport { BytesOutputParser } from \"./bytes.js\";\nimport { CommaSeparatedListOutputParser, CustomListOutputParser, ListOutputParser, MarkdownListOutputParser, NumberedListOutputParser } from \"./list.js\";\nimport { StringOutputParser } from \"./string.js\";\nimport { AsymmetricStructuredOutputParser, JsonMarkdownStructuredOutputParser, StructuredOutputParser } from \"./structured.js\";\nimport { JsonOutputParser } from \"./json.js\";\nimport { XMLOutputParser, XML_FORMAT_INSTRUCTIONS, parseXMLMarkdown } from \"./xml.js\";\n\n//#region src/output_parsers/index.ts\nvar output_parsers_exports = /* @__PURE__ */ __exportAll({\n\tAsymmetricStructuredOutputParser: () => AsymmetricStructuredOutputParser,\n\tBaseCumulativeTransformOutputParser: () => BaseCumulativeTransformOutputParser,\n\tBaseLLMOutputParser: () => BaseLLMOutputParser,\n\tBaseOutputParser: () => BaseOutputParser,\n\tBaseTransformOutputParser: () => BaseTransformOutputParser,\n\tBytesOutputParser: () => BytesOutputParser,\n\tCommaSeparatedListOutputParser: () => CommaSeparatedListOutputParser,\n\tCustomListOutputParser: () => CustomListOutputParser,\n\tJsonMarkdownStructuredOutputParser: () => JsonMarkdownStructuredOutputParser,\n\tJsonOutputParser: () => JsonOutputParser,\n\tListOutputParser: () => ListOutputParser,\n\tMarkdownListOutputParser: () => MarkdownListOutputParser,\n\tNumberedListOutputParser: () => NumberedListOutputParser,\n\tOutputParserException: () => OutputParserException,\n\tStringOutputParser: () => StringOutputParser,\n\tStructuredOutputParser: () => StructuredOutputParser,\n\tXMLOutputParser: () => XMLOutputParser,\n\tXML_FORMAT_INSTRUCTIONS: () => XML_FORMAT_INSTRUCTIONS,\n\tparseJsonMarkdown: () => parseJsonMarkdown,\n\tparsePartialJson: () => parsePartialJson,\n\tparseXMLMarkdown: () => parseXMLMarkdown\n});\n\n//#endregion\nexport { AsymmetricStructuredOutputParser, BaseCumulativeTransformOutputParser, BaseLLMOutputParser, BaseOutputParser, BaseTransformOutputParser, BytesOutputParser, CommaSeparatedListOutputParser, CustomListOutputParser, JsonMarkdownStructuredOutputParser, JsonOutputParser, ListOutputParser, MarkdownListOutputParser, NumberedListOutputParser, OutputParserException, StringOutputParser, StructuredOutputParser, XMLOutputParser, XML_FORMAT_INSTRUCTIONS, output_parsers_exports, parseJsonMarkdown, parsePartialJson, parseXMLMarkdown };\n//# sourceMappingURL=index.js.map", "import type * as z3 from \"zod/v3\";\nimport type * as z4 from \"zod/v4/core\";\nimport { ChatGeneration, ChatGenerationChunk } from \"../../outputs.js\";\nimport { OutputParserException } from \"../base.js\";\nimport { parsePartialJson } from \"../json.js\";\nimport { InvalidToolCall, ToolCall } from \"../../messages/tool.js\";\nimport {\n  BaseCumulativeTransformOutputParser,\n  BaseCumulativeTransformOutputParserInput,\n} from \"../transform.js\";\nimport { isAIMessage } from \"../../messages/ai.js\";\nimport {\n  type InteropZodType,\n  interopSafeParseAsync,\n} from \"../../utils/types/zod.js\";\n\nexport type ParsedToolCall = {\n  id?: string;\n\n  type: string;\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  args: Record<string, any>;\n};\n\nexport type JsonOutputToolsParserParams = {\n  /** Whether to return the tool call id. */\n  returnId?: boolean;\n} & BaseCumulativeTransformOutputParserInput;\n\nexport function parseToolCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>,\n  options: { returnId?: boolean; partial: true }\n): ToolCall | undefined;\nexport function parseToolCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>,\n  options?: { returnId?: boolean; partial?: false }\n): ToolCall;\nexport function parseToolCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>,\n  options?: { returnId?: boolean; partial?: boolean }\n): ToolCall | undefined;\nexport function parseToolCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>,\n  options?: { returnId?: boolean; partial?: boolean }\n): ToolCall | undefined {\n  if (rawToolCall.function === undefined) {\n    return undefined;\n  }\n  let functionArgs;\n  if (options?.partial) {\n    try {\n      functionArgs = parsePartialJson(rawToolCall.function.arguments ?? \"{}\");\n    } catch {\n      return undefined;\n    }\n  } else {\n    try {\n      functionArgs = JSON.parse(rawToolCall.function.arguments);\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    } catch (e: any) {\n      throw new OutputParserException(\n        [\n          `Function \"${rawToolCall.function.name}\" arguments:`,\n          ``,\n          rawToolCall.function.arguments,\n          ``,\n          `are not valid JSON.`,\n          `Error: ${e.message}`,\n        ].join(\"\\n\")\n      );\n    }\n  }\n\n  const parsedToolCall: ToolCall = {\n    name: rawToolCall.function.name,\n    args: functionArgs,\n    type: \"tool_call\",\n  };\n\n  if (options?.returnId) {\n    parsedToolCall.id = rawToolCall.id;\n  }\n\n  return parsedToolCall;\n}\n\nexport function convertLangChainToolCallToOpenAI(toolCall: ToolCall) {\n  if (toolCall.id === undefined) {\n    throw new Error(`All OpenAI tool calls must have an \"id\" field.`);\n  }\n  return {\n    id: toolCall.id,\n    type: \"function\",\n    function: {\n      name: toolCall.name,\n      arguments: JSON.stringify(toolCall.args),\n    },\n  };\n}\n\nexport function makeInvalidToolCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>,\n  errorMsg?: string\n): InvalidToolCall {\n  return {\n    name: rawToolCall.function?.name,\n    args: rawToolCall.function?.arguments,\n    id: rawToolCall.id,\n    error: errorMsg,\n    type: \"invalid_tool_call\",\n  };\n}\n\n/**\n * Class for parsing the output of a tool-calling LLM into a JSON object.\n */\nexport class JsonOutputToolsParser<\n  T,\n> extends BaseCumulativeTransformOutputParser<T> {\n  static lc_name() {\n    return \"JsonOutputToolsParser\";\n  }\n\n  returnId = false;\n\n  lc_namespace = [\"langchain\", \"output_parsers\", \"openai_tools\"];\n\n  lc_serializable = true;\n\n  constructor(fields?: JsonOutputToolsParserParams) {\n    super(fields);\n    this.returnId = fields?.returnId ?? this.returnId;\n  }\n\n  protected _diff() {\n    throw new Error(\"Not supported.\");\n  }\n\n  async parse(): Promise<T> {\n    throw new Error(\"Not implemented.\");\n  }\n\n  async parseResult(generations: ChatGeneration[]): Promise<T> {\n    const result = await this.parsePartialResult(generations, false);\n    return result;\n  }\n\n  /**\n   * Parses the output and returns a JSON object. If `argsOnly` is true,\n   * only the arguments of the function call are returned.\n   * @param generations The output of the LLM to parse.\n   * @returns A JSON object representation of the function call or its arguments.\n   */\n  async parsePartialResult(\n    generations: ChatGenerationChunk[] | ChatGeneration[],\n    partial = true\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  ): Promise<any> {\n    const message = generations[0].message;\n    let toolCalls;\n    if (isAIMessage(message) && message.tool_calls?.length) {\n      toolCalls = message.tool_calls.map((toolCall) => {\n        const { id, ...rest } = toolCall;\n        if (!this.returnId) {\n          return rest;\n        }\n        return {\n          id,\n          ...rest,\n        };\n      });\n    } else if (message.additional_kwargs.tool_calls !== undefined) {\n      const rawToolCalls = JSON.parse(\n        JSON.stringify(message.additional_kwargs.tool_calls)\n      );\n      toolCalls = rawToolCalls.map((rawToolCall: Record<string, unknown>) => {\n        return parseToolCall(rawToolCall, { returnId: this.returnId, partial });\n      });\n    }\n    if (!toolCalls) {\n      return [];\n    }\n    const parsedToolCalls = [];\n    for (const toolCall of toolCalls) {\n      if (toolCall !== undefined) {\n        const backwardsCompatibleToolCall: ParsedToolCall = {\n          type: toolCall.name,\n          args: toolCall.args,\n          id: toolCall.id,\n        };\n        parsedToolCalls.push(backwardsCompatibleToolCall);\n      }\n    }\n    return parsedToolCalls;\n  }\n}\n\ntype JsonOutputKeyToolsParserParamsBase = {\n  keyName: string;\n  returnSingle?: boolean;\n} & JsonOutputToolsParserParams;\n\ntype JsonOutputKeyToolsParserParamsV3<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  T extends Record<string, any> = Record<string, any>,\n> = { zodSchema?: z3.ZodType<T> } & JsonOutputKeyToolsParserParamsBase;\n\ntype JsonOutputKeyToolsParserParamsV4<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  T extends Record<string, any> = Record<string, any>,\n> = { zodSchema?: z4.$ZodType<T, T> } & JsonOutputKeyToolsParserParamsBase;\n\nexport type JsonOutputKeyToolsParserParamsInterop<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  T extends Record<string, any> = Record<string, any>,\n> = { zodSchema?: InteropZodType<T> } & JsonOutputKeyToolsParserParamsBase;\n\n// Use Zod 3 for backwards compatibility\nexport type JsonOutputKeyToolsParserParams<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  T extends Record<string, any> = Record<string, any>,\n> = JsonOutputKeyToolsParserParamsV3<T>;\n\n/**\n * Class for parsing the output of a tool-calling LLM into a JSON object if you are\n * expecting only a single tool to be called.\n */\nexport class JsonOutputKeyToolsParser<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  T extends Record<string, any> = Record<string, any>,\n> extends JsonOutputToolsParser<T> {\n  static lc_name() {\n    return \"JsonOutputKeyToolsParser\";\n  }\n\n  lc_namespace = [\"langchain\", \"output_parsers\", \"openai_tools\"];\n\n  lc_serializable = true;\n\n  returnId = false;\n\n  /** The type of tool calls to return. */\n  keyName: string;\n\n  /** Whether to return only the first tool call. */\n  returnSingle = false;\n\n  zodSchema?: InteropZodType<T>;\n\n  constructor(params: JsonOutputKeyToolsParserParamsV3<T>);\n\n  constructor(params: JsonOutputKeyToolsParserParamsV4<T>);\n\n  constructor(params: JsonOutputKeyToolsParserParamsInterop<T>);\n\n  constructor(\n    params:\n      | JsonOutputKeyToolsParserParamsV3<T>\n      | JsonOutputKeyToolsParserParamsV4<T>\n      | JsonOutputKeyToolsParserParamsInterop<T>\n  ) {\n    super(params);\n    this.keyName = params.keyName;\n    this.returnSingle = params.returnSingle ?? this.returnSingle;\n    this.zodSchema = params.zodSchema;\n  }\n\n  protected async _validateResult(result: unknown): Promise<T> {\n    if (this.zodSchema === undefined) {\n      return result as T;\n    }\n    const zodParsedResult = await interopSafeParseAsync(this.zodSchema, result);\n    if (zodParsedResult.success) {\n      return zodParsedResult.data;\n    } else {\n      throw new OutputParserException(\n        `Failed to parse. Text: \"${JSON.stringify(\n          result,\n          null,\n          2\n        )}\". Error: ${JSON.stringify(zodParsedResult.error?.issues)}`,\n        JSON.stringify(result, null, 2)\n      );\n    }\n  }\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  async parsePartialResult(generations: ChatGeneration[]): Promise<any> {\n    const results = await super.parsePartialResult(generations);\n    const matchingResults = results.filter(\n      (result: ParsedToolCall) => result.type === this.keyName\n    );\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    let returnedValues: ParsedToolCall[] | Record<string, any>[] =\n      matchingResults;\n    if (!matchingResults.length) {\n      return undefined;\n    }\n    if (!this.returnId) {\n      returnedValues = matchingResults.map(\n        (result: ParsedToolCall) => result.args\n      );\n    }\n    if (this.returnSingle) {\n      return returnedValues[0];\n    }\n    return returnedValues;\n  }\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  async parseResult(generations: ChatGeneration[]): Promise<any> {\n    const results = await super.parsePartialResult(generations, false);\n    const matchingResults = results.filter(\n      (result: ParsedToolCall) => result.type === this.keyName\n    );\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    let returnedValues: ParsedToolCall[] | Record<string, any>[] =\n      matchingResults;\n    if (!matchingResults.length) {\n      return undefined;\n    }\n    if (!this.returnId) {\n      returnedValues = matchingResults.map(\n        (result: ParsedToolCall) => result.args\n      );\n    }\n    if (this.returnSingle) {\n      return this._validateResult(returnedValues[0]);\n    }\n    const toolCallResults = await Promise.all(\n      returnedValues.map((value) => this._validateResult(value))\n    );\n    return toolCallResults;\n  }\n}\n", "import { __exportAll } from \"../../_virtual/_rolldown/runtime.js\";\nimport { JsonOutputKeyToolsParser, JsonOutputToolsParser, convertLangChainToolCallToOpenAI, makeInvalidToolCall, parseToolCall } from \"./json_output_tools_parsers.js\";\n\n//#region src/output_parsers/openai_tools/index.ts\nvar openai_tools_exports = /* @__PURE__ */ __exportAll({\n\tJsonOutputKeyToolsParser: () => JsonOutputKeyToolsParser,\n\tJsonOutputToolsParser: () => JsonOutputToolsParser,\n\tconvertLangChainToolCallToOpenAI: () => convertLangChainToolCallToOpenAI,\n\tmakeInvalidToolCall: () => makeInvalidToolCall,\n\tparseToolCall: () => parseToolCall\n});\n\n//#endregion\nexport { JsonOutputKeyToolsParser, JsonOutputToolsParser, convertLangChainToolCallToOpenAI, makeInvalidToolCall, openai_tools_exports, parseToolCall };\n//# sourceMappingURL=index.js.map", "import OpenAI, { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { type ChatGeneration } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  BaseChatModel,\n  type LangSmithParams,\n  type BaseChatModelParams,\n  BaseChatModelCallOptions,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  isOpenAITool as isOpenAIFunctionTool,\n  type BaseFunctionCallOptions,\n  type BaseLanguageModelInput,\n  type FunctionDefinition,\n  type StructuredOutputMethodOptions,\n} from \"@langchain/core/language_models/base\";\nimport { ModelProfile } from \"@langchain/core/language_models/profile\";\nimport {\n  Runnable,\n  RunnableLambda,\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"@langchain/core/runnables\";\nimport {\n  JsonOutputParser,\n  StructuredOutputParser,\n} from \"@langchain/core/output_parsers\";\nimport { JsonOutputKeyToolsParser } from \"@langchain/core/output_parsers/openai_tools\";\nimport {\n  getSchemaDescription,\n  InteropZodType,\n  isInteropZodSchema,\n} from \"@langchain/core/utils/types\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\nimport {\n  type OpenAICallOptions,\n  type OpenAIChatInput,\n  type OpenAICoreRequestOptions,\n  type ChatOpenAIResponseFormat,\n  ResponseFormatConfiguration,\n  OpenAIVerbosityParam,\n  type OpenAIApiKey,\n  OpenAICacheRetentionParam,\n} from \"../types.js\";\nimport {\n  type OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"../utils/azure.js\";\nimport {\n  type FunctionDef,\n  formatFunctionDefinitions,\n  OpenAIToolChoice,\n  _convertToOpenAITool,\n  ChatOpenAIToolType,\n  convertResponsesCustomTool,\n  isBuiltInTool,\n  isCustomTool,\n  hasProviderToolDefinition,\n  ResponsesToolChoice,\n} from \"../utils/tools.js\";\nimport {\n  getStructuredOutputMethod,\n  interopZodResponseFormat,\n  _convertOpenAIResponsesUsageToLangChainUsage,\n} from \"../utils/output.js\";\nimport { isReasoningModel, messageToOpenAIRole } from \"../utils/misc.js\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\nimport PROFILES from \"./profiles.js\";\n\ninterface OpenAILLMOutput {\n  tokenUsage: {\n    completionTokens?: number;\n    promptTokens?: number;\n    totalTokens?: number;\n  };\n}\n\nexport type { OpenAICallOptions, OpenAIChatInput };\n\nexport interface BaseChatOpenAICallOptions\n  extends BaseChatModelCallOptions, BaseFunctionCallOptions {\n  /**\n   * Additional options to pass to the underlying axios request.\n   */\n  options?: OpenAICoreRequestOptions;\n\n  /**\n   * A list of tools that the model may use to generate responses.\n   * Each tool can be a function, a built-in tool, or a custom tool definition.\n   * If not provided, the model will not use any tools.\n   */\n  tools?: ChatOpenAIToolType[];\n\n  /**\n   * Specifies which tool the model should use to respond.\n   * Can be an {@link OpenAIToolChoice} or a {@link ResponsesToolChoice}.\n   * If not set, the model will decide which tool to use automatically.\n   */\n  // TODO: break OpenAIToolChoice and ResponsesToolChoice into options sub classes\n  tool_choice?: OpenAIToolChoice | ResponsesToolChoice;\n\n  /**\n   * Adds a prompt index to prompts passed to the model to track\n   * what prompt is being used for a given generation.\n   */\n  promptIndex?: number;\n\n  /**\n   * An object specifying the format that the model must output.\n   */\n  response_format?: ChatOpenAIResponseFormat;\n\n  /**\n   * When provided, the completions API will make a best effort to sample\n   * deterministically, such that repeated requests with the same `seed`\n   * and parameters should return the same result.\n   */\n  seed?: number;\n\n  /**\n   * Additional options to pass to streamed completions.\n   * If provided, this takes precedence over \"streamUsage\" set at\n   * initialization time.\n   */\n  stream_options?: OpenAIClient.Chat.ChatCompletionStreamOptions;\n\n  /**\n   * The model may choose to call multiple functions in a single turn. You can\n   * set parallel_tool_calls to false which ensures only one tool is called at most.\n   * [Learn more](https://platform.openai.com/docs/guides/function-calling#parallel-function-calling)\n   */\n  parallel_tool_calls?: boolean;\n\n  /**\n   * If `true`, model output is guaranteed to exactly match the JSON Schema\n   * provided in the tool definition. If `true`, the input schema will also be\n   * validated according to\n   * https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n   *\n   * If `false`, input schema will not be validated and model output will not\n   * be validated.\n   *\n   * If `undefined`, `strict` argument will not be passed to the model.\n   */\n  strict?: boolean;\n\n  /**\n   * Output types that you would like the model to generate for this request. Most\n   * models are capable of generating text, which is the default:\n   *\n   * `[\"text\"]`\n   *\n   * The `gpt-4o-audio-preview` model can also be used to\n   * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n   * this model generate both text and audio responses, you can use:\n   *\n   * `[\"text\", \"audio\"]`\n   */\n  modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n\n  /**\n   * Parameters for audio output. Required when audio output is requested with\n   * `modalities: [\"audio\"]`.\n   * [Learn more](https://platform.openai.com/docs/guides/audio).\n   */\n  audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n\n  /**\n   * Static predicted output content, such as the content of a text file that is being regenerated.\n   * [Learn more](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs).\n   */\n  prediction?: OpenAIClient.ChatCompletionPredictionContent;\n\n  /**\n   * Options for reasoning models.\n   *\n   * Note that some options, like reasoning summaries, are only available when using the responses\n   * API. If these options are set, the responses API will be used to fulfill the request.\n   *\n   * These options will be ignored when not using a reasoning model.\n   */\n  reasoning?: OpenAIClient.Reasoning;\n\n  /**\n   * Constrains effort on reasoning for reasoning models. Reduces reasoning in responses,\n   * which can reduce latency and cost at the expense of quality.\n   *\n   * Accepts values: \"low\", \"medium\", or \"high\".\n   *\n   * @deprecated This is a convenience option that will be merged into the `reasoning` object.\n   * Use `reasoning.effort` instead.\n   */\n  reasoningEffort?: OpenAIClient.Reasoning[\"effort\"];\n\n  /**\n   * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\"\n   * Specifies the service tier for prioritization and latency optimization.\n   */\n  service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates. Replaces the `user` field.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  promptCacheKey?: string;\n\n  /**\n   * Used by OpenAI to set cache retention time\n   */\n  promptCacheRetention?: OpenAICacheRetentionParam;\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n}\n\nexport interface BaseChatOpenAIFields\n  extends Partial<OpenAIChatInput>, BaseChatModelParams {\n  /**\n   * Optional configuration options for the OpenAI client.\n   */\n  configuration?: ClientOptions;\n}\n\nexport function getChatOpenAIModelParams<TParams extends BaseChatOpenAIFields>(\n  modelOrParams?: string | TParams,\n  paramsArg?: Omit<TParams, \"model\">\n): TParams | undefined {\n  if (typeof modelOrParams === \"string\") {\n    return { model: modelOrParams, ...(paramsArg ?? {}) } as TParams;\n  }\n  if (modelOrParams == null) {\n    return paramsArg as TParams | undefined;\n  }\n  return modelOrParams;\n}\n\n/** @internal */\nexport abstract class BaseChatOpenAI<\n  CallOptions extends BaseChatOpenAICallOptions,\n>\n  extends BaseChatModel<CallOptions, AIMessageChunk>\n  implements Partial<OpenAIChatInput>\n{\n  temperature?: number;\n\n  topP?: number;\n\n  frequencyPenalty?: number;\n\n  presencePenalty?: number;\n\n  n?: number;\n\n  logitBias?: Record<string, number>;\n\n  model = \"gpt-3.5-turbo\";\n\n  modelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n\n  stop?: string[];\n\n  stopSequences?: string[];\n\n  user?: string;\n\n  timeout?: number;\n\n  streaming = false;\n\n  streamUsage = true;\n\n  maxTokens?: number;\n\n  logprobs?: boolean;\n\n  topLogprobs?: number;\n\n  apiKey?: OpenAIApiKey;\n\n  organization?: string;\n\n  __includeRawResponse?: boolean;\n\n  /** @internal */\n  client: OpenAIClient;\n\n  /** @internal */\n  clientConfig: ClientOptions;\n\n  /**\n   * Whether the model supports the `strict` argument when passing in tools.\n   * If `undefined` the `strict` argument will not be passed to OpenAI.\n   */\n  supportsStrictToolCalling?: boolean;\n\n  audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n\n  modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n\n  reasoning?: OpenAIClient.Reasoning;\n\n  /**\n   * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable\n   * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your\n   * OpenAI organization or project. This must be configured directly with OpenAI.\n   *\n   * See:\n   * https://platform.openai.com/docs/guides/your-data\n   * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n   *\n   * @default false\n   */\n  zdrEnabled?: boolean | undefined;\n\n  /**\n   * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n   * Specifies the service tier for prioritization and latency optimization.\n   */\n  service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  promptCacheKey: string;\n\n  /**\n   * Used by OpenAI to set cache retention time\n   */\n  promptCacheRetention?: OpenAICacheRetentionParam;\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n\n  protected defaultOptions: CallOptions;\n\n  _llmType() {\n    return \"openai\";\n  }\n\n  static lc_name() {\n    return \"ChatOpenAI\";\n  }\n\n  get callKeys() {\n    return [\n      ...super.callKeys,\n      \"options\",\n      \"function_call\",\n      \"functions\",\n      \"tools\",\n      \"tool_choice\",\n      \"promptIndex\",\n      \"response_format\",\n      \"seed\",\n      \"reasoning\",\n      \"reasoning_effort\",\n      \"service_tier\",\n    ];\n  }\n\n  lc_serializable = true;\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\",\n    };\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      apiKey: \"openai_api_key\",\n      modelName: \"model\",\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [\n      \"configuration\",\n      \"logprobs\",\n      \"topLogprobs\",\n      \"prefixMessages\",\n      \"supportsStrictToolCalling\",\n      \"modalities\",\n      \"audio\",\n      \"temperature\",\n      \"maxTokens\",\n      \"topP\",\n      \"frequencyPenalty\",\n      \"presencePenalty\",\n      \"n\",\n      \"logitBias\",\n      \"user\",\n      \"streaming\",\n      \"streamUsage\",\n      \"model\",\n      \"modelName\",\n      \"modelKwargs\",\n      \"stop\",\n      \"stopSequences\",\n      \"timeout\",\n      \"apiKey\",\n      \"cache\",\n      \"maxConcurrency\",\n      \"maxRetries\",\n      \"verbose\",\n      \"callbacks\",\n      \"tags\",\n      \"metadata\",\n      \"disableStreaming\",\n      \"zdrEnabled\",\n      \"reasoning\",\n      \"promptCacheKey\",\n      \"promptCacheRetention\",\n      \"verbosity\",\n    ];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = this.invocationParams(options);\n    return {\n      ls_provider: \"openai\",\n      ls_model_name: this.model,\n      ls_model_type: \"chat\",\n      ls_temperature: params.temperature ?? undefined,\n      ls_max_tokens: params.max_tokens ?? undefined,\n      ls_stop: options.stop,\n    };\n  }\n\n  /** @ignore */\n  _identifyingParams(): Omit<\n    OpenAIClient.Chat.ChatCompletionCreateParams,\n    \"messages\"\n  > & {\n    model_name: string;\n  } & ClientOptions {\n    return {\n      model_name: this.model,\n      ...this.invocationParams(),\n      ...this.clientConfig,\n    };\n  }\n\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n\n  constructor(fields?: BaseChatOpenAIFields) {\n    super(fields ?? {});\n\n    const configApiKey =\n      typeof fields?.configuration?.apiKey === \"string\" ||\n      typeof fields?.configuration?.apiKey === \"function\"\n        ? fields?.configuration?.apiKey\n        : undefined;\n    this.apiKey =\n      fields?.apiKey ??\n      configApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.organization =\n      fields?.configuration?.organization ??\n      getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.timeout = fields?.timeout;\n\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.logprobs = fields?.logprobs;\n    this.topLogprobs = fields?.topLogprobs;\n    this.n = fields?.n ?? this.n;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stopSequences ?? fields?.stop;\n    this.stopSequences = this.stop;\n    this.user = fields?.user;\n    this.__includeRawResponse = fields?.__includeRawResponse;\n    this.audio = fields?.audio;\n    this.modalities = fields?.modalities;\n    this.reasoning = fields?.reasoning;\n    this.maxTokens = fields?.maxCompletionTokens ?? fields?.maxTokens;\n    this.promptCacheKey = fields?.promptCacheKey ?? this.promptCacheKey;\n    this.promptCacheRetention =\n      fields?.promptCacheRetention ?? this.promptCacheRetention;\n    this.verbosity = fields?.verbosity ?? this.verbosity;\n\n    this.disableStreaming = fields?.disableStreaming === true;\n    this.streaming = fields?.streaming === true;\n    if (this.disableStreaming) this.streaming = false;\n    // disable streaming in BaseChatModel if explicitly disabled\n    if (fields?.streaming === false) this.disableStreaming = true;\n\n    this.streamUsage = fields?.streamUsage ?? this.streamUsage;\n    if (this.disableStreaming) this.streamUsage = false;\n\n    this.clientConfig = {\n      apiKey: this.apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration,\n    };\n\n    // If `supportsStrictToolCalling` is explicitly set, use that value.\n    // Else leave undefined so it's not passed to OpenAI.\n    if (fields?.supportsStrictToolCalling !== undefined) {\n      this.supportsStrictToolCalling = fields.supportsStrictToolCalling;\n    }\n\n    if (fields?.service_tier !== undefined) {\n      this.service_tier = fields.service_tier;\n    }\n\n    this.zdrEnabled = fields?.zdrEnabled ?? false;\n  }\n\n  /**\n   * Returns backwards compatible reasoning parameters from constructor params and call options\n   * @internal\n   */\n  protected _getReasoningParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): OpenAIClient.Reasoning | undefined {\n    if (!isReasoningModel(this.model)) {\n      return;\n    }\n\n    // apply options in reverse order of importance -- newer options supersede older options\n    let reasoning: OpenAIClient.Reasoning | undefined;\n    if (this.reasoning !== undefined) {\n      reasoning = {\n        ...reasoning,\n        ...this.reasoning,\n      };\n    }\n    if (options?.reasoning !== undefined) {\n      reasoning = {\n        ...reasoning,\n        ...options.reasoning,\n      };\n    }\n\n    // Coalesce reasoningEffort into reasoning.effort if reasoning.effort is not already set\n    if (\n      options?.reasoningEffort !== undefined &&\n      reasoning?.effort === undefined\n    ) {\n      reasoning = {\n        ...reasoning,\n        effort: options.reasoningEffort,\n      };\n    }\n\n    return reasoning;\n  }\n\n  /**\n   * Returns an openai compatible response format from a set of options\n   * @internal\n   */\n  protected _getResponseFormat(\n    resFormat?: CallOptions[\"response_format\"]\n  ): ResponseFormatConfiguration | undefined {\n    if (\n      resFormat &&\n      resFormat.type === \"json_schema\" &&\n      resFormat.json_schema.schema &&\n      isInteropZodSchema(resFormat.json_schema.schema)\n    ) {\n      return interopZodResponseFormat(\n        resFormat.json_schema.schema,\n        resFormat.json_schema.name,\n        {\n          description: resFormat.json_schema.description,\n        }\n      );\n    }\n    return resFormat as ResponseFormatConfiguration | undefined;\n  }\n\n  protected _combineCallOptions(\n    additionalOptions?: this[\"ParsedCallOptions\"]\n  ): this[\"ParsedCallOptions\"] {\n    return {\n      ...this.defaultOptions,\n      ...(additionalOptions ?? {}),\n    };\n  }\n\n  /** @internal */\n  _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(params.defaultHeaders);\n\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options,\n    } as OpenAICoreRequestOptions;\n    return requestOptions;\n  }\n\n  // TODO: move to completions class\n  protected _convertChatOpenAIToolToCompletionsTool(\n    tool: ChatOpenAIToolType,\n    fields?: { strict?: boolean }\n  ): OpenAIClient.ChatCompletionTool {\n    if (isCustomTool(tool)) {\n      return convertResponsesCustomTool(tool.metadata.customTool);\n    }\n    if (isOpenAIFunctionTool(tool)) {\n      if (fields?.strict !== undefined) {\n        return {\n          ...tool,\n          function: {\n            ...tool.function,\n            strict: fields.strict,\n          },\n        };\n      }\n\n      return tool;\n    }\n    return _convertToOpenAITool(tool, fields);\n  }\n\n  override bindTools(\n    tools: ChatOpenAIToolType[],\n    kwargs?: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    let strict: boolean | undefined;\n    if (kwargs?.strict !== undefined) {\n      strict = kwargs.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n    return this.withConfig({\n      tools: tools.map((tool) => {\n        // Built-in tools and custom tools pass through as-is\n        if (isBuiltInTool(tool) || isCustomTool(tool)) {\n          return tool;\n        }\n        // Tools with providerToolDefinition (e.g., localShell, shell, computerUse, applyPatch)\n        // should use their provider-specific definition\n        if (hasProviderToolDefinition(tool)) {\n          return tool.extras.providerToolDefinition;\n        }\n        // Regular tools get converted to OpenAI function format\n        return this._convertChatOpenAIToolToCompletionsTool(tool, { strict });\n      }),\n      ...kwargs,\n    } as Partial<CallOptions>);\n  }\n\n  override async stream(input: BaseLanguageModelInput, options?: CallOptions) {\n    return super.stream(\n      input,\n      this._combineCallOptions(options) as CallOptions\n    );\n  }\n\n  override async invoke(input: BaseLanguageModelInput, options?: CallOptions) {\n    return super.invoke(\n      input,\n      this._combineCallOptions(options) as CallOptions\n    );\n  }\n\n  /** @ignore */\n  _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput {\n    return llmOutputs.reduce<{\n      [key in keyof OpenAILLMOutput]: Required<OpenAILLMOutput[key]>;\n    }>(\n      (acc, llmOutput) => {\n        if (llmOutput && llmOutput.tokenUsage) {\n          acc.tokenUsage.completionTokens +=\n            llmOutput.tokenUsage.completionTokens ?? 0;\n          acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n          acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n        }\n        return acc;\n      },\n      {\n        tokenUsage: {\n          completionTokens: 0,\n          promptTokens: 0,\n          totalTokens: 0,\n        },\n      }\n    );\n  }\n\n  async getNumTokensFromMessages(messages: BaseMessage[]) {\n    let totalCount = 0;\n    let tokensPerMessage = 0;\n    let tokensPerName = 0;\n\n    // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    if (this.model === \"gpt-3.5-turbo-0301\") {\n      tokensPerMessage = 4;\n      tokensPerName = -1;\n    } else {\n      tokensPerMessage = 3;\n      tokensPerName = 1;\n    }\n\n    const countPerMessage = await Promise.all(\n      messages.map(async (message) => {\n        const [textCount, roleCount] = await Promise.all([\n          this.getNumTokens(message.content),\n          this.getNumTokens(messageToOpenAIRole(message)),\n        ]);\n        const nameCount =\n          message.name !== undefined\n            ? tokensPerName + (await this.getNumTokens(message.name))\n            : 0;\n        let count = textCount + tokensPerMessage + roleCount + nameCount;\n\n        // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate\n        const openAIMessage = message;\n        if (openAIMessage._getType() === \"function\") {\n          count -= 2;\n        }\n        if (openAIMessage.additional_kwargs?.function_call) {\n          count += 3;\n        }\n        if (openAIMessage?.additional_kwargs.function_call?.name) {\n          count += await this.getNumTokens(\n            openAIMessage.additional_kwargs.function_call?.name\n          );\n        }\n        if (openAIMessage.additional_kwargs.function_call?.arguments) {\n          try {\n            count += await this.getNumTokens(\n              // Remove newlines and spaces\n              JSON.stringify(\n                JSON.parse(\n                  openAIMessage.additional_kwargs.function_call?.arguments\n                )\n              )\n            );\n          } catch (error) {\n            console.error(\n              \"Error parsing function arguments\",\n              error,\n              JSON.stringify(openAIMessage.additional_kwargs.function_call)\n            );\n            count += await this.getNumTokens(\n              openAIMessage.additional_kwargs.function_call?.arguments\n            );\n          }\n        }\n\n        totalCount += count;\n        return count;\n      })\n    );\n\n    totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n\n    return { totalCount, countPerMessage };\n  }\n\n  /** @internal */\n  protected async _getNumTokensFromGenerations(generations: ChatGeneration[]) {\n    const generationUsages = await Promise.all(\n      generations.map(async (generation) => {\n        if (generation.message.additional_kwargs?.function_call) {\n          return (await this.getNumTokensFromMessages([generation.message]))\n            .countPerMessage[0];\n        } else {\n          return await this.getNumTokens(generation.message.content);\n        }\n      })\n    );\n\n    return generationUsages.reduce((a, b) => a + b, 0);\n  }\n\n  /** @internal */\n  protected async _getEstimatedTokenCountFromPrompt(\n    messages: BaseMessage[],\n    functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[],\n    function_call?:\n      | \"none\"\n      | \"auto\"\n      | OpenAIClient.Chat.ChatCompletionFunctionCallOption\n  ): Promise<number> {\n    // It appears that if functions are present, the first system message is padded with a trailing newline. This\n    // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.\n\n    let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;\n\n    // If there are functions, add the function definitions as they count towards token usage\n    if (functions && function_call !== \"auto\") {\n      const promptDefinitions = formatFunctionDefinitions(\n        functions as unknown as FunctionDef[]\n      );\n      tokens += await this.getNumTokens(promptDefinitions);\n      tokens += 9; // Add nine per completion\n    }\n\n    // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because\n    // functions typically add a system message, but reuse the first one if it's already there. This offsets\n    // the extra 9 tokens added by the function definitions.\n    if (functions && messages.find((m) => m._getType() === \"system\")) {\n      tokens -= 4;\n    }\n\n    // If function_call is 'none', add one token.\n    // If it's a FunctionCall object, add 4 + the number of tokens in the function name.\n    // If it's undefined or 'auto', don't add anything.\n    if (function_call === \"none\") {\n      tokens += 1;\n    } else if (typeof function_call === \"object\") {\n      tokens += (await this.getNumTokens(function_call.name)) + 4;\n    }\n\n    return tokens;\n  }\n\n  /**\n   * Moderate content using OpenAI's Moderation API.\n   *\n   * This method checks whether content violates OpenAI's content policy by\n   * analyzing text for categories such as hate, harassment, self-harm,\n   * sexual content, violence, and more.\n   *\n   * @param input - The text or array of texts to moderate\n   * @param params - Optional parameters for the moderation request\n   * @param params.model - The moderation model to use. Defaults to \"omni-moderation-latest\".\n   * @param params.options - Additional options to pass to the underlying request\n   * @returns A promise that resolves to the moderation response containing results for each input\n   *\n   * @example\n   * ```typescript\n   * const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n   *\n   * // Moderate a single text\n   * const result = await model.moderateContent(\"This is a test message\");\n   * console.log(result.results[0].flagged); // false\n   * console.log(result.results[0].categories); // { hate: false, harassment: false, ... }\n   *\n   * // Moderate multiple texts\n   * const results = await model.moderateContent([\n   *   \"Hello, how are you?\",\n   *   \"This is inappropriate content\"\n   * ]);\n   * results.results.forEach((result, index) => {\n   *   console.log(`Text ${index + 1} flagged:`, result.flagged);\n   * });\n   *\n   * // Use a specific moderation model\n   * const stableResult = await model.moderateContent(\n   *   \"Test content\",\n   *   { model: \"omni-moderation-latest\" }\n   * );\n   * ```\n   */\n  async moderateContent(\n    input: string | string[],\n    params?: {\n      model?: OpenAI.ModerationModel;\n      options?: OpenAICoreRequestOptions;\n    }\n  ): Promise<OpenAIClient.ModerationCreateResponse> {\n    const clientOptions = this._getClientOptions(params?.options);\n    const moderationModel = params?.model ?? \"omni-moderation-latest\";\n    const moderationRequest: OpenAIClient.ModerationCreateParams = {\n      input,\n      model: moderationModel,\n    };\n\n    return this.caller.call(async () => {\n      try {\n        const response = await this.client.moderations.create(\n          moderationRequest,\n          clientOptions\n        );\n        return response;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /**\n   * Return profiling information for the model.\n   *\n   * Provides information about the model's capabilities and constraints,\n   * including token limits, multimodal support, and advanced features like\n   * tool calling and structured output.\n   *\n   * @returns {ModelProfile} An object describing the model's capabilities and constraints\n   *\n   * @example\n   * ```typescript\n   * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n   * const profile = model.profile;\n   * console.log(profile.maxInputTokens); // 128000\n   * console.log(profile.imageInputs); // true\n   * ```\n   */\n  get profile(): ModelProfile {\n    return PROFILES[this.model] ?? {};\n  }\n\n  /** @internal */\n  protected _getStructuredOutputMethod(\n    config: StructuredOutputMethodOptions<boolean>\n  ) {\n    const ensuredConfig = { ...config };\n    if (\n      !this.model.startsWith(\"gpt-3\") &&\n      !this.model.startsWith(\"gpt-4-\") &&\n      this.model !== \"gpt-4\"\n    ) {\n      if (ensuredConfig?.method === undefined) {\n        return \"jsonSchema\";\n      }\n    } else if (ensuredConfig.method === \"jsonSchema\") {\n      console.warn(\n        `[WARNING]: JSON Schema is not supported for model \"${this.model}\". Falling back to tool calling.`\n      );\n    }\n    return ensuredConfig.method;\n  }\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>,\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  /**\n   * Add structured output to the model.\n   *\n   * The OpenAI model family supports the following structured output methods:\n   * - `jsonSchema`: Use the `response_format` field in the response to return a JSON schema. Only supported with the `gpt-4o-mini`,\n   *   `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots and later.\n   * - `functionCalling`: Function calling is useful when you are building an application that bridges the models and functionality\n   *   of your application.\n   * - `jsonMode`: JSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model\n   *   output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify.\n   *   We recommend you use `functionCalling` or `jsonSchema` if it is supported for your use case.\n   *\n   * The default method is `functionCalling`.\n   *\n   * @see https://platform.openai.com/docs/guides/structured-outputs\n   * @param outputSchema - The schema to use for structured output.\n   * @param config - The structured output method options.\n   * @returns The model with structured output.\n   */\n  withStructuredOutput<\n    RunOutput extends Record<string, unknown> = Record<string, unknown>,\n  >(\n    outputSchema: InteropZodType<RunOutput> | Record<string, unknown>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ) {\n    let llm: Runnable<BaseLanguageModelInput>;\n    let outputParser: Runnable<AIMessageChunk, RunOutput>;\n\n    const { schema, name, includeRaw } = {\n      ...config,\n      schema: outputSchema,\n    };\n\n    if (config?.strict !== undefined && config.method === \"jsonMode\") {\n      throw new Error(\n        \"Argument `strict` is only supported for `method` = 'function_calling'\"\n      );\n    }\n\n    const method = getStructuredOutputMethod(this.model, config?.method);\n\n    if (method === \"jsonMode\") {\n      if (isInteropZodSchema(schema)) {\n        outputParser = StructuredOutputParser.fromZodSchema(schema);\n      } else {\n        outputParser = new JsonOutputParser<RunOutput>();\n      }\n      const asJsonSchema = toJsonSchema(schema);\n      llm = this.withConfig({\n        outputVersion: \"v0\",\n        response_format: { type: \"json_object\" },\n        ls_structured_output_format: {\n          kwargs: { method: \"json_mode\" },\n          schema: { title: name ?? \"extract\", ...asJsonSchema },\n        },\n      } as Partial<CallOptions>);\n    } else if (method === \"jsonSchema\") {\n      const openaiJsonSchemaParams = {\n        name: name ?? \"extract\",\n        description: getSchemaDescription(schema),\n        schema,\n        strict: config?.strict,\n      };\n      const asJsonSchema = toJsonSchema(openaiJsonSchemaParams.schema);\n      llm = this.withConfig({\n        outputVersion: \"v0\",\n        response_format: {\n          type: \"json_schema\",\n          json_schema: openaiJsonSchemaParams,\n        },\n        ls_structured_output_format: {\n          kwargs: { method: \"json_schema\" },\n          schema: {\n            title: openaiJsonSchemaParams.name,\n            description: openaiJsonSchemaParams.description,\n            ...asJsonSchema,\n          },\n        },\n      } as Partial<CallOptions>);\n      if (isInteropZodSchema(schema)) {\n        const altParser = StructuredOutputParser.fromZodSchema(schema);\n        outputParser = RunnableLambda.from<AIMessageChunk, RunOutput>(\n          (aiMessage: AIMessageChunk) => {\n            if (\"parsed\" in aiMessage.additional_kwargs) {\n              return aiMessage.additional_kwargs.parsed as RunOutput;\n            }\n            return altParser;\n          }\n        );\n      } else {\n        outputParser = new JsonOutputParser<RunOutput>();\n      }\n    } else {\n      let functionName = name ?? \"extract\";\n      // Is function calling\n      if (isInteropZodSchema(schema)) {\n        const asJsonSchema = toJsonSchema(schema);\n        llm = this.withConfig({\n          outputVersion: \"v0\",\n          tools: [\n            {\n              type: \"function\" as const,\n              function: {\n                name: functionName,\n                description: asJsonSchema.description,\n                parameters: asJsonSchema,\n              },\n            },\n          ],\n          tool_choice: {\n            type: \"function\" as const,\n            function: {\n              name: functionName,\n            },\n          },\n          ls_structured_output_format: {\n            kwargs: { method: \"function_calling\" },\n            schema: { title: functionName, ...asJsonSchema },\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser({\n          returnSingle: true,\n          keyName: functionName,\n          zodSchema: schema,\n        });\n      } else {\n        let openAIFunctionDefinition: FunctionDefinition;\n        if (\n          typeof schema.name === \"string\" &&\n          typeof schema.parameters === \"object\" &&\n          schema.parameters != null\n        ) {\n          openAIFunctionDefinition = schema as unknown as FunctionDefinition;\n          functionName = schema.name;\n        } else {\n          functionName = (schema.title as string) ?? functionName;\n          openAIFunctionDefinition = {\n            name: functionName,\n            description: (schema.description as string) ?? \"\",\n            parameters: schema,\n          };\n        }\n        const asJsonSchema = toJsonSchema(schema);\n        llm = this.withConfig({\n          outputVersion: \"v0\",\n          tools: [\n            {\n              type: \"function\" as const,\n              function: openAIFunctionDefinition,\n            },\n          ],\n          tool_choice: {\n            type: \"function\" as const,\n            function: {\n              name: functionName,\n            },\n          },\n          ls_structured_output_format: {\n            kwargs: { method: \"function_calling\" },\n            schema: { title: functionName, ...asJsonSchema },\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser<RunOutput>({\n          returnSingle: true,\n          keyName: functionName,\n        });\n      }\n    }\n\n    if (!includeRaw) {\n      return llm.pipe(outputParser) as Runnable<\n        BaseLanguageModelInput,\n        RunOutput\n      >;\n    }\n\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input: any, config) => outputParser.invoke(input.raw, config),\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null,\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone],\n    });\n    return RunnableSequence.from<\n      BaseLanguageModelInput,\n      { raw: BaseMessage; parsed: RunOutput }\n    >([{ raw: llm }, parsedWithFallback]);\n  }\n}\n", "import {\n  AIMessage,\n  AIMessageChunk,\n  BaseMessage,\n  BaseMessageChunk,\n  ChatMessage,\n  ChatMessageChunk,\n  FunctionMessageChunk,\n  HumanMessageChunk,\n  OpenAIToolCall,\n  SystemMessageChunk,\n  ToolCallChunk,\n  ToolMessage,\n  ToolMessageChunk,\n  parseBase64DataUrl,\n  parseMimeType,\n  StandardContentBlockConverter,\n  isDataContentBlock,\n  ContentBlock,\n  iife,\n  convertToProviderContentBlock,\n} from \"@langchain/core/messages\";\nimport {\n  convertLangChainToolCallToOpenAI,\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport { Converter } from \"@langchain/core/utils/format\";\nimport type {\n  ChatCompletionContentPartText,\n  ChatCompletionContentPartImage,\n  ChatCompletionContentPartInputAudio,\n  ChatCompletionContentPart,\n} from \"openai/resources/chat/completions\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { handleMultiModalOutput } from \"../utils/output.js\";\nimport {\n  getRequiredFilenameFromMetadata,\n  isReasoningModel,\n  messageToOpenAIRole,\n} from \"../utils/misc.js\";\n\n/**\n * @deprecated This converter is an internal detail of the OpenAI provider. Do not use it directly. This will be revisited in a future release.\n */\nexport const completionsApiContentBlockConverter: StandardContentBlockConverter<{\n  text: ChatCompletionContentPartText;\n  image: ChatCompletionContentPartImage;\n  audio: ChatCompletionContentPartInputAudio;\n  file: ChatCompletionContentPart.File;\n}> = {\n  providerName: \"ChatOpenAI\",\n\n  fromStandardTextBlock(block): ChatCompletionContentPartText {\n    return { type: \"text\", text: block.text };\n  },\n\n  fromStandardImageBlock(block): ChatCompletionContentPartImage {\n    if (block.source_type === \"url\") {\n      return {\n        type: \"image_url\",\n        image_url: {\n          url: block.url,\n          ...(block.metadata?.detail\n            ? { detail: block.metadata.detail as \"auto\" | \"low\" | \"high\" }\n            : {}),\n        },\n      };\n    }\n\n    if (block.source_type === \"base64\") {\n      const url = `data:${block.mime_type ?? \"\"};base64,${block.data}`;\n      return {\n        type: \"image_url\",\n        image_url: {\n          url,\n          ...(block.metadata?.detail\n            ? { detail: block.metadata.detail as \"auto\" | \"low\" | \"high\" }\n            : {}),\n        },\n      };\n    }\n\n    throw new Error(\n      `Image content blocks with source_type ${block.source_type} are not supported for ChatOpenAI`\n    );\n  },\n\n  fromStandardAudioBlock(block): ChatCompletionContentPartInputAudio {\n    if (block.source_type === \"url\") {\n      const data = parseBase64DataUrl({ dataUrl: block.url });\n      if (!data) {\n        throw new Error(\n          `URL audio blocks with source_type ${block.source_type} must be formatted as a data URL for ChatOpenAI`\n        );\n      }\n\n      const rawMimeType = data.mime_type || block.mime_type || \"\";\n      let mimeType: { type: string; subtype: string };\n\n      try {\n        mimeType = parseMimeType(rawMimeType);\n      } catch {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      if (\n        mimeType.type !== \"audio\" ||\n        (mimeType.subtype !== \"wav\" && mimeType.subtype !== \"mp3\")\n      ) {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      return {\n        type: \"input_audio\",\n        input_audio: {\n          format: mimeType.subtype,\n          data: data.data,\n        },\n      };\n    }\n\n    if (block.source_type === \"base64\") {\n      let mimeType: { type: string; subtype: string };\n\n      try {\n        mimeType = parseMimeType(block.mime_type ?? \"\");\n      } catch {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      if (\n        mimeType.type !== \"audio\" ||\n        (mimeType.subtype !== \"wav\" && mimeType.subtype !== \"mp3\")\n      ) {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      return {\n        type: \"input_audio\",\n        input_audio: {\n          format: mimeType.subtype,\n          data: block.data,\n        },\n      };\n    }\n\n    throw new Error(\n      `Audio content blocks with source_type ${block.source_type} are not supported for ChatOpenAI`\n    );\n  },\n\n  fromStandardFileBlock(block): ChatCompletionContentPart.File {\n    if (block.source_type === \"url\") {\n      const data = parseBase64DataUrl({ dataUrl: block.url });\n\n      const filename = getRequiredFilenameFromMetadata(block);\n\n      if (!data) {\n        throw new Error(\n          `URL file blocks with source_type ${block.source_type} must be formatted as a data URL for ChatOpenAI`\n        );\n      }\n\n      return {\n        type: \"file\",\n        file: {\n          file_data: block.url, // formatted as base64 data URL\n          ...(block.metadata?.filename || block.metadata?.name\n            ? {\n                filename,\n              }\n            : {}),\n        },\n      };\n    }\n\n    if (block.source_type === \"base64\") {\n      const filename = getRequiredFilenameFromMetadata(block);\n\n      return {\n        type: \"file\",\n        file: {\n          file_data: `data:${block.mime_type ?? \"\"};base64,${block.data}`,\n          ...(block.metadata?.filename ||\n          block.metadata?.name ||\n          block.metadata?.title\n            ? {\n                filename,\n              }\n            : {}),\n        },\n      };\n    }\n\n    if (block.source_type === \"id\") {\n      return {\n        type: \"file\",\n        file: {\n          file_id: block.id,\n        },\n      };\n    }\n\n    throw new Error(\n      `File content blocks with source_type ${block.source_type} are not supported for ChatOpenAI`\n    );\n  },\n};\n\n/**\n * Converts an OpenAI Chat Completions API message to a LangChain BaseMessage.\n *\n * This converter transforms messages from OpenAI's Chat Completions API format into\n * LangChain's internal message representation, handling various message types and\n * preserving metadata, tool calls, and other relevant information.\n *\n * @remarks\n * The converter handles the following message roles:\n * - `assistant`: Converted to {@link AIMessage} with support for tool calls, function calls,\n *   audio content, and multi-modal outputs\n * - Other roles: Converted to generic {@link ChatMessage}\n *\n * For assistant messages, the converter:\n * - Parses and validates tool calls, separating valid and invalid calls\n * - Preserves function call information in additional_kwargs\n * - Includes usage statistics and system fingerprint in response_metadata\n * - Handles multi-modal content (text, images, audio)\n * - Optionally includes the raw API response for debugging\n *\n * @param params - Conversion parameters\n * @param params.message - The OpenAI chat completion message to convert\n * @param params.rawResponse - The complete raw response from OpenAI's API, used to extract\n *   metadata like model name, usage statistics, and system fingerprint\n * @param params.includeRawResponse - If true, includes the raw OpenAI response in the\n *   message's additional_kwargs under the `__raw_response` key. Useful for debugging\n *   or accessing provider-specific fields. Defaults to false.\n *\n * @returns A LangChain BaseMessage instance:\n *   - {@link AIMessage} for assistant messages with tool calls, metadata, and content\n *   - {@link ChatMessage} for all other message types\n *\n * @example\n * ```typescript\n * const baseMessage = convertCompletionsMessageToBaseMessage({\n *   message: {\n *     role: \"assistant\",\n *     content: \"Hello! How can I help you?\",\n *     tool_calls: [\n *       {\n *         id: \"call_123\",\n *         type: \"function\",\n *         function: { name: \"get_weather\", arguments: '{\"location\":\"NYC\"}' }\n *       }\n *     ]\n *   },\n *   rawResponse: completionResponse,\n *   includeRawResponse: true\n * });\n * // Returns an AIMessage with parsed tool calls and metadata\n * ```\n *\n * @throws {Error} If tool call parsing fails, the invalid tool call is captured in\n *   the `invalid_tool_calls` array rather than throwing an error\n *\n */\nexport const convertCompletionsMessageToBaseMessage: Converter<\n  {\n    message: OpenAIClient.Chat.Completions.ChatCompletionMessage;\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletion;\n    includeRawResponse?: boolean;\n  },\n  BaseMessage\n> = ({ message, rawResponse, includeRawResponse }) => {\n  const rawToolCalls: OpenAIToolCall[] | undefined = message.tool_calls as\n    | OpenAIToolCall[]\n    | undefined;\n  switch (message.role) {\n    case \"assistant\": {\n      const toolCalls = [];\n      const invalidToolCalls = [];\n      for (const rawToolCall of rawToolCalls ?? []) {\n        try {\n          toolCalls.push(parseToolCall(rawToolCall, { returnId: true }));\n          // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        } catch (e: any) {\n          invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n        }\n      }\n      const additional_kwargs: Record<string, unknown> = {\n        function_call: message.function_call,\n        tool_calls: rawToolCalls,\n      };\n      if (includeRawResponse !== undefined) {\n        additional_kwargs.__raw_response = rawResponse;\n      }\n      const response_metadata: Record<string, unknown> | undefined = {\n        model_provider: \"openai\",\n        model_name: rawResponse.model,\n        ...(rawResponse.system_fingerprint\n          ? {\n              usage: { ...rawResponse.usage },\n              system_fingerprint: rawResponse.system_fingerprint,\n            }\n          : {}),\n      };\n\n      if (message.audio) {\n        additional_kwargs.audio = message.audio;\n      }\n\n      const content = handleMultiModalOutput(\n        message.content || \"\",\n        rawResponse.choices?.[0]?.message\n      );\n      return new AIMessage({\n        content,\n        tool_calls: toolCalls,\n        invalid_tool_calls: invalidToolCalls,\n        additional_kwargs,\n        response_metadata,\n        id: rawResponse.id,\n      });\n    }\n    default:\n      return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n  }\n};\n\n/**\n * Converts an OpenAI Chat Completions API delta (streaming chunk) to a LangChain BaseMessageChunk.\n *\n * This converter is used during streaming responses to transform incremental updates from OpenAI's\n * Chat Completions API into LangChain message chunks. It handles various message types, tool calls,\n * function calls, audio content, and role-specific message chunk creation.\n *\n * @param params - Conversion parameters\n * @param params.delta - The delta object from an OpenAI streaming chunk containing incremental\n *   message updates. May include content, role, tool_calls, function_call, audio, etc.\n * @param params.rawResponse - The complete raw ChatCompletionChunk response from OpenAI,\n *   containing metadata like model info, usage stats, and the delta\n * @param params.includeRawResponse - Optional flag to include the raw OpenAI response in the\n *   message chunk's additional_kwargs. Useful for debugging or accessing provider-specific data\n * @param params.defaultRole - Optional default role to use if the delta doesn't specify one.\n *   Typically used to maintain role consistency across chunks in a streaming response\n *\n * @returns A BaseMessageChunk subclass appropriate for the message role:\n *   - HumanMessageChunk for \"user\" role\n *   - AIMessageChunk for \"assistant\" role (includes tool call chunks)\n *   - SystemMessageChunk for \"system\" or \"developer\" roles\n *   - FunctionMessageChunk for \"function\" role\n *   - ToolMessageChunk for \"tool\" role\n *   - ChatMessageChunk for any other role\n *\n * @example\n * Basic streaming text chunk:\n * ```typescript\n * const chunk = convertCompletionsDeltaToBaseMessageChunk({\n *   delta: { role: \"assistant\", content: \"Hello\" },\n *   rawResponse: { id: \"chatcmpl-123\", model: \"gpt-4\", ... }\n * });\n * // Returns: AIMessageChunk with content \"Hello\"\n * ```\n *\n * @example\n * Streaming chunk with tool call:\n * ```typescript\n * const chunk = convertCompletionsDeltaToBaseMessageChunk({\n *   delta: {\n *     role: \"assistant\",\n *     tool_calls: [{\n *       index: 0,\n *       id: \"call_123\",\n *       function: { name: \"get_weather\", arguments: '{\"location\":' }\n *     }]\n *   },\n *   rawResponse: { id: \"chatcmpl-123\", ... }\n * });\n * // Returns: AIMessageChunk with tool_call_chunks containing partial tool call data\n * ```\n *\n * @remarks\n * - Tool calls are converted to ToolCallChunk objects with incremental data\n * - Audio content includes the chunk index from the raw response\n * - The \"developer\" role is mapped to SystemMessageChunk with a special marker\n * - Response metadata includes model provider info and usage statistics\n * - Function calls and tool calls are stored in additional_kwargs for compatibility\n */\nexport const convertCompletionsDeltaToBaseMessageChunk: Converter<\n  {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta: Record<string, any>;\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk;\n    includeRawResponse?: boolean;\n    defaultRole?: OpenAIClient.Chat.ChatCompletionRole;\n  },\n  BaseMessageChunk\n> = ({ delta, rawResponse, includeRawResponse, defaultRole }) => {\n  const role = delta.role ?? defaultRole;\n  const content = delta.content ?? \"\";\n  let additional_kwargs: Record<string, unknown>;\n  if (delta.function_call) {\n    additional_kwargs = {\n      function_call: delta.function_call,\n    };\n  } else if (delta.tool_calls) {\n    additional_kwargs = {\n      tool_calls: delta.tool_calls,\n    };\n  } else {\n    additional_kwargs = {};\n  }\n  if (includeRawResponse) {\n    additional_kwargs.__raw_response = rawResponse;\n  }\n\n  if (delta.audio) {\n    additional_kwargs.audio = {\n      ...delta.audio,\n      index: rawResponse.choices[0].index,\n    };\n  }\n\n  const response_metadata = {\n    model_provider: \"openai\",\n    usage: { ...rawResponse.usage },\n  };\n  if (role === \"user\") {\n    return new HumanMessageChunk({ content, response_metadata });\n  } else if (role === \"assistant\") {\n    const toolCallChunks: ToolCallChunk[] = [];\n    if (Array.isArray(delta.tool_calls)) {\n      for (const rawToolCall of delta.tool_calls) {\n        toolCallChunks.push({\n          name: rawToolCall.function?.name,\n          args: rawToolCall.function?.arguments,\n          id: rawToolCall.id,\n          index: rawToolCall.index,\n          type: \"tool_call_chunk\",\n        });\n      }\n    }\n    return new AIMessageChunk({\n      content,\n      tool_call_chunks: toolCallChunks,\n      additional_kwargs,\n      id: rawResponse.id,\n      response_metadata,\n    });\n  } else if (role === \"system\") {\n    return new SystemMessageChunk({ content, response_metadata });\n  } else if (role === \"developer\") {\n    return new SystemMessageChunk({\n      content,\n      response_metadata,\n      additional_kwargs: {\n        __openai_role__: \"developer\",\n      },\n    });\n  } else if (role === \"function\") {\n    return new FunctionMessageChunk({\n      content,\n      additional_kwargs,\n      name: delta.name,\n      response_metadata,\n    });\n  } else if (role === \"tool\") {\n    return new ToolMessageChunk({\n      content,\n      additional_kwargs,\n      tool_call_id: delta.tool_call_id,\n      response_metadata,\n    });\n  } else {\n    return new ChatMessageChunk({ content, role, response_metadata });\n  }\n};\n\n/**\n * Converts a standard LangChain content block to an OpenAI Completions API content part.\n *\n * This converter transforms LangChain's standardized content blocks (image, audio, file)\n * into the format expected by OpenAI's Chat Completions API. It handles various content\n * types including images (URL or base64), audio (base64), and files (data or file ID).\n *\n * @param block - The standard content block to convert. Can be an image, audio, or file block.\n *\n * @returns An OpenAI Chat Completions content part object, or undefined if the block\n *   cannot be converted (e.g., missing required data).\n *\n * @example\n * Image with URL:\n * ```typescript\n * const block = { type: \"image\", url: \"https://example.com/image.jpg\" };\n * const part = convertStandardContentBlockToCompletionsContentPart(block);\n * // Returns: { type: \"image_url\", image_url: { url: \"https://example.com/image.jpg\" } }\n * ```\n *\n * @example\n * Image with base64 data:\n * ```typescript\n * const block = { type: \"image\", data: \"iVBORw0KGgo...\", mimeType: \"image/png\" };\n * const part = convertStandardContentBlockToCompletionsContentPart(block);\n * // Returns: { type: \"image_url\", image_url: { url: \"data:image/png;base64,iVBORw0KGgo...\" } }\n * ```\n */\nexport const convertStandardContentBlockToCompletionsContentPart: Converter<\n  ContentBlock.Standard,\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPartImage\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPartInputAudio\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPart.File\n  | undefined\n> = (block) => {\n  if (block.type === \"image\") {\n    if (block.url) {\n      return {\n        type: \"image_url\",\n        image_url: {\n          url: block.url,\n        },\n      };\n    } else if (block.data) {\n      return {\n        type: \"image_url\",\n        image_url: {\n          url: `data:${block.mimeType};base64,${block.data}`,\n        },\n      };\n    }\n  }\n  if (block.type === \"audio\") {\n    if (block.data) {\n      const format = iife(() => {\n        const [, format] = block.mimeType.split(\"/\");\n        if (format === \"wav\" || format === \"mp3\") {\n          return format;\n        }\n        return \"wav\";\n      });\n      return {\n        type: \"input_audio\",\n        input_audio: {\n          data: block.data.toString(),\n          format,\n        },\n      };\n    }\n  }\n  if (block.type === \"file\") {\n    if (block.data) {\n      const filename = getRequiredFilenameFromMetadata(block);\n\n      return {\n        type: \"file\",\n        file: {\n          file_data: `data:${block.mimeType};base64,${block.data}`,\n          filename: filename,\n        },\n      };\n    }\n    if (block.fileId) {\n      return {\n        type: \"file\",\n        file: {\n          file_id: block.fileId,\n        },\n      };\n    }\n  }\n  return undefined;\n};\n\n/**\n * Converts a LangChain BaseMessage with standard content blocks to an OpenAI Chat Completions API message parameter.\n *\n * This converter transforms LangChain's standardized message format (using contentBlocks) into the format\n * expected by OpenAI's Chat Completions API. It handles role mapping, content filtering, and multi-modal\n * content conversion for various message types.\n *\n * @remarks\n * The converter performs the following transformations:\n * - Maps LangChain message roles to OpenAI API roles (user, assistant, system, developer, tool, function)\n * - For reasoning models, automatically converts \"system\" role to \"developer\" role\n * - Filters content blocks based on message role (most roles only include text blocks)\n * - For user messages, converts multi-modal content blocks (images, audio, files) to OpenAI format\n * - Preserves tool call IDs for tool messages and function names for function messages\n *\n * Role-specific behavior:\n * - **developer**: Returns only text content blocks (used for reasoning models)\n * - **system**: Returns only text content blocks\n * - **assistant**: Returns only text content blocks\n * - **tool**: Returns only text content blocks with tool_call_id preserved\n * - **function**: Returns text content blocks joined as a single string with function name\n * - **user** (default): Returns multi-modal content including text, images, audio, and files\n *\n * @param params - Conversion parameters\n * @param params.message - The LangChain BaseMessage to convert. Must have contentBlocks property\n *   containing an array of standard content blocks (text, image, audio, file, etc.)\n * @param params.model - Optional model name. Used to determine if special role mapping is needed\n *   (e.g., \"system\" -> \"developer\" for reasoning models like o1)\n *\n * @returns An OpenAI ChatCompletionMessageParam object formatted for the Chat Completions API.\n *   The structure varies by role:\n *   - Developer/System/Assistant: `{ role, content: TextBlock[] }`\n *   - Tool: `{ role: \"tool\", tool_call_id, content: TextBlock[] }`\n *   - Function: `{ role: \"function\", name, content: string }`\n *   - User: `{ role: \"user\", content: Array<TextPart | ImagePart | AudioPart | FilePart> }`\n *\n * @example\n * Simple text message:\n * ```typescript\n * const message = new HumanMessage({\n *   content: [{ type: \"text\", text: \"Hello!\" }]\n * });\n * const param = convertStandardContentMessageToCompletionsMessage({ message });\n * // Returns: { role: \"user\", content: [{ type: \"text\", text: \"Hello!\" }] }\n * ```\n *\n * @example\n * Multi-modal user message with image:\n * ```typescript\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"What's in this image?\" },\n *     { type: \"image\", url: \"https://example.com/image.jpg\" }\n *   ]\n * });\n * const param = convertStandardContentMessageToCompletionsMessage({ message });\n * // Returns: {\n * //   role: \"user\",\n * //   content: [\n * //     { type: \"text\", text: \"What's in this image?\" },\n * //     { type: \"image_url\", image_url: { url: \"https://example.com/image.jpg\" } }\n * //   ]\n * // }\n * ```\n */\nexport const convertStandardContentMessageToCompletionsMessage: Converter<\n  { message: BaseMessage; model?: string },\n  OpenAIClient.Chat.Completions.ChatCompletionMessageParam\n> = ({ message, model }) => {\n  let role = messageToOpenAIRole(message);\n  if (role === \"system\" && isReasoningModel(model)) {\n    role = \"developer\";\n  }\n  if (role === \"developer\") {\n    return {\n      role: \"developer\",\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"system\") {\n    return {\n      role: \"system\",\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"assistant\") {\n    return {\n      role: \"assistant\",\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"tool\" && ToolMessage.isInstance(message)) {\n    return {\n      role: \"tool\",\n      tool_call_id: message.tool_call_id,\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"function\") {\n    return {\n      role: \"function\",\n      name: message.name ?? \"\",\n      content: message.contentBlocks\n        .filter((block) => block.type === \"text\")\n        .join(\"\"),\n    };\n  }\n  // Default to user message handling\n  function* iterateUserContent(blocks: ContentBlock.Standard[]) {\n    for (const block of blocks) {\n      if (block.type === \"text\") {\n        yield {\n          type: \"text\" as const,\n          text: block.text,\n        };\n      }\n      const data = convertStandardContentBlockToCompletionsContentPart(block);\n      if (data) {\n        yield data;\n      }\n    }\n  }\n  return {\n    role: \"user\",\n    content: Array.from(iterateUserContent(message.contentBlocks)),\n  };\n};\n\n/**\n * Converts an array of LangChain BaseMessages to OpenAI Chat Completions API message parameters.\n *\n * This converter transforms LangChain's internal message representation into the format required\n * by OpenAI's Chat Completions API. It handles various message types, roles, content formats,\n * tool calls, function calls, audio messages, and special model-specific requirements.\n *\n * @remarks\n * The converter performs several key transformations:\n * - Maps LangChain message types to OpenAI roles (user, assistant, system, tool, function, developer)\n * - Converts standard content blocks (v1 format) using a specialized converter\n * - Handles multimodal content including text, images, audio, and data blocks\n * - Preserves tool calls and function calls with proper formatting\n * - Applies model-specific role mappings (e.g., \"system\"  \"developer\" for reasoning models)\n * - Splits audio messages into separate message parameters when needed\n *\n * @param params - Conversion parameters\n * @param params.messages - Array of LangChain BaseMessages to convert. Can include any message\n *   type: HumanMessage, AIMessage, SystemMessage, ToolMessage, FunctionMessage, etc.\n * @param params.model - Optional model name used to determine if special role mapping is needed.\n *   For reasoning models (o1, o3, etc.), \"system\" role is converted to \"developer\" role.\n *\n * @returns Array of ChatCompletionMessageParam objects formatted for OpenAI's Chat Completions API.\n *   Some messages may be split into multiple parameters (e.g., audio messages).\n *\n * @example\n * Basic message conversion:\n * ```typescript\n * const messages = [\n *   new HumanMessage(\"What's the weather like?\"),\n *   new AIMessage(\"Let me check that for you.\")\n * ];\n *\n * const params = convertMessagesToCompletionsMessageParams({\n *   messages,\n *   model: \"gpt-4\"\n * });\n * // Returns:\n * // [\n * //   { role: \"user\", content: \"What's the weather like?\" },\n * //   { role: \"assistant\", content: \"Let me check that for you.\" }\n * // ]\n * ```\n *\n * @example\n * Message with tool calls:\n * ```typescript\n * const messages = [\n *   new AIMessage({\n *     content: \"\",\n *     tool_calls: [{\n *       id: \"call_123\",\n *       name: \"get_weather\",\n *       args: { location: \"San Francisco\" }\n *     }]\n *   })\n * ];\n *\n * const params = convertMessagesToCompletionsMessageParams({ messages });\n * // Returns:\n * // [{\n * //   role: \"assistant\",\n * //   content: \"\",\n * //   tool_calls: [{\n * //     id: \"call_123\",\n * //     type: \"function\",\n * //     function: { name: \"get_weather\", arguments: '{\"location\":\"San Francisco\"}' }\n * //   }]\n * // }]\n * ```\n */\nexport const convertMessagesToCompletionsMessageParams: Converter<\n  { messages: BaseMessage[]; model?: string },\n  OpenAIClient.Chat.Completions.ChatCompletionMessageParam[]\n> = ({ messages, model }) => {\n  return messages.flatMap((message) => {\n    if (\n      \"output_version\" in message.response_metadata &&\n      message.response_metadata?.output_version === \"v1\"\n    ) {\n      return convertStandardContentMessageToCompletionsMessage({ message });\n    }\n    let role = messageToOpenAIRole(message);\n    if (role === \"system\" && isReasoningModel(model)) {\n      role = \"developer\";\n    }\n\n    const content =\n      typeof message.content === \"string\"\n        ? message.content\n        : message.content.flatMap((m) => {\n            if (isDataContentBlock(m)) {\n              return convertToProviderContentBlock(\n                m,\n                completionsApiContentBlockConverter\n              );\n            }\n            // Drop Anthropic tool_use blocks from content  these are\n            // already represented in message.tool_calls and would cause\n            // an API error if passed through to OpenAI.\n            if (\n              typeof m === \"object\" &&\n              m !== null &&\n              \"type\" in m &&\n              m.type === \"tool_use\"\n            ) {\n              return [];\n            }\n            return m;\n          });\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const completionParam: Record<string, any> = {\n      role,\n      content,\n    };\n    if (message.name != null) {\n      completionParam.name = message.name;\n    }\n    if (message.additional_kwargs.function_call != null) {\n      completionParam.function_call = message.additional_kwargs.function_call;\n    }\n    if (AIMessage.isInstance(message) && !!message.tool_calls?.length) {\n      completionParam.tool_calls = message.tool_calls.map(\n        convertLangChainToolCallToOpenAI\n      );\n    } else {\n      if (message.additional_kwargs.tool_calls != null) {\n        completionParam.tool_calls = message.additional_kwargs.tool_calls;\n      }\n      if (ToolMessage.isInstance(message) && message.tool_call_id != null) {\n        completionParam.tool_call_id = message.tool_call_id;\n      }\n    }\n\n    if (\n      message.additional_kwargs.audio &&\n      typeof message.additional_kwargs.audio === \"object\" &&\n      \"id\" in message.additional_kwargs.audio\n    ) {\n      const audioMessage = {\n        role: \"assistant\",\n        audio: {\n          id: message.additional_kwargs.audio.id,\n        },\n      };\n      return [\n        completionParam,\n        audioMessage,\n      ] as OpenAIClient.Chat.Completions.ChatCompletionMessageParam[];\n    }\n\n    return completionParam as OpenAIClient.Chat.Completions.ChatCompletionMessageParam;\n  });\n};\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  AIMessage,\n  AIMessageChunk,\n  type BaseMessage,\n  isAIMessage,\n  type UsageMetadata,\n  type AIMessageFields,\n  BaseMessageChunk,\n} from \"@langchain/core/messages\";\nimport {\n  ChatGenerationChunk,\n  type ChatGeneration,\n  type ChatResult,\n} from \"@langchain/core/outputs\";\nimport { NewTokenIndices } from \"@langchain/core/callbacks/base\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\nimport {\n  OpenAIToolChoice,\n  formatToOpenAIToolChoice,\n  _convertToOpenAITool,\n} from \"../utils/tools.js\";\nimport { isReasoningModel } from \"../utils/misc.js\";\nimport {\n  BaseChatOpenAI,\n  BaseChatOpenAICallOptions,\n  BaseChatOpenAIFields,\n  getChatOpenAIModelParams,\n} from \"./base.js\";\nimport {\n  convertCompletionsDeltaToBaseMessageChunk,\n  convertCompletionsMessageToBaseMessage,\n  convertMessagesToCompletionsMessageParams,\n} from \"../converters/completions.js\";\n\nexport interface ChatOpenAICompletionsCallOptions extends BaseChatOpenAICallOptions {}\n\ntype ChatCompletionsInvocationParams = Omit<\n  OpenAIClient.Chat.Completions.ChatCompletionCreateParams,\n  \"messages\"\n>;\n\n/**\n * OpenAI Completions API implementation.\n * @internal\n */\nexport class ChatOpenAICompletions<\n  CallOptions extends ChatOpenAICompletionsCallOptions =\n    ChatOpenAICompletionsCallOptions,\n> extends BaseChatOpenAI<CallOptions> {\n  constructor(model: string, fields?: Omit<BaseChatOpenAIFields, \"model\">);\n  constructor(fields?: BaseChatOpenAIFields);\n  constructor(\n    modelOrFields?: string | BaseChatOpenAIFields,\n    fieldsArg?: Omit<BaseChatOpenAIFields, \"model\">\n  ) {\n    super(getChatOpenAIModelParams(modelOrFields, fieldsArg));\n  }\n\n  /** @internal */\n  override invocationParams(\n    options?: this[\"ParsedCallOptions\"],\n    extra?: { streaming?: boolean }\n  ): ChatCompletionsInvocationParams {\n    let strict: boolean | undefined;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n\n    let streamOptionsConfig = {};\n    if (options?.stream_options !== undefined) {\n      streamOptionsConfig = { stream_options: options.stream_options };\n    } else if (this.streamUsage && (this.streaming || extra?.streaming)) {\n      streamOptionsConfig = { stream_options: { include_usage: true } };\n    }\n\n    const params: Partial<ChatCompletionsInvocationParams> = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      logprobs: this.logprobs,\n      top_logprobs: this.topLogprobs,\n      n: this.n,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stopSequences,\n      user: this.user,\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      functions: options?.functions,\n      function_call: options?.function_call,\n      tools: options?.tools?.length\n        ? options.tools.map((tool) =>\n            this._convertChatOpenAIToolToCompletionsTool(tool, { strict })\n          )\n        : undefined,\n      tool_choice: formatToOpenAIToolChoice(\n        options?.tool_choice as OpenAIToolChoice\n      ),\n      response_format: this._getResponseFormat(options?.response_format),\n      seed: options?.seed,\n      ...streamOptionsConfig,\n      parallel_tool_calls: options?.parallel_tool_calls,\n      ...(this.audio || options?.audio\n        ? { audio: this.audio || options?.audio }\n        : {}),\n      ...(this.modalities || options?.modalities\n        ? { modalities: this.modalities || options?.modalities }\n        : {}),\n      ...this.modelKwargs,\n      prompt_cache_key: options?.promptCacheKey ?? this.promptCacheKey,\n      prompt_cache_retention:\n        options?.promptCacheRetention ?? this.promptCacheRetention,\n      verbosity: options?.verbosity ?? this.verbosity,\n    };\n    if (options?.prediction !== undefined) {\n      params.prediction = options.prediction;\n    }\n    if (this.service_tier !== undefined) {\n      params.service_tier = this.service_tier;\n    }\n    if (options?.service_tier !== undefined) {\n      params.service_tier = options.service_tier;\n    }\n    const reasoning = this._getReasoningParams(options);\n    if (reasoning !== undefined && reasoning.effort !== undefined) {\n      params.reasoning_effort = reasoning.effort;\n    }\n    if (isReasoningModel(params.model)) {\n      params.max_completion_tokens =\n        this.maxTokens === -1 ? undefined : this.maxTokens;\n    } else {\n      params.max_tokens = this.maxTokens === -1 ? undefined : this.maxTokens;\n    }\n\n    return params as ChatCompletionsInvocationParams;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    options.signal?.throwIfAborted();\n    const usageMetadata = {} as UsageMetadata;\n    const params = this.invocationParams(options);\n    const messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[] =\n      convertMessagesToCompletionsMessageParams({\n        messages,\n        model: this.model,\n      });\n\n    if (params.stream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks: Record<number, ChatGenerationChunk> = {};\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata,\n        };\n        const index =\n          (chunk.generationInfo as NewTokenIndices)?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks)\n        .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n        .map(([_, value]) => value);\n\n      const { functions, function_call } = this.invocationParams(options);\n\n      // OpenAI does not support token usage report under stream mode,\n      // fallback to estimation.\n\n      const promptTokenUsage = await this._getEstimatedTokenCountFromPrompt(\n        messages,\n        functions,\n        function_call\n      );\n      const completionTokenUsage =\n        await this._getNumTokensFromGenerations(generations);\n\n      usageMetadata.input_tokens = promptTokenUsage;\n      usageMetadata.output_tokens = completionTokenUsage;\n      usageMetadata.total_tokens = promptTokenUsage + completionTokenUsage;\n      return {\n        generations,\n        llmOutput: {\n          estimatedTokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens,\n          },\n        },\n      };\n    } else {\n      const data = await this.completionWithRetry(\n        {\n          ...params,\n          stream: false,\n          messages: messagesMapped,\n        },\n        {\n          signal: options?.signal,\n          ...options?.options,\n        }\n      );\n\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens,\n        prompt_tokens_details: promptTokensDetails,\n        completion_tokens_details: completionTokensDetails,\n      } = data?.usage ?? {};\n\n      if (completionTokens) {\n        usageMetadata.output_tokens =\n          (usageMetadata.output_tokens ?? 0) + completionTokens;\n      }\n\n      if (promptTokens) {\n        usageMetadata.input_tokens =\n          (usageMetadata.input_tokens ?? 0) + promptTokens;\n      }\n\n      if (totalTokens) {\n        usageMetadata.total_tokens =\n          (usageMetadata.total_tokens ?? 0) + totalTokens;\n      }\n\n      if (\n        promptTokensDetails?.audio_tokens !== null ||\n        promptTokensDetails?.cached_tokens !== null\n      ) {\n        usageMetadata.input_token_details = {\n          ...(promptTokensDetails?.audio_tokens !== null && {\n            audio: promptTokensDetails?.audio_tokens,\n          }),\n          ...(promptTokensDetails?.cached_tokens !== null && {\n            cache_read: promptTokensDetails?.cached_tokens,\n          }),\n        };\n      }\n\n      if (\n        completionTokensDetails?.audio_tokens !== null ||\n        completionTokensDetails?.reasoning_tokens !== null\n      ) {\n        usageMetadata.output_token_details = {\n          ...(completionTokensDetails?.audio_tokens !== null && {\n            audio: completionTokensDetails?.audio_tokens,\n          }),\n          ...(completionTokensDetails?.reasoning_tokens !== null && {\n            reasoning: completionTokensDetails?.reasoning_tokens,\n          }),\n        };\n      }\n\n      const generations: ChatGeneration[] = [];\n      for (const part of data?.choices ?? []) {\n        const text = part.message?.content ?? \"\";\n        const generation: ChatGeneration = {\n          text,\n          message: this._convertCompletionsMessageToBaseMessage(\n            part.message ?? { role: \"assistant\" },\n            data\n          ),\n        };\n        generation.generationInfo = {\n          ...(part.finish_reason ? { finish_reason: part.finish_reason } : {}),\n          ...(part.logprobs ? { logprobs: part.logprobs } : {}),\n        };\n        if (isAIMessage(generation.message)) {\n          generation.message.usage_metadata = usageMetadata;\n        }\n        // Fields are not serialized unless passed to the constructor\n        // Doing this ensures all fields on the message are serialized\n        generation.message = new AIMessage(\n          Object.fromEntries(\n            Object.entries(generation.message).filter(\n              ([key]) => !key.startsWith(\"lc_\")\n            )\n          ) as AIMessageFields\n        );\n        generations.push(generation);\n      }\n      return {\n        generations,\n        llmOutput: {\n          tokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens,\n          },\n        },\n      };\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[] =\n      convertMessagesToCompletionsMessageParams({\n        messages,\n        model: this.model,\n      });\n\n    const params = {\n      ...this.invocationParams(options, {\n        streaming: true,\n      }),\n      messages: messagesMapped,\n      stream: true as const,\n    };\n    let defaultRole: OpenAIClient.Chat.ChatCompletionRole | undefined;\n\n    const streamIterable = await this.completionWithRetry(params, options);\n    let usage: OpenAIClient.Completions.CompletionUsage | undefined;\n    for await (const data of streamIterable) {\n      if (options.signal?.aborted) {\n        return;\n      }\n      const choice = data?.choices?.[0];\n      if (data.usage) {\n        usage = data.usage;\n      }\n      if (!choice) {\n        continue;\n      }\n\n      const { delta } = choice;\n      if (!delta) {\n        continue;\n      }\n      const chunk = this._convertCompletionsDeltaToBaseMessageChunk(\n        delta,\n        data,\n        defaultRole\n      );\n      defaultRole = delta.role ?? defaultRole;\n      const newTokenIndices = {\n        prompt: options.promptIndex ?? 0,\n        completion: choice.index ?? 0,\n      };\n      if (typeof chunk.content !== \"string\") {\n        console.log(\n          \"[WARNING]: Received non-string content from OpenAI. This is currently not supported.\"\n        );\n        continue;\n      }\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      const generationInfo: Record<string, any> = { ...newTokenIndices };\n      if (choice.finish_reason != null) {\n        generationInfo.finish_reason = choice.finish_reason;\n        // Only include system fingerprint in the last chunk for now\n        // to avoid concatenation issues\n        generationInfo.system_fingerprint = data.system_fingerprint;\n        generationInfo.model_name = data.model;\n        generationInfo.service_tier = data.service_tier;\n      }\n      if (this.logprobs) {\n        generationInfo.logprobs = choice.logprobs;\n      }\n      const generationChunk = new ChatGenerationChunk({\n        message: chunk,\n        text: chunk.content,\n        generationInfo,\n      });\n      yield generationChunk;\n      await runManager?.handleLLMNewToken(\n        generationChunk.text ?? \"\",\n        newTokenIndices,\n        undefined,\n        undefined,\n        undefined,\n        { chunk: generationChunk }\n      );\n    }\n    if (usage) {\n      const inputTokenDetails = {\n        ...(usage.prompt_tokens_details?.audio_tokens !== null && {\n          audio: usage.prompt_tokens_details?.audio_tokens,\n        }),\n        ...(usage.prompt_tokens_details?.cached_tokens !== null && {\n          cache_read: usage.prompt_tokens_details?.cached_tokens,\n        }),\n      };\n      const outputTokenDetails = {\n        ...(usage.completion_tokens_details?.audio_tokens !== null && {\n          audio: usage.completion_tokens_details?.audio_tokens,\n        }),\n        ...(usage.completion_tokens_details?.reasoning_tokens !== null && {\n          reasoning: usage.completion_tokens_details?.reasoning_tokens,\n        }),\n      };\n      const generationChunk = new ChatGenerationChunk({\n        message: new AIMessageChunk({\n          content: \"\",\n          response_metadata: {\n            usage: { ...usage },\n          },\n          usage_metadata: {\n            input_tokens: usage.prompt_tokens,\n            output_tokens: usage.completion_tokens,\n            total_tokens: usage.total_tokens,\n            ...(Object.keys(inputTokenDetails).length > 0 && {\n              input_token_details: inputTokenDetails,\n            }),\n            ...(Object.keys(outputTokenDetails).length > 0 && {\n              output_token_details: outputTokenDetails,\n            }),\n          },\n        }),\n        text: \"\",\n      });\n      yield generationChunk;\n      await runManager?.handleLLMNewToken(\n        generationChunk.text ?? \"\",\n        { prompt: 0, completion: 0 },\n        undefined,\n        undefined,\n        undefined,\n        { chunk: generationChunk }\n      );\n    }\n    if (options.signal?.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParamsStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Chat.Completions.ChatCompletionChunk>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParamsNonStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<OpenAIClient.Chat.Completions.ChatCompletion>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParams,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<\n    | AsyncIterable<OpenAIClient.Chat.Completions.ChatCompletionChunk>\n    | OpenAIClient.Chat.Completions.ChatCompletion\n  > {\n    const clientOptions = this._getClientOptions(requestOptions);\n    const isParseableFormat =\n      request.response_format && request.response_format.type === \"json_schema\";\n    return this.caller.call(async () => {\n      try {\n        if (isParseableFormat && !request.stream) {\n          return await this.client.chat.completions.parse(\n            request,\n            clientOptions\n          );\n        } else {\n          return await this.client.chat.completions.create(\n            request,\n            clientOptions\n          );\n        }\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /**\n   * @deprecated\n   * This function was hoisted into a publicly accessible function from a\n   * different export, but to maintain backwards compatibility with chat models\n   * that depend on ChatOpenAICompletions, we'll keep it here as an overridable\n   * method. This will be removed in a future release\n   */\n  protected _convertCompletionsDeltaToBaseMessageChunk(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta: Record<string, any>,\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk,\n    defaultRole?: OpenAIClient.Chat.ChatCompletionRole\n  ): BaseMessageChunk {\n    return convertCompletionsDeltaToBaseMessageChunk({\n      delta,\n      rawResponse,\n      includeRawResponse: this.__includeRawResponse,\n      defaultRole,\n    });\n  }\n\n  /**\n   * @deprecated\n   * This function was hoisted into a publicly accessible function from a\n   * different export, but to maintain backwards compatibility with chat models\n   * that depend on ChatOpenAICompletions, we'll keep it here as an overridable\n   * method. This will be removed in a future release\n   */\n  protected _convertCompletionsMessageToBaseMessage(\n    message: OpenAIClient.ChatCompletionMessage,\n    rawResponse: OpenAIClient.ChatCompletion\n  ): BaseMessage {\n    return convertCompletionsMessageToBaseMessage({\n      message,\n      rawResponse,\n      includeRawResponse: this.__includeRawResponse,\n    });\n  }\n}\n", "import { AzureOpenAI as AzureOpenAIClient, type ClientOptions } from \"openai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport type { Serialized } from \"@langchain/core/load/serializable\";\nimport { ChatOpenAICallOptions } from \"../../chat_models/index.js\";\nimport {\n  OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"../../utils/azure.js\";\nimport { AzureOpenAIChatInput, OpenAICoreRequestOptions } from \"../../types.js\";\nimport {\n  BaseChatOpenAI,\n  BaseChatOpenAIFields,\n} from \"../../chat_models/base.js\";\n\nexport const AZURE_ALIASES = {\n  openAIApiKey: \"openai_api_key\",\n  openAIApiVersion: \"openai_api_version\",\n  openAIBasePath: \"openai_api_base\",\n  deploymentName: \"deployment_name\",\n  azureOpenAIEndpoint: \"azure_endpoint\",\n  azureOpenAIApiVersion: \"openai_api_version\",\n  azureOpenAIBasePath: \"openai_api_base\",\n  azureOpenAIApiDeploymentName: \"deployment_name\",\n};\n\nexport const AZURE_SECRETS = {\n  azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n};\n\nexport const AZURE_SERIALIZABLE_KEYS = [\n  \"azureOpenAIApiKey\",\n  \"azureOpenAIApiVersion\",\n  \"azureOpenAIBasePath\",\n  \"azureOpenAIEndpoint\",\n  \"azureOpenAIApiInstanceName\",\n  \"azureOpenAIApiDeploymentName\",\n  \"deploymentName\",\n  \"openAIApiKey\",\n  \"openAIApiVersion\",\n];\n\nexport interface AzureChatOpenAIFields\n  extends BaseChatOpenAIFields, Partial<AzureOpenAIChatInput> {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi?: boolean;\n}\n\nexport function getAzureChatOpenAIParams(\n  modelOrFields?: string | AzureChatOpenAIFields,\n  fieldsArg?: Omit<\n    AzureChatOpenAIFields,\n    \"deploymentName\" | \"azureOpenAIApiDeploymentName\" | \"model\"\n  >\n): AzureChatOpenAIFields | undefined {\n  if (typeof modelOrFields === \"string\") {\n    return {\n      model: modelOrFields,\n      deploymentName: modelOrFields,\n      azureOpenAIApiDeploymentName: modelOrFields,\n      ...(fieldsArg ?? {}),\n    };\n  }\n  if (modelOrFields == null) {\n    return fieldsArg as AzureChatOpenAIFields | undefined;\n  }\n  return modelOrFields;\n}\n\nexport function _constructAzureFields(\n  this: Partial<AzureOpenAIChatInput>,\n  fields?: AzureChatOpenAIFields\n) {\n  this.azureOpenAIApiKey =\n    fields?.azureOpenAIApiKey ??\n    (typeof fields?.openAIApiKey === \"string\"\n      ? fields?.openAIApiKey\n      : undefined) ??\n    (typeof fields?.apiKey === \"string\" ? fields?.apiKey : undefined) ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n\n  this.azureOpenAIApiInstanceName =\n    fields?.azureOpenAIApiInstanceName ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n\n  this.azureOpenAIApiDeploymentName =\n    fields?.azureOpenAIApiDeploymentName ??\n    fields?.deploymentName ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\");\n\n  this.azureOpenAIApiVersion =\n    fields?.azureOpenAIApiVersion ??\n    fields?.openAIApiVersion ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n\n  this.azureOpenAIBasePath =\n    fields?.azureOpenAIBasePath ??\n    getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n\n  this.azureOpenAIEndpoint =\n    fields?.azureOpenAIEndpoint ??\n    getEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\");\n\n  this.azureADTokenProvider = fields?.azureADTokenProvider;\n\n  if (!this.azureOpenAIApiKey && !this.apiKey && !this.azureADTokenProvider) {\n    throw new Error(\"Azure OpenAI API key or Token Provider not found\");\n  }\n}\n\nexport function _getAzureClientOptions(\n  this: BaseChatOpenAI<ChatOpenAICallOptions> & Partial<AzureOpenAIChatInput>,\n  options: OpenAICoreRequestOptions | undefined\n): OpenAICoreRequestOptions {\n  if (!this.client) {\n    const openAIEndpointConfig: OpenAIEndpointConfig = {\n      azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n      azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n      azureOpenAIApiKey: this.azureOpenAIApiKey,\n      azureOpenAIBasePath: this.azureOpenAIBasePath,\n      azureADTokenProvider: this.azureADTokenProvider,\n      baseURL: this.clientConfig.baseURL,\n      azureOpenAIEndpoint: this.azureOpenAIEndpoint,\n    };\n\n    const endpoint = getEndpoint(openAIEndpointConfig);\n\n    const { apiKey: existingApiKey, ...clientConfigRest } = this.clientConfig;\n    const params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string } = {\n      ...clientConfigRest,\n      baseURL: endpoint,\n      timeout: this.timeout,\n      maxRetries: 0,\n    };\n\n    if (!this.azureADTokenProvider) {\n      params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;\n    }\n\n    if (!params.baseURL) {\n      delete params.baseURL;\n    }\n\n    params.defaultHeaders = getHeadersWithUserAgent(\n      params.defaultHeaders,\n      true,\n      \"2.0.0\"\n    );\n\n    this.client = new AzureOpenAIClient({\n      apiVersion: this.azureOpenAIApiVersion,\n      azureADTokenProvider: this.azureADTokenProvider,\n      deployment: this.azureOpenAIApiDeploymentName,\n      ...params,\n    });\n  }\n  const requestOptions = {\n    ...this.clientConfig,\n    ...options,\n  } as OpenAICoreRequestOptions;\n  if (this.azureOpenAIApiKey) {\n    requestOptions.headers = {\n      \"api-key\": this.azureOpenAIApiKey,\n      ...requestOptions.headers,\n    };\n    requestOptions.query = {\n      \"api-version\": this.azureOpenAIApiVersion,\n      ...requestOptions.query,\n    };\n  }\n  return requestOptions;\n}\n\nexport function _serializeAzureChat(\n  this: BaseChatOpenAI<ChatOpenAICallOptions> & Partial<AzureOpenAIChatInput>,\n  input: Serialized\n) {\n  const json = input;\n\n  function isRecord(obj: unknown): obj is Record<string, unknown> {\n    return typeof obj === \"object\" && obj != null;\n  }\n\n  if (isRecord(json) && isRecord(json.kwargs)) {\n    delete json.kwargs.azure_openai_base_path;\n    delete json.kwargs.azure_openai_api_deployment_name;\n    delete json.kwargs.azure_openai_api_key;\n    delete json.kwargs.azure_openai_api_version;\n    delete json.kwargs.azure_open_ai_base_path;\n\n    if (!json.kwargs.azure_endpoint && this.azureOpenAIEndpoint) {\n      json.kwargs.azure_endpoint = this.azureOpenAIEndpoint;\n    }\n    if (!json.kwargs.azure_endpoint && this.azureOpenAIBasePath) {\n      const parts = this.azureOpenAIBasePath.split(\"/openai/deployments/\");\n      if (parts.length === 2 && parts[0].startsWith(\"http\")) {\n        const [endpoint] = parts;\n        json.kwargs.azure_endpoint = endpoint;\n      }\n    }\n    if (!json.kwargs.azure_endpoint && this.azureOpenAIApiInstanceName) {\n      json.kwargs.azure_endpoint = `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/`;\n    }\n    if (!json.kwargs.deployment_name && this.azureOpenAIApiDeploymentName) {\n      json.kwargs.deployment_name = this.azureOpenAIApiDeploymentName;\n    }\n    if (!json.kwargs.deployment_name && this.azureOpenAIBasePath) {\n      const parts = this.azureOpenAIBasePath.split(\"/openai/deployments/\");\n      if (parts.length === 2) {\n        const [, deployment] = parts;\n        json.kwargs.deployment_name = deployment;\n      }\n    }\n\n    if (\n      json.kwargs.azure_endpoint &&\n      json.kwargs.deployment_name &&\n      json.kwargs.openai_api_base\n    ) {\n      delete json.kwargs.openai_api_base;\n    }\n    if (\n      json.kwargs.azure_openai_api_instance_name &&\n      json.kwargs.azure_endpoint\n    ) {\n      delete json.kwargs.azure_openai_api_instance_name;\n    }\n  }\n\n  return json;\n}\n", "import { LangSmithParams } from \"@langchain/core/language_models/chat_models\";\nimport { Serialized } from \"@langchain/core/load/serializable\";\nimport {\n  ChatOpenAICompletions,\n  ChatOpenAICompletionsCallOptions,\n} from \"../../chat_models/completions.js\";\nimport { AzureOpenAIChatInput, OpenAICoreRequestOptions } from \"../../types.js\";\nimport {\n  _constructAzureFields,\n  _getAzureClientOptions,\n  _serializeAzureChat,\n  AZURE_ALIASES,\n  AZURE_SECRETS,\n  AZURE_SERIALIZABLE_KEYS,\n  AzureChatOpenAIFields,\n  getAzureChatOpenAIParams,\n} from \"./common.js\";\n\nexport class AzureChatOpenAICompletions<\n  CallOptions extends ChatOpenAICompletionsCallOptions =\n    ChatOpenAICompletionsCallOptions,\n>\n  extends ChatOpenAICompletions<CallOptions>\n  implements Partial<AzureOpenAIChatInput>\n{\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  _llmType(): string {\n    return \"azure_openai\";\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      ...AZURE_ALIASES,\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      ...AZURE_SECRETS,\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, ...AZURE_SERIALIZABLE_KEYS];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = super.getLsParams(options);\n    params.ls_provider = \"azure\";\n    return params;\n  }\n\n  constructor(\n    deploymentName: string,\n    fields?: Omit<\n      AzureChatOpenAIFields,\n      \"deploymentName\" | \"azureOpenAIApiDeploymentName\" | \"model\"\n    >\n  );\n  constructor(fields?: AzureChatOpenAIFields);\n  constructor(\n    deploymentOrFields?: string | AzureChatOpenAIFields,\n    fieldsArg?: Omit<\n      AzureChatOpenAIFields,\n      \"deploymentName\" | \"azureOpenAIApiDeploymentName\" | \"model\"\n    >\n  ) {\n    const fields = getAzureChatOpenAIParams(deploymentOrFields, fieldsArg);\n    super(fields);\n    _constructAzureFields.call(this, fields);\n  }\n\n  override _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    return _getAzureClientOptions.call(this, options);\n  }\n\n  override toJSON(): Serialized {\n    return _serializeAzureChat.call(this, super.toJSON());\n  }\n}\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport {\n  AIMessage,\n  AIMessageChunk,\n  type BaseMessage,\n  type UsageMetadata,\n  type BaseMessageFields,\n  type MessageContent,\n  type InvalidToolCall,\n  MessageContentImageUrl,\n  isDataContentBlock,\n  convertToProviderContentBlock,\n  ContentBlock,\n} from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport {\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport type {\n  ToolCall,\n  ToolCallChunk,\n  ToolMessage,\n} from \"@langchain/core/messages/tool\";\nimport { ResponseInputMessageContentList } from \"openai/resources/responses/responses.js\";\nimport { ChatOpenAIReasoningSummary } from \"../types.js\";\nimport {\n  isComputerToolCall,\n  isCustomToolCall,\n  parseComputerCall,\n  parseCustomToolCall,\n} from \"../utils/tools.js\";\nimport {\n  getFilenameFromMetadata,\n  getRequiredFilenameFromMetadata,\n  iife,\n  isReasoningModel,\n  messageToOpenAIRole,\n} from \"../utils/misc.js\";\nimport { Converter } from \"@langchain/core/utils/format\";\nimport { completionsApiContentBlockConverter } from \"./completions.js\";\n\nconst _FUNCTION_CALL_IDS_MAP_KEY = \"__openai_function_call_ids__\";\n\ntype OpenAIAnnotation =\n  OpenAIClient.Responses.ResponseOutputText[\"annotations\"][number];\n\n/**\n * Converts an OpenAI annotation to a LangChain Citation or BaseContentBlock.\n *\n * OpenAI has several annotation types:\n * - `url_citation`: Web citations with url, title, start_index, end_index\n * - `file_citation`: File citations with file_id, filename, index\n * - `container_file_citation`: Container file citations with container_id, file_id, filename, start_index, end_index\n * - `file_path`: File paths with file_id, index\n *\n * This function maps them to LangChain's Citation format or preserves them as non-standard blocks.\n */\nfunction convertOpenAIAnnotationToLangChain(\n  annotation: OpenAIAnnotation\n): ContentBlock.Citation | ContentBlock.NonStandard {\n  if (annotation.type === \"url_citation\") {\n    return {\n      type: \"citation\",\n      source: \"url_citation\",\n      url: annotation.url,\n      title: annotation.title,\n      startIndex: annotation.start_index,\n      endIndex: annotation.end_index,\n    } satisfies ContentBlock.Citation;\n  }\n\n  if (annotation.type === \"file_citation\") {\n    return {\n      type: \"citation\",\n      source: \"file_citation\",\n      title: annotation.filename,\n      startIndex: annotation.index,\n      // Store file_id in a way that can be retrieved\n      file_id: annotation.file_id,\n    } as ContentBlock.Citation;\n  }\n\n  if (annotation.type === \"container_file_citation\") {\n    return {\n      type: \"citation\",\n      source: \"container_file_citation\",\n      title: annotation.filename,\n      startIndex: annotation.start_index,\n      endIndex: annotation.end_index,\n      // Store additional IDs\n      file_id: annotation.file_id,\n      container_id: annotation.container_id,\n    } as ContentBlock.Citation;\n  }\n\n  if (annotation.type === \"file_path\") {\n    return {\n      type: \"citation\",\n      source: \"file_path\",\n      startIndex: annotation.index,\n      file_id: annotation.file_id,\n    } as ContentBlock.Citation;\n  }\n\n  // For unknown annotation types, preserve them as non-standard blocks\n  return {\n    type: \"non_standard\",\n    value: annotation as unknown as Record<string, unknown>,\n  } satisfies ContentBlock.NonStandard;\n}\n\n/**\n * Converts a LangChain Citation or BaseContentBlock back to an OpenAI annotation.\n *\n * This is the inverse of `convertOpenAIAnnotationToLangChain`. It handles all four\n * annotation types (url_citation, file_citation, container_file_citation, file_path)\n * and also passes through annotations that are already in OpenAI format.\n */\nfunction convertLangChainAnnotationToOpenAI(\n  annotation:\n    | ContentBlock.Citation\n    | ContentBlock.NonStandard\n    | Record<string, unknown>\n): OpenAIAnnotation {\n  // If it's already in OpenAI format, pass through unchanged\n  if (\n    annotation.type === \"url_citation\" ||\n    annotation.type === \"file_citation\" ||\n    annotation.type === \"container_file_citation\" ||\n    annotation.type === \"file_path\"\n  ) {\n    return annotation as unknown as OpenAIAnnotation;\n  }\n\n  // Convert from LangChain citation format back to OpenAI format\n  if (annotation.type === \"citation\") {\n    const citation = annotation as ContentBlock.Citation & {\n      file_id?: string;\n      container_id?: string;\n    };\n\n    if (citation.source === \"url_citation\") {\n      return {\n        type: \"url_citation\",\n        url: citation.url ?? \"\",\n        title: citation.title ?? \"\",\n        start_index: citation.startIndex ?? 0,\n        end_index: citation.endIndex ?? 0,\n      } as OpenAIAnnotation;\n    }\n\n    if (citation.source === \"file_citation\") {\n      return {\n        type: \"file_citation\",\n        file_id: citation.file_id ?? \"\",\n        filename: citation.title ?? \"\",\n        index: citation.startIndex ?? 0,\n      } as OpenAIAnnotation;\n    }\n\n    if (citation.source === \"container_file_citation\") {\n      return {\n        type: \"container_file_citation\",\n        file_id: citation.file_id ?? \"\",\n        filename: citation.title ?? \"\",\n        container_id: citation.container_id ?? \"\",\n        start_index: citation.startIndex ?? 0,\n        end_index: citation.endIndex ?? 0,\n      } as OpenAIAnnotation;\n    }\n\n    if (citation.source === \"file_path\") {\n      return {\n        type: \"file_path\",\n        file_id: citation.file_id ?? \"\",\n        index: citation.startIndex ?? 0,\n      } as OpenAIAnnotation;\n    }\n  }\n\n  // For non_standard blocks, unwrap the value\n  if (annotation.type === \"non_standard\") {\n    return (annotation as ContentBlock.NonStandard)\n      .value as unknown as OpenAIAnnotation;\n  }\n\n  // Unknown format, pass through as-is\n  return annotation as unknown as OpenAIAnnotation;\n}\n\ntype ExcludeController<T> = T extends { controller: unknown } ? never : T;\n\nexport type ResponsesCreate = OpenAIClient.Responses[\"create\"];\nexport type ResponsesParse = OpenAIClient.Responses[\"parse\"];\n\nexport type ResponsesCreateInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesCreate>>\n>;\nexport type ResponsesParseInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesParse>>\n>;\n\nexport type ResponsesInputItem = OpenAIClient.Responses.ResponseInputItem;\n\n/**\n * Converts OpenAI Responses API usage statistics to LangChain's UsageMetadata format.\n *\n * This converter transforms token usage information from OpenAI's Responses API into\n * the standardized UsageMetadata format used throughout LangChain. It handles both\n * basic token counts and detailed token breakdowns including cached tokens and\n * reasoning tokens.\n *\n * @param usage - The usage statistics object from OpenAI's Responses API containing\n *                token counts and optional detailed breakdowns.\n *\n * @returns A UsageMetadata object containing:\n *   - `input_tokens`: Total number of tokens in the input/prompt (defaults to 0 if not provided)\n *   - `output_tokens`: Total number of tokens in the model's output (defaults to 0 if not provided)\n *   - `total_tokens`: Combined total of input and output tokens (defaults to 0 if not provided)\n *   - `input_token_details`: Object containing detailed input token information:\n *     - `cache_read`: Number of tokens read from cache (only included if available)\n *   - `output_token_details`: Object containing detailed output token information:\n *     - `reasoning`: Number of tokens used for reasoning (only included if available)\n *\n * @example\n * ```typescript\n * const usage = {\n *   input_tokens: 100,\n *   output_tokens: 50,\n *   total_tokens: 150,\n *   input_tokens_details: { cached_tokens: 20 },\n *   output_tokens_details: { reasoning_tokens: 10 }\n * };\n *\n * const metadata = convertResponsesUsageToUsageMetadata(usage);\n * // Returns:\n * // {\n * //   input_tokens: 100,\n * //   output_tokens: 50,\n * //   total_tokens: 150,\n * //   input_token_details: { cache_read: 20 },\n * //   output_token_details: { reasoning: 10 }\n * // }\n * ```\n *\n * @remarks\n * - The function safely handles undefined or null values by using optional chaining\n *   and nullish coalescing operators\n * - Detailed token information (cache_read, reasoning) is only included in the result\n *   if the corresponding values are present in the input\n * - Token counts default to 0 if not provided in the usage object\n * - This converter is specifically designed for OpenAI's Responses API format and\n *   may differ from other OpenAI API endpoints\n */\nexport const convertResponsesUsageToUsageMetadata: Converter<\n  OpenAIClient.Responses.ResponseUsage | undefined,\n  UsageMetadata\n> = (usage) => {\n  const inputTokenDetails = {\n    ...(usage?.input_tokens_details?.cached_tokens != null && {\n      cache_read: usage?.input_tokens_details?.cached_tokens,\n    }),\n  };\n  const outputTokenDetails = {\n    ...(usage?.output_tokens_details?.reasoning_tokens != null && {\n      reasoning: usage?.output_tokens_details?.reasoning_tokens,\n    }),\n  };\n  return {\n    input_tokens: usage?.input_tokens ?? 0,\n    output_tokens: usage?.output_tokens ?? 0,\n    total_tokens: usage?.total_tokens ?? 0,\n    input_token_details: inputTokenDetails,\n    output_token_details: outputTokenDetails,\n  };\n};\n\n/**\n * Converts an OpenAI Responses API response to a LangChain AIMessage.\n *\n * This converter processes the output from OpenAI's Responses API (both `create` and `parse` methods)\n * and transforms it into a LangChain AIMessage object with all relevant metadata, tool calls, and content.\n *\n * @param response - The response object from OpenAI's Responses API. Can be either:\n *   - ResponsesCreateInvoke: Result from `responses.create()`\n *   - ResponsesParseInvoke: Result from `responses.parse()`\n *\n * @returns An AIMessage containing:\n *   - `id`: The message ID from the response output\n *   - `content`: Array of message content blocks (text, images, etc.)\n *   - `tool_calls`: Array of successfully parsed tool calls\n *   - `invalid_tool_calls`: Array of tool calls that failed to parse\n *   - `usage_metadata`: Token usage information converted to LangChain format\n *   - `additional_kwargs`: Extra data including:\n *     - `refusal`: Refusal text if the model refused to respond\n *     - `reasoning`: Reasoning output for reasoning models\n *     - `tool_outputs`: Results from built-in tools (web search, file search, etc.)\n *     - `parsed`: Parsed structured output when using json_schema format\n *     - Function call ID mappings for tracking\n *   - `response_metadata`: Metadata about the response including model, timestamps, status, etc.\n *\n * @throws Error if the response contains an error object. The error message and code are extracted\n *   from the response.error field.\n *\n * @example\n * ```typescript\n * const response = await client.responses.create({\n *   model: \"gpt-4\",\n *   input: [{ type: \"message\", content: \"Hello\" }]\n * });\n * const message = convertResponsesMessageToAIMessage(response);\n * console.log(message.content); // Message content\n * console.log(message.tool_calls); // Any tool calls made\n * ```\n *\n * @remarks\n * The converter handles multiple output item types:\n * - `message`: Text and structured content from the model\n * - `function_call`: Tool/function calls that need to be executed\n * - `reasoning`: Reasoning traces from reasoning models (o1, o3, etc.)\n * - `custom_tool_call`: Custom tool invocations\n * - Built-in tool outputs: web_search, file_search, code_interpreter, etc.\n *\n * Tool calls are parsed and validated. Invalid tool calls (malformed JSON, etc.) are captured\n * in the `invalid_tool_calls` array rather than throwing errors.\n */\nexport const convertResponsesMessageToAIMessage: Converter<\n  ResponsesCreateInvoke | ResponsesParseInvoke,\n  AIMessage\n> = (response) => {\n  if (response.error) {\n    // TODO: add support for `addLangChainErrorFields`\n    const error = new Error(response.error.message);\n    error.name = response.error.code;\n    throw error;\n  }\n\n  let messageId: string | undefined;\n  const content: MessageContent = [];\n  const tool_calls: ToolCall[] = [];\n  const invalid_tool_calls: InvalidToolCall[] = [];\n  // Preserve the raw output items so that convertMessagesToResponsesInput can\n  // return them verbatim on the fast path.  We strip `parsed_arguments` from\n  // function_call items because the OpenAI SDK injects it when using\n  // responses.parse(), but the API rejects it when sent back as input.\n  const cleanedOutput = response.output.map((item) => {\n    if (item.type === \"function_call\" && \"parsed_arguments\" in item) {\n      const cleaned = { ...item };\n      delete (cleaned as Record<string, unknown>).parsed_arguments;\n      return cleaned;\n    }\n    return item;\n  });\n\n  const response_metadata: Record<string, unknown> = {\n    model_provider: \"openai\",\n    model: response.model,\n    created_at: response.created_at,\n    id: response.id,\n    incomplete_details: response.incomplete_details,\n    metadata: response.metadata,\n    object: response.object,\n    output: cleanedOutput,\n    status: response.status,\n    user: response.user,\n    service_tier: response.service_tier,\n    // for compatibility with chat completion calls.\n    model_name: response.model,\n  };\n\n  const additional_kwargs: {\n    [key: string]: unknown;\n    refusal?: string;\n    reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n    tool_outputs?: unknown[];\n    parsed?: unknown;\n    [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n  } = {};\n\n  for (const item of response.output) {\n    if (item.type === \"message\") {\n      messageId = item.id;\n      content.push(\n        ...item.content.flatMap((part) => {\n          if (part.type === \"output_text\") {\n            if (\"parsed\" in part && part.parsed != null) {\n              additional_kwargs.parsed = part.parsed;\n            }\n            return {\n              type: \"text\",\n              text: part.text,\n              annotations: part.annotations.map(\n                convertOpenAIAnnotationToLangChain\n              ),\n            } satisfies ContentBlock.Text;\n          }\n\n          if (part.type === \"refusal\") {\n            additional_kwargs.refusal = part.refusal;\n            return [];\n          }\n\n          return part;\n        })\n      );\n    } else if (item.type === \"function_call\") {\n      const fnAdapter = {\n        function: { name: item.name, arguments: item.arguments },\n        id: item.call_id,\n      };\n\n      try {\n        tool_calls.push(parseToolCall(fnAdapter, { returnId: true }));\n      } catch (e: unknown) {\n        let errMessage: string | undefined;\n        if (\n          typeof e === \"object\" &&\n          e != null &&\n          \"message\" in e &&\n          typeof e.message === \"string\"\n        ) {\n          errMessage = e.message;\n        }\n        invalid_tool_calls.push(makeInvalidToolCall(fnAdapter, errMessage));\n      }\n\n      additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] ??= {};\n      if (item.id) {\n        additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY][item.call_id] = item.id;\n      }\n    } else if (item.type === \"reasoning\") {\n      additional_kwargs.reasoning = item;\n      // Also elevate reasoning to content for UI rendering\n      const reasoningText = item.summary\n        ?.map((s) => s.text)\n        .filter(Boolean)\n        .join(\"\");\n      if (reasoningText) {\n        content.push({\n          type: \"reasoning\",\n          reasoning: reasoningText,\n        });\n      }\n    } else if (item.type === \"custom_tool_call\") {\n      const parsed = parseCustomToolCall(item);\n      if (parsed) {\n        tool_calls.push(parsed);\n      } else {\n        invalid_tool_calls.push(\n          makeInvalidToolCall(item, \"Malformed custom tool call\")\n        );\n      }\n    } else if (item.type === \"computer_call\") {\n      const parsed = parseComputerCall(item);\n      if (parsed) {\n        tool_calls.push(parsed);\n      } else {\n        invalid_tool_calls.push(\n          makeInvalidToolCall(item, \"Malformed computer call\")\n        );\n      }\n    } else if (item.type === \"image_generation_call\") {\n      // Add image as proper content block if result is available\n      if (item.result) {\n        content.push({\n          type: \"image\",\n          mimeType: \"image/png\",\n          data: item.result,\n          id: item.id,\n          metadata: {\n            status: item.status,\n          },\n        } satisfies ContentBlock.Multimodal.Image);\n      }\n      // Also store in tool_outputs for backwards compatibility and multi-turn editing (needs id)\n      additional_kwargs.tool_outputs ??= [];\n      additional_kwargs.tool_outputs.push(item);\n    } else {\n      additional_kwargs.tool_outputs ??= [];\n      additional_kwargs.tool_outputs.push(item);\n    }\n  }\n\n  return new AIMessage({\n    id: messageId,\n    content,\n    tool_calls,\n    invalid_tool_calls,\n    usage_metadata: convertResponsesUsageToUsageMetadata(response.usage),\n    additional_kwargs,\n    response_metadata,\n  });\n};\n\n/**\n * Converts a LangChain ChatOpenAI reasoning summary to an OpenAI Responses API reasoning item.\n *\n * This converter transforms reasoning summaries that have been accumulated during streaming\n * (where summary parts may arrive in multiple chunks with the same index) into the final\n * consolidated format expected by OpenAI's Responses API. It combines summary parts that\n * share the same index and removes the index field from the final output.\n *\n * @param reasoning - A ChatOpenAI reasoning summary object containing:\n *   - `id`: The reasoning item ID\n *   - `type`: The type of reasoning (typically \"reasoning\")\n *   - `summary`: Array of summary parts, each with:\n *     - `text`: The summary text content\n *     - `type`: The summary type (e.g., \"summary_text\")\n *     - `index`: The index used to group related summary parts during streaming\n *\n * @returns An OpenAI Responses API ResponseReasoningItem with:\n *   - All properties from the input reasoning object\n *   - `summary`: Consolidated array of summary objects with:\n *     - `text`: Combined text from all parts with the same index\n *     - `type`: The summary type\n *     - No `index` field (removed after consolidation)\n *\n * @example\n * ```typescript\n * // Input: Reasoning summary with multiple parts at the same index\n * const reasoning = {\n *   id: \"reasoning_123\",\n *   type: \"reasoning\",\n *   summary: [\n *     { text: \"First \", type: \"summary_text\", index: 0 },\n *     { text: \"part\", type: \"summary_text\", index: 0 },\n *     { text: \"Second part\", type: \"summary_text\", index: 1 }\n *   ]\n * };\n *\n * const result = convertReasoningSummaryToResponsesReasoningItem(reasoning);\n * // Returns:\n * // {\n * //   id: \"reasoning_123\",\n * //   type: \"reasoning\",\n * //   summary: [\n * //     { text: \"First part\", type: \"summary_text\" },\n * //     { text: \"Second part\", type: \"summary_text\" }\n * //   ]\n * // }\n * ```\n *\n * @remarks\n * - This converter is primarily used when reconstructing complete reasoning items from\n *   streaming chunks, where summary parts may arrive incrementally with index markers\n * - Summary parts with the same index are concatenated in the order they appear\n * - If the reasoning summary contains only one part, no reduction is performed\n * - The index field is used internally during streaming to track which summary parts\n *   belong together, but is removed from the final output as it's not part of the\n *   OpenAI Responses API schema\n * - This is the inverse operation of the streaming accumulation that happens in\n *   `convertResponsesDeltaToChatGenerationChunk`\n */\nexport const convertReasoningSummaryToResponsesReasoningItem: Converter<\n  ChatOpenAIReasoningSummary,\n  OpenAIClient.Responses.ResponseReasoningItem\n> = (reasoning) => {\n  // combine summary parts that have the the same index and then remove the indexes\n  const summary = (\n    reasoning.summary.length > 1\n      ? reasoning.summary.reduce(\n          (acc, curr) => {\n            const last = acc[acc.length - 1];\n\n            if (last!.index === curr.index) {\n              last!.text += curr.text;\n            } else {\n              acc.push(curr);\n            }\n            return acc;\n          },\n          [{ ...reasoning.summary[0] }]\n        )\n      : reasoning.summary\n  ).map((s) =>\n    Object.fromEntries(Object.entries(s).filter(([k]) => k !== \"index\"))\n  ) as OpenAIClient.Responses.ResponseReasoningItem.Summary[];\n\n  return {\n    ...reasoning,\n    summary,\n  } as OpenAIClient.Responses.ResponseReasoningItem;\n};\n\n/**\n * Converts OpenAI Responses API stream events to LangChain ChatGenerationChunk objects.\n *\n * This converter processes streaming events from OpenAI's Responses API and transforms them\n * into LangChain ChatGenerationChunk objects that can be used in streaming chat applications.\n * It handles various event types including text deltas, tool calls, reasoning, and metadata updates.\n *\n * @param event - A streaming event from OpenAI's Responses API\n *\n * @returns A ChatGenerationChunk containing:\n *   - `text`: Concatenated text content from all text parts in the event\n *   - `message`: An AIMessageChunk with:\n *     - `id`: Message ID (set when a message output item is added)\n *     - `content`: Array of content blocks (text with optional annotations)\n *     - `tool_call_chunks`: Incremental tool call data (name, args, id)\n *     - `usage_metadata`: Token usage information (only in completion events)\n *     - `additional_kwargs`: Extra data including:\n *       - `refusal`: Refusal text if the model refused to respond\n *       - `reasoning`: Reasoning output for reasoning models (id, type, summary)\n *       - `tool_outputs`: Results from built-in tools (web search, file search, etc.)\n *       - `parsed`: Parsed structured output when using json_schema format\n *       - Function call ID mappings for tracking\n *     - `response_metadata`: Metadata about the response (model, id, etc.)\n *   - `generationInfo`: Additional generation information (e.g., tool output status)\n *\n *   Returns `null` for events that don't produce meaningful chunks:\n *   - Partial image generation events (to avoid storing all partial images in history)\n *   - Unrecognized event types\n *\n * @example\n * ```typescript\n * const stream = await client.responses.create({\n *   model: \"gpt-4\",\n *   input: [{ type: \"message\", content: \"Hello\" }],\n *   stream: true\n * });\n *\n * for await (const event of stream) {\n *   const chunk = convertResponsesDeltaToChatGenerationChunk(event);\n *   if (chunk) {\n *     console.log(chunk.text); // Incremental text\n *     console.log(chunk.message.tool_call_chunks); // Tool call updates\n *   }\n * }\n * ```\n *\n * @remarks\n * - Text content is accumulated in an array with index tracking for proper ordering\n * - Tool call chunks include incremental arguments that need to be concatenated by the consumer\n * - Reasoning summaries are built incrementally across multiple events\n * - Function call IDs are tracked in `additional_kwargs` to map call_id to item id\n * - The `text` field is provided for legacy compatibility with `onLLMNewToken` callbacks\n * - Usage metadata is only available in `response.completed` events\n * - Partial images are intentionally ignored to prevent memory bloat in conversation history\n */\nexport const convertResponsesDeltaToChatGenerationChunk: Converter<\n  OpenAIClient.Responses.ResponseStreamEvent,\n  ChatGenerationChunk | null\n> = (event) => {\n  const content: ContentBlock[] = [];\n  let generationInfo: Record<string, unknown> = {};\n  let usage_metadata: UsageMetadata | undefined;\n  const tool_call_chunks: ToolCallChunk[] = [];\n  const response_metadata: Record<string, unknown> = {\n    model_provider: \"openai\",\n  };\n  const additional_kwargs: {\n    [key: string]: unknown;\n    reasoning?: Partial<ChatOpenAIReasoningSummary>;\n    tool_outputs?: unknown[];\n  } = {};\n  let id: string | undefined;\n  if (event.type === \"response.output_text.delta\") {\n    content.push({\n      type: \"text\",\n      text: event.delta,\n      index: event.content_index,\n    } satisfies ContentBlock.Text);\n  } else if (event.type === \"response.output_text.annotation.added\") {\n    content.push({\n      type: \"text\",\n      text: \"\",\n      annotations: [\n        convertOpenAIAnnotationToLangChain(\n          event.annotation as OpenAIAnnotation\n        ),\n      ],\n      index: event.content_index,\n    } satisfies ContentBlock.Text);\n  } else if (\n    event.type === \"response.output_item.added\" &&\n    event.item.type === \"message\"\n  ) {\n    id = event.item.id;\n  } else if (\n    event.type === \"response.output_item.added\" &&\n    event.item.type === \"function_call\"\n  ) {\n    tool_call_chunks.push({\n      type: \"tool_call_chunk\",\n      name: event.item.name,\n      args: event.item.arguments,\n      id: event.item.call_id,\n      index: event.output_index,\n    });\n\n    additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] = {\n      [event.item.call_id]: event.item.id,\n    };\n  } else if (\n    event.type === \"response.output_item.done\" &&\n    event.item.type === \"computer_call\"\n  ) {\n    // Handle computer_call as a tool call so ToolNode can process it\n    tool_call_chunks.push({\n      type: \"tool_call_chunk\",\n      name: \"computer_use\",\n      args: JSON.stringify({ action: event.item.action }),\n      id: event.item.call_id,\n      index: event.output_index,\n    });\n    // Also store the raw item for additional context (pending_safety_checks, etc.)\n    additional_kwargs.tool_outputs = [event.item];\n  } else if (\n    event.type === \"response.output_item.done\" &&\n    event.item.type === \"image_generation_call\"\n  ) {\n    // Add image as proper content block if result is available\n    if (event.item.result) {\n      content.push({\n        type: \"image\",\n        mimeType: \"image/png\",\n        data: event.item.result,\n        id: event.item.id,\n        metadata: {\n          status: event.item.status,\n        },\n      } satisfies ContentBlock.Multimodal.Image);\n    }\n    // Also store in tool_outputs for backwards compatibility and multi-turn editing (needs id)\n    additional_kwargs.tool_outputs = [event.item];\n  } else if (\n    event.type === \"response.output_item.done\" &&\n    [\n      \"web_search_call\",\n      \"file_search_call\",\n      \"code_interpreter_call\",\n      \"shell_call\",\n      \"local_shell_call\",\n      \"mcp_call\",\n      \"mcp_list_tools\",\n      \"mcp_approval_request\",\n      \"custom_tool_call\",\n    ].includes(event.item.type)\n  ) {\n    additional_kwargs.tool_outputs = [event.item];\n  } else if (event.type === \"response.created\") {\n    response_metadata.id = event.response.id;\n    response_metadata.model_name = event.response.model;\n    response_metadata.model = event.response.model;\n  } else if (event.type === \"response.completed\") {\n    const msg = convertResponsesMessageToAIMessage(event.response);\n\n    usage_metadata = convertResponsesUsageToUsageMetadata(event.response.usage);\n\n    if (event.response.text?.format?.type === \"json_schema\") {\n      additional_kwargs.parsed ??= JSON.parse(msg.text);\n    }\n    for (const [key, value] of Object.entries(event.response)) {\n      if (key === \"id\") continue;\n      // Use the cleaned output from the converted message so that\n      // SDK-only fields like parsed_arguments are not persisted.\n      if (key === \"output\") {\n        response_metadata[key] = msg.response_metadata.output;\n      } else {\n        response_metadata[key] = value;\n      }\n    }\n  } else if (\n    event.type === \"response.function_call_arguments.delta\" ||\n    event.type === \"response.custom_tool_call_input.delta\"\n  ) {\n    tool_call_chunks.push({\n      type: \"tool_call_chunk\",\n      args: event.delta,\n      index: event.output_index,\n    });\n  } else if (\n    event.type === \"response.web_search_call.completed\" ||\n    event.type === \"response.file_search_call.completed\"\n  ) {\n    generationInfo = {\n      tool_outputs: {\n        id: event.item_id,\n        type: event.type.replace(\"response.\", \"\").replace(\".completed\", \"\"),\n        status: \"completed\",\n      },\n    };\n  } else if (event.type === \"response.refusal.done\") {\n    additional_kwargs.refusal = event.refusal;\n  } else if (\n    event.type === \"response.output_item.added\" &&\n    \"item\" in event &&\n    event.item.type === \"reasoning\"\n  ) {\n    const summary: ChatOpenAIReasoningSummary[\"summary\"] | undefined = event\n      .item.summary\n      ? event.item.summary.map((s, index) => ({\n          ...s,\n          index,\n        }))\n      : undefined;\n\n    additional_kwargs.reasoning = {\n      // We only capture ID in the first event or else the concatenated result of all chunks will\n      // have an ID field that is repeated once per event. There is special handling for the `type`\n      // field that prevents this, however.\n      id: event.item.id,\n      type: event.item.type,\n      ...(summary ? { summary } : {}),\n    };\n\n    // Also elevate reasoning to content for UI rendering\n    const reasoningText = event.item.summary\n      ?.map((s) => s.text)\n      .filter(Boolean)\n      .join(\"\");\n    if (reasoningText) {\n      content.push({\n        type: \"reasoning\",\n        reasoning: reasoningText,\n      });\n    }\n  } else if (event.type === \"response.reasoning_summary_part.added\") {\n    additional_kwargs.reasoning = {\n      type: \"reasoning\",\n      summary: [{ ...event.part, index: event.summary_index }],\n    };\n\n    // Also elevate reasoning to content for UI rendering\n    if (event.part.text) {\n      content.push({\n        type: \"reasoning\",\n        reasoning: event.part.text,\n      });\n    }\n  } else if (event.type === \"response.reasoning_summary_text.delta\") {\n    additional_kwargs.reasoning = {\n      type: \"reasoning\",\n      summary: [\n        {\n          text: event.delta,\n          type: \"summary_text\",\n          index: event.summary_index,\n        },\n      ],\n    };\n\n    // Also elevate reasoning to content for UI rendering\n    if (event.delta) {\n      content.push({\n        type: \"reasoning\",\n        reasoning: event.delta,\n      });\n    }\n  } else if (event.type === \"response.image_generation_call.partial_image\") {\n    // noop/fixme: retaining partial images in a message chunk means that _all_\n    // partial images get kept in history, so we don't do anything here.\n    return null;\n  } else {\n    return null;\n  }\n\n  return new ChatGenerationChunk({\n    // Legacy reasons, `onLLMNewToken` should pulls this out\n    text: content.map((part) => part.text).join(\"\"),\n    message: new AIMessageChunk({\n      id,\n      content,\n      tool_call_chunks,\n      usage_metadata,\n      additional_kwargs,\n      response_metadata,\n    }),\n    generationInfo,\n  });\n};\n\n/**\n * Converts a single LangChain BaseMessage to OpenAI Responses API input format.\n *\n * This converter transforms a LangChain message into one or more ResponseInputItem objects\n * that can be used with OpenAI's Responses API. It handles complex message structures including\n * tool calls, reasoning blocks, multimodal content, and various content block types.\n *\n * @param message - The LangChain BaseMessage to convert. Can be any message type including\n *   HumanMessage, AIMessage, SystemMessage, ToolMessage, etc.\n *\n * @returns An array of ResponseInputItem objects.\n *\n * @example\n * Basic text message conversion:\n * ```typescript\n * const message = new HumanMessage(\"Hello, how are you?\");\n * const items = convertStandardContentMessageToResponsesInput(message);\n * // Returns: [{ type: \"message\", role: \"user\", content: [{ type: \"input_text\", text: \"Hello, how are you?\" }] }]\n * ```\n *\n * @example\n * AI message with tool calls:\n * ```typescript\n * const message = new AIMessage({\n *   content: \"I'll check the weather for you.\",\n *   tool_calls: [{\n *     id: \"call_123\",\n *     name: \"get_weather\",\n *     args: { location: \"San Francisco\" }\n *   }]\n * });\n * const items = convertStandardContentMessageToResponsesInput(message);\n * // Returns:\n * // [\n * //   { type: \"message\", role: \"assistant\", content: [{ type: \"input_text\", text: \"I'll check the weather for you.\" }] },\n * //   { type: \"function_call\", call_id: \"call_123\", name: \"get_weather\", arguments: '{\"location\":\"San Francisco\"}' }\n * // ]\n * ```\n */\nexport const convertStandardContentMessageToResponsesInput: Converter<\n  BaseMessage,\n  OpenAIClient.Responses.ResponseInputItem[]\n> = (message) => {\n  const isResponsesMessage =\n    AIMessage.isInstance(message) &&\n    message.response_metadata?.model_provider === \"openai\";\n\n  function* iterateItems(): Generator<OpenAIClient.Responses.ResponseInputItem> {\n    const messageRole = iife(() => {\n      try {\n        const role = messageToOpenAIRole(message);\n        if (\n          role === \"system\" ||\n          role === \"developer\" ||\n          role === \"assistant\" ||\n          role === \"user\"\n        ) {\n          return role;\n        }\n        return \"assistant\";\n      } catch {\n        return \"assistant\";\n      }\n    });\n\n    let currentMessage: OpenAIClient.Responses.EasyInputMessage | undefined =\n      undefined;\n\n    const functionCallIdsWithBlocks = new Set<string>();\n    const serverFunctionCallIdsWithBlocks = new Set<string>();\n\n    const pendingFunctionChunks = new Map<\n      string,\n      { name?: string; args: string[] }\n    >();\n    const pendingServerFunctionChunks = new Map<\n      string,\n      { name?: string; args: string[] }\n    >();\n\n    function* flushMessage() {\n      if (!currentMessage) return;\n      const content = currentMessage.content;\n      if (\n        (typeof content === \"string\" && content.length > 0) ||\n        (Array.isArray(content) && content.length > 0)\n      ) {\n        yield currentMessage;\n      }\n      currentMessage = undefined;\n    }\n\n    const pushMessageContent: (\n      content: OpenAIClient.Responses.ResponseInputMessageContentList\n    ) => void = (content) => {\n      if (!currentMessage) {\n        currentMessage = {\n          type: \"message\",\n          role: messageRole,\n          content: [],\n        };\n      }\n      if (typeof currentMessage.content === \"string\") {\n        currentMessage.content =\n          currentMessage.content.length > 0\n            ? [{ type: \"input_text\", text: currentMessage.content }, ...content]\n            : [...content];\n      } else {\n        currentMessage.content.push(...content);\n      }\n    };\n\n    const toJsonString = (value: unknown) => {\n      if (typeof value === \"string\") {\n        return value;\n      }\n      try {\n        return JSON.stringify(value ?? {});\n      } catch {\n        return \"{}\";\n      }\n    };\n\n    const resolveImageItem = (\n      block: ContentBlock.Multimodal.Image\n    ): OpenAIClient.Responses.ResponseInputImage | undefined => {\n      const detail = iife(() => {\n        const raw = block.metadata?.detail;\n        if (raw === \"low\" || raw === \"high\" || raw === \"auto\") {\n          return raw;\n        }\n        return \"auto\";\n      });\n      if (block.fileId) {\n        return {\n          type: \"input_image\",\n          detail,\n          file_id: block.fileId,\n        };\n      }\n      if (block.url) {\n        return {\n          type: \"input_image\",\n          detail,\n          image_url: block.url,\n        };\n      }\n      if (block.data) {\n        const base64Data =\n          typeof block.data === \"string\"\n            ? block.data\n            : Buffer.from(block.data).toString(\"base64\");\n        const mimeType = block.mimeType ?? \"image/png\";\n        return {\n          type: \"input_image\",\n          detail,\n          image_url: `data:${mimeType};base64,${base64Data}`,\n        };\n      }\n      return undefined;\n    };\n\n    const resolveFileItem = (\n      block: ContentBlock.Multimodal.File | ContentBlock.Multimodal.Video\n    ): OpenAIClient.Responses.ResponseInputFile | undefined => {\n      if (block.fileId) {\n        const filename = getFilenameFromMetadata(block);\n        return {\n          type: \"input_file\",\n          file_id: block.fileId,\n          ...(filename ? { filename } : {}),\n        };\n      }\n      if (block.url) {\n        const filename = getFilenameFromMetadata(block);\n        return {\n          ...(filename ? { filename } : {}),\n          type: \"input_file\",\n          file_url: block.url,\n        };\n      }\n      if (block.data) {\n        const filename = getRequiredFilenameFromMetadata(block);\n        const encoded =\n          typeof block.data === \"string\"\n            ? block.data\n            : Buffer.from(block.data).toString(\"base64\");\n        const mimeType = block.mimeType ?? \"application/octet-stream\";\n        return {\n          type: \"input_file\",\n          file_data: `data:${mimeType};base64,${encoded}`,\n          ...(filename ? { filename } : {}),\n        };\n      }\n      return undefined;\n    };\n\n    const convertReasoningBlock = (\n      block: ContentBlock.Reasoning\n    ): OpenAIClient.Responses.ResponseReasoningItem => {\n      const summaryEntries = iife(() => {\n        if (Array.isArray(block.summary)) {\n          const candidate = block.summary;\n          const mapped =\n            candidate\n              ?.map((item) => item?.text)\n              .filter((text): text is string => typeof text === \"string\") ?? [];\n          if (mapped.length > 0) {\n            return mapped;\n          }\n        }\n        return block.reasoning ? [block.reasoning] : [];\n      });\n\n      const summary =\n        summaryEntries.length > 0\n          ? summaryEntries.map((text) => ({\n              type: \"summary_text\" as const,\n              text,\n            }))\n          : [{ type: \"summary_text\" as const, text: \"\" }];\n\n      const reasoningItem: OpenAIClient.Responses.ResponseReasoningItem = {\n        type: \"reasoning\",\n        id: block.id ?? \"\",\n        summary,\n      };\n\n      if (block.reasoning) {\n        reasoningItem.content = [\n          {\n            type: \"reasoning_text\" as const,\n            text: block.reasoning,\n          },\n        ];\n      }\n      return reasoningItem;\n    };\n\n    const convertFunctionCall = (\n      block: ContentBlock.Tools.ToolCall | ContentBlock.Tools.ServerToolCall\n    ): OpenAIClient.Responses.ResponseFunctionToolCall => ({\n      type: \"function_call\",\n      name: block.name ?? \"\",\n      call_id: block.id ?? \"\",\n      arguments: toJsonString(block.args),\n    });\n\n    const convertFunctionCallOutput = (\n      block: ContentBlock.Tools.ServerToolCallResult\n    ): OpenAIClient.Responses.ResponseInputItem.FunctionCallOutput => {\n      const output = toJsonString(block.output);\n      const status =\n        block.status === \"success\"\n          ? \"completed\"\n          : block.status === \"error\"\n            ? \"incomplete\"\n            : undefined;\n      return {\n        type: \"function_call_output\",\n        call_id: block.toolCallId ?? \"\",\n        output,\n        ...(status ? { status } : {}),\n      };\n    };\n\n    for (const block of message.contentBlocks) {\n      if (block.type === \"text\") {\n        pushMessageContent([{ type: \"input_text\", text: block.text }]);\n      } else if (block.type === \"invalid_tool_call\") {\n        // no-op\n      } else if (block.type === \"reasoning\") {\n        yield* flushMessage();\n        yield convertReasoningBlock(\n          block as ContentBlock.Standard & { type: \"reasoning\" }\n        );\n      } else if (block.type === \"tool_call\") {\n        yield* flushMessage();\n        const id = block.id ?? \"\";\n        if (id) {\n          functionCallIdsWithBlocks.add(id);\n          pendingFunctionChunks.delete(id);\n        }\n        yield convertFunctionCall(\n          block as ContentBlock.Standard & { type: \"tool_call\" }\n        );\n      } else if (block.type === \"tool_call_chunk\") {\n        if (block.id) {\n          const existing = pendingFunctionChunks.get(block.id) ?? {\n            name: block.name,\n            args: [],\n          };\n          if (block.name) existing.name = block.name;\n          if (block.args) existing.args.push(block.args);\n          pendingFunctionChunks.set(block.id, existing);\n        }\n      } else if (block.type === \"server_tool_call\") {\n        yield* flushMessage();\n        const id = block.id ?? \"\";\n        if (id) {\n          serverFunctionCallIdsWithBlocks.add(id);\n          pendingServerFunctionChunks.delete(id);\n        }\n        yield convertFunctionCall(block);\n      } else if (block.type === \"server_tool_call_chunk\") {\n        if (block.id) {\n          const existing = pendingServerFunctionChunks.get(block.id) ?? {\n            name: block.name,\n            args: [],\n          };\n          if (block.name) existing.name = block.name;\n          if (block.args) existing.args.push(block.args);\n          pendingServerFunctionChunks.set(block.id, existing);\n        }\n      } else if (block.type === \"server_tool_call_result\") {\n        yield* flushMessage();\n        yield convertFunctionCallOutput(block);\n      } else if (block.type === \"audio\") {\n        // no-op\n      } else if (block.type === \"file\") {\n        const fileItem = resolveFileItem(block);\n        if (fileItem) {\n          pushMessageContent([fileItem]);\n        }\n      } else if (block.type === \"image\") {\n        const imageItem = resolveImageItem(block);\n        if (imageItem) {\n          pushMessageContent([imageItem]);\n        }\n      } else if (block.type === \"video\") {\n        const videoItem = resolveFileItem(block);\n        if (videoItem) {\n          pushMessageContent([videoItem]);\n        }\n      } else if (block.type === \"text-plain\") {\n        if (block.text) {\n          pushMessageContent([\n            {\n              type: \"input_text\",\n              text: block.text,\n            },\n          ]);\n        }\n      } else if (block.type === \"non_standard\" && isResponsesMessage) {\n        yield* flushMessage();\n        yield block.value as ResponsesInputItem;\n      }\n    }\n    yield* flushMessage();\n\n    for (const [id, chunk] of pendingFunctionChunks) {\n      if (!id || functionCallIdsWithBlocks.has(id)) continue;\n      const args = chunk.args.join(\"\");\n      if (!chunk.name && !args) continue;\n      yield {\n        type: \"function_call\",\n        call_id: id,\n        name: chunk.name ?? \"\",\n        arguments: args,\n      };\n    }\n\n    for (const [id, chunk] of pendingServerFunctionChunks) {\n      if (!id || serverFunctionCallIdsWithBlocks.has(id)) continue;\n      const args = chunk.args.join(\"\");\n      if (!chunk.name && !args) continue;\n      yield {\n        type: \"function_call\",\n        call_id: id,\n        name: chunk.name ?? \"\",\n        arguments: args,\n      };\n    }\n  }\n  return Array.from(iterateItems());\n};\n\n/**\n * - MCP (Model Context Protocol) approval responses\n * - Zero Data Retention (ZDR) mode handling\n *\n * @param params - Conversion parameters\n * @param params.messages - Array of LangChain BaseMessages to convert\n * @param params.zdrEnabled - Whether Zero Data Retention mode is enabled. When true, certain\n *   metadata like message IDs and function call IDs are omitted from the output\n * @param params.model - The model name being used. Used to determine if special role mapping\n *   is needed (e.g., \"system\" -> \"developer\" for reasoning models)\n *\n * @returns Array of ResponsesInputItem objects formatted for the OpenAI Responses API\n *\n * @throws {Error} When a function message is encountered (not supported)\n * @throws {Error} When computer call output format is invalid\n *\n * @example\n * ```typescript\n * const messages = [\n *   new HumanMessage(\"Hello\"),\n *   new AIMessage({ content: \"Hi there!\", tool_calls: [...] })\n * ];\n *\n * const input = convertMessagesToResponsesInput({\n *   messages,\n *   zdrEnabled: false,\n *   model: \"gpt-4\"\n * });\n * ```\n */\nexport const convertMessagesToResponsesInput: Converter<\n  { messages: BaseMessage[]; zdrEnabled: boolean; model: string },\n  ResponsesInputItem[]\n> = ({ messages, zdrEnabled, model }) => {\n  return messages.flatMap(\n    (lcMsg): ResponsesInputItem | ResponsesInputItem[] => {\n      const responseMetadata = lcMsg.response_metadata as\n        | Record<string, unknown>\n        | undefined;\n      if (responseMetadata?.output_version === \"v1\") {\n        return convertStandardContentMessageToResponsesInput(lcMsg);\n      }\n\n      const additional_kwargs =\n        lcMsg.additional_kwargs as BaseMessageFields[\"additional_kwargs\"] & {\n          [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n          reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n          type?: string;\n          refusal?: string;\n        };\n\n      let role = messageToOpenAIRole(lcMsg);\n      if (role === \"system\" && isReasoningModel(model)) role = \"developer\";\n\n      if (role === \"function\") {\n        throw new Error(\"Function messages are not supported in Responses API\");\n      }\n\n      if (role === \"tool\") {\n        const toolMessage = lcMsg as ToolMessage;\n\n        // Handle computer call output\n        if (additional_kwargs?.type === \"computer_call_output\") {\n          const output = (() => {\n            if (typeof toolMessage.content === \"string\") {\n              return {\n                type: \"input_image\" as const,\n                image_url: toolMessage.content,\n              };\n            }\n\n            if (Array.isArray(toolMessage.content)) {\n              /**\n               * Check for input_image type first (computer-use-preview format)\n               */\n              const inputImage = toolMessage.content.find(\n                (i) => i.type === \"input_image\"\n              ) as { type: \"input_image\"; image_url: string } | undefined;\n\n              if (inputImage) return inputImage;\n\n              /**\n               * Check for computer_screenshot type (legacy format)\n               */\n              const oaiScreenshot = toolMessage.content.find(\n                (i) => i.type === \"computer_screenshot\"\n              ) as\n                | { type: \"computer_screenshot\"; image_url: string }\n                | undefined;\n\n              if (oaiScreenshot) return oaiScreenshot;\n\n              /**\n               * Convert image_url content block to input_image format\n               */\n              const lcImage = toolMessage.content.find(\n                (i) => i.type === \"image_url\"\n              ) as MessageContentImageUrl;\n\n              if (lcImage) {\n                return {\n                  type: \"input_image\" as const,\n                  image_url:\n                    typeof lcImage.image_url === \"string\"\n                      ? lcImage.image_url\n                      : lcImage.image_url.url,\n                };\n              }\n            }\n\n            throw new Error(\"Invalid computer call output\");\n          })();\n\n          /**\n           * Cast needed because OpenAI SDK types don't yet include input_image\n           * for computer-use-preview model output format\n           */\n          return {\n            type: \"computer_call_output\",\n            output,\n            call_id: toolMessage.tool_call_id,\n          } as ResponsesInputItem;\n        }\n\n        // Handle custom tool output\n        if (toolMessage.additional_kwargs?.customTool) {\n          return {\n            type: \"custom_tool_call_output\",\n            call_id: toolMessage.tool_call_id,\n            output: toolMessage.content as string,\n          };\n        }\n\n        // Check if content contains provider-native OpenAI content blocks\n        // that should be passed through without stringification\n        const isProviderNativeContent =\n          Array.isArray(toolMessage.content) &&\n          toolMessage.content.every(\n            (item) =>\n              typeof item === \"object\" &&\n              item !== null &&\n              \"type\" in item &&\n              (item.type === \"input_file\" ||\n                item.type === \"input_image\" ||\n                item.type === \"input_text\")\n          );\n\n        return {\n          type: \"function_call_output\",\n          call_id: toolMessage.tool_call_id,\n          id: toolMessage.id?.startsWith(\"fc_\") ? toolMessage.id : undefined,\n          output: isProviderNativeContent\n            ? (toolMessage.content as OpenAIClient.Responses.ResponseFunctionCallOutputItemList)\n            : typeof toolMessage.content !== \"string\"\n              ? JSON.stringify(toolMessage.content)\n              : toolMessage.content,\n        };\n      }\n\n      if (role === \"assistant\") {\n        // if we have the original response items, just reuse them\n        if (\n          !zdrEnabled &&\n          responseMetadata?.output != null &&\n          Array.isArray(responseMetadata?.output) &&\n          responseMetadata?.output.length > 0 &&\n          responseMetadata?.output.every((item) => \"type\" in item)\n        ) {\n          return responseMetadata?.output;\n        }\n\n        // otherwise, try to reconstruct the response from what we have\n\n        const input: ResponsesInputItem[] = [];\n\n        // reasoning items\n        const reasoning = additional_kwargs?.reasoning;\n        const hasEncryptedContent = !!reasoning?.encrypted_content;\n        /**\n         * With ZDR enabled, OpenAI does not retain reasoning items, so we only send\n         * them when encrypted content is available (via include: [\"reasoning.encrypted_content\"]).\n         * With ZDR disabled, we include reasoning item ids so OpenAI can reference them, as it's storing them.\n         */\n        if (reasoning && (!zdrEnabled || hasEncryptedContent)) {\n          const reasoningItem =\n            convertReasoningSummaryToResponsesReasoningItem(reasoning);\n          input.push(reasoningItem);\n        }\n\n        // ai content\n        let { content } = lcMsg as { content: ContentBlock[] };\n        if (additional_kwargs?.refusal) {\n          if (typeof content === \"string\") {\n            content = [{ type: \"output_text\", text: content, annotations: [] }];\n          }\n          content = [\n            ...(content as ContentBlock[]),\n            { type: \"refusal\", refusal: additional_kwargs.refusal },\n          ];\n        }\n\n        if (typeof content === \"string\" || content.length > 0) {\n          input.push({\n            type: \"message\",\n            role: \"assistant\",\n            ...(lcMsg.id && !zdrEnabled && lcMsg.id.startsWith(\"msg_\")\n              ? { id: lcMsg.id }\n              : {}),\n            content: iife(() => {\n              if (typeof content === \"string\") {\n                return content;\n              }\n              return content.flatMap((item) => {\n                if (item.type === \"text\") {\n                  return {\n                    type: \"output_text\",\n                    text: item.text,\n                    annotations: (item.annotations ?? []).map(\n                      convertLangChainAnnotationToOpenAI\n                    ),\n                  };\n                }\n\n                if (item.type === \"output_text\" || item.type === \"refusal\") {\n                  return item;\n                }\n\n                return [];\n              });\n            }) as ResponseInputMessageContentList,\n          });\n        }\n\n        const functionCallIds = additional_kwargs?.[_FUNCTION_CALL_IDS_MAP_KEY];\n\n        if (AIMessage.isInstance(lcMsg) && !!lcMsg.tool_calls?.length) {\n          input.push(\n            ...lcMsg.tool_calls.map((toolCall): ResponsesInputItem => {\n              if (isCustomToolCall(toolCall)) {\n                return {\n                  type: \"custom_tool_call\",\n                  id: toolCall.call_id,\n                  call_id: toolCall.id ?? \"\",\n                  input: toolCall.args.input,\n                  name: toolCall.name,\n                };\n              }\n              if (isComputerToolCall(toolCall)) {\n                return {\n                  type: \"computer_call\",\n                  id: toolCall.call_id,\n                  call_id: toolCall.id ?? \"\",\n                  action: toolCall.args.action,\n                } as ResponsesInputItem;\n              }\n              return {\n                type: \"function_call\",\n                name: toolCall.name,\n                arguments: JSON.stringify(toolCall.args),\n                call_id: toolCall.id!,\n                ...(!zdrEnabled ? { id: functionCallIds?.[toolCall.id!] } : {}),\n              };\n            })\n          );\n        } else if (additional_kwargs?.tool_calls) {\n          input.push(\n            ...additional_kwargs.tool_calls.map(\n              (toolCall): ResponsesInputItem => ({\n                type: \"function_call\",\n                name: toolCall.function.name,\n                call_id: toolCall.id,\n                arguments: toolCall.function.arguments,\n                ...(!zdrEnabled ? { id: functionCallIds?.[toolCall.id] } : {}),\n              })\n            )\n          );\n        }\n\n        const toolOutputs = (\n          responseMetadata?.output as Array<ResponsesInputItem>\n        )?.length\n          ? responseMetadata?.output\n          : additional_kwargs.tool_outputs;\n\n        const fallthroughCallTypes: ResponsesInputItem[\"type\"][] = [\n          \"computer_call\",\n          \"mcp_call\",\n          \"code_interpreter_call\",\n          \"image_generation_call\",\n          \"shell_call\",\n          \"local_shell_call\",\n        ];\n\n        if (toolOutputs != null) {\n          const castToolOutputs = toolOutputs as Array<ResponsesInputItem>;\n          const fallthroughCalls = castToolOutputs?.filter((item) =>\n            fallthroughCallTypes.includes(item.type)\n          );\n          if (fallthroughCalls.length > 0) input.push(...fallthroughCalls);\n        }\n\n        return input;\n      }\n\n      if (role === \"user\" || role === \"system\" || role === \"developer\") {\n        if (typeof lcMsg.content === \"string\") {\n          return { type: \"message\", role, content: lcMsg.content };\n        }\n\n        const messages: ResponsesInputItem[] = [];\n        const content = (lcMsg.content as ContentBlock[]).flatMap((item) => {\n          if (item.type === \"mcp_approval_response\") {\n            messages.push({\n              type: \"mcp_approval_response\",\n              approval_request_id: item.approval_request_id as string,\n              approve: item.approve as boolean,\n            });\n          }\n          if (isDataContentBlock(item)) {\n            return convertToProviderContentBlock(\n              item,\n              completionsApiContentBlockConverter\n            );\n          }\n          if (item.type === \"text\") {\n            return {\n              type: \"input_text\",\n              text: item.text,\n            };\n          }\n          if (item.type === \"image_url\") {\n            const imageUrl = iife(() => {\n              if (typeof item.image_url === \"string\") {\n                return item.image_url;\n              } else if (\n                typeof item.image_url === \"object\" &&\n                item.image_url !== null &&\n                \"url\" in item.image_url\n              ) {\n                return item.image_url.url;\n              }\n              return undefined;\n            });\n            const detail = iife(() => {\n              if (typeof item.image_url === \"string\") {\n                return \"auto\";\n              } else if (\n                typeof item.image_url === \"object\" &&\n                item.image_url !== null &&\n                \"detail\" in item.image_url\n              ) {\n                return item.image_url.detail;\n              }\n              return undefined;\n            });\n            return {\n              type: \"input_image\",\n              image_url: imageUrl,\n              detail,\n            };\n          }\n          if (\n            item.type === \"input_text\" ||\n            item.type === \"input_image\" ||\n            item.type === \"input_file\"\n          ) {\n            return item;\n          }\n          return [];\n        });\n\n        if (content.length > 0) {\n          messages.push({\n            type: \"message\",\n            role,\n            content: content as ResponseInputMessageContentList,\n          });\n        }\n        return messages;\n      }\n\n      console.warn(\n        `Unsupported role found when converting to OpenAI Responses API: ${role}`\n      );\n      return [];\n    }\n  );\n};\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { AIMessage, type BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatResult } from \"@langchain/core/outputs\";\nimport { isOpenAITool as isOpenAIFunctionTool } from \"@langchain/core/language_models/base\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\nimport {\n  ChatOpenAIToolType,\n  convertCompletionsCustomTool,\n  formatToOpenAIToolChoice,\n  isBuiltInTool,\n  isBuiltInToolChoice,\n  isCustomTool,\n  isOpenAICustomTool,\n  ResponsesTool,\n} from \"../utils/tools.js\";\nimport {\n  BaseChatOpenAI,\n  BaseChatOpenAICallOptions,\n  BaseChatOpenAIFields,\n  getChatOpenAIModelParams,\n} from \"./base.js\";\nimport {\n  convertMessagesToResponsesInput,\n  convertResponsesDeltaToChatGenerationChunk,\n  convertResponsesMessageToAIMessage,\n} from \"../converters/responses.js\";\nimport { OpenAIVerbosityParam } from \"../types.js\";\n\nexport interface ChatOpenAIResponsesCallOptions extends BaseChatOpenAICallOptions {\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data.\n   */\n  text?: OpenAIClient.Responses.ResponseCreateParams[\"text\"];\n\n  /**\n   * The truncation strategy to use for the model response.\n   */\n  truncation?: OpenAIClient.Responses.ResponseCreateParams[\"truncation\"];\n\n  /**\n   * Specify additional output data to include in the model response.\n   */\n  include?: OpenAIClient.Responses.ResponseCreateParams[\"include\"];\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create multi-turn\n   * conversations.\n   */\n  previous_response_id?: OpenAIClient.Responses.ResponseCreateParams[\"previous_response_id\"];\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n}\n\nexport type ChatResponsesInvocationParams = Omit<\n  OpenAIClient.Responses.ResponseCreateParams,\n  \"input\"\n>;\n\n/**\n * OpenAI Responses API implementation.\n *\n * Will be exported in a later version of @langchain/openai.\n *\n * @internal\n */\nexport class ChatOpenAIResponses<\n  CallOptions extends ChatOpenAIResponsesCallOptions =\n    ChatOpenAIResponsesCallOptions,\n> extends BaseChatOpenAI<CallOptions> {\n  constructor(model: string, fields?: Omit<BaseChatOpenAIFields, \"model\">);\n  constructor(fields?: BaseChatOpenAIFields);\n  constructor(\n    modelOrFields?: string | BaseChatOpenAIFields,\n    fieldsArg?: Omit<BaseChatOpenAIFields, \"model\">\n  ) {\n    super(getChatOpenAIModelParams(modelOrFields, fieldsArg));\n  }\n\n  override invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): ChatResponsesInvocationParams {\n    let strict: boolean | undefined;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    }\n    if (strict === undefined && this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n\n    const params: ChatResponsesInvocationParams = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      user: this.user,\n      service_tier: this.service_tier,\n\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      previous_response_id: options?.previous_response_id,\n      truncation: options?.truncation,\n      include: options?.include,\n      tools: options?.tools?.length\n        ? this._reduceChatOpenAITools(options.tools, {\n            stream: this.streaming,\n            strict,\n          })\n        : undefined,\n      tool_choice: isBuiltInToolChoice(options?.tool_choice)\n        ? options?.tool_choice\n        : (() => {\n            const formatted = formatToOpenAIToolChoice(options?.tool_choice);\n            if (typeof formatted === \"object\" && \"type\" in formatted) {\n              if (formatted.type === \"function\") {\n                return { type: \"function\", name: formatted.function.name };\n              } else if (formatted.type === \"allowed_tools\") {\n                return {\n                  type: \"allowed_tools\",\n                  mode: formatted.allowed_tools.mode,\n                  tools: formatted.allowed_tools.tools,\n                };\n              } else if (formatted.type === \"custom\") {\n                return {\n                  type: \"custom\",\n                  name: formatted.custom.name,\n                };\n              }\n            }\n            return undefined;\n          })(),\n      text: (() => {\n        if (options?.text) return options.text;\n        const format = this._getResponseFormat(options?.response_format);\n        if (format?.type === \"json_schema\") {\n          if (format.json_schema.schema != null) {\n            return {\n              format: {\n                type: \"json_schema\",\n                schema: format.json_schema.schema,\n                description: format.json_schema.description,\n                name: format.json_schema.name,\n                strict: format.json_schema.strict,\n              },\n              verbosity: options?.verbosity,\n            };\n          }\n          return undefined;\n        }\n        return { format, verbosity: options?.verbosity };\n      })(),\n      parallel_tool_calls: options?.parallel_tool_calls,\n      max_output_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n      prompt_cache_key: options?.promptCacheKey ?? this.promptCacheKey,\n      prompt_cache_retention:\n        options?.promptCacheRetention ?? this.promptCacheRetention,\n      ...(this.zdrEnabled ? { store: false } : {}),\n      ...this.modelKwargs,\n    };\n\n    const reasoning = this._getReasoningParams(options);\n\n    if (reasoning !== undefined) {\n      params.reasoning = reasoning;\n    }\n\n    return params;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    options.signal?.throwIfAborted();\n    const invocationParams = this.invocationParams(options);\n    if (invocationParams.stream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      let finalChunk: ChatGenerationChunk | undefined;\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata,\n        };\n        finalChunk = finalChunk?.concat(chunk) ?? chunk;\n      }\n\n      return {\n        generations: finalChunk ? [finalChunk] : [],\n        llmOutput: {\n          estimatedTokenUsage: (finalChunk?.message as AIMessage | undefined)\n            ?.usage_metadata,\n        },\n      };\n    } else {\n      const data = await this.completionWithRetry(\n        {\n          input: convertMessagesToResponsesInput({\n            messages,\n            zdrEnabled: this.zdrEnabled ?? false,\n            model: this.model,\n          }),\n          ...invocationParams,\n          stream: false,\n        },\n        { signal: options?.signal, ...options?.options }\n      );\n\n      return {\n        generations: [\n          {\n            text: data.output_text,\n            message: convertResponsesMessageToAIMessage(data),\n          },\n        ],\n        llmOutput: {\n          id: data.id,\n          estimatedTokenUsage: data.usage\n            ? {\n                promptTokens: data.usage.input_tokens,\n                completionTokens: data.usage.output_tokens,\n                totalTokens: data.usage.total_tokens,\n              }\n            : undefined,\n        },\n      };\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const streamIterable = await this.completionWithRetry(\n      {\n        ...this.invocationParams(options),\n        input: convertMessagesToResponsesInput({\n          messages,\n          zdrEnabled: this.zdrEnabled ?? false,\n          model: this.model,\n        }),\n        stream: true,\n      },\n      options\n    );\n\n    for await (const data of streamIterable) {\n      if (options.signal?.aborted) {\n        return;\n      }\n      const chunk = convertResponsesDeltaToChatGenerationChunk(data);\n      if (chunk == null) continue;\n      yield chunk;\n      await runManager?.handleLLMNewToken(\n        chunk.text || \"\",\n        {\n          prompt: options.promptIndex ?? 0,\n          completion: 0,\n        },\n        undefined,\n        undefined,\n        undefined,\n        { chunk }\n      );\n    }\n  }\n\n  /**\n   * Calls the Responses API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsNonStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<OpenAIClient.Responses.Response>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParams,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<\n    | AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>\n    | OpenAIClient.Responses.Response\n  > {\n    return this.caller.call(async () => {\n      const clientOptions = this._getClientOptions(requestOptions);\n      try {\n        // use parse if dealing with json_schema\n        if (request.text?.format?.type === \"json_schema\" && !request.stream) {\n          return await this.client.responses.parse(request, clientOptions);\n        }\n        return await this.client.responses.create(request, clientOptions);\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /** @internal */\n  protected _reduceChatOpenAITools(\n    tools: ChatOpenAIToolType[],\n    fields: { stream?: boolean; strict?: boolean }\n  ): ResponsesTool[] {\n    const reducedTools: ResponsesTool[] = [];\n    for (const tool of tools) {\n      if (isBuiltInTool(tool)) {\n        if (tool.type === \"image_generation\" && fields?.stream) {\n          // OpenAI sends a 400 error if partial_images is not set and we want to stream.\n          // We also set it to 1 since we don't support partial images yet.\n          tool.partial_images = 1;\n        }\n        reducedTools.push(tool);\n      } else if (isCustomTool(tool)) {\n        const customToolData = tool.metadata.customTool;\n        reducedTools.push({\n          type: \"custom\",\n          name: customToolData.name,\n          description: customToolData.description,\n          format: customToolData.format,\n        } as ResponsesTool);\n      } else if (isOpenAIFunctionTool(tool)) {\n        reducedTools.push({\n          type: \"function\",\n          name: tool.function.name,\n          parameters: tool.function.parameters,\n          description: tool.function.description,\n          strict: fields?.strict ?? null,\n        });\n      } else if (isOpenAICustomTool(tool)) {\n        reducedTools.push(convertCompletionsCustomTool(tool));\n      }\n    }\n    return reducedTools;\n  }\n}\n", "import { LangSmithParams } from \"@langchain/core/language_models/chat_models\";\nimport { Serialized } from \"@langchain/core/load/serializable\";\nimport {\n  ChatOpenAIResponsesCallOptions,\n  ChatOpenAIResponses,\n} from \"../../chat_models/responses.js\";\nimport { AzureOpenAIChatInput, OpenAICoreRequestOptions } from \"../../types.js\";\nimport {\n  _constructAzureFields,\n  _getAzureClientOptions,\n  _serializeAzureChat,\n  AZURE_ALIASES,\n  AZURE_SECRETS,\n  AZURE_SERIALIZABLE_KEYS,\n  AzureChatOpenAIFields,\n  getAzureChatOpenAIParams,\n} from \"./common.js\";\n\nexport class AzureChatOpenAIResponses<\n  CallOptions extends ChatOpenAIResponsesCallOptions =\n    ChatOpenAIResponsesCallOptions,\n>\n  extends ChatOpenAIResponses<CallOptions>\n  implements Partial<AzureOpenAIChatInput>\n{\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  _llmType(): string {\n    return \"azure_openai\";\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      ...AZURE_ALIASES,\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      ...AZURE_SECRETS,\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, ...AZURE_SERIALIZABLE_KEYS];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = super.getLsParams(options);\n    params.ls_provider = \"azure\";\n    return params;\n  }\n\n  constructor(\n    deploymentName: string,\n    fields?: Omit<\n      AzureChatOpenAIFields,\n      \"deploymentName\" | \"azureOpenAIApiDeploymentName\" | \"model\"\n    >\n  );\n  constructor(fields?: AzureChatOpenAIFields);\n  constructor(\n    deploymentOrFields?: string | AzureChatOpenAIFields,\n    fieldsArg?: Omit<\n      AzureChatOpenAIFields,\n      \"deploymentName\" | \"azureOpenAIApiDeploymentName\" | \"model\"\n    >\n  ) {\n    const fields = getAzureChatOpenAIParams(deploymentOrFields, fieldsArg);\n    super(fields);\n    _constructAzureFields.call(this, fields);\n  }\n\n  override _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    return _getAzureClientOptions.call(this, options);\n  }\n\n  override toJSON(): Serialized {\n    return _serializeAzureChat.call(this, super.toJSON());\n  }\n}\n", "import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatResult } from \"@langchain/core/outputs\";\nimport { type BaseLanguageModelInput } from \"@langchain/core/language_models/base\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { type OpenAICallOptions, type OpenAIChatInput } from \"../types.js\";\nimport {\n  _convertToOpenAITool,\n  isBuiltInTool,\n  isCustomTool,\n  isOpenAICustomTool,\n} from \"../utils/tools.js\";\nimport { _modelPrefersResponsesAPI } from \"../utils/misc.js\";\nimport { _convertOpenAIResponsesUsageToLangChainUsage } from \"../utils/output.js\";\nimport {\n  ChatOpenAICompletions,\n  ChatOpenAICompletionsCallOptions,\n} from \"./completions.js\";\nimport {\n  ChatOpenAIResponses,\n  ChatOpenAIResponsesCallOptions,\n} from \"./responses.js\";\nimport {\n  BaseChatOpenAI,\n  BaseChatOpenAIFields,\n  getChatOpenAIModelParams,\n} from \"./base.js\";\n\nexport type { OpenAICallOptions, OpenAIChatInput };\n\nexport type ChatOpenAICallOptions = ChatOpenAICompletionsCallOptions &\n  ChatOpenAIResponsesCallOptions;\n\nexport interface ChatOpenAIFields extends BaseChatOpenAIFields {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi?: boolean;\n  /**\n   * The completions chat instance\n   * @internal\n   */\n  completions?: ChatOpenAICompletions;\n  /**\n   * The responses chat instance\n   * @internal\n   */\n  responses?: ChatOpenAIResponses;\n}\n\n/**\n * OpenAI chat model integration.\n *\n * To use with Azure, import the `AzureChatOpenAI` class.\n *\n * Setup:\n * Install `@langchain/openai` and set an environment variable named `OPENAI_API_KEY`.\n *\n * ```bash\n * npm install @langchain/openai\n * export OPENAI_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from '@langchain/openai';\n *\n * const llm = new ChatOpenAI({\n *   model: \"gpt-4o-mini\",\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // configuration: {\n *   //   baseURL: \"...\",\n *   // }\n *   // organization: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools(\n *   [GetWeather, GetPopulation],\n *   {\n *     // strict: true  // enforce tool args schema is respected\n *   }\n * );\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, {\n *   name: \"Joke\",\n *   strict: true, // Optionally enable OpenAI structured outputs\n * });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.withConfig({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ model: \"gpt-4o-mini\", logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Schema Structured Output</strong></summary>\n *\n * ```typescript\n * const llmForJsonSchema = new ChatOpenAI({\n *   model: \"gpt-4o-2024-08-06\",\n * }).withStructuredOutput(\n *   z.object({\n *     command: z.string().describe(\"The command to execute\"),\n *     expectedOutput: z.string().describe(\"The expected output of the command\"),\n *     options: z\n *       .array(z.string())\n *       .describe(\"The options you can pass to the command\"),\n *   }),\n *   {\n *     method: \"jsonSchema\",\n *     strict: true, // Optional when using the `jsonSchema` method\n *   }\n * );\n *\n * const jsonSchemaRes = await llmForJsonSchema.invoke(\n *   \"What is the command to list files in a directory?\"\n * );\n * console.log(jsonSchemaRes);\n * ```\n *\n * ```txt\n * {\n *   command: 'ls',\n *   expectedOutput: 'A list of files and subdirectories within the specified directory.',\n *   options: [\n *     '-a: include directory entries whose names begin with a dot (.).',\n *     '-l: use a long listing format.',\n *     '-h: with -l, print sizes in human readable format (e.g., 1K, 234M, 2G).',\n *     '-t: sort by time, newest first.',\n *     '-r: reverse order while sorting.',\n *     '-S: sort by file size, largest first.',\n *     '-R: list subdirectories recursively.'\n *   ]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castMessageContent = audioOutputResult.content[0] as Record<string, any>;\n *\n * console.log({\n *   ...castMessageContent,\n *   data: castMessageContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\n *\n * console.log({\n *   ...castAudioContent,\n *   data: castAudioContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatOpenAI<\n  CallOptions extends ChatOpenAICallOptions = ChatOpenAICallOptions,\n> extends BaseChatOpenAI<CallOptions> {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi = false;\n\n  protected responses: ChatOpenAIResponses;\n\n  protected completions: ChatOpenAICompletions;\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, \"useResponsesApi\"];\n  }\n\n  get callKeys(): string[] {\n    return [...super.callKeys, \"useResponsesApi\"];\n  }\n\n  protected fields?: ChatOpenAIFields;\n\n  constructor(model: string, fields?: Omit<ChatOpenAIFields, \"model\">);\n  constructor(fields?: ChatOpenAIFields);\n  constructor(\n    modelOrFields?: string | ChatOpenAIFields,\n    fieldsArg?: Omit<ChatOpenAIFields, \"model\">\n  ) {\n    const fields = getChatOpenAIModelParams(modelOrFields, fieldsArg);\n    super(fields);\n    this.fields = fields;\n    this.useResponsesApi = fields?.useResponsesApi ?? false;\n    this.responses = fields?.responses ?? new ChatOpenAIResponses(fields);\n    this.completions = fields?.completions ?? new ChatOpenAICompletions(fields);\n  }\n\n  protected _useResponsesApi(options: this[\"ParsedCallOptions\"] | undefined) {\n    const usesBuiltInTools = options?.tools?.some(isBuiltInTool);\n    const hasResponsesOnlyKwargs =\n      options?.previous_response_id != null ||\n      options?.text != null ||\n      options?.truncation != null ||\n      options?.include != null ||\n      options?.reasoning?.summary != null ||\n      this.reasoning?.summary != null;\n    const hasCustomTools =\n      options?.tools?.some(isOpenAICustomTool) ||\n      options?.tools?.some(isCustomTool);\n\n    return (\n      this.useResponsesApi ||\n      usesBuiltInTools ||\n      hasResponsesOnlyKwargs ||\n      hasCustomTools ||\n      _modelPrefersResponsesAPI(this.model)\n    );\n  }\n\n  override getLsParams(options: this[\"ParsedCallOptions\"]) {\n    const optionsWithDefaults = this._combineCallOptions(options);\n    if (this._useResponsesApi(options)) {\n      return this.responses.getLsParams(optionsWithDefaults);\n    }\n    return this.completions.getLsParams(optionsWithDefaults);\n  }\n\n  override invocationParams(options?: this[\"ParsedCallOptions\"]) {\n    const optionsWithDefaults = this._combineCallOptions(options);\n    if (this._useResponsesApi(options)) {\n      return this.responses.invocationParams(optionsWithDefaults);\n    }\n    return this.completions.invocationParams(optionsWithDefaults);\n  }\n\n  /** @ignore */\n  override async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (this._useResponsesApi(options)) {\n      return this.responses._generate(messages, options, runManager);\n    }\n    return this.completions._generate(messages, options, runManager);\n  }\n\n  override async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    if (this._useResponsesApi(options)) {\n      yield* this.responses._streamResponseChunks(\n        messages,\n        this._combineCallOptions(options),\n        runManager\n      );\n      return;\n    }\n    yield* this.completions._streamResponseChunks(\n      messages,\n      this._combineCallOptions(options),\n      runManager\n    );\n  }\n\n  override withConfig(\n    config: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    const newModel = new ChatOpenAI<CallOptions>(this.fields);\n    newModel.defaultOptions = { ...this.defaultOptions, ...config };\n    return newModel;\n  }\n}\n", "import { StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport type { Serialized } from \"@langchain/core/load/serializable\";\nimport { LangSmithParams } from \"@langchain/core/language_models/chat_models\";\nimport { ChatOpenAI, ChatOpenAICallOptions } from \"../../chat_models/index.js\";\nimport { AzureOpenAIChatInput } from \"../../types.js\";\nimport {\n  _constructAzureFields,\n  _serializeAzureChat,\n  AZURE_ALIASES,\n  AZURE_SECRETS,\n  AZURE_SERIALIZABLE_KEYS,\n  AzureChatOpenAIFields,\n  getAzureChatOpenAIParams,\n} from \"./common.js\";\nimport { AzureChatOpenAICompletions } from \"./completions.js\";\nimport { AzureChatOpenAIResponses } from \"./responses.js\";\n\n/**\n * Azure OpenAI chat model integration.\n *\n * Setup:\n * Install `@langchain/openai` and set the following environment variables:\n *\n * ```bash\n * npm install @langchain/openai\n * export AZURE_OPENAI_API_KEY=\"your-api-key\"\n * export AZURE_OPENAI_API_DEPLOYMENT_NAME=\"your-deployment-name\"\n * export AZURE_OPENAI_API_VERSION=\"your-version\"\n * export AZURE_OPENAI_BASE_PATH=\"your-base-path\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { AzureChatOpenAI } from '@langchain/openai';\n *\n * const llm = new AzureChatOpenAI({\n *   azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n *   azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n *   azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n *   azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // baseUrl: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, { name: \"Joke\" });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.withConfig({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ model: \"gpt-4o-mini\", logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n */\nexport class AzureChatOpenAI<\n  CallOptions extends ChatOpenAICallOptions = ChatOpenAICallOptions,\n>\n  extends ChatOpenAI<CallOptions>\n  implements Partial<AzureOpenAIChatInput>\n{\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  _llmType(): string {\n    return \"azure_openai\";\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      ...AZURE_ALIASES,\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      ...AZURE_SECRETS,\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, ...AZURE_SERIALIZABLE_KEYS];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = super.getLsParams(options);\n    params.ls_provider = \"azure\";\n    return params;\n  }\n\n  constructor(\n    deploymentName: string,\n    fields?: Omit<\n      AzureChatOpenAIFields,\n      \"deploymentName\" | \"azureOpenAIApiDeploymentName\" | \"model\"\n    >\n  );\n  constructor(fields?: AzureChatOpenAIFields);\n  constructor(\n    deploymentOrFields?: string | AzureChatOpenAIFields,\n    fieldsArg?: Omit<\n      AzureChatOpenAIFields,\n      \"deploymentName\" | \"azureOpenAIApiDeploymentName\" | \"model\"\n    >\n  ) {\n    const fields = getAzureChatOpenAIParams(deploymentOrFields, fieldsArg);\n    super({\n      ...fields,\n      completions: new AzureChatOpenAICompletions(fields),\n      responses: new AzureChatOpenAIResponses(fields),\n    });\n    _constructAzureFields.call(this, fields);\n  }\n\n  /** @internal */\n  override _getStructuredOutputMethod(\n    config: StructuredOutputMethodOptions<boolean>\n  ) {\n    const ensuredConfig = { ...config };\n    // Not all Azure gpt-4o deployments models support jsonSchema yet\n    if (this.model.startsWith(\"gpt-4o\")) {\n      if (ensuredConfig?.method === undefined) {\n        return \"functionCalling\";\n      }\n    }\n    return super._getStructuredOutputMethod(ensuredConfig);\n  }\n\n  override toJSON(): Serialized {\n    return _serializeAzureChat.call(this, super.toJSON());\n  }\n}\n", "import type { BasePromptValueInterface } from \"../prompt_values.js\";\nimport {\n  type LLMResult,\n  RUN_KEY,\n  type Generation,\n  GenerationChunk,\n} from \"../outputs.js\";\nimport {\n  type BaseCallbackConfig,\n  CallbackManager,\n  type CallbackManagerForLLMRun,\n  type Callbacks,\n} from \"../callbacks/manager.js\";\nimport {\n  BaseLanguageModel,\n  type BaseLanguageModelCallOptions,\n  type BaseLanguageModelInput,\n  type BaseLanguageModelParams,\n} from \"./base.js\";\nimport type { RunnableConfig } from \"../runnables/config.js\";\nimport type { BaseCache } from \"../caches/index.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\n\nexport type SerializedLLM = {\n  _model: string;\n  _type: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport interface BaseLLMParams extends BaseLanguageModelParams {}\n\nexport interface BaseLLMCallOptions extends BaseLanguageModelCallOptions {}\n\n/**\n * LLM Wrapper. Takes in a prompt (or prompts) and returns a string.\n */\nexport abstract class BaseLLM<\n  CallOptions extends BaseLLMCallOptions = BaseLLMCallOptions,\n> extends BaseLanguageModel<string, CallOptions> {\n  // Backwards compatibility since fields have been moved to RunnableConfig\n  declare ParsedCallOptions: Omit<\n    CallOptions,\n    Exclude<keyof RunnableConfig, \"signal\" | \"timeout\" | \"maxConcurrency\">\n  >;\n\n  // Only ever instantiated in main LangChain\n  lc_namespace = [\"langchain\", \"llms\", this._llmType()];\n\n  /**\n   * This method takes an input and options, and returns a string. It\n   * converts the input to a prompt value and generates a result based on\n   * the prompt.\n   * @param input Input for the LLM.\n   * @param options Options for the LLM call.\n   * @returns A string result based on the prompt.\n   */\n  async invoke(\n    input: BaseLanguageModelInput,\n    options?: Partial<CallOptions>\n  ): Promise<string> {\n    const promptValue = BaseLLM._convertInputToPromptValue(input);\n    const result = await this.generatePrompt(\n      [promptValue],\n      options,\n      options?.callbacks\n    );\n    return result.generations[0][0].text;\n  }\n\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(\n    _input: string,\n    _options: this[\"ParsedCallOptions\"],\n    _runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    throw new Error(\"Not implemented.\");\n  }\n\n  protected _separateRunnableConfigFromCallOptionsCompat(\n    options?: Partial<CallOptions>\n  ): [RunnableConfig, this[\"ParsedCallOptions\"]] {\n    // For backwards compat, keep `signal` in both runnableConfig and callOptions\n    const [runnableConfig, callOptions] =\n      super._separateRunnableConfigFromCallOptions(options);\n    (callOptions as this[\"ParsedCallOptions\"]).signal = runnableConfig.signal;\n    return [runnableConfig, callOptions as this[\"ParsedCallOptions\"]];\n  }\n\n  async *_streamIterator(\n    input: BaseLanguageModelInput,\n    options?: Partial<CallOptions>\n  ): AsyncGenerator<string> {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (\n      this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks\n    ) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseLLM._convertInputToPromptValue(input);\n      const [runnableConfig, callOptions] =\n        this._separateRunnableConfigFromCallOptionsCompat(options);\n      const callbackManager_ = await CallbackManager.configure(\n        runnableConfig.callbacks,\n        this.callbacks,\n        runnableConfig.tags,\n        this.tags,\n        runnableConfig.metadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions),\n        batch_size: 1,\n      };\n      const runManagers = await callbackManager_?.handleLLMStart(\n        this.toJSON(),\n        [prompt.toString()],\n        runnableConfig.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        runnableConfig.runName\n      );\n      let generation = new GenerationChunk({\n        text: \"\",\n      });\n      try {\n        for await (const chunk of this._streamResponseChunks(\n          prompt.toString(),\n          callOptions,\n          runManagers?.[0]\n        )) {\n          if (!generation) {\n            generation = chunk;\n          } else {\n            generation = generation.concat(chunk);\n          }\n          if (typeof chunk.text === \"string\") {\n            yield chunk.text;\n          }\n        }\n      } catch (err) {\n        await Promise.all(\n          (runManagers ?? []).map((runManager) =>\n            runManager?.handleLLMError(err)\n          )\n        );\n        throw err;\n      }\n      await Promise.all(\n        (runManagers ?? []).map((runManager) =>\n          runManager?.handleLLMEnd({\n            generations: [[generation]],\n          })\n        )\n      );\n    }\n  }\n\n  /**\n   * This method takes prompt values, options, and callbacks, and generates\n   * a result based on the prompts.\n   * @param promptValues Prompt values for the LLM.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns An LLMResult based on the prompts.\n   */\n  async generatePrompt(\n    promptValues: BasePromptValueInterface[],\n    options?: string[] | Partial<CallOptions>,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    const prompts: string[] = promptValues.map((promptValue) =>\n      promptValue.toString()\n    );\n    return this.generate(prompts, options, callbacks);\n  }\n\n  /**\n   * Run the LLM on the given prompts and input.\n   */\n  abstract _generate(\n    prompts: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult>;\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options?: this[\"ParsedCallOptions\"]): any {\n    return {};\n  }\n\n  _flattenLLMResult(llmResult: LLMResult): LLMResult[] {\n    const llmResults: LLMResult[] = [];\n\n    for (let i = 0; i < llmResult.generations.length; i += 1) {\n      const genList = llmResult.generations[i];\n\n      if (i === 0) {\n        llmResults.push({\n          generations: [genList],\n          llmOutput: llmResult.llmOutput,\n        });\n      } else {\n        const llmOutput = llmResult.llmOutput\n          ? { ...llmResult.llmOutput, tokenUsage: {} }\n          : undefined;\n\n        llmResults.push({\n          generations: [genList],\n          llmOutput,\n        });\n      }\n    }\n\n    return llmResults;\n  }\n\n  /** @ignore */\n  async _generateUncached(\n    prompts: string[],\n    parsedOptions: this[\"ParsedCallOptions\"],\n    handledOptions: BaseCallbackConfig,\n    startedRunManagers?: CallbackManagerForLLMRun[]\n  ): Promise<LLMResult> {\n    let runManagers: CallbackManagerForLLMRun[] | undefined;\n    if (\n      startedRunManagers !== undefined &&\n      startedRunManagers.length === prompts.length\n    ) {\n      runManagers = startedRunManagers;\n    } else {\n      const callbackManager_ = await CallbackManager.configure(\n        handledOptions.callbacks,\n        this.callbacks,\n        handledOptions.tags,\n        this.tags,\n        handledOptions.metadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: parsedOptions,\n        invocation_params: this?.invocationParams(parsedOptions),\n        batch_size: prompts.length,\n      };\n      runManagers = await callbackManager_?.handleLLMStart(\n        this.toJSON(),\n        prompts,\n        handledOptions.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        handledOptions?.runName\n      );\n    }\n    // Even if stream is not explicitly called, check if model is implicitly\n    // called from streamEvents() or streamLog() to get all streamed events.\n    // Bail out if _streamResponseChunks not overridden\n    const hasStreamingHandler = !!runManagers?.[0].handlers.find(\n      callbackHandlerPrefersStreaming\n    );\n    let output: LLMResult;\n    if (\n      hasStreamingHandler &&\n      prompts.length === 1 &&\n      this._streamResponseChunks !== BaseLLM.prototype._streamResponseChunks\n    ) {\n      try {\n        const stream = await this._streamResponseChunks(\n          prompts[0],\n          parsedOptions,\n          runManagers?.[0]\n        );\n        let aggregated;\n        for await (const chunk of stream) {\n          if (aggregated === undefined) {\n            aggregated = chunk;\n          } else {\n            aggregated = concat(aggregated, chunk);\n          }\n        }\n        if (aggregated === undefined) {\n          throw new Error(\"Received empty response from chat model call.\");\n        }\n        output = { generations: [[aggregated]], llmOutput: {} };\n        await runManagers?.[0].handleLLMEnd(output);\n      } catch (e) {\n        await runManagers?.[0].handleLLMError(e);\n        throw e;\n      }\n    } else {\n      try {\n        output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n      } catch (err) {\n        await Promise.all(\n          (runManagers ?? []).map((runManager) =>\n            runManager?.handleLLMError(err)\n          )\n        );\n        throw err;\n      }\n\n      const flattenedOutputs: LLMResult[] = this._flattenLLMResult(output);\n      await Promise.all(\n        (runManagers ?? []).map((runManager, i) =>\n          runManager?.handleLLMEnd(flattenedOutputs[i])\n        )\n      );\n    }\n    const runIds = runManagers?.map((manager) => manager.runId) || undefined;\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runIds ? { runIds } : undefined,\n      configurable: true,\n    });\n    return output;\n  }\n\n  async _generateCached({\n    prompts,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions,\n    runId,\n  }: {\n    prompts: string[];\n    cache: BaseCache<Generation[]>;\n    llmStringKey: string;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    parsedOptions: any;\n    handledOptions: RunnableConfig;\n    runId?: string;\n  }): Promise<\n    LLMResult & {\n      missingPromptIndices: number[];\n      startedRunManagers?: CallbackManagerForLLMRun[];\n    }\n  > {\n    const callbackManager_ = await CallbackManager.configure(\n      handledOptions.callbacks,\n      this.callbacks,\n      handledOptions.tags,\n      this.tags,\n      handledOptions.metadata,\n      this.metadata,\n      { verbose: this.verbose }\n    );\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: prompts.length,\n    };\n    const runManagers = await callbackManager_?.handleLLMStart(\n      this.toJSON(),\n      prompts,\n      runId,\n      undefined,\n      extra,\n      undefined,\n      undefined,\n      handledOptions?.runName\n    );\n\n    // generate results\n    const missingPromptIndices: number[] = [];\n    const results = await Promise.allSettled(\n      prompts.map(async (prompt, index) => {\n        const result = await cache.lookup(prompt, llmStringKey);\n        if (result == null) {\n          missingPromptIndices.push(index);\n        }\n        return result;\n      })\n    );\n\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results\n      .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n      .filter(\n        ({ result }) =>\n          (result.status === \"fulfilled\" && result.value != null) ||\n          result.status === \"rejected\"\n      );\n\n    // Handle results and call run managers\n    const generations: Generation[][] = [];\n    await Promise.all(\n      cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n        if (promiseResult.status === \"fulfilled\") {\n          const result = promiseResult.value as Generation[];\n          generations[i] = result.map((result) => {\n            result.generationInfo = {\n              ...result.generationInfo,\n              tokenUsage: {},\n            };\n            return result;\n          });\n          if (result.length) {\n            await runManager?.handleLLMNewToken(result[0].text);\n          }\n          return runManager?.handleLLMEnd(\n            {\n              generations: [result],\n            },\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n        } else {\n          // status === \"rejected\"\n          await runManager?.handleLLMError(\n            promiseResult.reason,\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n          return Promise.reject(promiseResult.reason);\n        }\n      })\n    );\n\n    const output = {\n      generations,\n      missingPromptIndices,\n      startedRunManagers: runManagers,\n    };\n\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers\n        ? { runIds: runManagers?.map((manager) => manager.runId) }\n        : undefined,\n      configurable: true,\n    });\n\n    return output;\n  }\n\n  /**\n   * Run the LLM on the given prompts and input, handling caching.\n   */\n  async generate(\n    prompts: string[],\n    options?: string[] | Partial<CallOptions>,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    if (!Array.isArray(prompts)) {\n      throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n    }\n\n    let parsedOptions: Partial<CallOptions> | undefined;\n    if (Array.isArray(options)) {\n      parsedOptions = { stop: options } as Partial<CallOptions>;\n    } else {\n      parsedOptions = options;\n    }\n\n    const [runnableConfig, callOptions] =\n      this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n\n    if (!this.cache) {\n      return this._generateUncached(prompts, callOptions, runnableConfig);\n    }\n\n    const { cache } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(\n      callOptions as CallOptions\n    );\n    const { generations, missingPromptIndices, startedRunManagers } =\n      await this._generateCached({\n        prompts,\n        cache,\n        llmStringKey,\n        parsedOptions: callOptions,\n        handledOptions: runnableConfig,\n        runId: runnableConfig.runId,\n      });\n\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(\n        missingPromptIndices.map((i) => prompts[i]),\n        callOptions,\n        runnableConfig,\n        startedRunManagers !== undefined\n          ? missingPromptIndices.map((i) => startedRunManagers?.[i])\n          : undefined\n      );\n      await Promise.all(\n        results.generations.map(async (generation, index) => {\n          const promptIndex = missingPromptIndices[index];\n          generations[promptIndex] = generation;\n          return cache.update(prompts[promptIndex], llmStringKey, generation);\n        })\n      );\n      llmOutput = results.llmOutput ?? {};\n    }\n\n    return { generations, llmOutput } as LLMResult;\n  }\n\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams(): Record<string, any> {\n    return {};\n  }\n\n  /**\n   * Return the string type key uniquely identifying this class of LLM.\n   */\n  abstract _llmType(): string;\n\n  _modelType(): string {\n    return \"base_llm\" as const;\n  }\n}\n\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport abstract class LLM<\n  CallOptions extends BaseLLMCallOptions = BaseLLMCallOptions,\n> extends BaseLLM<CallOptions> {\n  /**\n   * Run the LLM on the given prompt and input.\n   */\n  abstract _call(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<string>;\n\n  async _generate(\n    prompts: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult> {\n    const generations: Generation[][] = await Promise.all(\n      prompts.map((prompt, promptIndex) =>\n        this._call(prompt, { ...options, promptIndex }, runManager).then(\n          (text) => [{ text }]\n        )\n      )\n    );\n    return { generations };\n  }\n}\n", "export const chunkArray = <T>(arr: T[], chunkSize: number) =>\n  arr.reduce((chunks, elem, index) => {\n    const chunkIndex = Math.floor(index / chunkSize);\n    const chunk = chunks[chunkIndex] || [];\n    chunks[chunkIndex] = chunk.concat([elem]);\n    return chunks;\n  }, [] as T[][]);\n", "import type { TiktokenModel } from \"js-tiktoken/lite\";\nimport { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { calculateMaxTokens } from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { GenerationChunk, type LLMResult } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  BaseLLM,\n  type BaseLLMParams,\n} from \"@langchain/core/language_models/llms\";\nimport { chunkArray } from \"@langchain/core/utils/chunk_array\";\nimport type {\n  OpenAIApiKey,\n  OpenAICallOptions,\n  OpenAICoreRequestOptions,\n  OpenAIInput,\n} from \"./types.js\";\nimport {\n  OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"./utils/azure.js\";\nimport { wrapOpenAIClientError } from \"./utils/client.js\";\n\nexport type { OpenAICallOptions, OpenAIInput };\n\n/**\n * Interface for tracking token usage in OpenAI calls.\n */\ninterface TokenUsage {\n  completionTokens?: number;\n  promptTokens?: number;\n  totalTokens?: number;\n}\n\n/**\n * Wrapper around OpenAI large language models.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure, import the `AzureOpenAI` class.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/completions/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n * @example\n * ```typescript\n * const model = new OpenAI({\n *   modelName: \"gpt-4\",\n *   temperature: 0.7,\n *   maxTokens: 1000,\n *   maxRetries: 5,\n * });\n *\n * const res = await model.invoke(\n *   \"Question: What would be a good company name for a company that makes colorful socks?\\nAnswer:\"\n * );\n * console.log({ res });\n * ```\n */\nexport class OpenAI<CallOptions extends OpenAICallOptions = OpenAICallOptions>\n  extends BaseLLM<CallOptions>\n  implements Partial<OpenAIInput>\n{\n  static lc_name() {\n    return \"OpenAI\";\n  }\n\n  get callKeys() {\n    return [...super.callKeys, \"options\"];\n  }\n\n  lc_serializable = true;\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      openAIApiKey: \"OPENAI_API_KEY\",\n      apiKey: \"OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\",\n    };\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      modelName: \"model\",\n      openAIApiKey: \"openai_api_key\",\n      apiKey: \"openai_api_key\",\n    };\n  }\n\n  temperature?: number;\n\n  maxTokens?: number;\n\n  topP?: number;\n\n  frequencyPenalty?: number;\n\n  presencePenalty?: number;\n\n  n = 1;\n\n  bestOf?: number;\n\n  logitBias?: Record<string, number>;\n\n  model = \"gpt-3.5-turbo-instruct\";\n\n  /** @deprecated Use \"model\" instead */\n  modelName: string;\n\n  modelKwargs?: OpenAIInput[\"modelKwargs\"];\n\n  batchSize = 20;\n\n  timeout?: number;\n\n  stop?: string[];\n\n  stopSequences?: string[];\n\n  user?: string;\n\n  streaming = false;\n\n  openAIApiKey?: OpenAIApiKey;\n\n  apiKey?: OpenAIApiKey;\n\n  organization?: string;\n\n  protected client: OpenAIClient;\n\n  protected clientConfig: ClientOptions;\n\n  constructor(\n    fields?: Partial<OpenAIInput> &\n      BaseLLMParams & {\n        configuration?: ClientOptions;\n      }\n  ) {\n    super(fields ?? {});\n\n    this.openAIApiKey =\n      fields?.apiKey ??\n      fields?.openAIApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.apiKey = this.openAIApiKey;\n\n    this.organization =\n      fields?.configuration?.organization ??\n      getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    if (\n      (this.model?.startsWith(\"gpt-3.5-turbo\") ||\n        this.model?.startsWith(\"gpt-4\") ||\n        this.model?.startsWith(\"o1\")) &&\n      !this.model?.includes(\"-instruct\")\n    ) {\n      throw new Error(\n        [\n          `Your chosen OpenAI model, \"${this.model}\", is a chat model and not a text-in/text-out LLM.`,\n          `Passing it into the \"OpenAI\" class is no longer supported.`,\n          `Please use the \"ChatOpenAI\" class instead.`,\n          \"\",\n          `See this page for more information:`,\n          \"|\",\n          `> https://js.langchain.com/docs/integrations/chat/openai`,\n        ].join(\"\\n\")\n      );\n    }\n    this.modelName = this.model;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.batchSize = fields?.batchSize ?? this.batchSize;\n    this.timeout = fields?.timeout;\n\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.n = fields?.n ?? this.n;\n    this.bestOf = fields?.bestOf ?? this.bestOf;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stopSequences ?? fields?.stop;\n    this.stopSequences = this.stop;\n    this.user = fields?.user;\n\n    this.streaming = fields?.streaming ?? false;\n\n    if (this.streaming && this.bestOf && this.bestOf > 1) {\n      throw new Error(\"Cannot stream results when bestOf > 1\");\n    }\n\n    this.clientConfig = {\n      apiKey: this.apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration,\n    };\n  }\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): Omit<OpenAIClient.CompletionCreateParams, \"prompt\"> {\n    return {\n      model: this.model,\n      temperature: this.temperature,\n      max_tokens: this.maxTokens,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      n: this.n,\n      best_of: this.bestOf,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stopSequences,\n      user: this.user,\n      stream: this.streaming,\n      ...this.modelKwargs,\n    };\n  }\n\n  /** @ignore */\n  _identifyingParams(): Omit<OpenAIClient.CompletionCreateParams, \"prompt\"> & {\n    model_name: string;\n  } & ClientOptions {\n    return {\n      model_name: this.model,\n      ...this.invocationParams(),\n      ...this.clientConfig,\n    };\n  }\n\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams(): Omit<OpenAIClient.CompletionCreateParams, \"prompt\"> & {\n    model_name: string;\n  } & ClientOptions {\n    return this._identifyingParams();\n  }\n\n  /**\n   * Call out to OpenAI's endpoint with k unique prompts\n   *\n   * @param [prompts] - The prompts to pass into the model.\n   * @param [options] - Optional list of stop words to use when generating.\n   * @param [runManager] - Optional callback manager to use when generating.\n   *\n   * @returns The full LLM output.\n   *\n   * @example\n   * ```ts\n   * import { OpenAI } from \"langchain/llms/openai\";\n   * const openai = new OpenAI();\n   * const response = await openai.generate([\"Tell me a joke.\"]);\n   * ```\n   */\n  async _generate(\n    prompts: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult> {\n    const subPrompts = chunkArray(prompts, this.batchSize);\n    const choices: OpenAIClient.CompletionChoice[] = [];\n    const tokenUsage: TokenUsage = {};\n\n    const params = this.invocationParams(options);\n\n    if (params.max_tokens === -1) {\n      if (prompts.length !== 1) {\n        throw new Error(\n          \"max_tokens set to -1 not supported for multiple inputs\"\n        );\n      }\n      params.max_tokens = await calculateMaxTokens({\n        prompt: prompts[0],\n        // Cast here to allow for other models that may not fit the union\n        modelName: this.model as TiktokenModel,\n      });\n    }\n\n    for (let i = 0; i < subPrompts.length; i += 1) {\n      const data = params.stream\n        ? await (async () => {\n            const choices: OpenAIClient.CompletionChoice[] = [];\n            let response: Omit<OpenAIClient.Completion, \"choices\"> | undefined;\n            const stream = await this.completionWithRetry(\n              {\n                ...params,\n                stream: true,\n                prompt: subPrompts[i],\n              },\n              options\n            );\n            for await (const message of stream) {\n              // on the first message set the response properties\n              if (!response) {\n                response = {\n                  id: message.id,\n                  object: message.object,\n                  created: message.created,\n                  model: message.model,\n                };\n              }\n\n              // on all messages, update choice\n              for (const part of message.choices) {\n                if (!choices[part.index]) {\n                  choices[part.index] = part;\n                } else {\n                  const choice = choices[part.index];\n                  choice.text += part.text;\n                  choice.finish_reason = part.finish_reason;\n                  choice.logprobs = part.logprobs;\n                }\n                // eslint-disable-next-line no-void\n                void runManager?.handleLLMNewToken(part.text, {\n                  prompt: Math.floor(part.index / this.n),\n                  completion: part.index % this.n,\n                });\n              }\n            }\n            if (options.signal?.aborted) {\n              throw new Error(\"AbortError\");\n            }\n            return { ...response, choices };\n          })()\n        : await this.completionWithRetry(\n            {\n              ...params,\n              stream: false,\n              prompt: subPrompts[i],\n            },\n            {\n              signal: options.signal,\n              ...options.options,\n            }\n          );\n\n      choices.push(...data.choices);\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens,\n      } = data.usage\n        ? data.usage\n        : {\n            completion_tokens: undefined,\n            prompt_tokens: undefined,\n            total_tokens: undefined,\n          };\n\n      if (completionTokens) {\n        tokenUsage.completionTokens =\n          (tokenUsage.completionTokens ?? 0) + completionTokens;\n      }\n\n      if (promptTokens) {\n        tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n      }\n\n      if (totalTokens) {\n        tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n      }\n    }\n\n    const generations = chunkArray(choices, this.n).map((promptChoices) =>\n      promptChoices.map((choice) => ({\n        text: choice.text ?? \"\",\n        generationInfo: {\n          finishReason: choice.finish_reason,\n          logprobs: choice.logprobs,\n        },\n      }))\n    );\n    return {\n      generations,\n      llmOutput: { tokenUsage },\n    };\n  }\n\n  // TODO(jacoblee): Refactor with _generate(..., {stream: true}) implementation?\n  async *_streamResponseChunks(\n    input: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    const params = {\n      ...this.invocationParams(options),\n      prompt: input,\n      stream: true as const,\n    };\n    const stream = await this.completionWithRetry(params, options);\n    for await (const data of stream) {\n      const choice = data?.choices[0];\n      if (!choice) {\n        continue;\n      }\n      const chunk = new GenerationChunk({\n        text: choice.text,\n        generationInfo: {\n          finishReason: choice.finish_reason,\n        },\n      });\n      yield chunk;\n      // eslint-disable-next-line no-void\n      void runManager?.handleLLMNewToken(chunk.text ?? \"\");\n    }\n    if (options.signal?.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n\n  /**\n   * Calls the OpenAI API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  async completionWithRetry(\n    request: OpenAIClient.CompletionCreateParamsStreaming,\n    options?: OpenAICoreRequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Completion>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.CompletionCreateParamsNonStreaming,\n    options?: OpenAICoreRequestOptions\n  ): Promise<OpenAIClient.Completions.Completion>;\n\n  async completionWithRetry(\n    request:\n      | OpenAIClient.CompletionCreateParamsStreaming\n      | OpenAIClient.CompletionCreateParamsNonStreaming,\n    options?: OpenAICoreRequestOptions\n  ): Promise<\n    AsyncIterable<OpenAIClient.Completion> | OpenAIClient.Completions.Completion\n  > {\n    const requestOptions = this._getClientOptions(options);\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.completions.create(\n          request,\n          requestOptions\n        );\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /**\n   * Calls the OpenAI API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  protected _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(params.defaultHeaders);\n\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options,\n    } as OpenAICoreRequestOptions;\n    return requestOptions;\n  }\n\n  _llmType() {\n    return \"openai\";\n  }\n}\n", "import { type ClientOptions, AzureOpenAI as AzureOpenAIClient } from \"openai\";\nimport { type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { OpenAI } from \"../llms.js\";\nimport {\n  OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"../utils/azure.js\";\nimport type {\n  OpenAIInput,\n  AzureOpenAIInput,\n  OpenAICoreRequestOptions,\n} from \"../types.js\";\n\nexport class AzureOpenAI extends OpenAI {\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      openAIApiKey: \"openai_api_key\",\n      openAIApiVersion: \"openai_api_version\",\n      openAIBasePath: \"openai_api_base\",\n      deploymentName: \"deployment_name\",\n      azureOpenAIEndpoint: \"azure_endpoint\",\n      azureOpenAIApiVersion: \"openai_api_version\",\n      azureOpenAIBasePath: \"openai_api_base\",\n      azureOpenAIApiDeploymentName: \"deployment_name\",\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n    };\n  }\n\n  constructor(\n    fields?: Partial<OpenAIInput> & {\n      openAIApiKey?: string;\n      openAIApiVersion?: string;\n      openAIBasePath?: string;\n      deploymentName?: string;\n    } & Partial<AzureOpenAIInput> &\n      BaseLLMParams & {\n        configuration?: ClientOptions;\n      }\n  ) {\n    super(fields);\n\n    this.azureOpenAIApiDeploymentName =\n      (fields?.azureOpenAIApiCompletionsDeploymentName ||\n        fields?.azureOpenAIApiDeploymentName) ??\n      (getEnvironmentVariable(\"AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\") ||\n        getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n\n    this.azureOpenAIApiKey =\n      fields?.azureOpenAIApiKey ??\n      (typeof fields?.openAIApiKey === \"string\"\n        ? fields?.openAIApiKey\n        : undefined) ??\n      (typeof fields?.apiKey === \"string\" ? fields?.apiKey : undefined) ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n\n    this.azureOpenAIApiInstanceName =\n      fields?.azureOpenAIApiInstanceName ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n\n    this.azureOpenAIApiVersion =\n      fields?.azureOpenAIApiVersion ??\n      fields?.openAIApiVersion ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n\n    this.azureOpenAIBasePath =\n      fields?.azureOpenAIBasePath ??\n      getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n\n    this.azureOpenAIEndpoint =\n      fields?.azureOpenAIEndpoint ??\n      getEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\");\n\n    this.azureADTokenProvider = fields?.azureADTokenProvider;\n\n    if (!this.azureOpenAIApiKey && !this.apiKey && !this.azureADTokenProvider) {\n      throw new Error(\"Azure OpenAI API key or Token Provider not found\");\n    }\n  }\n\n  protected _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n        azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n        azureOpenAIApiKey: this.azureOpenAIApiKey,\n        azureOpenAIBasePath: this.azureOpenAIBasePath,\n        azureADTokenProvider: this.azureADTokenProvider,\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const { apiKey: existingApiKey, ...clientConfigRest } = this.clientConfig;\n      const params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string } = {\n        ...clientConfigRest,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!this.azureADTokenProvider) {\n        params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;\n      }\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(\n        params.defaultHeaders,\n        true,\n        \"2.0.0\"\n      );\n\n      this.client = new AzureOpenAIClient({\n        apiVersion: this.azureOpenAIApiVersion,\n        azureADTokenProvider: this.azureADTokenProvider,\n        ...params,\n      });\n    }\n\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options,\n    } as OpenAICoreRequestOptions;\n    if (this.azureOpenAIApiKey) {\n      requestOptions.headers = {\n        \"api-key\": this.azureOpenAIApiKey,\n        ...requestOptions.headers,\n      };\n      requestOptions.query = {\n        \"api-version\": this.azureOpenAIApiVersion,\n        ...requestOptions.query,\n      };\n    }\n    return requestOptions;\n  }\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  toJSON(): any {\n    const json = super.toJSON() as unknown;\n\n    function isRecord(obj: unknown): obj is Record<string, unknown> {\n      return typeof obj === \"object\" && obj != null;\n    }\n\n    if (isRecord(json) && isRecord(json.kwargs)) {\n      delete json.kwargs.azure_openai_base_path;\n      delete json.kwargs.azure_openai_api_deployment_name;\n      delete json.kwargs.azure_openai_api_key;\n      delete json.kwargs.azure_openai_api_version;\n      delete json.kwargs.azure_open_ai_base_path;\n    }\n\n    return json;\n  }\n}\n", "import { AsyncCaller, AsyncCallerParams } from \"./utils/async_caller.js\";\n\n/**\n * The parameters required to initialize an instance of the Embeddings\n * class.\n */\nexport type EmbeddingsParams = AsyncCallerParams;\n\nexport interface EmbeddingsInterface<TOutput = number[]> {\n  /**\n   * An abstract method that takes an array of documents as input and\n   * returns a promise that resolves to an array of vectors for each\n   * document.\n   * @param documents An array of documents to be embedded.\n   * @returns A promise that resolves to an array of vectors for each document.\n   */\n  embedDocuments(documents: string[]): Promise<TOutput[]>;\n\n  /**\n   * An abstract method that takes a single document as input and returns a\n   * promise that resolves to a vector for the query document.\n   * @param document A single document to be embedded.\n   * @returns A promise that resolves to a vector for the query document.\n   */\n  embedQuery(document: string): Promise<TOutput>;\n}\n\n/**\n * An abstract class that provides methods for embedding documents and\n * queries using LangChain.\n */\nexport abstract class Embeddings<\n  TOutput = number[],\n> implements EmbeddingsInterface<TOutput> {\n  /**\n   * The async caller should be used by subclasses to make any async calls,\n   * which will thus benefit from the concurrency and retry logic.\n   */\n  caller: AsyncCaller;\n\n  constructor(params: EmbeddingsParams) {\n    this.caller = new AsyncCaller(params ?? {});\n  }\n\n  /**\n   * An abstract method that takes an array of documents as input and\n   * returns a promise that resolves to an array of vectors for each\n   * document.\n   * @param documents An array of documents to be embedded.\n   * @returns A promise that resolves to an array of vectors for each document.\n   */\n  abstract embedDocuments(documents: string[]): Promise<TOutput[]>;\n\n  /**\n   * An abstract method that takes a single document as input and returns a\n   * promise that resolves to a vector for the query document.\n   * @param document A single document to be embedded.\n   * @returns A promise that resolves to a vector for the query document.\n   */\n  abstract embedQuery(document: string): Promise<TOutput>;\n}\n", "import { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { Embeddings, type EmbeddingsParams } from \"@langchain/core/embeddings\";\nimport { chunkArray } from \"@langchain/core/utils/chunk_array\";\nimport type { OpenAIApiKey } from \"./types.js\";\nimport {\n  getEndpoint,\n  OpenAIEndpointConfig,\n  getHeadersWithUserAgent,\n} from \"./utils/azure.js\";\nimport { wrapOpenAIClientError } from \"./utils/client.js\";\n\n/**\n * @see https://platform.openai.com/docs/guides/embeddings#embedding-models\n */\nexport type OpenAIEmbeddingModelId =\n  | OpenAIClient.EmbeddingModel\n  | (string & NonNullable<unknown>);\n\n/**\n * Interface for OpenAIEmbeddings parameters. Extends EmbeddingsParams and\n * defines additional parameters specific to the OpenAIEmbeddings class.\n */\nexport interface OpenAIEmbeddingsParams extends EmbeddingsParams {\n  /**\n   * Model name to use\n   * Alias for `model`\n   * @deprecated Use \"model\" instead.\n   */\n  modelName: OpenAIEmbeddingModelId;\n  /** Model name to use */\n  model: OpenAIEmbeddingModelId;\n\n  /**\n   * The number of dimensions the resulting output embeddings should have.\n   * Only supported in `text-embedding-3` and later models.\n   */\n  dimensions?: number;\n\n  /**\n   * Timeout to use when making requests to OpenAI.\n   */\n  timeout?: number;\n\n  /**\n   * The maximum number of documents to embed in a single request. This is\n   * limited by the OpenAI API to a maximum of 2048.\n   */\n  batchSize?: number;\n\n  /**\n   * Whether to strip new lines from the input text. This is recommended by\n   * OpenAI for older models, but may not be suitable for all use cases.\n   * See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\n   */\n  stripNewLines?: boolean;\n\n  /**\n   * The format to return the embeddings in. Can be either 'float' or 'base64'.\n   */\n  encodingFormat?: \"float\" | \"base64\";\n}\n\n/**\n * Class for generating embeddings using the OpenAI API.\n *\n * To use with Azure, import the `AzureOpenAIEmbeddings` class.\n *\n * @example\n * ```typescript\n * // Embed a query using OpenAIEmbeddings to generate embeddings for a given text\n * const model = new OpenAIEmbeddings();\n * const res = await model.embedQuery(\n *   \"What would be a good company name for a company that makes colorful socks?\",\n * );\n * console.log({ res });\n *\n * ```\n */\nexport class OpenAIEmbeddings<TOutput = number[]>\n  extends Embeddings<TOutput>\n  implements Partial<OpenAIEmbeddingsParams>\n{\n  model = \"text-embedding-ada-002\";\n\n  /** @deprecated Use \"model\" instead */\n  modelName: string;\n\n  batchSize = 512;\n\n  // TODO: Update to `false` on next minor release (see: https://github.com/langchain-ai/langchainjs/pull/3612)\n  stripNewLines = true;\n\n  /**\n   * The number of dimensions the resulting output embeddings should have.\n   * Only supported in `text-embedding-3` and later models.\n   */\n  dimensions?: number;\n\n  timeout?: number;\n\n  organization?: string;\n\n  encodingFormat?: \"float\" | \"base64\";\n\n  protected client: OpenAIClient;\n\n  protected clientConfig: ClientOptions;\n\n  protected apiKey?: OpenAIApiKey;\n\n  constructor(\n    fields?: Partial<OpenAIEmbeddingsParams> & {\n      verbose?: boolean;\n      /**\n       * The OpenAI API key to use.\n       * Alias for `apiKey`.\n       */\n      openAIApiKey?: OpenAIApiKey;\n      /** The OpenAI API key to use. */\n      apiKey?: OpenAIApiKey;\n      configuration?: ClientOptions;\n    }\n  ) {\n    const fieldsWithDefaults = { maxConcurrency: 2, ...fields };\n\n    super(fieldsWithDefaults);\n\n    const apiKey =\n      fieldsWithDefaults?.apiKey ??\n      fieldsWithDefaults?.openAIApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n\n    this.organization =\n      fieldsWithDefaults?.configuration?.organization ??\n      getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    this.model =\n      fieldsWithDefaults?.model ?? fieldsWithDefaults?.modelName ?? this.model;\n    this.modelName = this.model;\n    this.batchSize = fieldsWithDefaults?.batchSize ?? this.batchSize;\n    this.stripNewLines =\n      fieldsWithDefaults?.stripNewLines ?? this.stripNewLines;\n    this.timeout = fieldsWithDefaults?.timeout;\n    this.dimensions = fieldsWithDefaults?.dimensions;\n    this.encodingFormat = fieldsWithDefaults?.encodingFormat;\n\n    this.clientConfig = {\n      apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration,\n    };\n  }\n\n  /**\n   * Method to generate embeddings for an array of documents. Splits the\n   * documents into batches and makes requests to the OpenAI API to generate\n   * embeddings.\n   * @param texts Array of documents to generate embeddings for.\n   * @returns Promise that resolves to a 2D array of embeddings for each document.\n   */\n  async embedDocuments(texts: string[]): Promise<TOutput[]> {\n    const batches = chunkArray(\n      this.stripNewLines ? texts.map((t) => t.replace(/\\n/g, \" \")) : texts,\n      this.batchSize\n    );\n\n    const batchRequests = batches.map((batch) => {\n      const params: OpenAIClient.EmbeddingCreateParams = {\n        model: this.model,\n        input: batch,\n      };\n      if (this.dimensions) {\n        params.dimensions = this.dimensions;\n      }\n      if (this.encodingFormat) {\n        params.encoding_format = this.encodingFormat;\n      }\n      return this.embeddingWithRetry(params);\n    });\n    const batchResponses = await Promise.all(batchRequests);\n\n    const embeddings: TOutput[] = [];\n    for (let i = 0; i < batchResponses.length; i += 1) {\n      const batch = batches[i];\n      const { data: batchResponse } = batchResponses[i];\n      for (let j = 0; j < batch.length; j += 1) {\n        embeddings.push(batchResponse[j].embedding as TOutput);\n      }\n    }\n    return embeddings;\n  }\n\n  /**\n   * Method to generate an embedding for a single document. Calls the\n   * embeddingWithRetry method with the document as the input.\n   * @param text Document to generate an embedding for.\n   * @returns Promise that resolves to an embedding for the document.\n   */\n  async embedQuery(text: string): Promise<TOutput> {\n    const params: OpenAIClient.EmbeddingCreateParams = {\n      model: this.model,\n      input: this.stripNewLines ? text.replace(/\\n/g, \" \") : text,\n    };\n    if (this.dimensions) {\n      params.dimensions = this.dimensions;\n    }\n    if (this.encodingFormat) {\n      params.encoding_format = this.encodingFormat;\n    }\n    const { data } = await this.embeddingWithRetry(params);\n    return data[0].embedding as TOutput;\n  }\n\n  /**\n   * Private method to make a request to the OpenAI API to generate\n   * embeddings. Handles the retry logic and returns the response from the\n   * API.\n   * @param request Request to send to the OpenAI API.\n   * @returns Promise that resolves to the response from the API.\n   */\n  protected async embeddingWithRetry(\n    request: OpenAIClient.EmbeddingCreateParams\n  ) {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(params.defaultHeaders);\n\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {};\n\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.embeddings.create(\n          request,\n          requestOptions\n        );\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n}\n", "import {\n  type ClientOptions,\n  AzureOpenAI as AzureOpenAIClient,\n  OpenAI as OpenAIClient,\n} from \"openai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { OpenAIEmbeddings, OpenAIEmbeddingsParams } from \"../embeddings.js\";\nimport { AzureOpenAIInput, OpenAICoreRequestOptions } from \"../types.js\";\nimport {\n  getEndpoint,\n  OpenAIEndpointConfig,\n  getHeadersWithUserAgent,\n} from \"../utils/azure.js\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\n\nexport class AzureOpenAIEmbeddings extends OpenAIEmbeddings {\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  constructor(\n    fields?: Partial<OpenAIEmbeddingsParams> &\n      Partial<AzureOpenAIInput> & {\n        verbose?: boolean;\n        /** The OpenAI API key to use. */\n        apiKey?: string;\n        configuration?: ClientOptions;\n        deploymentName?: string;\n        openAIApiVersion?: string;\n      }\n  ) {\n    super(fields);\n    this.batchSize = fields?.batchSize ?? 1;\n    this.azureOpenAIApiKey =\n      fields?.azureOpenAIApiKey ??\n      (typeof fields?.apiKey === \"string\" ? fields?.apiKey : undefined) ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n\n    this.azureOpenAIApiVersion =\n      fields?.azureOpenAIApiVersion ??\n      fields?.openAIApiVersion ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n\n    this.azureOpenAIBasePath =\n      fields?.azureOpenAIBasePath ??\n      getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n\n    this.azureOpenAIApiInstanceName =\n      fields?.azureOpenAIApiInstanceName ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n\n    this.azureOpenAIApiDeploymentName =\n      (fields?.azureOpenAIApiEmbeddingsDeploymentName ||\n        fields?.azureOpenAIApiDeploymentName) ??\n      (getEnvironmentVariable(\"AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\") ||\n        getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n\n    this.azureADTokenProvider = fields?.azureADTokenProvider;\n  }\n\n  protected async embeddingWithRetry(\n    request: OpenAIClient.EmbeddingCreateParams\n  ) {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n        azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n        azureOpenAIApiKey: this.azureOpenAIApiKey,\n        azureOpenAIBasePath: this.azureOpenAIBasePath,\n        azureADTokenProvider: this.azureADTokenProvider,\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const { apiKey: existingApiKey, ...clientConfigRest } = this.clientConfig;\n      const params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string } = {\n        ...clientConfigRest,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!this.azureADTokenProvider) {\n        params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;\n      }\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(\n        params.defaultHeaders,\n        true,\n        \"2.0.0\"\n      );\n\n      this.client = new AzureOpenAIClient({\n        apiVersion: this.azureOpenAIApiVersion,\n        azureADTokenProvider: this.azureADTokenProvider,\n        deployment: this.azureOpenAIApiDeploymentName,\n        ...params,\n      });\n    }\n    const requestOptions: OpenAICoreRequestOptions = {};\n    if (this.azureOpenAIApiKey) {\n      requestOptions.headers = {\n        \"api-key\": this.azureOpenAIApiKey,\n        ...requestOptions.headers,\n      };\n      requestOptions.query = {\n        \"api-version\": this.azureOpenAIApiVersion,\n        ...requestOptions.query,\n      };\n    }\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.embeddings.create(\n          request,\n          requestOptions\n        );\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n}\n", "import { z } from \"zod/v3\";\nimport { z as z4, ZodError } from \"zod/v4\";\nimport {\n  validate,\n  type Schema as ValidationSchema,\n} from \"@cfworker/json-schema\";\nimport {\n  CallbackManager,\n  CallbackManagerForToolRun,\n  parseCallbackConfigArg,\n} from \"../callbacks/manager.js\";\nimport { BaseLangChain } from \"../language_models/base.js\";\nimport {\n  mergeConfigs,\n  ensureConfig,\n  patchConfig,\n  pickRunnableConfigKeys,\n  type RunnableConfig,\n} from \"../runnables/config.js\";\nimport type { RunnableFunc } from \"../runnables/base.js\";\nimport { isDirectToolOutput, ToolCall, ToolMessage } from \"../messages/tool.js\";\nimport { AsyncLocalStorageProviderSingleton } from \"../singletons/index.js\";\nimport type { RunnableToolLike } from \"../runnables/base.js\";\nimport {\n  _configHasToolCallId,\n  _isToolCall,\n  ToolInputParsingException,\n} from \"./utils.js\";\nimport {\n  type InferInteropZodInput,\n  type InferInteropZodOutput,\n  type InteropZodObject,\n  type InteropZodType,\n  interopParseAsync,\n  isSimpleStringZodSchema,\n  isInteropZodError,\n  isInteropZodSchema,\n  type ZodStringV3,\n  type ZodStringV4,\n  type ZodObjectV3,\n  type ZodObjectV4,\n} from \"../utils/types/zod.js\";\nimport { getAbortSignalError } from \"../utils/signal.js\";\nimport type {\n  StructuredToolCallInput,\n  ToolInputSchemaBase,\n  ToolReturnType,\n  ResponseFormat,\n  ToolInputSchemaInputType,\n  ToolInputSchemaOutputType,\n  ToolParams,\n  ToolRunnableConfig,\n  StructuredToolInterface,\n  DynamicToolInput,\n  DynamicStructuredToolInput,\n  StringInputToolSchema,\n  ToolInterface,\n  ToolOutputType,\n  ToolRuntime,\n} from \"./types.js\";\nimport { type JSONSchema, validatesOnlyStrings } from \"../utils/json_schema.js\";\nimport { consumeAsyncGenerator, isAsyncGenerator } from \"../runnables/iter.js\";\n\nexport type {\n  BaseDynamicToolInput,\n  ContentAndArtifact,\n  DynamicToolInput,\n  DynamicStructuredToolInput,\n  ResponseFormat,\n  StructuredToolCallInput,\n  StructuredToolInterface,\n  StructuredToolParams,\n  ToolInterface,\n  ToolParams,\n  ToolReturnType,\n  ToolRunnableConfig,\n  ToolInputSchemaBase as ToolSchemaBase,\n} from \"./types.js\";\n\nexport {\n  isLangChainTool,\n  isRunnableToolLike,\n  isStructuredTool,\n  isStructuredToolParams,\n  type ToolRuntime,\n} from \"./types.js\";\n\nexport { ToolInputParsingException };\n/**\n * Base class for Tools that accept input of any shape defined by a Zod schema.\n */\nexport abstract class StructuredTool<\n  SchemaT = ToolInputSchemaBase,\n  SchemaOutputT = ToolInputSchemaOutputType<SchemaT>,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n>\n  extends BaseLangChain<\n    StructuredToolCallInput<SchemaT, SchemaInputT>,\n    ToolOutputT | ToolMessage\n  >\n  implements StructuredToolInterface<SchemaT, SchemaInputT, ToolOutputT>\n{\n  abstract name: string;\n\n  abstract description: string;\n\n  abstract schema: SchemaT;\n\n  /**\n   * Optional provider-specific extra fields for the tool.\n   *\n   * This is used to pass provider-specific configuration that doesn't fit into\n   * standard tool fields.\n   */\n  extras?: Record<string, unknown>;\n\n  /**\n   * Whether to return the tool's output directly.\n   *\n   * Setting this to true means that after the tool is called,\n   * an agent should stop looping.\n   */\n  returnDirect = false;\n\n  verboseParsingErrors = false;\n\n  get lc_namespace() {\n    return [\"langchain\", \"tools\"];\n  }\n\n  /**\n   * The tool response format.\n   *\n   * If \"content\" then the output of the tool is interpreted as the contents of a\n   * ToolMessage. If \"content_and_artifact\" then the output is expected to be a\n   * two-tuple corresponding to the (content, artifact) of a ToolMessage.\n   *\n   * @default \"content\"\n   */\n  responseFormat?: ResponseFormat = \"content\";\n\n  /**\n   * Default config object for the tool runnable.\n   */\n  defaultConfig?: ToolRunnableConfig;\n\n  constructor(fields?: ToolParams) {\n    super(fields ?? {});\n\n    this.verboseParsingErrors =\n      fields?.verboseParsingErrors ?? this.verboseParsingErrors;\n    this.responseFormat = fields?.responseFormat ?? this.responseFormat;\n    this.defaultConfig = fields?.defaultConfig ?? this.defaultConfig;\n    this.metadata = fields?.metadata ?? this.metadata;\n    this.extras = fields?.extras ?? this.extras;\n  }\n\n  protected abstract _call(\n    arg: SchemaOutputT,\n    runManager?: CallbackManagerForToolRun,\n    parentConfig?: ToolRunnableConfig\n  ): Promise<ToolOutputT> | AsyncGenerator<unknown, ToolOutputT>;\n\n  /**\n   * Invokes the tool with the provided input and configuration.\n   * @param input The input for the tool.\n   * @param config Optional configuration for the tool.\n   * @returns A Promise that resolves with the tool's output.\n   */\n  async invoke<\n    TInput extends StructuredToolCallInput<SchemaT, SchemaInputT>,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    input: TInput,\n    config?: TConfig\n  ): Promise<ToolReturnType<TInput, TConfig, ToolOutputT>> {\n    let toolInput: Exclude<\n      StructuredToolCallInput<SchemaT, SchemaInputT>,\n      ToolCall\n    >;\n\n    let enrichedConfig: ToolRunnableConfig = ensureConfig(\n      mergeConfigs(this.defaultConfig, config)\n    );\n    if (_isToolCall(input)) {\n      toolInput = input.args as Exclude<\n        StructuredToolCallInput<SchemaT, SchemaInputT>,\n        ToolCall\n      >;\n      enrichedConfig = {\n        ...enrichedConfig,\n        toolCall: input,\n      };\n    } else {\n      toolInput = input as Exclude<\n        StructuredToolCallInput<SchemaT, SchemaInputT>,\n        ToolCall\n      >;\n    }\n\n    return this.call(toolInput, enrichedConfig) as Promise<\n      ToolReturnType<TInput, TConfig, ToolOutputT>\n    >;\n  }\n\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.3.0.\n   *\n   * Calls the tool with the provided argument, configuration, and tags. It\n   * parses the input according to the schema, handles any errors, and\n   * manages callbacks.\n   * @param arg The input argument for the tool.\n   * @param configArg Optional configuration or callbacks for the tool.\n   * @param tags Optional tags for the tool.\n   * @returns A Promise that resolves with a string.\n   */\n  async call<\n    TArg extends StructuredToolCallInput<SchemaT, SchemaInputT>,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    arg: TArg,\n    configArg?: TConfig,\n    /** @deprecated */\n    tags?: string[]\n  ): Promise<ToolReturnType<TArg, TConfig, ToolOutputT>> {\n    // Determine the actual input that needs parsing/validation.\n    // If arg is a ToolCall, use its args; otherwise, use arg directly.\n    const inputForValidation = _isToolCall(arg) ? arg.args : arg;\n\n    let parsed: SchemaOutputT; // This will hold the successfully parsed input of the expected output type.\n    if (isInteropZodSchema(this.schema)) {\n      try {\n        // Validate the inputForValidation - TS needs help here as it can't exclude ToolCall based on the check\n        parsed = await interopParseAsync(\n          this.schema as InteropZodType,\n          inputForValidation as Exclude<TArg, ToolCall>\n        );\n      } catch (e) {\n        let message = `Received tool input did not match expected schema`;\n        if (this.verboseParsingErrors) {\n          message = `${message}\\nDetails: ${(e as Error).message}`;\n        }\n        if (isInteropZodError(e)) {\n          message = `${message}\\n\\n${z4.prettifyError(e as ZodError)}`;\n        }\n        // Pass the original raw input arg to the exception\n        throw new ToolInputParsingException(message, JSON.stringify(arg));\n      }\n    } else {\n      const result = validate(\n        inputForValidation,\n        this.schema as ValidationSchema\n      );\n      if (!result.valid) {\n        let message = `Received tool input did not match expected schema`;\n        if (this.verboseParsingErrors) {\n          message = `${message}\\nDetails: ${result.errors\n            .map((e) => `${e.keywordLocation}: ${e.error}`)\n            .join(\"\\n\")}`;\n        }\n        // Pass the original raw input arg to the exception\n        throw new ToolInputParsingException(message, JSON.stringify(arg));\n      }\n      // Assign the validated input to parsed\n      // We cast here because validate() doesn't narrow the type sufficiently for TS, but we know it's valid.\n      parsed = inputForValidation as SchemaOutputT;\n    }\n\n    const config = parseCallbackConfigArg(configArg);\n    const callbackManager_ = CallbackManager.configure(\n      config.callbacks,\n      this.callbacks,\n      config.tags || tags,\n      this.tags,\n      config.metadata,\n      this.metadata,\n      { verbose: this.verbose }\n    );\n\n    let toolCallId: string | undefined;\n    // Extract toolCallId ONLY if the original arg was a ToolCall\n    if (_isToolCall(arg)) {\n      toolCallId = arg.id;\n    }\n    // Or if it was provided in the config's toolCall property\n    if (!toolCallId && _configHasToolCallId(config)) {\n      toolCallId = config.toolCall.id;\n    }\n\n    const runManager = await callbackManager_?.handleToolStart(\n      this.toJSON(),\n      // Log the original raw input arg\n      typeof arg === \"string\" ? arg : JSON.stringify(arg),\n      config.runId,\n      undefined,\n      undefined,\n      undefined,\n      config.runName,\n      toolCallId\n    );\n    delete config.runId;\n\n    let result;\n    try {\n      const raw = await this._call(parsed, runManager, config);\n      result = isAsyncGenerator(raw)\n        ? await consumeAsyncGenerator(raw, async (chunk) => {\n            try {\n              await runManager?.handleToolEvent(chunk);\n            } catch (streamError) {\n              await runManager?.handleToolError(streamError);\n            }\n          })\n        : raw;\n    } catch (e) {\n      await runManager?.handleToolError(e);\n      throw e;\n    }\n\n    let content;\n    let artifact;\n    if (this.responseFormat === \"content_and_artifact\") {\n      if (Array.isArray(result) && result.length === 2) {\n        [content, artifact] = result;\n      } else {\n        throw new Error(\n          `Tool response format is \"content_and_artifact\" but the output was not a two-tuple.\\nResult: ${JSON.stringify(\n            result\n          )}`\n        );\n      }\n    } else {\n      content = result;\n    }\n\n    const formattedOutput = _formatToolOutput<ToolOutputT>({\n      content,\n      artifact,\n      toolCallId,\n      name: this.name,\n      metadata: this.metadata,\n    });\n    await runManager?.handleToolEnd(formattedOutput);\n    return formattedOutput as ToolReturnType<TArg, TConfig, ToolOutputT>;\n  }\n}\n\n/**\n * Base class for Tools that accept input as a string.\n */\nexport abstract class Tool<ToolOutputT = ToolOutputType>\n  extends StructuredTool<\n    StringInputToolSchema,\n    ToolInputSchemaOutputType<StringInputToolSchema>,\n    ToolInputSchemaInputType<StringInputToolSchema>,\n    ToolOutputT\n  >\n  implements\n    ToolInterface<\n      StringInputToolSchema,\n      ToolInputSchemaInputType<StringInputToolSchema>,\n      ToolOutputT\n    >\n{\n  schema = z\n    .object({ input: z.string().optional() })\n    .transform((obj) => obj.input);\n\n  constructor(fields?: ToolParams) {\n    super(fields);\n  }\n\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.3.0.\n   *\n   * Calls the tool with the provided argument and callbacks. It handles\n   * string inputs specifically.\n   * @param arg The input argument for the tool, which can be a string, undefined, or an input of the tool's schema.\n   * @param callbacks Optional callbacks for the tool.\n   * @returns A Promise that resolves with a string.\n   */\n  // Match the base class signature including the generics and conditional return type\n  call<\n    TArg extends string | undefined | z.input<this[\"schema\"]> | ToolCall,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    arg: TArg,\n    callbacks?: TConfig\n  ): Promise<ToolReturnType<NonNullable<TArg>, TConfig, ToolOutputT>> {\n    // Prepare the input for the base class call method.\n    // If arg is string or undefined, wrap it; otherwise, pass ToolCall or { input: ... } directly.\n    const structuredArg =\n      typeof arg === \"string\" || arg == null ? { input: arg } : arg;\n\n    // Ensure TConfig is passed to super.call\n    return super.call(structuredArg, callbacks);\n  }\n}\n\n/**\n * A tool that can be created dynamically from a function, name, and description.\n */\nexport class DynamicTool<\n  ToolOutputT = ToolOutputType,\n> extends Tool<ToolOutputT> {\n  static lc_name() {\n    return \"DynamicTool\";\n  }\n\n  name: string;\n\n  description: string;\n\n  func: DynamicToolInput<ToolOutputT>[\"func\"];\n\n  constructor(fields: DynamicToolInput<ToolOutputT>) {\n    super(fields);\n    this.name = fields.name;\n    this.description = fields.description;\n    this.func = fields.func;\n    this.returnDirect = fields.returnDirect ?? this.returnDirect;\n  }\n\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.3.0.\n   */\n  async call<\n    TArg extends string | undefined | z.input<this[\"schema\"]> | ToolCall,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    arg: TArg,\n    configArg?: TConfig\n  ): Promise<ToolReturnType<NonNullable<TArg>, TConfig, ToolOutputT>> {\n    const config = parseCallbackConfigArg(configArg);\n    if (config.runName === undefined) {\n      config.runName = this.name;\n    }\n    // Call the Tool class's call method, passing generics through\n    // Cast config to TConfig to satisfy the super.call signature\n    return super.call<TArg, TConfig>(arg, config as TConfig);\n  }\n\n  /** @ignore */\n  _call(\n    input: string, // DynamicTool's _call specifically expects a string after schema transformation\n    runManager?: CallbackManagerForToolRun,\n    parentConfig?: ToolRunnableConfig\n  ): Promise<ToolOutputT> | AsyncGenerator<unknown, ToolOutputT> {\n    return this.func(input, runManager, parentConfig);\n  }\n}\n\n/**\n * A tool that can be created dynamically from a function, name, and\n * description, designed to work with structured data. It extends the\n * StructuredTool class and overrides the _call method to execute the\n * provided function when the tool is called.\n *\n * Schema can be passed as Zod or JSON schema. The tool will not validate\n * input if JSON schema is passed.\n *\n * @template SchemaT The input schema type for the tool (Zod schema or JSON schema). Defaults to `ToolInputSchemaBase`.\n * @template SchemaOutputT The output type derived from the schema after parsing/validation. Defaults to `ToolInputSchemaOutputType<SchemaT>`.\n * @template SchemaInputT The input type derived from the schema before parsing. Defaults to `ToolInputSchemaInputType<SchemaT>`.\n * @template ToolOutputT The return type of the tool's function. Defaults to `ToolOutputType`.\n * @template NameT The literal type of the tool name (for discriminated union support). Defaults to `string`.\n */\nexport class DynamicStructuredTool<\n  SchemaT = ToolInputSchemaBase,\n  SchemaOutputT = ToolInputSchemaOutputType<SchemaT>,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n  NameT extends string = string,\n> extends StructuredTool<SchemaT, SchemaOutputT, SchemaInputT, ToolOutputT> {\n  static lc_name() {\n    return \"DynamicStructuredTool\";\n  }\n\n  declare name: NameT;\n\n  description: string;\n\n  func: DynamicStructuredToolInput<SchemaT, SchemaOutputT, ToolOutputT>[\"func\"];\n\n  schema: SchemaT;\n\n  constructor(\n    fields: DynamicStructuredToolInput<SchemaT, SchemaOutputT, ToolOutputT> & {\n      name: NameT;\n    }\n  ) {\n    super(fields);\n    this.name = fields.name;\n    this.description = fields.description;\n    this.func = fields.func;\n    this.returnDirect = fields.returnDirect ?? this.returnDirect;\n    this.schema = fields.schema;\n  }\n\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.3.0.\n   */\n  // Match the base class signature\n  async call<\n    TArg extends StructuredToolCallInput<SchemaT, SchemaInputT>,\n    TConfig extends ToolRunnableConfig | undefined,\n  >(\n    arg: TArg,\n    configArg?: TConfig,\n    /** @deprecated */\n    tags?: string[]\n  ): Promise<ToolReturnType<NonNullable<TArg>, TConfig, ToolOutputT>> {\n    const config = parseCallbackConfigArg(configArg);\n    if (config.runName === undefined) {\n      config.runName = this.name;\n    }\n\n    // Call the base class method, passing generics through\n    // Cast config to TConfig to satisfy the super.call signature\n    return super.call<TArg, TConfig>(arg, config as TConfig, tags);\n  }\n\n  protected _call(\n    arg: Parameters<\n      DynamicStructuredToolInput<SchemaT, SchemaOutputT>[\"func\"]\n    >[0],\n    runManager?: CallbackManagerForToolRun,\n    parentConfig?: RunnableConfig\n  ): Promise<ToolOutputT> | AsyncGenerator<unknown, ToolOutputT> {\n    return this.func(arg, runManager, parentConfig);\n  }\n}\n\n/**\n * Abstract base class for toolkits in LangChain. Toolkits are collections\n * of tools that agents can use. Subclasses must implement the `tools`\n * property to provide the specific tools for the toolkit.\n */\nexport abstract class BaseToolkit {\n  abstract tools: StructuredToolInterface[];\n\n  getTools(): StructuredToolInterface[] {\n    return this.tools;\n  }\n}\n\n/**\n * Parameters for the tool function.\n * Schema can be provided as Zod or JSON schema.\n * Both schema types will be validated.\n * @template {ToolInputSchemaBase} RunInput The input schema for the tool.\n * @template {string} NameT The literal name type for discriminated union support.\n */\ninterface ToolWrapperParams<\n  RunInput = ToolInputSchemaBase | undefined,\n  NameT extends string = string,\n> extends ToolParams {\n  /**\n   * The name of the tool. If using with an LLM, this\n   * will be passed as the tool name.\n   */\n  name: NameT;\n  /**\n   * The description of the tool.\n   * @default `${fields.name} tool`\n   */\n  description?: string;\n  /**\n   * The input schema for the tool. If using an LLM, this\n   * will be passed as the tool schema to generate arguments\n   * for.\n   */\n  schema?: RunInput;\n  /**\n   * The tool response format.\n   *\n   * If \"content\" then the output of the tool is interpreted as the contents of a\n   * ToolMessage. If \"content_and_artifact\" then the output is expected to be a\n   * two-tuple corresponding to the (content, artifact) of a ToolMessage.\n   *\n   * @default \"content\"\n   */\n  responseFormat?: ResponseFormat;\n  /**\n   * Whether to return the tool's output directly.\n   *\n   * Setting this to true means that after the tool is called,\n   * an agent should stop looping.\n   */\n  returnDirect?: boolean;\n}\n\n/**\n * Creates a new StructuredTool instance with the provided function, name, description, and schema.\n *\n * Schema can be provided as Zod or JSON schema, and both will be validated.\n *\n * @function\n * @template {ToolInputSchemaBase} SchemaT The input schema for the tool.\n * @template {ToolReturnType} ToolOutputT The output type of the tool.\n *\n * @param {RunnableFunc<z.output<SchemaT>, ToolOutputT>} func - The function to invoke when the tool is called.\n * @param {ToolWrapperParams<SchemaT>} fields - An object containing the following properties:\n * @param {string} fields.name The name of the tool.\n * @param {string | undefined} fields.description The description of the tool. Defaults to either the description on the Zod schema, or `${fields.name} tool`.\n * @param {z.AnyZodObject | z.ZodString | undefined} fields.schema The Zod schema defining the input for the tool. If undefined, it will default to a Zod string schema.\n *\n * @returns {DynamicStructuredTool<SchemaT>} A new StructuredTool instance.\n */\nexport function tool<SchemaT extends ZodStringV3, ToolOutputT = ToolOutputType>(\n  func: RunnableFunc<\n    InferInteropZodOutput<SchemaT>,\n    ToolOutputT,\n    ToolRunnableConfig\n  >,\n  fields: ToolWrapperParams<SchemaT>\n): DynamicTool<ToolOutputT>;\n\nexport function tool<SchemaT extends ZodStringV4, ToolOutputT = ToolOutputType>(\n  func: RunnableFunc<\n    InferInteropZodOutput<SchemaT>,\n    ToolOutputT,\n    ToolRunnableConfig\n  >,\n  fields: ToolWrapperParams<SchemaT>\n): DynamicTool<ToolOutputT>;\n\nexport function tool<\n  SchemaT extends ZodObjectV3,\n  NameT extends string,\n  SchemaOutputT = InferInteropZodOutput<SchemaT>,\n  SchemaInputT = InferInteropZodInput<SchemaT>,\n  ToolOutputT = ToolOutputType,\n>(\n  func: RunnableFunc<SchemaOutputT, ToolOutputT, ToolRunnableConfig>,\n  fields: ToolWrapperParams<SchemaT, NameT>\n): DynamicStructuredTool<\n  SchemaT,\n  SchemaOutputT,\n  SchemaInputT,\n  ToolOutputT,\n  NameT\n>;\n\nexport function tool<\n  SchemaT extends ZodObjectV4,\n  NameT extends string,\n  SchemaOutputT = InferInteropZodOutput<SchemaT>,\n  SchemaInputT = InferInteropZodInput<SchemaT>,\n  ToolOutputT = ToolOutputType,\n>(\n  func: RunnableFunc<SchemaOutputT, ToolOutputT, ToolRunnableConfig>,\n  fields: ToolWrapperParams<SchemaT, NameT>\n): DynamicStructuredTool<\n  SchemaT,\n  SchemaOutputT,\n  SchemaInputT,\n  ToolOutputT,\n  NameT\n>;\n\nexport function tool<\n  SchemaT extends JSONSchema,\n  NameT extends string,\n  SchemaOutputT = ToolInputSchemaOutputType<SchemaT>,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n>(\n  func: RunnableFunc<\n    Parameters<DynamicStructuredToolInput<SchemaT>[\"func\"]>[0],\n    ToolOutputT,\n    ToolRunnableConfig\n  >,\n  fields: ToolWrapperParams<SchemaT, NameT>\n): DynamicStructuredTool<\n  SchemaT,\n  SchemaOutputT,\n  SchemaInputT,\n  ToolOutputT,\n  NameT\n>;\n\nexport function tool<\n  SchemaT extends InteropZodObject | InteropZodType<string> | JSONSchema =\n    InteropZodObject,\n  NameT extends string = string,\n  SchemaOutputT = ToolInputSchemaOutputType<SchemaT>,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n>(\n  func: RunnableFunc<SchemaOutputT, ToolOutputT, ToolRunnableConfig>,\n  fields: ToolWrapperParams<SchemaT, NameT>\n):\n  | DynamicStructuredTool<\n      SchemaT,\n      SchemaOutputT,\n      SchemaInputT,\n      ToolOutputT,\n      NameT\n    >\n  | DynamicTool<ToolOutputT>;\n\n// Overloads with ToolRuntime as CallOptions\nexport function tool<\n  SchemaT extends ZodStringV3,\n  ToolOutputT = ToolOutputType,\n  TState = unknown,\n  TContext = unknown,\n>(\n  func: (\n    input: InferInteropZodOutput<SchemaT>,\n    runtime: ToolRuntime<TState, TContext>\n  ) => ToolOutputT | Promise<ToolOutputT>,\n  fields: ToolWrapperParams<SchemaT>\n): DynamicTool<ToolOutputT>;\n\nexport function tool<\n  SchemaT extends ZodStringV4,\n  ToolOutputT = ToolOutputType,\n  TState = unknown,\n  TContext = unknown,\n>(\n  func: (\n    input: InferInteropZodOutput<SchemaT>,\n    runtime: ToolRuntime<TState, TContext>\n  ) => ToolOutputT | Promise<ToolOutputT>,\n  fields: ToolWrapperParams<SchemaT>\n): DynamicTool<ToolOutputT>;\n\nexport function tool<\n  SchemaT extends ZodObjectV3,\n  NameT extends string,\n  SchemaOutputT = InferInteropZodOutput<SchemaT>,\n  SchemaInputT = InferInteropZodInput<SchemaT>,\n  ToolOutputT = ToolOutputType,\n  TState = unknown,\n  TContext = unknown,\n>(\n  func: (\n    input: SchemaOutputT,\n    runtime: ToolRuntime<TState, TContext>\n  ) => ToolOutputT | Promise<ToolOutputT>,\n  fields: ToolWrapperParams<SchemaT, NameT>\n): DynamicStructuredTool<\n  SchemaT,\n  SchemaOutputT,\n  SchemaInputT,\n  ToolOutputT,\n  NameT\n>;\n\nexport function tool<\n  SchemaT extends ZodObjectV4,\n  NameT extends string,\n  SchemaOutputT = InferInteropZodOutput<SchemaT>,\n  SchemaInputT = InferInteropZodInput<SchemaT>,\n  ToolOutputT = ToolOutputType,\n  TState = unknown,\n  TContext = unknown,\n>(\n  func: (\n    input: SchemaOutputT,\n    runtime: ToolRuntime<TState, TContext>\n  ) => ToolOutputT | Promise<ToolOutputT>,\n  fields: ToolWrapperParams<SchemaT, NameT>\n): DynamicStructuredTool<\n  SchemaT,\n  SchemaOutputT,\n  SchemaInputT,\n  ToolOutputT,\n  NameT\n>;\n\nexport function tool<\n  SchemaT extends JSONSchema,\n  NameT extends string,\n  SchemaOutputT = ToolInputSchemaOutputType<SchemaT>,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n  TState = unknown,\n  TContext = unknown,\n>(\n  func: (\n    input: Parameters<DynamicStructuredToolInput<SchemaT>[\"func\"]>[0],\n    runtime: ToolRuntime<TState, TContext>\n  ) => ToolOutputT | Promise<ToolOutputT>,\n  fields: ToolWrapperParams<SchemaT, NameT>\n): DynamicStructuredTool<\n  SchemaT,\n  SchemaOutputT,\n  SchemaInputT,\n  ToolOutputT,\n  NameT\n>;\n\nexport function tool<\n  SchemaT extends InteropZodObject | InteropZodType<string> | JSONSchema =\n    InteropZodObject,\n  NameT extends string = string,\n  SchemaOutputT = ToolInputSchemaOutputType<SchemaT>,\n  SchemaInputT = ToolInputSchemaInputType<SchemaT>,\n  ToolOutputT = ToolOutputType,\n  TState = unknown,\n  TContext = unknown,\n>(\n  func: (\n    input: SchemaOutputT,\n    runtime: ToolRuntime<TState, TContext>\n  ) => ToolOutputT | Promise<ToolOutputT>,\n  fields: ToolWrapperParams<SchemaT, NameT>\n):\n  | DynamicStructuredTool<\n      SchemaT,\n      SchemaOutputT,\n      SchemaInputT,\n      ToolOutputT,\n      NameT\n    >\n  | DynamicTool<ToolOutputT> {\n  const isSimpleStringSchema = isSimpleStringZodSchema(fields.schema);\n  const isStringJSONSchema = validatesOnlyStrings(fields.schema);\n\n  // If the schema is not provided, or it's a simple string schema, create a DynamicTool\n  if (!fields.schema || isSimpleStringSchema || isStringJSONSchema) {\n    return new DynamicTool<ToolOutputT>({\n      ...fields,\n      description:\n        fields.description ??\n        (fields.schema as { description?: string } | undefined)?.description ??\n        `${fields.name} tool`,\n      func: async (input, runManager, config) => {\n        return new Promise<ToolOutputT>((resolve, reject) => {\n          const childConfig = patchConfig(config, {\n            callbacks: runManager?.getChild(),\n          });\n          // eslint-disable-next-line no-void\n          void AsyncLocalStorageProviderSingleton.runWithConfig(\n            pickRunnableConfigKeys(childConfig),\n            async () => {\n              try {\n                // eslint-disable-next-line @typescript-eslint/no-explicit-any\n                resolve(func(input as any, childConfig as any));\n              } catch (e) {\n                reject(e);\n              }\n            }\n          );\n        });\n      },\n    });\n  }\n\n  const schema = fields.schema as InteropZodObject | JSONSchema;\n\n  const description =\n    fields.description ??\n    (fields.schema as { description?: string }).description ??\n    `${fields.name} tool`;\n\n  return new DynamicStructuredTool<\n    typeof schema,\n    SchemaOutputT,\n    SchemaInputT,\n    ToolOutputT,\n    NameT\n  >({\n    ...fields,\n    description,\n    schema,\n    func: async (input, runManager, config) => {\n      return new Promise<ToolOutputT>((resolve, reject) => {\n        let listener: (() => void) | undefined;\n        const cleanup = () => {\n          if (config?.signal && listener) {\n            config.signal.removeEventListener(\"abort\", listener);\n          }\n        };\n\n        if (config?.signal) {\n          listener = () => {\n            cleanup();\n            reject(getAbortSignalError(config.signal));\n          };\n          config.signal.addEventListener(\"abort\", listener, { once: true });\n        }\n\n        const childConfig = patchConfig(config, {\n          callbacks: runManager?.getChild(),\n        });\n        // eslint-disable-next-line no-void\n        void AsyncLocalStorageProviderSingleton.runWithConfig(\n          pickRunnableConfigKeys(childConfig),\n          async () => {\n            try {\n              // eslint-disable-next-line @typescript-eslint/no-explicit-any\n              const result = await func(input as any, childConfig as any);\n\n              /**\n               * If the signal is aborted, we don't want to resolve the promise\n               * as the promise is already rejected.\n               */\n              if (config?.signal?.aborted) {\n                cleanup();\n                return;\n              }\n\n              cleanup();\n              resolve(result);\n            } catch (e) {\n              cleanup();\n              reject(e);\n            }\n          }\n        );\n      });\n    },\n  }) as DynamicStructuredTool<\n    SchemaT,\n    SchemaOutputT,\n    SchemaInputT,\n    ToolOutputT,\n    NameT\n  >;\n}\n\nfunction _formatToolOutput<TOutput extends ToolOutputType>(params: {\n  content: TOutput;\n  name: string;\n  artifact?: unknown;\n  toolCallId?: string;\n  metadata?: Record<string, unknown>;\n}): ToolMessage | TOutput {\n  const { content, artifact, toolCallId, metadata } = params;\n  if (toolCallId && !isDirectToolOutput(content)) {\n    if (\n      typeof content === \"string\" ||\n      (Array.isArray(content) &&\n        content.every((item) => typeof item === \"object\"))\n    ) {\n      return new ToolMessage({\n        status: \"success\",\n        content,\n        artifact,\n        tool_call_id: toolCallId,\n        name: params.name,\n        metadata,\n      });\n    } else {\n      return new ToolMessage({\n        status: \"success\",\n        content: _stringify(content),\n        artifact,\n        tool_call_id: toolCallId,\n        name: params.name,\n        metadata,\n      });\n    }\n  } else {\n    return content;\n  }\n}\n\nfunction _stringify(content: unknown): string {\n  try {\n    return JSON.stringify(content) ?? \"\";\n  } catch (_noOp) {\n    return `${content}`;\n  }\n}\n\nexport type ServerTool = Record<string, unknown>;\nexport type ClientTool =\n  | StructuredToolInterface\n  | DynamicTool\n  | RunnableToolLike;\n", "import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { Tool, ToolParams } from \"@langchain/core/tools\";\nimport {\n  MessageContentComplex,\n  MessageContentImageUrl,\n} from \"@langchain/core/messages\";\n\n/**\n * @see https://platform.openai.com/docs/api-reference/images/create\n */\nexport type OpenAIImageModelId =\n  | OpenAIClient.ImageModel\n  | (string & NonNullable<unknown>);\n\n/**\n * An interface for the Dall-E API Wrapper.\n */\nexport interface DallEAPIWrapperParams extends ToolParams {\n  /**\n   * The OpenAI API key\n   * Alias for `apiKey`\n   */\n  openAIApiKey?: string;\n  /**\n   * The OpenAI API key\n   */\n  apiKey?: string;\n  /**\n   * The model to use.\n   * Alias for `model`\n   * @params \"dall-e-2\" | \"dall-e-3\"\n   * @default \"dall-e-3\"\n   * @deprecated Use `model` instead.\n   */\n  modelName?: OpenAIImageModelId;\n  /**\n   * The model to use.\n   * @params \"dall-e-2\" | \"dall-e-3\"\n   * @default \"dall-e-3\"\n   */\n  model?: OpenAIImageModelId;\n  /**\n   * The style of the generated images. Must be one of vivid or natural.\n   * Vivid causes the model to lean towards generating hyper-real and dramatic images.\n   * Natural causes the model to produce more natural, less hyper-real looking images.\n   * @default \"vivid\"\n   */\n  style?: \"natural\" | \"vivid\";\n  /**\n   * The quality of the image that will be generated. hd creates images with finer\n   * details and greater consistency across the image.\n   * @default \"standard\"\n   */\n  quality?: \"standard\" | \"hd\";\n  /**\n   * The number of images to generate.\n   * Must be between 1 and 10.\n   * For dall-e-3, only `n: 1` is supported.\n   * @default 1\n   */\n  n?: number;\n  /**\n   * The size of the generated images.\n   * Must be one of 256x256, 512x512, or 1024x1024 for DALLE-2 models.\n   * Must be one of 1024x1024, 1792x1024, or 1024x1792 for DALLE-3 models.\n   * @default \"1024x1024\"\n   */\n  size?: \"256x256\" | \"512x512\" | \"1024x1024\" | \"1792x1024\" | \"1024x1792\";\n  /**\n   * The format in which the generated images are returned.\n   * Must be one of \"url\" or \"b64_json\".\n   * @default \"url\"\n   */\n  dallEResponseFormat?: \"url\" | \"b64_json\";\n  /**\n   * @deprecated Use dallEResponseFormat instead for the Dall-E response type.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  responseFormat?: any;\n  /**\n   * A unique identifier representing your end-user, which will help\n   * OpenAI to monitor and detect abuse.\n   */\n  user?: string;\n  /**\n   * The organization to use\n   */\n  organization?: string;\n  /**\n   * The base URL of the OpenAI API.\n   */\n  baseUrl?: string;\n}\n\n/**\n * A tool for generating images with Open AIs Dall-E 2 or 3 API.\n */\nexport class DallEAPIWrapper extends Tool {\n  static lc_name() {\n    return \"DallEAPIWrapper\";\n  }\n\n  name = \"dalle_api_wrapper\";\n\n  description =\n    \"A wrapper around OpenAI DALL-E API. Useful for when you need to generate images from a text description. Input should be an image description.\";\n\n  protected client: OpenAIClient;\n\n  static readonly toolName = \"dalle_api_wrapper\";\n\n  private model = \"dall-e-3\";\n\n  private style: \"natural\" | \"vivid\" = \"vivid\";\n\n  private quality: \"standard\" | \"hd\" = \"standard\";\n\n  private n = 1;\n\n  private size:\n    | \"256x256\"\n    | \"512x512\"\n    | \"1024x1024\"\n    | \"1792x1024\"\n    | \"1024x1792\" = \"1024x1024\";\n\n  private dallEResponseFormat: \"url\" | \"b64_json\" = \"url\";\n\n  private user?: string;\n\n  constructor(fields?: DallEAPIWrapperParams) {\n    // Shim for new base tool param name\n    if (\n      fields?.responseFormat !== undefined &&\n      [\"url\", \"b64_json\"].includes(fields.responseFormat)\n    ) {\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      fields.dallEResponseFormat = fields.responseFormat as any;\n      fields.responseFormat = \"content\";\n    }\n    super(fields);\n    const openAIApiKey =\n      fields?.apiKey ??\n      fields?.openAIApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n\n    const organization =\n      fields?.organization ?? getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    const clientConfig = {\n      apiKey: openAIApiKey,\n      organization,\n      dangerouslyAllowBrowser: true,\n      baseURL: fields?.baseUrl,\n    };\n    this.client = new OpenAIClient(clientConfig);\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    this.style = fields?.style ?? this.style;\n    this.quality = fields?.quality ?? this.quality;\n    this.n = fields?.n ?? this.n;\n    this.size = fields?.size ?? this.size;\n    this.dallEResponseFormat =\n      fields?.dallEResponseFormat ?? this.dallEResponseFormat;\n    this.user = fields?.user;\n  }\n\n  /**\n   * Processes the API response if multiple images are generated.\n   * Returns a list of MessageContentImageUrl objects. If the response\n   * format is `url`, then the `image_url` field will contain the URL.\n   * If it is `b64_json`, then the `image_url` field will contain an object\n   * with a `url` field with the base64 encoded image.\n   *\n   * @param {OpenAIClient.Images.ImagesResponse[]} response The API response\n   * @returns {MessageContentImageUrl[]}\n   */\n  private processMultipleGeneratedUrls(\n    response: OpenAIClient.Images.ImagesResponse[]\n  ): MessageContentImageUrl[] {\n    if (this.dallEResponseFormat === \"url\") {\n      return response.flatMap((res) => {\n        const imageUrlContent =\n          res.data\n            ?.flatMap((item) => {\n              if (!item.url) return [];\n              return {\n                type: \"image_url\" as const,\n                image_url: item.url,\n              };\n            })\n            .filter(\n              (item) =>\n                item !== undefined &&\n                item.type === \"image_url\" &&\n                typeof item.image_url === \"string\" &&\n                item.image_url !== undefined\n            ) ?? [];\n        return imageUrlContent;\n      });\n    } else {\n      return response.flatMap((res) => {\n        const b64Content =\n          res.data\n            ?.flatMap((item) => {\n              if (!item.b64_json) return [];\n              return {\n                type: \"image_url\" as const,\n                image_url: {\n                  url: item.b64_json,\n                },\n              };\n            })\n            .filter(\n              (item) =>\n                item !== undefined &&\n                item.type === \"image_url\" &&\n                typeof item.image_url === \"object\" &&\n                \"url\" in item.image_url &&\n                typeof item.image_url.url === \"string\" &&\n                item.image_url.url !== undefined\n            ) ?? [];\n        return b64Content;\n      });\n    }\n  }\n\n  /** @ignore */\n  async _call(input: string): Promise<string | MessageContentComplex[]> {\n    const generateImageFields = {\n      model: this.model,\n      prompt: input,\n      n: 1,\n      size: this.size,\n      response_format: this.dallEResponseFormat,\n      style: this.style,\n      quality: this.quality,\n      user: this.user,\n    };\n\n    if (this.n > 1) {\n      const results = await Promise.all(\n        Array.from({ length: this.n }).map(() =>\n          this.client.images.generate(generateImageFields)\n        )\n      );\n\n      return this.processMultipleGeneratedUrls(results);\n    }\n\n    const response = await this.client.images.generate(generateImageFields);\n\n    let data = \"\";\n    if (this.dallEResponseFormat === \"url\") {\n      [data] =\n        response.data\n          ?.map((item) => item.url)\n          .filter((url): url is string => url !== \"undefined\") ?? [];\n    } else {\n      [data] =\n        response.data\n          ?.map((item) => item.b64_json)\n          .filter((b64_json): b64_json is string => b64_json !== \"undefined\") ??\n        [];\n    }\n    return data;\n  }\n}\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * User location configuration for geographic search refinement.\n */\nexport interface WebSearchUserLocation {\n  /**\n   * The type of location. Currently only \"approximate\" is supported.\n   */\n  type: \"approximate\";\n  /**\n   * Two-letter ISO country code (e.g., \"US\", \"GB\").\n   * @see https://en.wikipedia.org/wiki/ISO_3166-1\n   */\n  country?: string;\n  /**\n   * City name (e.g., \"San Francisco\", \"London\").\n   */\n  city?: string;\n  /**\n   * Region or state name (e.g., \"California\", \"London\").\n   */\n  region?: string;\n  /**\n   * IANA timezone (e.g., \"America/Los_Angeles\", \"Europe/London\").\n   * @see https://timeapi.io/documentation/iana-timezones\n   */\n  timezone?: string;\n}\n\n/**\n * Domain filtering configuration for web search.\n */\nexport interface WebSearchFilters {\n  /**\n   * Allow-list of up to 100 domains to limit search results.\n   * Omit the HTTP/HTTPS prefix (e.g., \"openai.com\" instead of \"https://openai.com/\").\n   * Includes subdomains automatically.\n   */\n  allowedDomains?: string[];\n}\n\n/**\n * Options for the OpenAI web search tool.\n */\nexport interface WebSearchOptions {\n  /**\n   * Domain filtering configuration.\n   * Limit results to a specific set of up to 100 domains.\n   */\n  filters?: WebSearchFilters;\n  /**\n   * Approximate user location for geographic search refinement.\n   * Not supported for deep research models.\n   */\n  userLocation?: WebSearchUserLocation;\n  /**\n   * High level guidance for the amount of context window space to use for the\n   * search. One of `low`, `medium`, or `high`. `medium` is the default.\n   */\n  search_context_size?: \"low\" | \"medium\" | \"high\";\n}\n\n/**\n * OpenAI web search tool type for the Responses API.\n */\nexport type WebSearchTool = OpenAIClient.Responses.WebSearchTool;\n\n/**\n * Creates a web search tool that allows OpenAI models to search the web\n * for up-to-date information before generating a response.\n *\n * Web search supports three main types:\n * 1. **Non-reasoning web search**: Quick lookups where the model passes queries\n *    directly to the search tool.\n * 2. **Agentic search with reasoning models**: The model actively manages the\n *    search process, analyzing results and deciding whether to keep searching.\n * 3. **Deep research**: Extended investigations using models like `o3-deep-research`\n *    or `gpt-5` with high reasoning effort.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-web-search | OpenAI Web Search Documentation}\n * @param options - Configuration options for the web search tool\n * @returns A web search tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({\n *   model: \"gpt-4o\",\n * });\n *\n * // Basic usage\n * const response = await model.invoke(\"What was a positive news story from today?\", {\n *   tools: [tools.webSearch()],\n * });\n *\n * // With domain filtering\n * const filteredResponse = await model.invoke(\"Latest AI research news\", {\n *   tools: [tools.webSearch({\n *     filters: {\n *       allowedDomains: [\"arxiv.org\", \"nature.com\", \"science.org\"],\n *     },\n *   })],\n * });\n *\n * // With user location for geographic relevance\n * const localResponse = await model.invoke(\"What are the best restaurants near me?\", {\n *   tools: [tools.webSearch({\n *     userLocation: {\n *       type: \"approximate\",\n *       country: \"US\",\n *       city: \"San Francisco\",\n *       region: \"California\",\n *       timezone: \"America/Los_Angeles\",\n *     },\n *   })],\n * });\n *\n * // Cache-only mode (no live internet access)\n * const cachedResponse = await model.invoke(\"Find information about OpenAI\", {\n *   tools: [tools.webSearch({\n *     externalWebAccess: false,\n *   })],\n * });\n * ```\n */\nexport function webSearch(options?: WebSearchOptions): ServerTool {\n  return {\n    type: \"web_search\",\n    filters: options?.filters?.allowedDomains\n      ? { allowed_domains: options.filters.allowedDomains }\n      : undefined,\n    user_location: options?.userLocation,\n    search_context_size: options?.search_context_size,\n  } satisfies WebSearchTool;\n}\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Available connector IDs for OpenAI's built-in service connectors.\n * These are OpenAI-maintained MCP wrappers for popular services.\n */\nexport type McpConnectorId =\n  | \"connector_dropbox\"\n  | \"connector_gmail\"\n  | \"connector_googlecalendar\"\n  | \"connector_googledrive\"\n  | \"connector_microsoftteams\"\n  | \"connector_outlookcalendar\"\n  | \"connector_outlookemail\"\n  | \"connector_sharepoint\";\n\n/**\n * Filter object to specify which tools are allowed.\n */\nexport interface McpToolFilter {\n  /**\n   * List of allowed tool names.\n   */\n  toolNames?: string[];\n  /**\n   * Indicates whether or not a tool modifies data or is read-only.\n   * If an MCP server is annotated with `readOnlyHint`, it will match this filter.\n   */\n  readOnly?: boolean;\n}\n\n/**\n * Filter object for approval requirements.\n */\nexport interface McpApprovalFilter {\n  /**\n   * Tools that always require approval before execution.\n   */\n  always?: McpToolFilter;\n  /**\n   * Tools that never require approval.\n   */\n  never?: McpToolFilter;\n}\n\n/**\n * Base options shared between remote MCP servers and connectors.\n */\ninterface McpBaseOptions {\n  /**\n   * A label for this MCP server, used to identify it in tool calls.\n   */\n  serverLabel: string;\n  /**\n   * List of allowed tool names or a filter object.\n   * Use this to limit which tools from the MCP server are available to the model.\n   */\n  allowedTools?: string[] | McpToolFilter;\n  /**\n   * An OAuth access token for authentication with the MCP server.\n   * Your application must handle the OAuth authorization flow and provide the token here.\n   */\n  authorization?: string;\n  /**\n   * Optional HTTP headers to send to the MCP server.\n   * Use for authentication or other purposes.\n   */\n  headers?: Record<string, string>;\n  /**\n   * Specify which of the MCP server's tools require approval before execution.\n   * - `\"always\"`: All tools require approval\n   * - `\"never\"`: No tools require approval\n   * - `McpApprovalFilter`: Fine-grained control over which tools require approval\n   *\n   * @default \"always\" (approval required for all tools)\n   */\n  requireApproval?: \"always\" | \"never\" | McpApprovalFilter;\n  /**\n   * Optional description of the MCP server, used to provide more context to the model.\n   */\n  serverDescription?: string;\n}\n\n/**\n * Options for connecting to a remote MCP server via URL.\n */\nexport interface McpRemoteServerOptions extends McpBaseOptions {\n  /**\n   * The URL for the MCP server.\n   * The server must implement the Streamable HTTP or HTTP/SSE transport protocol.\n   */\n  serverUrl: string;\n}\n\n/**\n * Options for connecting to an OpenAI-maintained service connector.\n */\nexport interface McpConnectorOptions extends McpBaseOptions {\n  /**\n   * Identifier for the service connector.\n   * These are OpenAI-maintained MCP wrappers for popular services.\n   *\n   * Available connectors:\n   * - `connector_dropbox`: Dropbox file access\n   * - `connector_gmail`: Gmail email access\n   * - `connector_googlecalendar`: Google Calendar access\n   * - `connector_googledrive`: Google Drive file access\n   * - `connector_microsoftteams`: Microsoft Teams access\n   * - `connector_outlookcalendar`: Outlook Calendar access\n   * - `connector_outlookemail`: Outlook Email access\n   * - `connector_sharepoint`: SharePoint file access\n   */\n  connectorId: McpConnectorId;\n}\n\n/**\n * OpenAI MCP tool type for the Responses API.\n */\nexport type McpTool = OpenAIClient.Responses.Tool.Mcp;\n\n/**\n * Converts a McpToolFilter to the API format.\n */\nfunction convertToolFilter(\n  filter: McpToolFilter\n): OpenAIClient.Responses.Tool.Mcp.McpToolFilter {\n  return {\n    tool_names: filter.toolNames,\n    read_only: filter.readOnly,\n  };\n}\n\n/**\n * Converts allowed_tools option to API format.\n */\nfunction convertAllowedTools(\n  allowedTools: string[] | McpToolFilter | undefined\n): Array<string> | OpenAIClient.Responses.Tool.Mcp.McpToolFilter | undefined {\n  if (!allowedTools) return undefined;\n  if (Array.isArray(allowedTools)) return allowedTools;\n  return convertToolFilter(allowedTools);\n}\n\n/**\n * Converts require_approval option to API format.\n */\nfunction convertRequireApproval(\n  requireApproval: \"always\" | \"never\" | McpApprovalFilter | undefined\n):\n  | OpenAIClient.Responses.Tool.Mcp.McpToolApprovalFilter\n  | \"always\"\n  | \"never\"\n  | undefined {\n  if (!requireApproval) return undefined;\n  if (typeof requireApproval === \"string\") return requireApproval;\n  return {\n    always: requireApproval.always\n      ? convertToolFilter(requireApproval.always)\n      : undefined,\n    never: requireApproval.never\n      ? convertToolFilter(requireApproval.never)\n      : undefined,\n  };\n}\n\n/**\n * Creates an MCP tool that connects to a remote MCP server or OpenAI service connector.\n * This allows OpenAI models to access external tools and services via the Model Context Protocol.\n *\n * There are two ways to use MCP tools:\n * 1. **Remote MCP servers**: Connect to any server on the public Internet that implements\n *    the MCP protocol using `serverUrl`.\n * 2. **Connectors**: Use OpenAI-maintained MCP wrappers for popular services like\n *    Google Workspace or Dropbox using `connectorId`.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-remote-mcp | OpenAI MCP Documentation}\n *\n * @param options - Configuration options for the MCP tool\n * @returns An MCP tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n *\n * // Using a remote MCP server\n * const response = await model.invoke(\"Roll 2d4+1\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"dmcp\",\n *     serverDescription: \"A D&D MCP server for dice rolling\",\n *     serverUrl: \"https://dmcp-server.deno.dev/sse\",\n *     requireApproval: \"never\",\n *   })],\n * });\n *\n * // Using a connector (e.g., Google Calendar)\n * const calendarResponse = await model.invoke(\"What's on my calendar today?\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"google_calendar\",\n *     connectorId: \"connector_googlecalendar\",\n *     authorization: \"<oauth-access-token>\",\n *     requireApproval: \"never\",\n *   })],\n * });\n *\n * // With tool filtering - only allow specific tools\n * const filteredResponse = await model.invoke(\"Roll some dice\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"dmcp\",\n *     serverUrl: \"https://dmcp-server.deno.dev/sse\",\n *     allowedTools: [\"roll\"],  // Only allow the \"roll\" tool\n *     requireApproval: \"never\",\n *   })],\n * });\n *\n * // With fine-grained approval control\n * const controlledResponse = await model.invoke(\"Search and modify files\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"deepwiki\",\n *     serverUrl: \"https://mcp.deepwiki.com/mcp\",\n *     requireApproval: {\n *       never: { toolNames: [\"ask_question\", \"read_wiki_structure\"] },\n *       // All other tools will require approval\n *     },\n *   })],\n * });\n * ```\n */\nexport function mcp(options: McpRemoteServerOptions): ServerTool;\nexport function mcp(options: McpConnectorOptions): ServerTool;\nexport function mcp(\n  options: McpRemoteServerOptions | McpConnectorOptions\n): ServerTool {\n  const baseConfig: McpTool = {\n    type: \"mcp\",\n    server_label: options.serverLabel,\n    allowed_tools: convertAllowedTools(options.allowedTools),\n    authorization: options.authorization,\n    headers: options.headers,\n    require_approval: convertRequireApproval(options.requireApproval),\n    server_description: options.serverDescription,\n  };\n\n  if (\"serverUrl\" in options) {\n    return {\n      ...baseConfig,\n      server_url: options.serverUrl,\n    } satisfies McpTool;\n  }\n\n  return {\n    ...baseConfig,\n    connector_id: options.connectorId,\n  } satisfies McpTool;\n}\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Memory limit options for the Code Interpreter container.\n * Higher tiers offer more RAM and are billed at different rates.\n */\nexport type CodeInterpreterMemoryLimit = \"1g\" | \"4g\" | \"16g\" | \"64g\";\n\n/**\n * Auto container configuration for Code Interpreter.\n * Creates a new container automatically or reuses an active one.\n */\nexport interface CodeInterpreterAutoContainer {\n  /**\n   * Memory limit for the container.\n   * - `\"1g\"` (default): 1 GB RAM\n   * - `\"4g\"`: 4 GB RAM\n   * - `\"16g\"`: 16 GB RAM\n   * - `\"64g\"`: 64 GB RAM\n   */\n  memoryLimit?: CodeInterpreterMemoryLimit;\n  /**\n   * Optional list of uploaded file IDs to make available to the code.\n   * Files in the model input are automatically uploaded, so this is only\n   * needed for additional files.\n   */\n  fileIds?: string[];\n}\n\n/**\n * Options for the Code Interpreter tool.\n */\nexport interface CodeInterpreterOptions {\n  /**\n   * The container configuration for the Code Interpreter.\n   *\n   * Can be either:\n   * - A string container ID for explicit mode (created via `/v1/containers` endpoint)\n   * - An auto configuration object that creates/reuses containers automatically\n   *\n   * If not provided, defaults to auto mode with default settings.\n   */\n  container?: string | CodeInterpreterAutoContainer;\n}\n\n/**\n * OpenAI Code Interpreter tool type for the Responses API.\n */\nexport type CodeInterpreterTool = OpenAIClient.Responses.Tool.CodeInterpreter;\n\n/**\n * Converts container options to the API format.\n */\nfunction convertContainer(\n  container: string | CodeInterpreterAutoContainer | undefined\n):\n  | string\n  | OpenAIClient.Responses.Tool.CodeInterpreter.CodeInterpreterToolAuto {\n  // If a string container ID is provided, use it directly\n  if (typeof container === \"string\") {\n    return container;\n  }\n\n  // Auto mode configuration\n  return {\n    type: \"auto\",\n    file_ids: container?.fileIds,\n    memory_limit: container?.memoryLimit,\n  };\n}\n\n/**\n * Creates a Code Interpreter tool that allows models to write and run Python code\n * in a sandboxed environment to solve complex problems.\n *\n * Use Code Interpreter for:\n * - **Data analysis**: Processing files with diverse data and formatting\n * - **File generation**: Creating files with data and images of graphs\n * - **Iterative coding**: Writing and running code iteratively to solve problems\n * - **Visual intelligence**: Cropping, zooming, rotating, and transforming images\n *\n * The tool runs in a container, which is a fully sandboxed virtual machine.\n * Containers can be created automatically (auto mode) or explicitly via the\n * `/v1/containers` endpoint.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-code-interpreter | OpenAI Code Interpreter Documentation}\n *\n * @param options - Configuration options for the Code Interpreter tool\n * @returns A Code Interpreter tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4.1\" });\n *\n * // Basic usage with auto container (default 1GB memory)\n * const response = await model.invoke(\n *   \"Solve the equation 3x + 11 = 14\",\n *   { tools: [tools.codeInterpreter()] }\n * );\n *\n * // With increased memory limit for larger computations\n * const response = await model.invoke(\n *   \"Analyze this large dataset and create visualizations\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: { memoryLimit: \"4g\" }\n *     })]\n *   }\n * );\n *\n * // With specific files available to the code\n * const response = await model.invoke(\n *   \"Process the uploaded CSV file\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: {\n *         memoryLimit: \"4g\",\n *         fileIds: [\"file-abc123\", \"file-def456\"]\n *       }\n *     })]\n *   }\n * );\n *\n * // Using an explicit container ID (created via /v1/containers)\n * const response = await model.invoke(\n *   \"Continue working with the data\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: \"cntr_abc123\"\n *     })]\n *   }\n * );\n * ```\n *\n * @remarks\n * - Containers expire after 20 minutes of inactivity\n * - While called \"Code Interpreter\", the model knows it as the \"python tool\"\n * - For explicit prompting, ask for \"the python tool\" in your prompts\n * - Files in model input are automatically uploaded to the container\n * - Generated files are returned as `container_file_citation` annotations\n */\nexport function codeInterpreter(options?: CodeInterpreterOptions): ServerTool {\n  return {\n    type: \"code_interpreter\",\n    container: convertContainer(options?.container),\n  } satisfies CodeInterpreterTool;\n}\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Comparison operators for file attribute filtering.\n */\nexport type FileSearchComparisonType =\n  | \"eq\"\n  | \"ne\"\n  | \"gt\"\n  | \"gte\"\n  | \"lt\"\n  | \"lte\";\n\n/**\n * A filter used to compare a specified attribute key to a given value.\n */\nexport interface FileSearchComparisonFilter {\n  /**\n   * The comparison operator to use.\n   * - `eq`: equals\n   * - `ne`: not equal\n   * - `gt`: greater than\n   * - `gte`: greater than or equal\n   * - `lt`: less than\n   * - `lte`: less than or equal\n   */\n  type: FileSearchComparisonType;\n  /**\n   * The attribute key to compare against.\n   */\n  key: string;\n  /**\n   * The value to compare against the attribute key.\n   * Supports string, number, boolean, or array of strings/numbers.\n   */\n  value: string | number | boolean | Array<string | number>;\n}\n\n/**\n * Combine multiple filters using `and` or `or`.\n */\nexport interface FileSearchCompoundFilter {\n  /**\n   * Type of operation: `and` or `or`.\n   */\n  type: \"and\" | \"or\";\n  /**\n   * Array of filters to combine.\n   */\n  filters: Array<FileSearchComparisonFilter | FileSearchCompoundFilter>;\n}\n\n/**\n * Filter type for file search - can be a comparison or compound filter.\n */\nexport type FileSearchFilter =\n  | FileSearchComparisonFilter\n  | FileSearchCompoundFilter;\n\n/**\n * Weights for hybrid search balancing semantic and keyword matches.\n */\nexport interface FileSearchHybridSearchWeights {\n  /**\n   * The weight of semantic embedding matches (0-1).\n   */\n  embeddingWeight: number;\n  /**\n   * The weight of keyword/text matches (0-1).\n   */\n  textWeight: number;\n}\n\n/**\n * Ranking options for file search results.\n */\nexport interface FileSearchRankingOptions {\n  /**\n   * The ranker to use for the file search.\n   * - `auto`: Automatically select the best ranker\n   * - `default-2024-11-15`: Default ranker as of November 2024\n   */\n  ranker?: \"auto\" | \"default-2024-11-15\";\n  /**\n   * The score threshold for results (0-1).\n   * Numbers closer to 1 return only the most relevant results but may return fewer.\n   */\n  scoreThreshold?: number;\n  /**\n   * Weights that control how reciprocal rank fusion balances semantic\n   * embedding matches versus sparse keyword matches when hybrid search is enabled.\n   */\n  hybridSearch?: FileSearchHybridSearchWeights;\n}\n\n/**\n * Options for the File Search tool.\n */\nexport interface FileSearchOptions {\n  /**\n   * The IDs of the vector stores to search.\n   * You must have previously created vector stores and uploaded files to them.\n   */\n  vectorStoreIds: string[];\n  /**\n   * The maximum number of results to return.\n   * Must be between 1 and 50 inclusive.\n   */\n  maxNumResults?: number;\n  /**\n   * A filter to apply based on file attributes/metadata.\n   * Use this to narrow down search results to specific categories or file types.\n   */\n  filters?: FileSearchFilter;\n  /**\n   * Ranking options to customize how results are scored and ordered.\n   */\n  rankingOptions?: FileSearchRankingOptions;\n}\n\n/**\n * OpenAI File Search tool type for the Responses API.\n */\nexport type FileSearchTool = OpenAIClient.Responses.FileSearchTool;\n\n/**\n * Converts ranking options to the API format.\n */\nfunction convertRankingOptions(\n  options: FileSearchRankingOptions | undefined\n): OpenAIClient.Responses.FileSearchTool.RankingOptions | undefined {\n  if (!options) return undefined;\n  return {\n    ranker: options.ranker,\n    score_threshold: options.scoreThreshold,\n    hybrid_search: options.hybridSearch\n      ? {\n          embedding_weight: options.hybridSearch.embeddingWeight,\n          text_weight: options.hybridSearch.textWeight,\n        }\n      : undefined,\n  };\n}\n\n/**\n * Creates a File Search tool that allows models to search your files\n * for relevant information using semantic and keyword search.\n *\n * File Search enables models to retrieve information from a knowledge base\n * of previously uploaded files stored in vector stores. This is a hosted tool\n * managed by OpenAI - you don't need to implement the search execution yourself.\n *\n * **Prerequisites**: Before using File Search, you must:\n * 1. Upload files to the File API with `purpose: \"assistants\"`\n * 2. Create a vector store\n * 3. Add files to the vector store\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-file-search | OpenAI File Search Documentation}\n *\n * @param options - Configuration options for the File Search tool\n * @returns A File Search tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4.1\" });\n *\n * // Basic usage with a vector store\n * const response = await model.invoke(\n *   \"What is deep research by OpenAI?\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"]\n *     })]\n *   }\n * );\n *\n * // Limit the number of results for lower latency\n * const response = await model.invoke(\n *   \"Find information about pricing\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       maxNumResults: 5\n *     })]\n *   }\n * );\n *\n * // With metadata filtering\n * const response = await model.invoke(\n *   \"Find recent blog posts about AI\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       filters: {\n *         type: \"eq\",\n *         key: \"category\",\n *         value: \"blog\"\n *       }\n *     })]\n *   }\n * );\n *\n * // With compound filters (AND/OR)\n * const response = await model.invoke(\n *   \"Find technical docs from 2024\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       filters: {\n *         type: \"and\",\n *         filters: [\n *           { type: \"eq\", key: \"category\", value: \"technical\" },\n *           { type: \"gte\", key: \"year\", value: 2024 }\n *         ]\n *       }\n *     })]\n *   }\n * );\n *\n * // With ranking options for more relevant results\n * const response = await model.invoke(\n *   \"Find the most relevant information\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       rankingOptions: {\n *         scoreThreshold: 0.8,\n *         ranker: \"auto\"\n *       }\n *     })]\n *   }\n * );\n *\n * // Search multiple vector stores\n * const response = await model.invoke(\n *   \"Search across all knowledge bases\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\", \"vs_def456\"]\n *     })]\n *   }\n * );\n * ```\n *\n * @remarks\n * - Vector stores must be created and populated before using this tool\n * - The tool returns file citations in the response annotations\n * - Use `include: [\"file_search_call.results\"]` in the API call to get search results\n * - Supported file types include PDF, DOCX, TXT, MD, and many code file formats\n */\nexport function fileSearch(options: FileSearchOptions): ServerTool {\n  return {\n    type: \"file_search\",\n    vector_store_ids: options.vectorStoreIds,\n    max_num_results: options.maxNumResults,\n    filters: options.filters as FileSearchTool[\"filters\"],\n    ranking_options: convertRankingOptions(options.rankingOptions),\n  } satisfies FileSearchTool;\n}\n", "import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Optional mask for inpainting. Allows you to specify areas of the image\n * that should be regenerated.\n */\nexport interface ImageGenerationInputMask {\n  /**\n   * Base64-encoded mask image URL.\n   */\n  imageUrl?: string;\n  /**\n   * File ID for the mask image (uploaded via OpenAI File API).\n   */\n  fileId?: string;\n}\n\n/**\n * Options for the Image Generation tool.\n */\nexport interface ImageGenerationOptions {\n  /**\n   * Whether to generate a new image or edit an existing image.\n   * - `generate`: Generate a new image from scratch\n   * - `edit`: Edit an existing image\n   * - `auto`: Let the model decide based on the input\n   * @default \"auto\"\n   */\n  action?: \"generate\" | \"edit\" | \"auto\";\n\n  /**\n   * Background type for the generated image.\n   * - `transparent`: Generate image with transparent background\n   * - `opaque`: Generate image with opaque background\n   * - `auto`: Let the model decide based on the prompt\n   * @default \"auto\"\n   */\n  background?: \"transparent\" | \"opaque\" | \"auto\";\n\n  /**\n   * Control how much effort the model will exert to match the style and features,\n   * especially facial features, of input images. This parameter is only supported\n   * for `gpt-image-1`. Unsupported for `gpt-image-1-mini`.\n   * - `high`: Higher fidelity to input images\n   * - `low`: Lower fidelity to input images\n   * @default \"low\"\n   */\n  inputFidelity?: \"high\" | \"low\";\n\n  /**\n   * Optional mask for inpainting. Use this to specify areas of an image\n   * that should be regenerated.\n   */\n  inputImageMask?: ImageGenerationInputMask;\n\n  /**\n   * The image generation model to use.\n   * @default \"gpt-image-1\"\n   */\n  model?: \"gpt-image-1\" | \"gpt-image-1-mini\" | \"gpt-image-1.5\";\n\n  /**\n   * Moderation level for the generated image.\n   * - `auto`: Standard moderation\n   * - `low`: Less restrictive moderation\n   * @default \"auto\"\n   */\n  moderation?: \"auto\" | \"low\";\n\n  /**\n   * Compression level for the output image (0-100).\n   * Only applies to JPEG and WebP formats.\n   * @default 100\n   */\n  outputCompression?: number;\n\n  /**\n   * The output format of the generated image.\n   * @default \"png\"\n   */\n  outputFormat?: \"png\" | \"webp\" | \"jpeg\";\n\n  /**\n   * Number of partial images to generate in streaming mode (0-3).\n   * When set, the model will return partial images as they are generated,\n   * providing faster visual feedback.\n   * @default 0\n   */\n  partialImages?: number;\n\n  /**\n   * The quality of the generated image.\n   * - `low`: Faster generation, lower quality\n   * - `medium`: Balanced generation time and quality\n   * - `high`: Slower generation, higher quality\n   * - `auto`: Let the model decide based on the prompt\n   * @default \"auto\"\n   */\n  quality?: \"low\" | \"medium\" | \"high\" | \"auto\";\n\n  /**\n   * The size of the generated image.\n   * - `1024x1024`: Square format\n   * - `1024x1536`: Portrait format\n   * - `1536x1024`: Landscape format\n   * - `auto`: Let the model decide based on the prompt\n   * @default \"auto\"\n   */\n  size?: \"1024x1024\" | \"1024x1536\" | \"1536x1024\" | \"auto\";\n}\n\n/**\n * OpenAI Image Generation tool type for the Responses API.\n */\nexport type ImageGenerationTool = OpenAIClient.Responses.Tool.ImageGeneration;\n\n/**\n * Converts input mask options to the API format.\n */\nfunction convertInputImageMask(\n  mask: ImageGenerationInputMask | undefined\n): ImageGenerationTool[\"input_image_mask\"] {\n  if (!mask) return undefined;\n  return {\n    image_url: mask.imageUrl,\n    file_id: mask.fileId,\n  };\n}\n\n/**\n * Creates an Image Generation tool that allows models to generate or edit images\n * using text prompts and optional image inputs.\n *\n * The image generation tool leverages the GPT Image model and automatically\n * optimizes text inputs for improved performance. When included in a request,\n * the model can decide when and how to generate images as part of the conversation.\n *\n * **Key Features**:\n * - Generate images from text descriptions\n * - Edit existing images with text instructions\n * - Multi-turn image editing by referencing previous responses\n * - Configurable output options (size, quality, format)\n * - Streaming support for partial image generation\n *\n * **Prompting Tips**:\n * - Use terms like \"draw\" or \"edit\" in your prompt for best results\n * - For combining images, say \"edit the first image by adding this element\" instead of \"combine\"\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-image-generation | OpenAI Image Generation Documentation}\n *\n * @param options - Configuration options for the Image Generation tool\n * @returns An Image Generation tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n *\n * // Basic usage - generate an image\n * const response = await model.invoke(\n *   \"Generate an image of a gray tabby cat hugging an otter with an orange scarf\",\n *   { tools: [tools.imageGeneration()] }\n * );\n *\n * // Access the generated image\n * const imageData = response.additional_kwargs.tool_outputs?.find(\n *   (output) => output.type === \"image_generation_call\"\n * );\n * if (imageData?.result) {\n *   // imageData.result contains the base64-encoded image\n *   const fs = await import(\"fs\");\n *   fs.writeFileSync(\"output.png\", Buffer.from(imageData.result, \"base64\"));\n * }\n *\n * // With custom options\n * const response = await model.invoke(\n *   \"Draw a beautiful sunset over mountains\",\n *   {\n *     tools: [tools.imageGeneration({\n *       size: \"1536x1024\",      // Landscape format\n *       quality: \"high\",        // Higher quality output\n *       outputFormat: \"jpeg\",   // JPEG format\n *       outputCompression: 90,  // 90% compression\n *     })]\n *   }\n * );\n *\n * // With transparent background\n * const response = await model.invoke(\n *   \"Create a logo with a transparent background\",\n *   {\n *     tools: [tools.imageGeneration({\n *       background: \"transparent\",\n *       outputFormat: \"png\",\n *     })]\n *   }\n * );\n *\n * // Force the model to use image generation\n * const response = await model.invoke(\n *   \"A serene lake at dawn\",\n *   {\n *     tools: [tools.imageGeneration()],\n *     tool_choice: { type: \"image_generation\" },\n *   }\n * );\n *\n * // Enable streaming with partial images\n * const response = await model.invoke(\n *   \"Draw a detailed fantasy castle\",\n *   {\n *     tools: [tools.imageGeneration({\n *       partialImages: 2,  // Get 2 partial images during generation\n *     })]\n *   }\n * );\n * ```\n *\n * @remarks\n * - Supported models: gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3\n * - The image generation process always uses `gpt-image-1` model internally\n * - The model will automatically revise prompts for improved performance\n * - Access the revised prompt via `revised_prompt` field in the output\n * - Multi-turn editing is supported by passing previous response messages\n */\nexport function imageGeneration(options?: ImageGenerationOptions): ServerTool {\n  return {\n    type: \"image_generation\",\n    action: options?.action,\n    background: options?.background,\n    input_fidelity: options?.inputFidelity,\n    input_image_mask: convertInputImageMask(options?.inputImageMask),\n    model: options?.model,\n    moderation: options?.moderation,\n    output_compression: options?.outputCompression,\n    output_format: options?.outputFormat,\n    partial_images: options?.partialImages,\n    quality: options?.quality,\n    size: options?.size,\n  } satisfies ImageGenerationTool;\n}\n", "/* eslint-disable @typescript-eslint/no-explicit-any */\nimport { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\nimport { type ToolRuntime } from \"@langchain/core/tools\";\nimport {\n  ToolMessage,\n  type AIMessage,\n  type BaseMessage,\n} from \"@langchain/core/messages\";\n\n/**\n * The type of computer environment to control.\n */\nexport type ComputerUseEnvironment =\n  | \"browser\"\n  | \"mac\"\n  | \"windows\"\n  | \"linux\"\n  | \"ubuntu\";\n\n/**\n * Re-export action types from OpenAI SDK for convenience.\n */\nexport type ComputerUseClickAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Click;\nexport type ComputerUseDoubleClickAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.DoubleClick;\nexport type ComputerUseDragAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Drag;\nexport type ComputerUseKeypressAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Keypress;\nexport type ComputerUseMoveAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Move;\nexport type ComputerUseScreenshotAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Screenshot;\nexport type ComputerUseScrollAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Scroll;\nexport type ComputerUseTypeAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Type;\nexport type ComputerUseWaitAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Wait;\n\n/**\n * Union type of all computer use actions from OpenAI SDK.\n */\nexport type ComputerUseAction =\n  OpenAIClient.Responses.ResponseComputerToolCall[\"action\"];\n\n// Zod schemas for computer use actions\nconst ComputerUseScreenshotActionSchema = z.object({\n  type: z.literal(\"screenshot\"),\n});\n\nconst ComputerUseClickActionSchema = z.object({\n  type: z.literal(\"click\"),\n  x: z.number(),\n  y: z.number(),\n  button: z.enum([\"left\", \"right\", \"wheel\", \"back\", \"forward\"]).default(\"left\"),\n});\n\nconst ComputerUseDoubleClickActionSchema = z.object({\n  type: z.literal(\"double_click\"),\n  x: z.number(),\n  y: z.number(),\n  button: z.enum([\"left\", \"right\", \"wheel\", \"back\", \"forward\"]).default(\"left\"),\n});\n\nconst ComputerUseDragActionSchema = z.object({\n  type: z.literal(\"drag\"),\n  path: z.array(z.object({ x: z.number(), y: z.number() })),\n});\n\nconst ComputerUseKeypressActionSchema = z.object({\n  type: z.literal(\"keypress\"),\n  keys: z.array(z.string()),\n});\n\nconst ComputerUseMoveActionSchema = z.object({\n  type: z.literal(\"move\"),\n  x: z.number(),\n  y: z.number(),\n});\n\nconst ComputerUseScrollActionSchema = z.object({\n  type: z.literal(\"scroll\"),\n  x: z.number(),\n  y: z.number(),\n  scroll_x: z.number(),\n  scroll_y: z.number(),\n});\n\nconst ComputerUseTypeActionSchema = z.object({\n  type: z.literal(\"type\"),\n  text: z.string(),\n});\n\nconst ComputerUseWaitActionSchema = z.object({\n  type: z.literal(\"wait\"),\n  duration: z.number().optional(),\n});\n\n// Union schema for individual action types\nconst ComputerUseActionUnionSchema = z.union([\n  ComputerUseScreenshotActionSchema,\n  ComputerUseClickActionSchema,\n  ComputerUseDoubleClickActionSchema,\n  ComputerUseDragActionSchema,\n  ComputerUseKeypressActionSchema,\n  ComputerUseMoveActionSchema,\n  ComputerUseScrollActionSchema,\n  ComputerUseTypeActionSchema,\n  ComputerUseWaitActionSchema,\n]);\n\n// Schema for the input structure received from parseComputerCall\n// The action is wrapped in an `action` property: { action: { type: 'screenshot' } }\nexport const ComputerUseActionSchema = z.object({\n  action: ComputerUseActionUnionSchema,\n});\n\n// TypeScript types derived from Zod schemas\nexport type ComputerUseScreenshotActionType = z.infer<\n  typeof ComputerUseScreenshotActionSchema\n>;\nexport type ComputerUseClickActionType = z.infer<\n  typeof ComputerUseClickActionSchema\n>;\nexport type ComputerUseDoubleClickActionType = z.infer<\n  typeof ComputerUseDoubleClickActionSchema\n>;\nexport type ComputerUseDragActionType = z.infer<\n  typeof ComputerUseDragActionSchema\n>;\nexport type ComputerUseKeypressActionType = z.infer<\n  typeof ComputerUseKeypressActionSchema\n>;\nexport type ComputerUseMoveActionType = z.infer<\n  typeof ComputerUseMoveActionSchema\n>;\nexport type ComputerUseScrollActionType = z.infer<\n  typeof ComputerUseScrollActionSchema\n>;\nexport type ComputerUseTypeActionType = z.infer<\n  typeof ComputerUseTypeActionSchema\n>;\nexport type ComputerUseWaitActionType = z.infer<\n  typeof ComputerUseWaitActionSchema\n>;\n\n/**\n * Input structure for the Computer Use tool.\n * The action is wrapped in an `action` property.\n */\nexport interface ComputerUseInput {\n  action: ComputerUseAction;\n}\n\nexport type ComputerUseReturnType =\n  | string\n  | Promise<string>\n  | ToolMessage<any>\n  | Promise<ToolMessage<any>>;\n\n/**\n * Options for the Computer Use tool.\n */\nexport interface ComputerUseOptions {\n  /**\n   * The width of the computer display in pixels.\n   */\n  displayWidth: number;\n\n  /**\n   * The height of the computer display in pixels.\n   */\n  displayHeight: number;\n\n  /**\n   * The type of computer environment to control.\n   * - `browser`: Browser automation (recommended for most use cases)\n   * - `mac`: macOS environment\n   * - `windows`: Windows environment\n   * - `linux`: Linux environment\n   * - `ubuntu`: Ubuntu environment\n   */\n  environment: ComputerUseEnvironment;\n\n  /**\n   * Execute function that handles computer action execution.\n   * This function receives the action input and should return a base64-encoded\n   * screenshot of the result.\n   */\n  execute: (\n    action: ComputerUseAction,\n    runtime: ToolRuntime<any, any>\n  ) => ComputerUseReturnType;\n}\n\n/**\n * OpenAI Computer Use tool type for the Responses API.\n */\nexport type ComputerUseTool = OpenAIClient.Responses.ComputerTool;\n\nconst TOOL_NAME = \"computer_use\";\n\n/**\n * Creates a Computer Use tool that allows models to control computer interfaces\n * and perform tasks by simulating mouse clicks, keyboard input, scrolling, and more.\n *\n * **Computer Use** is a practical application of OpenAI's Computer-Using Agent (CUA)\n * model (`computer-use-preview`), which combines vision capabilities with advanced\n * reasoning to simulate controlling computer interfaces.\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends computer actions (click, type, scroll, etc.)\n * 2. Your code executes these actions in a controlled environment\n * 3. You capture a screenshot of the result\n * 4. Send the screenshot back to the model\n * 5. Repeat until the task is complete\n *\n * **Important**: Computer use is in beta and requires careful consideration:\n * - Use in sandboxed environments only\n * - Do not use for high-stakes or authenticated tasks\n * - Always implement human-in-the-loop for important decisions\n * - Handle safety checks appropriately\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-computer-use | OpenAI Computer Use Documentation}\n *\n * @param options - Configuration options for the Computer Use tool\n * @returns A Computer Use tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"computer-use-preview\" });\n *\n * // With execute callback for automatic action handling\n * const computer = tools.computerUse({\n *   displayWidth: 1024,\n *   displayHeight: 768,\n *   environment: \"browser\",\n *   execute: async (action) => {\n *     if (action.type === \"screenshot\") {\n *       return captureScreenshot();\n *     }\n *     if (action.type === \"click\") {\n *       await page.mouse.click(action.x, action.y, { button: action.button });\n *       return captureScreenshot();\n *     }\n *     if (action.type === \"type\") {\n *       await page.keyboard.type(action.text);\n *       return captureScreenshot();\n *     }\n *     // Handle other actions...\n *     return captureScreenshot();\n *   },\n * });\n *\n * const llmWithComputer = model.bindTools([computer]);\n * const response = await llmWithComputer.invoke(\n *   \"Check the latest news on bing.com\"\n * );\n * ```\n *\n * @example\n * ```typescript\n * // Without execute callback (manual action handling)\n * const computer = tools.computerUse({\n *   displayWidth: 1024,\n *   displayHeight: 768,\n *   environment: \"browser\",\n * });\n *\n * const response = await model.invoke(\"Check the news\", {\n *   tools: [computer],\n * });\n *\n * // Access the computer call from the response\n * const computerCall = response.additional_kwargs.tool_outputs?.find(\n *   (output) => output.type === \"computer_call\"\n * );\n * if (computerCall) {\n *   console.log(\"Action to execute:\", computerCall.action);\n *   // Execute the action manually, then send back a screenshot\n * }\n * ```\n *\n * @example\n * ```typescript\n * // For macOS desktop automation with Docker\n * const computer = tools.computerUse({\n *   displayWidth: 1920,\n *   displayHeight: 1080,\n *   environment: \"mac\",\n *   execute: async (action) => {\n *     if (action.type === \"click\") {\n *       await dockerExec(\n *         `DISPLAY=:99 xdotool mousemove ${action.x} ${action.y} click 1`,\n *         containerName\n *       );\n *     }\n *     // Capture screenshot from container\n *     return await getDockerScreenshot(containerName);\n *   },\n * });\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Requires `computer-use-preview` model\n * - Actions include: click, double_click, drag, keypress, move, screenshot, scroll, type, wait\n * - Safety checks may be returned that require acknowledgment before proceeding\n * - Use `truncation: \"auto\"` parameter when making requests\n * - Recommended to use with `reasoning.summary` for debugging\n */\nexport function computerUse(options: ComputerUseOptions) {\n  const computerTool = tool(\n    async (\n      input: ComputerUseInput,\n      runtime: ToolRuntime<{ messages: BaseMessage[] }>\n    ) => {\n      /**\n       * get computer_use call id from runtime\n       */\n      const aiMessage = runtime.state?.messages.at(-1) as AIMessage | undefined;\n      const computerToolCall = aiMessage?.tool_calls?.find(\n        (tc) => tc.name === \"computer_use\"\n      );\n      const computerToolCallId = computerToolCall?.id;\n      if (!computerToolCallId) {\n        throw new Error(\"Computer use call id not found\");\n      }\n\n      const result = await options.execute(input.action, runtime);\n\n      /**\n       * make sure {@link ToolMessage} is returned with the correct additional kwargs\n       */\n      if (typeof result === \"string\") {\n        return new ToolMessage({\n          content: result,\n          tool_call_id: computerToolCallId,\n          additional_kwargs: {\n            type: \"computer_call_output\",\n          },\n        });\n      }\n\n      /**\n       * make sure {@link ToolMessage} is returned with the correct additional kwargs\n       */\n      return new ToolMessage({\n        ...result,\n        tool_call_id: computerToolCallId,\n        additional_kwargs: {\n          type: \"computer_call_output\",\n          ...result.additional_kwargs,\n        },\n      });\n    },\n    {\n      name: TOOL_NAME,\n      description:\n        \"Control a computer interface by executing mouse clicks, keyboard input, scrolling, and other actions.\",\n      schema: ComputerUseActionSchema,\n    }\n  );\n\n  computerTool.extras = {\n    ...(computerTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"computer_use_preview\",\n      display_width: options.displayWidth,\n      display_height: options.displayHeight,\n      environment: options.environment,\n    } satisfies ComputerUseTool,\n  };\n\n  /**\n   * return as typed {@link DynamicStructuredTool} so we don't get any type\n   * errors like \"can't export tool without reference\"\n   */\n  return computerTool as DynamicStructuredTool<\n    typeof ComputerUseActionSchema,\n    ComputerUseInput,\n    unknown,\n    ComputerUseReturnType\n  >;\n}\n", "import { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\n\n/**\n * Re-export action type from OpenAI SDK for convenience.\n * The action contains command details like argv tokens, environment variables,\n * working directory, timeout, and user.\n */\nexport type LocalShellAction =\n  OpenAIClient.Responses.ResponseOutputItem.LocalShellCall.Action;\n\n// Zod schema for local shell exec action\nexport const LocalShellExecActionSchema = z.object({\n  type: z.literal(\"exec\"),\n  command: z.array(z.string()),\n  env: z.record(z.string(), z.string()).optional(),\n  working_directory: z.string().optional(),\n  timeout_ms: z.number().optional(),\n  user: z.string().optional(),\n});\n\n// Schema for all local shell actions (currently only exec)\nexport const LocalShellActionSchema = z.union([LocalShellExecActionSchema]);\n\n/**\n * Options for the Local Shell tool.\n */\nexport interface LocalShellOptions {\n  /**\n   * Optional execute function that handles shell command execution.\n   * This function receives the action input and should return the command output\n   * (stdout + stderr combined).\n   *\n   * If not provided, you'll need to handle action execution manually by\n   * checking `local_shell_call` outputs in the response.\n   *\n   * @example\n   * ```typescript\n   * execute: async (action) => {\n   *   const result = await exec(action.command.join(' '), {\n   *     cwd: action.working_directory,\n   *     env: { ...process.env, ...action.env },\n   *     timeout: action.timeout_ms,\n   *   });\n   *   return result.stdout + result.stderr;\n   * }\n   * ```\n   */\n  execute: (action: LocalShellAction) => string | Promise<string>;\n}\n\n/**\n * OpenAI Local Shell tool type for the Responses API.\n */\nexport type LocalShellTool = OpenAIClient.Responses.Tool.LocalShell;\n\nconst TOOL_NAME = \"local_shell\";\n\n/**\n * Creates a Local Shell tool that allows models to run shell commands locally\n * on a machine you provide. Commands are executed inside your own runtime\n * the API only returns the instructions, but does not execute them on OpenAI infrastructure.\n *\n * **Important**: The local shell tool is designed to work with\n * [Codex CLI](https://github.com/openai/codex) and the `codex-mini-latest` model.\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends shell commands (`local_shell_call` with `exec` action)\n * 2. Your code executes the command locally\n * 3. You return the output back to the model\n * 4. Repeat until the task is complete\n *\n * **Security Warning**: Running arbitrary shell commands can be dangerous.\n * Always sandbox execution or add strict allow/deny-lists before forwarding\n * a command to the system shell.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-local-shell | OpenAI Local Shell Documentation}\n *\n * @param options - Optional configuration for the Local Shell tool\n * @returns A Local Shell tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n * import { exec } from \"child_process\";\n * import { promisify } from \"util\";\n *\n * const execAsync = promisify(exec);\n * const model = new ChatOpenAI({ model: \"codex-mini-latest\" });\n *\n * // With execute callback for automatic command handling\n * const shell = tools.localShell({\n *   execute: async (action) => {\n *     const { command, env, working_directory, timeout_ms } = action;\n *     const result = await execAsync(command.join(' '), {\n *       cwd: working_directory ?? process.cwd(),\n *       env: { ...process.env, ...env },\n *       timeout: timeout_ms ?? undefined,\n *     });\n *     return result.stdout + result.stderr;\n *   },\n * });\n *\n * const llmWithShell = model.bindTools([shell]);\n * const response = await llmWithShell.invoke(\n *   \"List files in the current directory\"\n * );\n * ```\n *\n * @example\n * ```typescript\n * // Without execute callback (manual handling)\n * const shell = tools.localShell();\n *\n * const response = await model.invoke(\"List files\", {\n *   tools: [shell],\n * });\n *\n * // Access the shell call from the response\n * const shellCall = response.additional_kwargs.tool_outputs?.find(\n *   (output) => output.type === \"local_shell_call\"\n * );\n * if (shellCall) {\n *   console.log(\"Command to execute:\", shellCall.action.command);\n *   // Execute the command manually, then send back the output\n * }\n * ```\n *\n * @example\n * ```typescript\n * // Full shell loop example\n * async function shellLoop(model, task) {\n *   let response = await model.invoke(task, {\n *     tools: [tools.localShell()],\n *   });\n *\n *   while (true) {\n *     const shellCall = response.additional_kwargs.tool_outputs?.find(\n *       (output) => output.type === \"local_shell_call\"\n *     );\n *\n *     if (!shellCall) break;\n *\n *     // Execute command (with proper sandboxing!)\n *     const output = await executeCommand(shellCall.action);\n *\n *     // Send output back to model\n *     response = await model.invoke([\n *       response,\n *       {\n *         type: \"local_shell_call_output\",\n *         id: shellCall.call_id,\n *         output: output,\n *       },\n *     ], {\n *       tools: [tools.localShell()],\n *     });\n *   }\n *\n *   return response;\n * }\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Designed for use with `codex-mini-latest` model\n * - Commands are provided as argv tokens in `action.command`\n * - Action includes: `command`, `env`, `working_directory`, `timeout_ms`, `user`\n * - Always sandbox or validate commands before execution\n * - The `timeout_ms` from the model is only a hintenforce your own limits\n */\nexport function localShell(options: LocalShellOptions) {\n  const shellTool = tool(options.execute, {\n    name: TOOL_NAME,\n    description:\n      \"Execute shell commands locally on the machine. Commands are provided as argv tokens.\",\n    schema: LocalShellActionSchema,\n  });\n\n  shellTool.extras = {\n    ...(shellTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"local_shell\",\n    } satisfies LocalShellTool,\n  };\n\n  return shellTool as DynamicStructuredTool<\n    typeof LocalShellActionSchema,\n    LocalShellAction,\n    unknown,\n    string\n  >;\n}\n", "import { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\n\n/**\n * Re-export action type from OpenAI SDK for convenience.\n * The action contains command details like commands array, timeout, and max output length.\n */\nexport type ShellAction =\n  OpenAIClient.Responses.ResponseFunctionShellToolCall.Action;\n\n// Zod schema for shell action\nexport const ShellActionSchema = z.object({\n  commands: z.array(z.string()).describe(\"Array of shell commands to execute\"),\n  timeout_ms: z\n    .number()\n    .optional()\n    .describe(\"Optional timeout in milliseconds for the commands\"),\n  max_output_length: z\n    .number()\n    .optional()\n    .describe(\n      \"Optional maximum number of characters to return from each command\"\n    ),\n});\n\n/**\n * Result of a single shell command execution.\n * Contains stdout, stderr, and the outcome (exit code or timeout).\n */\nexport type ShellCommandOutput =\n  OpenAIClient.Responses.ResponseFunctionShellCallOutputContent;\n\n/**\n * Outcome type for shell command execution - either exit with code or timeout.\n */\nexport type ShellCallOutcome = ShellCommandOutput[\"outcome\"];\n\n/**\n * Result of executing shell commands.\n * Contains an array of outputs (one per command) and the max_output_length parameter.\n */\nexport interface ShellResult {\n  /**\n   * Array of command outputs. Each entry corresponds to a command from the action.\n   * The order should match the order of commands in the action.\n   */\n  output: ShellCommandOutput[];\n  /**\n   * The max_output_length from the action, which must be passed back to the API.\n   * If not provided in the action, can be omitted.\n   */\n  maxOutputLength?: number | null;\n}\n\n/**\n * Options for the Shell tool.\n */\nexport interface ShellOptions {\n  /**\n   * Execute function that handles shell command execution.\n   * This function receives the action input containing the commands and limits,\n   * and should return a ShellResult with stdout, stderr, and outcome for each command.\n   *\n   * @example\n   * ```typescript\n   * execute: async (action) => {\n   *   const outputs = await Promise.all(\n   *     action.commands.map(async (cmd) => {\n   *       try {\n   *         const { stdout, stderr } = await exec(cmd, {\n   *           timeout: action.timeout_ms ?? undefined,\n   *         });\n   *         return {\n   *           stdout,\n   *           stderr,\n   *           outcome: { type: \"exit\" as const, exit_code: 0 },\n   *         };\n   *       } catch (error) {\n   *         const timedOut = error.killed && error.signal === \"SIGTERM\";\n   *         return {\n   *           stdout: error.stdout ?? \"\",\n   *           stderr: error.stderr ?? String(error),\n   *           outcome: timedOut\n   *             ? { type: \"timeout\" as const }\n   *             : { type: \"exit\" as const, exit_code: error.code ?? 1 },\n   *         };\n   *       }\n   *     })\n   *   );\n   *   return {\n   *     output: outputs,\n   *     maxOutputLength: action.max_output_length,\n   *   };\n   * }\n   * ```\n   */\n  execute: (action: ShellAction) => ShellResult | Promise<ShellResult>;\n}\n\n/**\n * OpenAI Shell tool type for the Responses API.\n */\nexport type ShellTool = OpenAIClient.Responses.FunctionShellTool;\n\nconst TOOL_NAME = \"shell\";\n\n/**\n * Creates a Shell tool that allows models to run shell commands through your integration.\n *\n * The shell tool allows the model to interact with your local computer through a controlled\n * command-line interface. The model proposes shell commands; your integration executes them\n * and returns the outputs. This creates a simple plan-execute loop that lets models inspect\n * the system, run utilities, and gather data until they can finish the task.\n *\n * **Important**: The shell tool is available through the Responses API for use with `GPT-5.1`.\n * It is not available on other models, or via the Chat Completions API.\n *\n * **When to use**:\n * - **Automating filesystem or process diagnostics**  For example, \"find the largest PDF\n *   under ~/Documents\" or \"show running gunicorn processes.\"\n * - **Extending the model's capabilities**  Using built-in UNIX utilities, python runtime\n *   and other CLIs in your environment.\n * - **Running multi-step build and test flows**  Chaining commands like `pip install` and `pytest`.\n * - **Complex agentic coding workflows**  Using other tools like `apply_patch` to complete\n *   workflows that involve complex file operations.\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends shell commands (`shell_call` with `commands` array)\n * 2. Your code executes the commands (can be concurrent)\n * 3. You return stdout, stderr, and outcome for each command\n * 4. Repeat until the task is complete\n *\n * **Security Warning**: Running arbitrary shell commands can be dangerous.\n * Always sandbox execution or add strict allow/deny-lists before forwarding\n * a command to the system shell.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-shell | OpenAI Shell Documentation}\n * @see {@link https://github.com/openai/codex | Codex CLI} for reference implementation.\n *\n * @param options - Configuration for the Shell tool\n * @returns A Shell tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n * import { exec } from \"child_process/promises\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-5.1\" });\n *\n * // With execute callback for automatic command handling\n * const shellTool = tools.shell({\n *   execute: async (action) => {\n *     const outputs = await Promise.all(\n *       action.commands.map(async (cmd) => {\n *         try {\n *           const { stdout, stderr } = await exec(cmd, {\n *             timeout: action.timeout_ms ?? undefined,\n *           });\n *           return {\n *             stdout,\n *             stderr,\n *             outcome: { type: \"exit\" as const, exit_code: 0 },\n *           };\n *         } catch (error) {\n *           const timedOut = error.killed && error.signal === \"SIGTERM\";\n *           return {\n *             stdout: error.stdout ?? \"\",\n *             stderr: error.stderr ?? String(error),\n *             outcome: timedOut\n *               ? { type: \"timeout\" as const }\n *               : { type: \"exit\" as const, exit_code: error.code ?? 1 },\n *           };\n *         }\n *       })\n *     );\n *     return {\n *       output: outputs,\n *       maxOutputLength: action.max_output_length,\n *     };\n *   },\n * });\n *\n * const llmWithShell = model.bindTools([shellTool]);\n * const response = await llmWithShell.invoke(\n *   \"Find the largest PDF file in ~/Documents\"\n * );\n * ```\n *\n * @example\n * ```typescript\n * // Full shell loop example\n * async function shellLoop(model, task) {\n *   let response = await model.invoke(task, {\n *     tools: [tools.shell({ execute: myExecutor })],\n *   });\n *\n *   while (true) {\n *     const shellCall = response.additional_kwargs.tool_outputs?.find(\n *       (output) => output.type === \"shell_call\"\n *     );\n *\n *     if (!shellCall) break;\n *\n *     // Execute commands (with proper sandboxing!)\n *     const result = await executeCommands(shellCall.action);\n *\n *     // Send output back to model\n *     response = await model.invoke([\n *       response,\n *       {\n *         type: \"shell_call_output\",\n *         call_id: shellCall.call_id,\n *         output: result.output,\n *         max_output_length: result.maxOutputLength,\n *       },\n *     ], {\n *       tools: [tools.shell({ execute: myExecutor })],\n *     });\n *   }\n *\n *   return response;\n * }\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Designed for use with `gpt-5.1` model\n * - Commands are provided as an array of strings that can be executed concurrently\n * - Action includes: `commands`, `timeout_ms`, `max_output_length`\n * - Always sandbox or validate commands before execution\n * - The `timeout_ms` from the model is only a hintenforce your own limits\n * - If `max_output_length` exists in the action, always pass it back in the output\n * - Many CLI tools return non-zero exit codes for warnings; still capture stdout/stderr\n */\nexport function shell(options: ShellOptions) {\n  // Wrapper that converts ShellResult to string for LangChain tool compatibility\n  const executeWrapper = async (action: ShellAction): Promise<string> => {\n    const result = await options.execute(action);\n    // Return a JSON string representation for the tool result\n    return JSON.stringify({\n      output: result.output,\n      max_output_length: result.maxOutputLength,\n    });\n  };\n\n  const shellTool = tool(executeWrapper, {\n    name: TOOL_NAME,\n    description:\n      \"Execute shell commands in a managed environment. Commands can be run concurrently.\",\n    schema: ShellActionSchema,\n  });\n\n  shellTool.extras = {\n    ...(shellTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"shell\",\n    } satisfies ShellTool,\n  };\n\n  return shellTool as DynamicStructuredTool<\n    typeof ShellActionSchema,\n    ShellAction,\n    unknown,\n    string\n  >;\n}\n", "import { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\n\n/**\n * Re-export operation types from OpenAI SDK for convenience.\n */\nexport type ApplyPatchCreateFileOperation =\n  OpenAIClient.Responses.ResponseApplyPatchToolCall.CreateFile;\nexport type ApplyPatchUpdateFileOperation =\n  OpenAIClient.Responses.ResponseApplyPatchToolCall.UpdateFile;\nexport type ApplyPatchDeleteFileOperation =\n  OpenAIClient.Responses.ResponseApplyPatchToolCall.DeleteFile;\n\n/**\n * Union type of all apply patch operations from OpenAI SDK.\n */\nexport type ApplyPatchOperation = NonNullable<\n  OpenAIClient.Responses.ResponseApplyPatchToolCall[\"operation\"]\n>;\n\n// Zod schemas for apply patch operations\nexport const ApplyPatchCreateFileOperationSchema = z.object({\n  type: z.literal(\"create_file\"),\n  path: z.string(),\n  diff: z.string(),\n});\n\nexport const ApplyPatchUpdateFileOperationSchema = z.object({\n  type: z.literal(\"update_file\"),\n  path: z.string(),\n  diff: z.string(),\n});\n\nexport const ApplyPatchDeleteFileOperationSchema = z.object({\n  type: z.literal(\"delete_file\"),\n  path: z.string(),\n});\n\n// Union schema for all apply patch operations\nexport const ApplyPatchOperationSchema = z.union([\n  ApplyPatchCreateFileOperationSchema,\n  ApplyPatchUpdateFileOperationSchema,\n  ApplyPatchDeleteFileOperationSchema,\n]);\n\n/**\n * Options for the Apply Patch tool.\n */\nexport interface ApplyPatchOptions {\n  /**\n   * Execute function that handles patch operations.\n   * This function receives the operation input and should return a string\n   * describing the result (success or failure message).\n   *\n   * The operation types are:\n   * - `create_file`: Create a new file at the specified path with the diff content\n   * - `update_file`: Modify an existing file using V4A diff format\n   * - `delete_file`: Remove a file at the specified path\n   *\n   * @example\n   * ```typescript\n   * execute: async (operation) => {\n   *   if (operation.type === \"create_file\") {\n   *     const content = applyDiff(\"\", operation.diff, \"create\");\n   *     await fs.writeFile(operation.path, content);\n   *     return `Created ${operation.path}`;\n   *   }\n   *   if (operation.type === \"update_file\") {\n   *     const current = await fs.readFile(operation.path, \"utf-8\");\n   *     const newContent = applyDiff(current, operation.diff);\n   *     await fs.writeFile(operation.path, newContent);\n   *     return `Updated ${operation.path}`;\n   *   }\n   *   if (operation.type === \"delete_file\") {\n   *     await fs.unlink(operation.path);\n   *     return `Deleted ${operation.path}`;\n   *   }\n   *   return \"Unknown operation type\";\n   * }\n   * ```\n   */\n  execute: (operation: ApplyPatchOperation) => string | Promise<string>;\n}\n\n/**\n * OpenAI Apply Patch tool type for the Responses API.\n */\nexport type ApplyPatchTool = OpenAIClient.Responses.ApplyPatchTool;\n\nconst TOOL_NAME = \"apply_patch\";\n\n/**\n * Creates an Apply Patch tool that allows models to propose structured diffs\n * that your integration applies. This enables iterative, multi-step code\n * editing workflows.\n *\n * **Apply Patch** lets GPT-5.1 create, update, and delete files in your codebase\n * using structured diffs. Instead of just suggesting edits, the model emits\n * patch operations that your application applies and then reports back on.\n *\n * **When to use**:\n * - **Multi-file refactors**  Rename symbols, extract helpers, or reorganize modules\n * - **Bug fixes**  Have the model both diagnose issues and emit precise patches\n * - **Tests & docs generation**  Create new test files, fixtures, and documentation\n * - **Migrations & mechanical edits**  Apply repetitive, structured updates\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends patch operations (`apply_patch_call` with operation type)\n * 2. Your code applies the patch to your working directory or repo\n * 3. You return success/failure status and optional output\n * 4. Repeat until the task is complete\n *\n * **Security Warning**: Applying patches can modify files in your codebase.\n * Always validate paths, implement backups, and consider sandboxing.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-apply-patch | OpenAI Apply Patch Documentation}\n *\n * @param options - Configuration options for the Apply Patch tool\n * @returns An Apply Patch tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n * import { applyDiff } from \"@openai/agents\";\n * import * as fs from \"fs/promises\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-5.1\" });\n *\n * // With execute callback for automatic patch handling\n * const patchTool = tools.applyPatch({\n *   execute: async (operation) => {\n *     if (operation.type === \"create_file\") {\n *       const content = applyDiff(\"\", operation.diff, \"create\");\n *       await fs.writeFile(operation.path, content);\n *       return `Created ${operation.path}`;\n *     }\n *     if (operation.type === \"update_file\") {\n *       const current = await fs.readFile(operation.path, \"utf-8\");\n *       const newContent = applyDiff(current, operation.diff);\n *       await fs.writeFile(operation.path, newContent);\n *       return `Updated ${operation.path}`;\n *     }\n *     if (operation.type === \"delete_file\") {\n *       await fs.unlink(operation.path);\n *       return `Deleted ${operation.path}`;\n *     }\n *     return \"Unknown operation type\";\n *   },\n * });\n *\n * const llmWithPatch = model.bindTools([patchTool]);\n * const response = await llmWithPatch.invoke(\n *   \"Rename the fib() function to fibonacci() in lib/fib.py\"\n * );\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Designed for use with `gpt-5.1` model\n * - Operations include: `create_file`, `update_file`, `delete_file`\n * - Patches use V4A diff format for updates\n * - Always validate paths to prevent directory traversal attacks\n * - Consider backing up files before applying patches\n * - Implement \"all-or-nothing\" semantics if atomicity is required\n */\nexport function applyPatch(options: ApplyPatchOptions) {\n  const patchTool = tool(options.execute, {\n    name: TOOL_NAME,\n    description:\n      \"Apply structured diffs to create, update, or delete files in the codebase.\",\n    schema: ApplyPatchOperationSchema,\n  });\n\n  patchTool.extras = {\n    ...(patchTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"apply_patch\",\n    } satisfies ApplyPatchTool,\n  };\n\n  return patchTool as DynamicStructuredTool<\n    typeof ApplyPatchOperationSchema,\n    ApplyPatchOperation,\n    unknown,\n    string\n  >;\n}\n", "export * from \"./dalle.js\";\n\nimport { webSearch } from \"./webSearch.js\";\nexport type {\n  WebSearchTool,\n  WebSearchFilters,\n  WebSearchOptions,\n} from \"./webSearch.js\";\n\nimport { mcp } from \"./mcp.js\";\nexport type {\n  McpTool,\n  McpConnectorId,\n  McpToolFilter,\n  McpApprovalFilter,\n  McpRemoteServerOptions,\n  McpConnectorOptions,\n} from \"./mcp.js\";\n\nimport { codeInterpreter } from \"./codeInterpreter.js\";\nexport type {\n  CodeInterpreterTool,\n  CodeInterpreterOptions,\n  CodeInterpreterAutoContainer,\n  CodeInterpreterMemoryLimit,\n} from \"./codeInterpreter.js\";\n\nimport { fileSearch } from \"./fileSearch.js\";\nexport type {\n  FileSearchTool,\n  FileSearchOptions,\n  FileSearchFilter,\n  FileSearchComparisonFilter,\n  FileSearchCompoundFilter,\n  FileSearchComparisonType,\n  FileSearchRankingOptions,\n  FileSearchHybridSearchWeights,\n} from \"./fileSearch.js\";\n\nimport { imageGeneration } from \"./imageGeneration.js\";\nexport type {\n  ImageGenerationTool,\n  ImageGenerationOptions,\n  ImageGenerationInputMask,\n} from \"./imageGeneration.js\";\n\nimport { computerUse } from \"./computerUse.js\";\nexport type {\n  ComputerUseTool,\n  ComputerUseInput,\n  ComputerUseOptions,\n  ComputerUseEnvironment,\n  ComputerUseAction,\n  ComputerUseClickAction,\n  ComputerUseDoubleClickAction,\n  ComputerUseDragAction,\n  ComputerUseKeypressAction,\n  ComputerUseMoveAction,\n  ComputerUseScreenshotAction,\n  ComputerUseScrollAction,\n  ComputerUseTypeAction,\n  ComputerUseWaitAction,\n} from \"./computerUse.js\";\n\nimport { localShell } from \"./localShell.js\";\nexport type {\n  LocalShellTool,\n  LocalShellOptions,\n  LocalShellAction,\n} from \"./localShell.js\";\n\nimport { shell } from \"./shell.js\";\nexport type {\n  ShellTool,\n  ShellOptions,\n  ShellAction,\n  ShellResult,\n  ShellCommandOutput,\n  ShellCallOutcome,\n} from \"./shell.js\";\n\nimport { applyPatch } from \"./applyPatch.js\";\nexport type {\n  ApplyPatchTool,\n  ApplyPatchOptions,\n  ApplyPatchOperation,\n  ApplyPatchCreateFileOperation,\n  ApplyPatchUpdateFileOperation,\n  ApplyPatchDeleteFileOperation,\n} from \"./applyPatch.js\";\n\nexport const tools = {\n  webSearch,\n  mcp,\n  codeInterpreter,\n  fileSearch,\n  imageGeneration,\n  computerUse,\n  localShell,\n  shell,\n  applyPatch,\n};\n", "import {\n  patchConfig,\n  pickRunnableConfigKeys,\n  RunnableFunc,\n} from \"@langchain/core/runnables\";\nimport { AsyncLocalStorageProviderSingleton } from \"@langchain/core/singletons\";\nimport { DynamicTool, ToolRunnableConfig } from \"@langchain/core/tools\";\nimport OpenAI from \"openai\";\n\nexport type CustomToolFields = Omit<OpenAI.Responses.CustomTool, \"type\">;\n\nexport function customTool(\n  func: RunnableFunc<string, string, ToolRunnableConfig>,\n  fields: CustomToolFields\n): DynamicTool<string> {\n  return new DynamicTool({\n    ...fields,\n    description: \"\",\n    metadata: {\n      customTool: fields,\n    },\n    func: async (input, runManager, config) =>\n      new Promise<string>((resolve, reject) => {\n        const childConfig = patchConfig(config, {\n          callbacks: runManager?.getChild(),\n        });\n        // eslint-disable-next-line no-void\n        void AsyncLocalStorageProviderSingleton.runWithConfig(\n          pickRunnableConfigKeys(childConfig),\n          async () => {\n            try {\n              resolve(func(input, childConfig));\n            } catch (e) {\n              reject(e);\n            }\n          }\n        );\n      }),\n  });\n}\n", "import type { BasePromptValue } from \"@langchain/core/prompt_values\";\nimport type { OpenAI } from \"openai\";\nimport { convertMessagesToCompletionsMessageParams } from \"../converters/completions.js\";\n\n/**\n * Convert a formatted LangChain prompt (e.g. pulled from the hub) into\n * a format expected by OpenAI's JS SDK.\n *\n * Requires the \"@langchain/openai\" package to be installed in addition\n * to the OpenAI SDK.\n *\n * @example\n * ```ts\n * import { convertPromptToOpenAI } from \"langsmith/utils/hub/openai\";\n * import { pull } from \"langchain/hub\";\n *\n * import OpenAI from 'openai';\n *\n * const prompt = await pull(\"jacob/joke-generator\");\n * const formattedPrompt = await prompt.invoke({\n *   topic: \"cats\",\n * });\n *\n * const { messages } = convertPromptToOpenAI(formattedPrompt);\n *\n * const openAIClient = new OpenAI();\n *\n * const openaiResponse = await openAIClient.chat.completions.create({\n *   model: \"gpt-4o-mini\",\n *   messages,\n * });\n * ```\n * @param formattedPrompt\n * @returns A partial OpenAI payload.\n */\n// TODO: make this a converter\nexport function convertPromptToOpenAI(formattedPrompt: BasePromptValue): {\n  messages: OpenAI.Chat.ChatCompletionMessageParam[];\n} {\n  const messages = formattedPrompt.toChatMessages();\n  return {\n    messages: convertMessagesToCompletionsMessageParams({\n      messages,\n    }) as OpenAI.Chat.ChatCompletionMessageParam[],\n  };\n}\n"],
  "mappings": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAEA,YAAQ,aAAa;AACrB,YAAQ,cAAc;AACtB,YAAQ,gBAAgB;AAExB,QAAI,SAAS,CAAC;AACd,QAAI,YAAY,CAAC;AACjB,QAAI,MAAM,OAAO,eAAe,cAAc,aAAa;AAE3D,QAAI,OAAO;AACX,SAAS,IAAI,GAAG,MAAM,KAAK,QAAQ,IAAI,KAAK,EAAE,GAAG;AAC/C,aAAO,CAAC,IAAI,KAAK,CAAC;AAClB,gBAAU,KAAK,WAAW,CAAC,CAAC,IAAI;AAAA,IAClC;AAHS;AAAO;AAOhB,cAAU,IAAI,WAAW,CAAC,CAAC,IAAI;AAC/B,cAAU,IAAI,WAAW,CAAC,CAAC,IAAI;AAE/B,aAAS,QAAS,KAAK;AACrB,UAAIA,OAAM,IAAI;AAEd,UAAIA,OAAM,IAAI,GAAG;AACf,cAAM,IAAI,MAAM,gDAAgD;AAAA,MAClE;AAIA,UAAI,WAAW,IAAI,QAAQ,GAAG;AAC9B,UAAI,aAAa,GAAI,YAAWA;AAEhC,UAAI,kBAAkB,aAAaA,OAC/B,IACA,IAAK,WAAW;AAEpB,aAAO,CAAC,UAAU,eAAe;AAAA,IACnC;AAGA,aAAS,WAAY,KAAK;AACxB,UAAI,OAAO,QAAQ,GAAG;AACtB,UAAI,WAAW,KAAK,CAAC;AACrB,UAAI,kBAAkB,KAAK,CAAC;AAC5B,cAAS,WAAW,mBAAmB,IAAI,IAAK;AAAA,IAClD;AAEA,aAAS,YAAa,KAAK,UAAU,iBAAiB;AACpD,cAAS,WAAW,mBAAmB,IAAI,IAAK;AAAA,IAClD;AAEA,aAAS,YAAa,KAAK;AACzB,UAAI;AACJ,UAAI,OAAO,QAAQ,GAAG;AACtB,UAAI,WAAW,KAAK,CAAC;AACrB,UAAI,kBAAkB,KAAK,CAAC;AAE5B,UAAI,MAAM,IAAI,IAAI,YAAY,KAAK,UAAU,eAAe,CAAC;AAE7D,UAAI,UAAU;AAGd,UAAIA,OAAM,kBAAkB,IACxB,WAAW,IACX;AAEJ,UAAIC;AACJ,WAAKA,KAAI,GAAGA,KAAID,MAAKC,MAAK,GAAG;AAC3B,cACG,UAAU,IAAI,WAAWA,EAAC,CAAC,KAAK,KAChC,UAAU,IAAI,WAAWA,KAAI,CAAC,CAAC,KAAK,KACpC,UAAU,IAAI,WAAWA,KAAI,CAAC,CAAC,KAAK,IACrC,UAAU,IAAI,WAAWA,KAAI,CAAC,CAAC;AACjC,YAAI,SAAS,IAAK,OAAO,KAAM;AAC/B,YAAI,SAAS,IAAK,OAAO,IAAK;AAC9B,YAAI,SAAS,IAAI,MAAM;AAAA,MACzB;AAEA,UAAI,oBAAoB,GAAG;AACzB,cACG,UAAU,IAAI,WAAWA,EAAC,CAAC,KAAK,IAChC,UAAU,IAAI,WAAWA,KAAI,CAAC,CAAC,KAAK;AACvC,YAAI,SAAS,IAAI,MAAM;AAAA,MACzB;AAEA,UAAI,oBAAoB,GAAG;AACzB,cACG,UAAU,IAAI,WAAWA,EAAC,CAAC,KAAK,KAChC,UAAU,IAAI,WAAWA,KAAI,CAAC,CAAC,KAAK,IACpC,UAAU,IAAI,WAAWA,KAAI,CAAC,CAAC,KAAK;AACvC,YAAI,SAAS,IAAK,OAAO,IAAK;AAC9B,YAAI,SAAS,IAAI,MAAM;AAAA,MACzB;AAEA,aAAO;AAAA,IACT;AAEA,aAAS,gBAAiB,KAAK;AAC7B,aAAO,OAAO,OAAO,KAAK,EAAI,IAC5B,OAAO,OAAO,KAAK,EAAI,IACvB,OAAO,OAAO,IAAI,EAAI,IACtB,OAAO,MAAM,EAAI;AAAA,IACrB;AAEA,aAAS,YAAa,OAAO,OAAO,KAAK;AACvC,UAAI;AACJ,UAAI,SAAS,CAAC;AACd,eAASA,KAAI,OAAOA,KAAI,KAAKA,MAAK,GAAG;AACnC,eACI,MAAMA,EAAC,KAAK,KAAM,aAClB,MAAMA,KAAI,CAAC,KAAK,IAAK,UACtB,MAAMA,KAAI,CAAC,IAAI;AAClB,eAAO,KAAK,gBAAgB,GAAG,CAAC;AAAA,MAClC;AACA,aAAO,OAAO,KAAK,EAAE;AAAA,IACvB;AAEA,aAAS,cAAe,OAAO;AAC7B,UAAI;AACJ,UAAID,OAAM,MAAM;AAChB,UAAI,aAAaA,OAAM;AACvB,UAAI,QAAQ,CAAC;AACb,UAAI,iBAAiB;AAGrB,eAASC,KAAI,GAAGC,QAAOF,OAAM,YAAYC,KAAIC,OAAMD,MAAK,gBAAgB;AACtE,cAAM,KAAK,YAAY,OAAOA,IAAIA,KAAI,iBAAkBC,QAAOA,QAAQD,KAAI,cAAe,CAAC;AAAA,MAC7F;AAGA,UAAI,eAAe,GAAG;AACpB,cAAM,MAAMD,OAAM,CAAC;AACnB,cAAM;AAAA,UACJ,OAAO,OAAO,CAAC,IACf,OAAQ,OAAO,IAAK,EAAI,IACxB;AAAA,QACF;AAAA,MACF,WAAW,eAAe,GAAG;AAC3B,eAAO,MAAMA,OAAM,CAAC,KAAK,KAAK,MAAMA,OAAM,CAAC;AAC3C,cAAM;AAAA,UACJ,OAAO,OAAO,EAAE,IAChB,OAAQ,OAAO,IAAK,EAAI,IACxB,OAAQ,OAAO,IAAK,EAAI,IACxB;AAAA,QACF;AAAA,MACF;AAEA,aAAO,MAAM,KAAK,EAAE;AAAA,IACtB;AAAA;AAAA;;;ACvIA,SAAgBG,yBACd,OACA,eACA;AACC,QAAc,gBAAgB;AAC/B,QAAM,UAAU,GAAG,MAAM,OAAA;;kFAA8F,aAAA;;AACvH,SAAO;;;;ACpBT,SAAS,uBAAuB,UAAU,OAAO,OAAO,MAAM,GAAG;AAC7D,MAAI,SAAS;AACT,UAAM,IAAI,UAAU,gCAAgC;AACxD,MAAI,SAAS,OAAO,CAAC;AACjB,UAAM,IAAI,UAAU,+CAA+C;AACvE,MAAI,OAAO,UAAU,aAAa,aAAa,SAAS,CAAC,IAAI,CAAC,MAAM,IAAI,QAAQ;AAC5E,UAAM,IAAI,UAAU,yEAAyE;AACjG,SAAO,SAAS,MAAM,EAAE,KAAK,UAAU,KAAK,IAAI,IAAK,EAAE,QAAQ,QAAS,MAAM,IAAI,UAAU,KAAK,GAAG;AACxG;AACA,SAAS,uBAAuB,UAAU,OAAO,MAAM,GAAG;AACtD,MAAI,SAAS,OAAO,CAAC;AACjB,UAAM,IAAI,UAAU,+CAA+C;AACvE,MAAI,OAAO,UAAU,aAAa,aAAa,SAAS,CAAC,IAAI,CAAC,MAAM,IAAI,QAAQ;AAC5E,UAAM,IAAI,UAAU,0EAA0E;AAClG,SAAO,SAAS,MAAM,IAAI,SAAS,MAAM,EAAE,KAAK,QAAQ,IAAI,IAAI,EAAE,QAAQ,MAAM,IAAI,QAAQ;AAChG;;;ACVO,IAAI,QAAQ,WAAA;AACjB,QAAM,EAAE,QAAAC,QAAM,IAAK;AACnB,MAAIA,SAAQ,YAAY;AACtB,YAAQA,QAAO,WAAW,KAAKA,OAAM;AACrC,WAAOA,QAAO,WAAU;EAC1B;AACA,QAAM,KAAK,IAAI,WAAW,CAAC;AAC3B,QAAM,aAAaA,UAAS,MAAMA,QAAO,gBAAgB,EAAE,EAAE,CAAC,IAAK,MAAO,KAAK,OAAM,IAAK,MAAQ;AAClG,SAAO,uCAAuC,QAAQ,UAAU,CAAC,OAC9D,CAAC,IAAK,WAAU,IAAM,MAAO,CAAC,IAAI,GAAM,SAAS,EAAE,CAAC;AAEzD;;;ACdM,SAAU,aAAa,KAAY;AACvC,SACE,OAAO,QAAQ,YACf,QAAQ;GAEN,UAAU,OAAQ,IAAY,SAAS;EAEtC,aAAa,OAAO,OAAQ,IAAY,OAAO,EAAE,SAAS,+BAA+B;AAEhG;AAEO,IAAM,cAAc,CAAC,QAAmB;AAC7C,MAAI,eAAe;AAAO,WAAO;AACjC,MAAI,OAAO,QAAQ,YAAY,QAAQ,MAAM;AAC3C,QAAI;AACF,UAAI,OAAO,UAAU,SAAS,KAAK,GAAG,MAAM,kBAAkB;AAE5D,cAAM,QAAQ,IAAI,MAAM,IAAI,SAAS,IAAI,QAAQ,EAAE,OAAO,IAAI,MAAK,IAAK,CAAA,CAAE;AAC1E,YAAI,IAAI;AAAO,gBAAM,QAAQ,IAAI;AAEjC,YAAI,IAAI,SAAS,CAAC,MAAM;AAAO,gBAAM,QAAQ,IAAI;AACjD,YAAI,IAAI;AAAM,gBAAM,OAAO,IAAI;AAC/B,eAAO;MACT;IACF,QAAQ;IAAC;AACT,QAAI;AACF,aAAO,IAAI,MAAM,KAAK,UAAU,GAAG,CAAC;IACtC,QAAQ;IAAC;EACX;AACA,SAAO,IAAI,MAAM,GAAG;AACtB;;;AC5BM,IAAO,cAAP,cAA2B,MAAK;;AAEhC,IAAO,WAAP,MAAO,kBAIH,YAAW;EAcnB,YAAY,QAAiB,OAAe,SAA6B,SAAiB;AACxF,UAAM,GAAG,UAAS,YAAY,QAAQ,OAAO,OAAO,CAAC,EAAE;AACvD,SAAK,SAAS;AACd,SAAK,UAAU;AACf,SAAK,YAAY,SAAS,IAAI,cAAc;AAC5C,SAAK,QAAQ;AAEb,UAAM,OAAO;AACb,SAAK,OAAO,OAAO,MAAM;AACzB,SAAK,QAAQ,OAAO,OAAO;AAC3B,SAAK,OAAO,OAAO,MAAM;EAC3B;EAEQ,OAAO,YAAY,QAA4B,OAAY,SAA2B;AAC5F,UAAM,MACJ,OAAO,UACL,OAAO,MAAM,YAAY,WACvB,MAAM,UACN,KAAK,UAAU,MAAM,OAAO,IAC9B,QAAQ,KAAK,UAAU,KAAK,IAC5B;AAEJ,QAAI,UAAU,KAAK;AACjB,aAAO,GAAG,MAAM,IAAI,GAAG;IACzB;AACA,QAAI,QAAQ;AACV,aAAO,GAAG,MAAM;IAClB;AACA,QAAI,KAAK;AACP,aAAO;IACT;AACA,WAAO;EACT;EAEA,OAAO,SACL,QACA,eACA,SACA,SAA4B;AAE5B,QAAI,CAAC,UAAU,CAAC,SAAS;AACvB,aAAO,IAAI,mBAAmB,EAAE,SAAS,OAAO,YAAY,aAAa,EAAC,CAAE;IAC9E;AAEA,UAAM,QAAS,gBAAwC,OAAO;AAE9D,QAAI,WAAW,KAAK;AAClB,aAAO,IAAI,gBAAgB,QAAQ,OAAO,SAAS,OAAO;IAC5D;AAEA,QAAI,WAAW,KAAK;AAClB,aAAO,IAAI,oBAAoB,QAAQ,OAAO,SAAS,OAAO;IAChE;AAEA,QAAI,WAAW,KAAK;AAClB,aAAO,IAAI,sBAAsB,QAAQ,OAAO,SAAS,OAAO;IAClE;AAEA,QAAI,WAAW,KAAK;AAClB,aAAO,IAAI,cAAc,QAAQ,OAAO,SAAS,OAAO;IAC1D;AAEA,QAAI,WAAW,KAAK;AAClB,aAAO,IAAI,cAAc,QAAQ,OAAO,SAAS,OAAO;IAC1D;AAEA,QAAI,WAAW,KAAK;AAClB,aAAO,IAAI,yBAAyB,QAAQ,OAAO,SAAS,OAAO;IACrE;AAEA,QAAI,WAAW,KAAK;AAClB,aAAO,IAAI,eAAe,QAAQ,OAAO,SAAS,OAAO;IAC3D;AAEA,QAAI,UAAU,KAAK;AACjB,aAAO,IAAI,oBAAoB,QAAQ,OAAO,SAAS,OAAO;IAChE;AAEA,WAAO,IAAI,UAAS,QAAQ,OAAO,SAAS,OAAO;EACrD;;AAGI,IAAO,oBAAP,cAAiC,SAAyC;EAC9E,YAAY,EAAE,QAAO,IAA2B,CAAA,GAAE;AAChD,UAAM,QAAW,QAAW,WAAW,wBAAwB,MAAS;EAC1E;;AAGI,IAAO,qBAAP,cAAkC,SAAyC;EAC/E,YAAY,EAAE,SAAS,MAAK,GAA+D;AACzF,UAAM,QAAW,QAAW,WAAW,qBAAqB,MAAS;AAGrE,QAAI;AAAO,WAAK,QAAQ;EAC1B;;AAGI,IAAO,4BAAP,cAAyC,mBAAkB;EAC/D,YAAY,EAAE,QAAO,IAA2B,CAAA,GAAE;AAChD,UAAM,EAAE,SAAS,WAAW,qBAAoB,CAAE;EACpD;;AAGI,IAAO,kBAAP,cAA+B,SAAsB;;AAErD,IAAO,sBAAP,cAAmC,SAAsB;;AAEzD,IAAO,wBAAP,cAAqC,SAAsB;;AAE3D,IAAO,gBAAP,cAA6B,SAAsB;;AAEnD,IAAO,gBAAP,cAA6B,SAAsB;;AAEnD,IAAO,2BAAP,cAAwC,SAAsB;;AAE9D,IAAO,iBAAP,cAA8B,SAAsB;;AAEpD,IAAO,sBAAP,cAAmC,SAAyB;;AAE5D,IAAO,0BAAP,cAAuC,YAAW;EACtD,cAAA;AACE,UAAM,kEAAkE;EAC1E;;AAGI,IAAO,iCAAP,cAA8C,YAAW;EAC7D,cAAA;AACE,UAAM,oFAAoF;EAC5F;;AAGI,IAAO,+BAAP,cAA4C,MAAK;EACrD,YAAY,SAAe;AACzB,UAAM,OAAO;EACf;;;;ACzJF,IAAM,yBAAyB;AAExB,IAAM,gBAAgB,CAACC,SAAwB;AACpD,SAAO,uBAAuB,KAAKA,IAAG;AACxC;AAEO,IAAI,UAAU,CAAC,SAAqC,UAAU,MAAM,SAAU,QAAQ,GAAG;AACzF,IAAI,kBAAkB;AAGvB,SAAU,SAAS,GAAU;AACjC,MAAI,OAAO,MAAM,UAAU;AACzB,WAAO,CAAA;EACT;AAEA,SAAO,KAAK,CAAA;AACd;AAGM,SAAU,WAAW,KAA8B;AACvD,MAAI,CAAC;AAAK,WAAO;AACjB,aAAW,MAAM;AAAK,WAAO;AAC7B,SAAO;AACT;AAGM,SAAU,OAAkC,KAAQ,KAAgB;AACxE,SAAO,OAAO,UAAU,eAAe,KAAK,KAAK,GAAG;AACtD;AAEM,SAAU,MAAM,KAAY;AAChC,SAAO,OAAO,QAAQ,OAAO,QAAQ,YAAY,CAAC,MAAM,QAAQ,GAAG;AACrE;AAUO,IAAM,0BAA0B,CAAC,MAAc,MAAsB;AAC1E,MAAI,OAAO,MAAM,YAAY,CAAC,OAAO,UAAU,CAAC,GAAG;AACjD,UAAM,IAAI,YAAY,GAAG,IAAI,qBAAqB;EACpD;AACA,MAAI,IAAI,GAAG;AACT,UAAM,IAAI,YAAY,GAAG,IAAI,6BAA6B;EAC5D;AACA,SAAO;AACT;AA2CO,IAAM,WAAW,CAAC,SAAgB;AACvC,MAAI;AACF,WAAO,KAAK,MAAM,IAAI;EACxB,SAAS,KAAK;AACZ,WAAO;EACT;AACF;;;ACtGO,IAAM,QAAQ,CAAC,OAAe,IAAI,QAAc,CAAC,YAAY,WAAW,SAAS,EAAE,CAAC;;;ACFpF,IAAM,UAAU;;;ACIhB,IAAM,qBAAqB,MAAK;AACrC;;IAEE,OAAO,WAAW;IAElB,OAAO,OAAO,aAAa;IAE3B,OAAO,cAAc;;AAEzB;AAOA,SAAS,sBAAmB;AAC1B,MAAI,OAAO,SAAS,eAAe,KAAK,SAAS,MAAM;AACrD,WAAO;EACT;AACA,MAAI,OAAO,gBAAgB,aAAa;AACtC,WAAO;EACT;AACA,MACE,OAAO,UAAU,SAAS,KACxB,OAAQ,WAAmB,YAAY,cAAe,WAAmB,UAAU,CAAC,MAChF,oBACN;AACA,WAAO;EACT;AACA,SAAO;AACT;AAwBA,IAAM,wBAAwB,MAAyB;AACrD,QAAM,mBAAmB,oBAAmB;AAC5C,MAAI,qBAAqB,QAAQ;AAC/B,WAAO;MACL,oBAAoB;MACpB,+BAA+B;MAC/B,kBAAkB,kBAAkB,KAAK,MAAM,EAAE;MACjD,oBAAoB,cAAc,KAAK,MAAM,IAAI;MACjD,uBAAuB;MACvB,+BACE,OAAO,KAAK,YAAY,WAAW,KAAK,UAAU,KAAK,SAAS,QAAQ;;EAE9E;AACA,MAAI,OAAO,gBAAgB,aAAa;AACtC,WAAO;MACL,oBAAoB;MACpB,+BAA+B;MAC/B,kBAAkB;MAClB,oBAAoB,SAAS,WAAW;MACxC,uBAAuB;MACvB,+BAAgC,WAAmB,QAAQ;;EAE/D;AAEA,MAAI,qBAAqB,QAAQ;AAC/B,WAAO;MACL,oBAAoB;MACpB,+BAA+B;MAC/B,kBAAkB,kBAAmB,WAAmB,QAAQ,YAAY,SAAS;MACrF,oBAAoB,cAAe,WAAmB,QAAQ,QAAQ,SAAS;MAC/E,uBAAuB;MACvB,+BAAgC,WAAmB,QAAQ,WAAW;;EAE1E;AAEA,QAAM,cAAc,eAAc;AAClC,MAAI,aAAa;AACf,WAAO;MACL,oBAAoB;MACpB,+BAA+B;MAC/B,kBAAkB;MAClB,oBAAoB;MACpB,uBAAuB,WAAW,YAAY,OAAO;MACrD,+BAA+B,YAAY;;EAE/C;AAGA,SAAO;IACL,oBAAoB;IACpB,+BAA+B;IAC/B,kBAAkB;IAClB,oBAAoB;IACpB,uBAAuB;IACvB,+BAA+B;;AAEnC;AAUA,SAAS,iBAAc;AACrB,MAAI,OAAO,cAAc,eAAe,CAAC,WAAW;AAClD,WAAO;EACT;AAGA,QAAM,kBAAkB;IACtB,EAAE,KAAK,QAAiB,SAAS,uCAAsC;IACvE,EAAE,KAAK,MAAe,SAAS,uCAAsC;IACrE,EAAE,KAAK,MAAe,SAAS,6CAA4C;IAC3E,EAAE,KAAK,UAAmB,SAAS,yCAAwC;IAC3E,EAAE,KAAK,WAAoB,SAAS,0CAAyC;IAC7E,EAAE,KAAK,UAAmB,SAAS,oEAAmE;;AAIxG,aAAW,EAAE,KAAK,QAAO,KAAM,iBAAiB;AAC9C,UAAM,QAAQ,QAAQ,KAAK,UAAU,SAAS;AAC9C,QAAI,OAAO;AACT,YAAM,QAAQ,MAAM,CAAC,KAAK;AAC1B,YAAM,QAAQ,MAAM,CAAC,KAAK;AAC1B,YAAM,QAAQ,MAAM,CAAC,KAAK;AAE1B,aAAO,EAAE,SAAS,KAAK,SAAS,GAAG,KAAK,IAAI,KAAK,IAAI,KAAK,GAAE;IAC9D;EACF;AAEA,SAAO;AACT;AAEA,IAAM,gBAAgB,CAAC,SAAsB;AAK3C,MAAI,SAAS;AAAO,WAAO;AAC3B,MAAI,SAAS,YAAY,SAAS;AAAO,WAAO;AAChD,MAAI,SAAS;AAAO,WAAO;AAC3B,MAAI,SAAS,aAAa,SAAS;AAAS,WAAO;AACnD,MAAI;AAAM,WAAO,SAAS,IAAI;AAC9B,SAAO;AACT;AAEA,IAAM,oBAAoB,CAAC,aAAkC;AAO3D,aAAW,SAAS,YAAW;AAM/B,MAAI,SAAS,SAAS,KAAK;AAAG,WAAO;AACrC,MAAI,aAAa;AAAW,WAAO;AACnC,MAAI,aAAa;AAAU,WAAO;AAClC,MAAI,aAAa;AAAS,WAAO;AACjC,MAAI,aAAa;AAAW,WAAO;AACnC,MAAI,aAAa;AAAW,WAAO;AACnC,MAAI,aAAa;AAAS,WAAO;AACjC,MAAI;AAAU,WAAO,SAAS,QAAQ;AACtC,SAAO;AACT;AAEA,IAAI;AACG,IAAM,qBAAqB,MAAK;AACrC,SAAQ,qBAAA,mBAAqB,sBAAqB;AACpD;;;ACvLM,SAAU,kBAAe;AAC7B,MAAI,OAAO,UAAU,aAAa;AAChC,WAAO;EACT;AAEA,QAAM,IAAI,MACR,mJAAmJ;AAEvJ;AAIM,SAAU,sBAAsB,MAAwB;AAC5D,QAAMC,kBAAkB,WAAmB;AAC3C,MAAI,OAAOA,oBAAmB,aAAa;AAGzC,UAAM,IAAI,MACR,yHAAyH;EAE7H;AAEA,SAAO,IAAIA,gBAAe,GAAG,IAAI;AACnC;AAEM,SAAU,mBAAsB,UAAwC;AAC5E,MAAI,OACF,OAAO,iBAAiB,WAAW,SAAS,OAAO,aAAa,EAAC,IAAK,SAAS,OAAO,QAAQ,EAAC;AAEjG,SAAO,mBAAmB;IACxB,QAAK;IAAI;IACT,MAAM,KAAK,YAAe;AACxB,YAAM,EAAE,MAAM,MAAK,IAAK,MAAM,KAAK,KAAI;AACvC,UAAI,MAAM;AACR,mBAAW,MAAK;MAClB,OAAO;AACL,mBAAW,QAAQ,KAAK;MAC1B;IACF;IACA,MAAM,SAAM;AACV,YAAM,KAAK,SAAQ;IACrB;GACD;AACH;AAQM,SAAU,8BAAiC,QAAW;AAC1D,MAAI,OAAO,OAAO,aAAa;AAAG,WAAO;AAEzC,QAAM,SAAS,OAAO,UAAS;AAC/B,SAAO;IACL,MAAM,OAAI;AACR,UAAI;AACF,cAAM,SAAS,MAAM,OAAO,KAAI;AAChC,YAAI,QAAQ;AAAM,iBAAO,YAAW;AACpC,eAAO;MACT,SAAS,GAAG;AACV,eAAO,YAAW;AAClB,cAAM;MACR;IACF;IACA,MAAM,SAAM;AACV,YAAM,gBAAgB,OAAO,OAAM;AACnC,aAAO,YAAW;AAClB,YAAM;AACN,aAAO,EAAE,MAAM,MAAM,OAAO,OAAS;IACvC;IACA,CAAC,OAAO,aAAa,IAAC;AACpB,aAAO;IACT;;AAEJ;AAMA,eAAsB,qBAAqB,QAAW;AACpD,MAAI,WAAW,QAAQ,OAAO,WAAW;AAAU;AAEnD,MAAI,OAAO,OAAO,aAAa,GAAG;AAChC,UAAM,OAAO,OAAO,aAAa,EAAC,EAAG,SAAQ;AAC7C;EACF;AAEA,QAAM,SAAS,OAAO,UAAS;AAC/B,QAAM,gBAAgB,OAAO,OAAM;AACnC,SAAO,YAAW;AAClB,QAAM;AACR;;;ACnBO,IAAM,kBAAkC,CAAC,EAAE,SAAS,KAAI,MAAM;AACnE,SAAO;IACL,aAAa;MACX,gBAAgB;;IAElB,MAAM,KAAK,UAAU,IAAI;;AAE7B;;;AC5FO,IAAM,iBAAyB;AAC/B,IAAM,oBAAoB,CAAC,MAAmB,OAAO,CAAC;AACtD,IAAM,aAA2D;EACtE,SAAS,CAAC,MAAmB,OAAO,CAAC,EAAE,QAAQ,QAAQ,GAAG;EAC1D,SAAS;;AAEJ,IAAM,UAAU;;;ACJhB,IAAI,MAAM,CAAC,KAAa,SAC5B,MAAO,OAAe,UAAU,SAAS,UAAU,KAAK,KAAK,OAAO,UAAU,cAAc,GAC7F,IAAI,KAAK,GAAG;AAGd,IAAM,aAA6B,MAAK;AACtC,QAAMC,SAAQ,CAAA;AACd,WAAS,IAAI,GAAG,IAAI,KAAK,EAAE,GAAG;AAC5B,IAAAA,OAAM,KAAK,QAAQ,IAAI,KAAK,MAAM,MAAM,EAAE,SAAS,EAAE,GAAG,YAAW,CAAE;EACvE;AAEA,SAAOA;AACT,GAAE;AAqHF,IAAM,QAAQ;AAEP,IAAM,SAMC,CAACC,MAAK,iBAAiB,SAAS,OAAO,WAAkB;AAGrE,MAAIA,KAAI,WAAW,GAAG;AACpB,WAAOA;EACT;AAEA,MAAIC,UAASD;AACb,MAAI,OAAOA,SAAQ,UAAU;AAC3B,IAAAC,UAAS,OAAO,UAAU,SAAS,KAAKD,IAAG;EAC7C,WAAW,OAAOA,SAAQ,UAAU;AAClC,IAAAC,UAAS,OAAOD,IAAG;EACrB;AAEA,MAAI,YAAY,cAAc;AAC5B,WAAO,OAAOC,OAAM,EAAE,QAAQ,mBAAmB,SAAU,IAAE;AAC3D,aAAO,WAAW,SAAS,GAAG,MAAM,CAAC,GAAG,EAAE,IAAI;IAChD,CAAC;EACH;AAEA,MAAI,MAAM;AACV,WAAS,IAAI,GAAG,IAAIA,QAAO,QAAQ,KAAK,OAAO;AAC7C,UAAM,UAAUA,QAAO,UAAU,QAAQA,QAAO,MAAM,GAAG,IAAI,KAAK,IAAIA;AACtE,UAAM,MAAM,CAAA;AAEZ,aAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,EAAE,GAAG;AACvC,UAAI,IAAI,QAAQ,WAAW,CAAC;AAC5B,UACE,MAAM;MACN,MAAM;MACN,MAAM;MACN,MAAM;MACL,KAAK,MAAQ,KAAK;MAClB,KAAK,MAAQ,KAAK;MAClB,KAAK,MAAQ,KAAK;MAClB,WAAW,YAAY,MAAM,MAAQ,MAAM,KAC5C;AACA,YAAI,IAAI,MAAM,IAAI,QAAQ,OAAO,CAAC;AAClC;MACF;AAEA,UAAI,IAAI,KAAM;AACZ,YAAI,IAAI,MAAM,IAAI,UAAU,CAAC;AAC7B;MACF;AAEA,UAAI,IAAI,MAAO;AACb,YAAI,IAAI,MAAM,IAAI,UAAU,MAAQ,KAAK,CAAE,IAAK,UAAU,MAAQ,IAAI,EAAK;AAC3E;MACF;AAEA,UAAI,IAAI,SAAU,KAAK,OAAQ;AAC7B,YAAI,IAAI,MAAM,IACZ,UAAU,MAAQ,KAAK,EAAG,IAAK,UAAU,MAAS,KAAK,IAAK,EAAK,IAAI,UAAU,MAAQ,IAAI,EAAK;AAClG;MACF;AAEA,WAAK;AACL,UAAI,UAAa,IAAI,SAAU,KAAO,QAAQ,WAAW,CAAC,IAAI;AAE9D,UAAI,IAAI,MAAM,IACZ,UAAU,MAAQ,KAAK,EAAG,IAC1B,UAAU,MAAS,KAAK,KAAM,EAAK,IACnC,UAAU,MAAS,KAAK,IAAK,EAAK,IAClC,UAAU,MAAQ,IAAI,EAAK;IAC/B;AAEA,WAAO,IAAI,KAAK,EAAE;EACpB;AAEA,SAAO;AACT;AA+BM,SAAU,UAAU,KAAQ;AAChC,MAAI,CAAC,OAAO,OAAO,QAAQ,UAAU;AACnC,WAAO;EACT;AAEA,SAAO,CAAC,EAAE,IAAI,eAAe,IAAI,YAAY,YAAY,IAAI,YAAY,SAAS,GAAG;AACvF;AAMM,SAAU,UAAa,KAAU,IAAe;AACpD,MAAI,QAAQ,GAAG,GAAG;AAChB,UAAM,SAAS,CAAA;AACf,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK,GAAG;AACtC,aAAO,KAAK,GAAG,IAAI,CAAC,CAAE,CAAC;IACzB;AACA,WAAO;EACT;AACA,SAAO,GAAG,GAAG;AACf;;;ACnQA,IAAM,0BAA0B;EAC9B,SAAS,QAAmB;AAC1B,WAAO,OAAO,MAAM,IAAI;EAC1B;EACA,OAAO;EACP,QAAQ,QAAqB,KAAW;AACtC,WAAO,OAAO,MAAM,IAAI,MAAM,MAAM;EACtC;EACA,OAAO,QAAmB;AACxB,WAAO,OAAO,MAAM;EACtB;;AAGF,IAAM,gBAAgB,SAAU,KAAY,gBAAmB;AAC7D,QAAM,UAAU,KAAK,MAAM,KAAK,QAAQ,cAAc,IAAI,iBAAiB,CAAC,cAAc,CAAC;AAC7F;AAEA,IAAI;AAEJ,IAAM,WAAW;EACf,gBAAgB;EAChB,WAAW;EACX,kBAAkB;EAClB,aAAa;EACb,SAAS;EACT,iBAAiB;EACjB,WAAW;EACX,QAAQ;EACR,iBAAiB;EACjB,SAAS;EACT,kBAAkB;EAClB,QAAQ;EACR,WAAW;;EAEX,SAAS;EACT,cAAcC,OAAI;AAChB,YAAQ,gBAAA,cAAgB,SAAS,UAAU,KAAK,KAAK,KAAK,UAAU,WAAW,IAAGA,KAAI;EACxF;EACA,WAAW;EACX,oBAAoB;;AAGtB,SAAS,yBAAyB,GAAU;AAC1C,SACE,OAAO,MAAM,YACb,OAAO,MAAM,YACb,OAAO,MAAM,aACb,OAAO,MAAM,YACb,OAAO,MAAM;AAEjB;AAEA,IAAM,WAAW,CAAA;AAEjB,SAAS,gBACPC,SACA,QACA,qBACA,gBACA,kBACA,oBACA,WACA,iBACA,SACA,QACA,MACA,WACA,eACA,QACA,WACA,kBACA,SACA,aAA8B;AAE9B,MAAI,MAAMA;AAEV,MAAI,SAAS;AACb,MAAI,OAAO;AACX,MAAI,YAAY;AAChB,UAAQ,SAAS,OAAO,IAAI,QAAQ,OAAO,UAAkB,CAAC,WAAW;AAEvE,UAAM,MAAM,OAAO,IAAIA,OAAM;AAC7B,YAAQ;AACR,QAAI,OAAO,QAAQ,aAAa;AAC9B,UAAI,QAAQ,MAAM;AAChB,cAAM,IAAI,WAAW,qBAAqB;MAC5C,OAAO;AACL,oBAAY;MACd;IACF;AACA,QAAI,OAAO,OAAO,IAAI,QAAQ,MAAM,aAAa;AAC/C,aAAO;IACT;EACF;AAEA,MAAI,OAAO,WAAW,YAAY;AAChC,UAAM,OAAO,QAAQ,GAAG;EAC1B,WAAW,eAAe,MAAM;AAC9B,UAAM,gBAAgB,GAAG;EAC3B,WAAW,wBAAwB,WAAW,QAAQ,GAAG,GAAG;AAC1D,UAAM,UAAU,KAAK,SAAU,OAAK;AAClC,UAAI,iBAAiB,MAAM;AACzB,eAAO,gBAAgB,KAAK;MAC9B;AACA,aAAO;IACT,CAAC;EACH;AAEA,MAAI,QAAQ,MAAM;AAChB,QAAI,oBAAoB;AACtB,aAAO,WAAW,CAAC;;QAEf,QAAQ,QAAQ,SAAS,SAAS,SAAS,OAAO,MAAM;UACxD;IACN;AAEA,UAAM;EACR;AAEA,MAAI,yBAAyB,GAAG,KAAK,UAAU,GAAG,GAAG;AACnD,QAAI,SAAS;AACX,YAAM,YACJ,mBAAmB,SAEjB,QAAQ,QAAQ,SAAS,SAAS,SAAS,OAAO,MAAM;AAC5D,aAAO;QACL,YAAY,SAAS,IACnB;QAEA,YAAY,QAAQ,KAAK,SAAS,SAAS,SAAS,SAAS,MAAM,CAAC;;IAE1E;AACA,WAAO,CAAC,YAAY,MAAM,IAAI,MAAM,YAAY,OAAO,GAAG,CAAC,CAAC;EAC9D;AAEA,QAAM,SAAmB,CAAA;AAEzB,MAAI,OAAO,QAAQ,aAAa;AAC9B,WAAO;EACT;AAEA,MAAI;AACJ,MAAI,wBAAwB,WAAW,QAAQ,GAAG,GAAG;AAEnD,QAAI,oBAAoB,SAAS;AAE/B,YAAM,UAAU,KAAK,OAAO;IAC9B;AACA,eAAW,CAAC,EAAE,OAAO,IAAI,SAAS,IAAI,IAAI,KAAK,GAAG,KAAK,OAAO,OAAc,CAAE;EAChF,WAAW,QAAQ,MAAM,GAAG;AAC1B,eAAW;EACb,OAAO;AACL,UAAM,OAAO,OAAO,KAAK,GAAG;AAC5B,eAAW,OAAO,KAAK,KAAK,IAAI,IAAI;EACtC;AAEA,QAAM,iBAAiB,kBAAkB,OAAO,MAAM,EAAE,QAAQ,OAAO,KAAK,IAAI,OAAO,MAAM;AAE7F,QAAM,kBACJ,kBAAkB,QAAQ,GAAG,KAAK,IAAI,WAAW,IAAI,iBAAiB,OAAO;AAE/E,MAAI,oBAAoB,QAAQ,GAAG,KAAK,IAAI,WAAW,GAAG;AACxD,WAAO,kBAAkB;EAC3B;AAEA,WAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,EAAE,GAAG;AACxC,UAAM,MAAM,SAAS,CAAC;AACtB,UAAM;;MAEJ,OAAO,QAAQ,YAAY,OAAO,IAAI,UAAU,cAAc,IAAI,QAAQ,IAAI,GAAU;;AAE1F,QAAI,aAAa,UAAU,MAAM;AAC/B;IACF;AAGA,UAAM,cAAc,aAAa,kBAAmB,IAAY,QAAQ,OAAO,KAAK,IAAI;AACxF,UAAM,aACJ,QAAQ,GAAG,IACT,OAAO,wBAAwB,aAC7B,oBAAoB,iBAAiB,WAAW,IAChD,kBACF,mBAAmB,YAAY,MAAM,cAAc,MAAM,cAAc;AAE3E,gBAAY,IAAIA,SAAQ,IAAI;AAC5B,UAAM,mBAAmB,oBAAI,QAAO;AACpC,qBAAiB,IAAI,UAAU,WAAW;AAC1C,kBACE,QACA;MACE;MACA;MACA;MACA;MACA;MACA;MACA;MACA;;MAEA,wBAAwB,WAAW,oBAAoB,QAAQ,GAAG,IAAI,OAAO;MAC7E;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;IAAgB,CACjB;EAEL;AAEA,SAAO;AACT;AAEA,SAAS,4BACP,OAAyB,UAAQ;AAEjC,MAAI,OAAO,KAAK,qBAAqB,eAAe,OAAO,KAAK,qBAAqB,WAAW;AAC9F,UAAM,IAAI,UAAU,wEAAwE;EAC9F;AAEA,MAAI,OAAO,KAAK,oBAAoB,eAAe,OAAO,KAAK,oBAAoB,WAAW;AAC5F,UAAM,IAAI,UAAU,uEAAuE;EAC7F;AAEA,MAAI,KAAK,YAAY,QAAQ,OAAO,KAAK,YAAY,eAAe,OAAO,KAAK,YAAY,YAAY;AACtG,UAAM,IAAI,UAAU,+BAA+B;EACrD;AAEA,QAAM,UAAU,KAAK,WAAW,SAAS;AACzC,MAAI,OAAO,KAAK,YAAY,eAAe,KAAK,YAAY,WAAW,KAAK,YAAY,cAAc;AACpG,UAAM,IAAI,UAAU,mEAAmE;EACzF;AAEA,MAAI,SAAS;AACb,MAAI,OAAO,KAAK,WAAW,aAAa;AACtC,QAAI,CAAC,IAAI,YAAY,KAAK,MAAM,GAAG;AACjC,YAAM,IAAI,UAAU,iCAAiC;IACvD;AACA,aAAS,KAAK;EAChB;AACA,QAAM,YAAY,WAAW,MAAM;AAEnC,MAAI,SAAS,SAAS;AACtB,MAAI,OAAO,KAAK,WAAW,cAAc,QAAQ,KAAK,MAAM,GAAG;AAC7D,aAAS,KAAK;EAChB;AAEA,MAAI;AACJ,MAAI,KAAK,eAAe,KAAK,eAAe,yBAAyB;AACnE,kBAAc,KAAK;EACrB,WAAW,aAAa,MAAM;AAC5B,kBAAc,KAAK,UAAU,YAAY;EAC3C,OAAO;AACL,kBAAc,SAAS;EACzB;AAEA,MAAI,oBAAoB,QAAQ,OAAO,KAAK,mBAAmB,WAAW;AACxE,UAAM,IAAI,UAAU,+CAA+C;EACrE;AAEA,QAAM,YACJ,OAAO,KAAK,cAAc,cACxB,CAAC,CAAC,KAAK,oBAAoB,OACzB,OACA,SAAS,YACX,CAAC,CAAC,KAAK;AAEX,SAAO;IACL,gBAAgB,OAAO,KAAK,mBAAmB,YAAY,KAAK,iBAAiB,SAAS;;IAE1F;IACA,kBACE,OAAO,KAAK,qBAAqB,YAAY,CAAC,CAAC,KAAK,mBAAmB,SAAS;IAClF;IACA;IACA,iBACE,OAAO,KAAK,oBAAoB,YAAY,KAAK,kBAAkB,SAAS;IAC9E,gBAAgB,CAAC,CAAC,KAAK;IACvB,WAAW,OAAO,KAAK,cAAc,cAAc,SAAS,YAAY,KAAK;IAC7E,QAAQ,OAAO,KAAK,WAAW,YAAY,KAAK,SAAS,SAAS;IAClE,iBACE,OAAO,KAAK,oBAAoB,YAAY,KAAK,kBAAkB,SAAS;IAC9E,SAAS,OAAO,KAAK,YAAY,aAAa,KAAK,UAAU,SAAS;IACtE,kBACE,OAAO,KAAK,qBAAqB,YAAY,KAAK,mBAAmB,SAAS;IAChF;IACA;IACA;IACA,eAAe,OAAO,KAAK,kBAAkB,aAAa,KAAK,gBAAgB,SAAS;IACxF,WAAW,OAAO,KAAK,cAAc,YAAY,KAAK,YAAY,SAAS;;IAE3E,MAAM,OAAO,KAAK,SAAS,aAAa,KAAK,OAAO;IACpD,oBACE,OAAO,KAAK,uBAAuB,YAAY,KAAK,qBAAqB,SAAS;;AAExF;AAEM,SAAU,UAAUA,SAAa,OAAyB,CAAA,GAAE;AAChE,MAAI,MAAMA;AACV,QAAM,UAAU,4BAA4B,IAAI;AAEhD,MAAI;AACJ,MAAI;AAEJ,MAAI,OAAO,QAAQ,WAAW,YAAY;AACxC,aAAS,QAAQ;AACjB,UAAM,OAAO,IAAI,GAAG;EACtB,WAAW,QAAQ,QAAQ,MAAM,GAAG;AAClC,aAAS,QAAQ;AACjB,eAAW;EACb;AAEA,QAAM,OAAiB,CAAA;AAEvB,MAAI,OAAO,QAAQ,YAAY,QAAQ,MAAM;AAC3C,WAAO;EACT;AAEA,QAAM,sBAAsB,wBAAwB,QAAQ,WAAW;AACvE,QAAM,iBAAiB,wBAAwB,WAAW,QAAQ;AAElE,MAAI,CAAC,UAAU;AACb,eAAW,OAAO,KAAK,GAAG;EAC5B;AAEA,MAAI,QAAQ,MAAM;AAChB,aAAS,KAAK,QAAQ,IAAI;EAC5B;AAEA,QAAM,cAAc,oBAAI,QAAO;AAC/B,WAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,EAAE,GAAG;AACxC,UAAM,MAAM,SAAS,CAAC;AAEtB,QAAI,QAAQ,aAAa,IAAI,GAAG,MAAM,MAAM;AAC1C;IACF;AACA,kBACE,MACA;MACE,IAAI,GAAG;MACP;;MAEA;MACA;MACA,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR,QAAQ,SAAS,QAAQ,UAAU;MACnC,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR,QAAQ;MACR;IAAW,CACZ;EAEL;AAEA,QAAM,SAAS,KAAK,KAAK,QAAQ,SAAS;AAC1C,MAAI,SAAS,QAAQ,mBAAmB,OAAO,MAAM;AAErD,MAAI,QAAQ,iBAAiB;AAC3B,QAAI,QAAQ,YAAY,cAAc;AAEpC,gBAAU;IACZ,OAAO;AAEL,gBAAU;IACZ;EACF;AAEA,SAAO,OAAO,SAAS,IAAI,SAAS,SAAS;AAC/C;;;AChYM,SAAU,YAAY,SAAqB;AAC/C,MAAI,SAAS;AACb,aAAW,UAAU,SAAS;AAC5B,cAAU,OAAO;EACnB;AACA,QAAM,SAAS,IAAI,WAAW,MAAM;AACpC,MAAI,QAAQ;AACZ,aAAW,UAAU,SAAS;AAC5B,WAAO,IAAI,QAAQ,KAAK;AACxB,aAAS,OAAO;EAClB;AAEA,SAAO;AACT;AAEA,IAAI;AACE,SAAU,WAAWC,MAAW;AACpC,MAAI;AACJ,UACE,gBACE,UAAU,IAAK,WAAmB,YAAW,GAAM,cAAc,QAAQ,OAAO,KAAK,OAAO,IAC9FA,IAAG;AACP;AAEA,IAAI;AACE,SAAU,WAAW,OAAiB;AAC1C,MAAI;AACJ,UACE,gBACE,UAAU,IAAK,WAAmB,YAAW,GAAM,cAAc,QAAQ,OAAO,KAAK,OAAO,IAC9F,KAAK;AACT;;;;;ACrBM,IAAO,cAAP,MAAkB;EAQtB,cAAA;AAHA,wBAAA,IAAA,MAAA,MAAA;AACA,qCAAA,IAAA,MAAA,MAAA;AAGE,2BAAA,MAAI,qBAAW,IAAI,WAAU,GAAE,GAAA;AAC/B,2BAAA,MAAI,kCAAwB,MAAI,GAAA;EAClC;EAEA,OAAO,OAAY;AACjB,QAAI,SAAS,MAAM;AACjB,aAAO,CAAA;IACT;AAEA,UAAM,cACJ,iBAAiB,cAAc,IAAI,WAAW,KAAK,IACjD,OAAO,UAAU,WAAW,WAAW,KAAK,IAC5C;AAEJ,2BAAA,MAAI,qBAAW,YAAY,CAAC,uBAAA,MAAI,qBAAA,GAAA,GAAU,WAAW,CAAC,GAAC,GAAA;AAEvD,UAAM,QAAkB,CAAA;AACxB,QAAI;AACJ,YAAQ,eAAe,iBAAiB,uBAAA,MAAI,qBAAA,GAAA,GAAU,uBAAA,MAAI,kCAAA,GAAA,CAAqB,MAAM,MAAM;AACzF,UAAI,aAAa,YAAY,uBAAA,MAAI,kCAAA,GAAA,KAAyB,MAAM;AAE9D,+BAAA,MAAI,kCAAwB,aAAa,OAAK,GAAA;AAC9C;MACF;AAGA,UACE,uBAAA,MAAI,kCAAA,GAAA,KAAyB,SAC5B,aAAa,UAAU,uBAAA,MAAI,kCAAA,GAAA,IAAwB,KAAK,aAAa,WACtE;AACA,cAAM,KAAK,WAAW,uBAAA,MAAI,qBAAA,GAAA,EAAS,SAAS,GAAG,uBAAA,MAAI,kCAAA,GAAA,IAAwB,CAAC,CAAC,CAAC;AAC9E,+BAAA,MAAI,qBAAW,uBAAA,MAAI,qBAAA,GAAA,EAAS,SAAS,uBAAA,MAAI,kCAAA,GAAA,CAAqB,GAAC,GAAA;AAC/D,+BAAA,MAAI,kCAAwB,MAAI,GAAA;AAChC;MACF;AAEA,YAAM,WACJ,uBAAA,MAAI,kCAAA,GAAA,MAA0B,OAAO,aAAa,YAAY,IAAI,aAAa;AAEjF,YAAM,OAAO,WAAW,uBAAA,MAAI,qBAAA,GAAA,EAAS,SAAS,GAAG,QAAQ,CAAC;AAC1D,YAAM,KAAK,IAAI;AAEf,6BAAA,MAAI,qBAAW,uBAAA,MAAI,qBAAA,GAAA,EAAS,SAAS,aAAa,KAAK,GAAC,GAAA;AACxD,6BAAA,MAAI,kCAAwB,MAAI,GAAA;IAClC;AAEA,WAAO;EACT;EAEA,QAAK;AACH,QAAI,CAAC,uBAAA,MAAI,qBAAA,GAAA,EAAS,QAAQ;AACxB,aAAO,CAAA;IACT;AACA,WAAO,KAAK,OAAO,IAAI;EACzB;;;AA7DO,YAAA,gBAAgB,oBAAI,IAAI,CAAC,MAAM,IAAI,CAAC;AACpC,YAAA,iBAAiB;AAwE1B,SAAS,iBACP,QACA,YAAyB;AAEzB,QAAM,UAAU;AAChB,QAAM,WAAW;AAEjB,WAAS,IAAI,cAAc,GAAG,IAAI,OAAO,QAAQ,KAAK;AACpD,QAAI,OAAO,CAAC,MAAM,SAAS;AACzB,aAAO,EAAE,WAAW,GAAG,OAAO,IAAI,GAAG,UAAU,MAAK;IACtD;AAEA,QAAI,OAAO,CAAC,MAAM,UAAU;AAC1B,aAAO,EAAE,WAAW,GAAG,OAAO,IAAI,GAAG,UAAU,KAAI;IACrD;EACF;AAEA,SAAO;AACT;AAEM,SAAU,uBAAuB,QAAkB;AAIvD,QAAM,UAAU;AAChB,QAAM,WAAW;AAEjB,WAAS,IAAI,GAAG,IAAI,OAAO,SAAS,GAAG,KAAK;AAC1C,QAAI,OAAO,CAAC,MAAM,WAAW,OAAO,IAAI,CAAC,MAAM,SAAS;AAEtD,aAAO,IAAI;IACb;AACA,QAAI,OAAO,CAAC,MAAM,YAAY,OAAO,IAAI,CAAC,MAAM,UAAU;AAExD,aAAO,IAAI;IACb;AACA,QACE,OAAO,CAAC,MAAM,YACd,OAAO,IAAI,CAAC,MAAM,WAClB,IAAI,IAAI,OAAO,UACf,OAAO,IAAI,CAAC,MAAM,YAClB,OAAO,IAAI,CAAC,MAAM,SAClB;AAEA,aAAO,IAAI;IACb;EACF;AAEA,SAAO;AACT;;;ACvHA,IAAM,eAAe;EACnB,KAAK;EACL,OAAO;EACP,MAAM;EACN,MAAM;EACN,OAAO;;AAGF,IAAM,gBAAgB,CAC3B,YACA,YACA,WACwB;AACxB,MAAI,CAAC,YAAY;AACf,WAAO;EACT;AACA,MAAI,OAAO,cAAc,UAAU,GAAG;AACpC,WAAO;EACT;AACA,YAAU,MAAM,EAAE,KAChB,GAAG,UAAU,eAAe,KAAK,UAAU,UAAU,CAAC,qBAAqB,KAAK,UAC9E,OAAO,KAAK,YAAY,CAAC,CAC1B,EAAE;AAEL,SAAO;AACT;AAEA,SAAS,OAAI;AAAI;AAEjB,SAAS,UAAU,SAAuB,QAA4B,UAAkB;AACtF,MAAI,CAAC,UAAU,aAAa,OAAO,IAAI,aAAa,QAAQ,GAAG;AAC7D,WAAO;EACT,OAAO;AAEL,WAAO,OAAO,OAAO,EAAE,KAAK,MAAM;EACpC;AACF;AAEA,IAAM,aAAa;EACjB,OAAO;EACP,MAAM;EACN,MAAM;EACN,OAAO;;AAGT,IAAI,gBAAgC,oBAAI,QAAO;AAEzC,SAAU,UAAU,QAAc;AACtC,QAAM,SAAS,OAAO;AACtB,QAAM,WAAW,OAAO,YAAY;AACpC,MAAI,CAAC,QAAQ;AACX,WAAO;EACT;AAEA,QAAM,eAAe,cAAc,IAAI,MAAM;AAC7C,MAAI,gBAAgB,aAAa,CAAC,MAAM,UAAU;AAChD,WAAO,aAAa,CAAC;EACvB;AAEA,QAAM,cAAc;IAClB,OAAO,UAAU,SAAS,QAAQ,QAAQ;IAC1C,MAAM,UAAU,QAAQ,QAAQ,QAAQ;IACxC,MAAM,UAAU,QAAQ,QAAQ,QAAQ;IACxC,OAAO,UAAU,SAAS,QAAQ,QAAQ;;AAG5C,gBAAc,IAAI,QAAQ,CAAC,UAAU,WAAW,CAAC;AAEjD,SAAO;AACT;AAEO,IAAM,uBAAuB,CAAC,YAWhC;AACH,MAAI,QAAQ,SAAS;AACnB,YAAQ,UAAU,EAAE,GAAG,QAAQ,QAAO;AACtC,WAAO,QAAQ,QAAQ,SAAS;EAClC;AACA,MAAI,QAAQ,SAAS;AACnB,YAAQ,UAAU,OAAO,aACtB,QAAQ,mBAAmB,UAAU,CAAC,GAAG,QAAQ,OAAO,IAAI,OAAO,QAAQ,QAAQ,OAAO,GAAG,IAC5F,CAAC,CAAC,MAAM,KAAK,MAAM;MACjB;MAEE,KAAK,YAAW,MAAO,mBACvB,KAAK,YAAW,MAAO,YACvB,KAAK,YAAW,MAAO,eAEvB,QACA;KACH,CACF;EAEL;AACA,MAAI,yBAAyB,SAAS;AACpC,QAAI,QAAQ,qBAAqB;AAC/B,cAAQ,UAAU,QAAQ;IAC5B;AACA,WAAO,QAAQ;EACjB;AACA,SAAO;AACT;;;;ACzGM,IAAO,SAAP,MAAO,QAAM;EAIjB,YACU,UACR,YACA,QAAe;AAFP,SAAA,WAAA;AAHV,mBAAA,IAAA,MAAA,MAAA;AAOE,SAAK,aAAa;AAClB,2BAAA,MAAI,gBAAW,QAAM,GAAA;EACvB;EAEA,OAAO,gBACL,UACA,YACA,QACA,qBAA6B;AAE7B,QAAI,WAAW;AACf,UAAM,SAAS,SAAS,UAAU,MAAM,IAAI;AAE5C,oBAAgB,WAAQ;AACtB,UAAI,UAAU;AACZ,cAAM,IAAI,YAAY,0EAA0E;MAClG;AACA,iBAAW;AACX,UAAI,OAAO;AACX,UAAI;AACF,yBAAiB,OAAO,iBAAiB,UAAU,UAAU,GAAG;AAC9D,cAAI;AAAM;AAEV,cAAI,IAAI,KAAK,WAAW,QAAQ,GAAG;AACjC,mBAAO;AACP;UACF;AAEA,cAAI,IAAI,UAAU,QAAQ,CAAC,IAAI,MAAM,WAAW,SAAS,GAAG;AAC1D,gBAAI;AAEJ,gBAAI;AACF,qBAAO,KAAK,MAAM,IAAI,IAAI;YAC5B,SAAS,GAAG;AACV,qBAAO,MAAM,sCAAsC,IAAI,IAAI;AAC3D,qBAAO,MAAM,eAAe,IAAI,GAAG;AACnC,oBAAM;YACR;AAEA,gBAAI,QAAQ,KAAK,OAAO;AACtB,oBAAM,IAAI,SAAS,QAAW,KAAK,OAAO,QAAW,SAAS,OAAO;YACvE;AAEA,kBAAM,sBAAsB,EAAE,OAAO,IAAI,OAAO,KAAI,IAAK;UAC3D,OAAO;AACL,gBAAI;AACJ,gBAAI;AACF,qBAAO,KAAK,MAAM,IAAI,IAAI;YAC5B,SAAS,GAAG;AACV,sBAAQ,MAAM,sCAAsC,IAAI,IAAI;AAC5D,sBAAQ,MAAM,eAAe,IAAI,GAAG;AACpC,oBAAM;YACR;AAEA,gBAAI,IAAI,SAAS,SAAS;AACxB,oBAAM,IAAI,SAAS,QAAW,KAAK,OAAO,KAAK,SAAS,MAAS;YACnE;AACA,kBAAM,EAAE,OAAO,IAAI,OAAO,KAAU;UACtC;QACF;AACA,eAAO;MACT,SAAS,GAAG;AAEV,YAAI,aAAa,CAAC;AAAG;AACrB,cAAM;MACR;AAEE,YAAI,CAAC;AAAM,qBAAW,MAAK;MAC7B;IACF;AAEA,WAAO,IAAI,QAAO,UAAU,YAAY,MAAM;EAChD;;;;;EAMA,OAAO,mBACL,gBACA,YACA,QAAe;AAEf,QAAI,WAAW;AAEf,oBAAgB,YAAS;AACvB,YAAM,cAAc,IAAI,YAAW;AAEnC,YAAM,OAAO,8BAAqC,cAAc;AAChE,uBAAiB,SAAS,MAAM;AAC9B,mBAAW,QAAQ,YAAY,OAAO,KAAK,GAAG;AAC5C,gBAAM;QACR;MACF;AAEA,iBAAW,QAAQ,YAAY,MAAK,GAAI;AACtC,cAAM;MACR;IACF;AAEA,oBAAgB,WAAQ;AACtB,UAAI,UAAU;AACZ,cAAM,IAAI,YAAY,0EAA0E;MAClG;AACA,iBAAW;AACX,UAAI,OAAO;AACX,UAAI;AACF,yBAAiB,QAAQ,UAAS,GAAI;AACpC,cAAI;AAAM;AACV,cAAI;AAAM,kBAAM,KAAK,MAAM,IAAI;QACjC;AACA,eAAO;MACT,SAAS,GAAG;AAEV,YAAI,aAAa,CAAC;AAAG;AACrB,cAAM;MACR;AAEE,YAAI,CAAC;AAAM,qBAAW,MAAK;MAC7B;IACF;AAEA,WAAO,IAAI,QAAO,UAAU,YAAY,MAAM;EAChD;EAEA,EAAA,iBAAA,oBAAA,QAAA,GAAC,OAAO,cAAa,IAAC;AACpB,WAAO,KAAK,SAAQ;EACtB;;;;;EAMA,MAAG;AACD,UAAM,OAA6C,CAAA;AACnD,UAAM,QAA8C,CAAA;AACpD,UAAM,WAAW,KAAK,SAAQ;AAE9B,UAAM,cAAc,CAAC,UAAoE;AACvF,aAAO;QACL,MAAM,MAAK;AACT,cAAI,MAAM,WAAW,GAAG;AACtB,kBAAM,SAAS,SAAS,KAAI;AAC5B,iBAAK,KAAK,MAAM;AAChB,kBAAM,KAAK,MAAM;UACnB;AACA,iBAAO,MAAM,MAAK;QACpB;;IAEJ;AAEA,WAAO;MACL,IAAI,QAAO,MAAM,YAAY,IAAI,GAAG,KAAK,YAAY,uBAAA,MAAI,gBAAA,GAAA,CAAQ;MACjE,IAAI,QAAO,MAAM,YAAY,KAAK,GAAG,KAAK,YAAY,uBAAA,MAAI,gBAAA,GAAA,CAAQ;;EAEtE;;;;;;EAOA,mBAAgB;AACd,UAAM,OAAO;AACb,QAAI;AAEJ,WAAO,mBAAmB;MACxB,MAAM,QAAK;AACT,eAAO,KAAK,OAAO,aAAa,EAAC;MACnC;MACA,MAAM,KAAK,MAAS;AAClB,YAAI;AACF,gBAAM,EAAE,OAAO,KAAI,IAAK,MAAM,KAAK,KAAI;AACvC,cAAI;AAAM,mBAAO,KAAK,MAAK;AAE3B,gBAAM,QAAQ,WAAW,KAAK,UAAU,KAAK,IAAI,IAAI;AAErD,eAAK,QAAQ,KAAK;QACpB,SAAS,KAAK;AACZ,eAAK,MAAM,GAAG;QAChB;MACF;MACA,MAAM,SAAM;AACV,cAAM,KAAK,SAAQ;MACrB;KACD;EACH;;AAGF,gBAAuB,iBACrB,UACA,YAA2B;AAE3B,MAAI,CAAC,SAAS,MAAM;AAClB,eAAW,MAAK;AAChB,QACE,OAAQ,WAAmB,cAAc,eACxC,WAAmB,UAAU,YAAY,eAC1C;AACA,YAAM,IAAI,YACR,gKAAgK;IAEpK;AACA,UAAM,IAAI,YAAY,mDAAmD;EAC3E;AAEA,QAAM,aAAa,IAAI,WAAU;AACjC,QAAM,cAAc,IAAI,YAAW;AAEnC,QAAM,OAAO,8BAAqC,SAAS,IAAI;AAC/D,mBAAiB,YAAY,cAAc,IAAI,GAAG;AAChD,eAAW,QAAQ,YAAY,OAAO,QAAQ,GAAG;AAC/C,YAAM,MAAM,WAAW,OAAO,IAAI;AAClC,UAAI;AAAK,cAAM;IACjB;EACF;AAEA,aAAW,QAAQ,YAAY,MAAK,GAAI;AACtC,UAAM,MAAM,WAAW,OAAO,IAAI;AAClC,QAAI;AAAK,YAAM;EACjB;AACF;AAMA,gBAAgB,cAAc,UAAsC;AAClE,MAAI,OAAO,IAAI,WAAU;AAEzB,mBAAiB,SAAS,UAAU;AAClC,QAAI,SAAS,MAAM;AACjB;IACF;AAEA,UAAM,cACJ,iBAAiB,cAAc,IAAI,WAAW,KAAK,IACjD,OAAO,UAAU,WAAW,WAAW,KAAK,IAC5C;AAEJ,QAAI,UAAU,IAAI,WAAW,KAAK,SAAS,YAAY,MAAM;AAC7D,YAAQ,IAAI,IAAI;AAChB,YAAQ,IAAI,aAAa,KAAK,MAAM;AACpC,WAAO;AAEP,QAAI;AACJ,YAAQ,eAAe,uBAAuB,IAAI,OAAO,IAAI;AAC3D,YAAM,KAAK,MAAM,GAAG,YAAY;AAChC,aAAO,KAAK,MAAM,YAAY;IAChC;EACF;AAEA,MAAI,KAAK,SAAS,GAAG;AACnB,UAAM;EACR;AACF;AAEA,IAAM,aAAN,MAAgB;EAKd,cAAA;AACE,SAAK,QAAQ;AACb,SAAK,OAAO,CAAA;AACZ,SAAK,SAAS,CAAA;EAChB;EAEA,OAAO,MAAY;AACjB,QAAI,KAAK,SAAS,IAAI,GAAG;AACvB,aAAO,KAAK,UAAU,GAAG,KAAK,SAAS,CAAC;IAC1C;AAEA,QAAI,CAAC,MAAM;AAET,UAAI,CAAC,KAAK,SAAS,CAAC,KAAK,KAAK;AAAQ,eAAO;AAE7C,YAAM,MAAuB;QAC3B,OAAO,KAAK;QACZ,MAAM,KAAK,KAAK,KAAK,IAAI;QACzB,KAAK,KAAK;;AAGZ,WAAK,QAAQ;AACb,WAAK,OAAO,CAAA;AACZ,WAAK,SAAS,CAAA;AAEd,aAAO;IACT;AAEA,SAAK,OAAO,KAAK,IAAI;AAErB,QAAI,KAAK,WAAW,GAAG,GAAG;AACxB,aAAO;IACT;AAEA,QAAI,CAAC,WAAW,GAAG,KAAK,IAAI,UAAU,MAAM,GAAG;AAE/C,QAAI,MAAM,WAAW,GAAG,GAAG;AACzB,cAAQ,MAAM,UAAU,CAAC;IAC3B;AAEA,QAAI,cAAc,SAAS;AACzB,WAAK,QAAQ;IACf,WAAW,cAAc,QAAQ;AAC/B,WAAK,KAAK,KAAK,KAAK;IACtB;AAEA,WAAO;EACT;;AAGF,SAAS,UAAUC,MAAa,WAAiB;AAC/C,QAAM,QAAQA,KAAI,QAAQ,SAAS;AACnC,MAAI,UAAU,IAAI;AAChB,WAAO,CAACA,KAAI,UAAU,GAAG,KAAK,GAAG,WAAWA,KAAI,UAAU,QAAQ,UAAU,MAAM,CAAC;EACrF;AAEA,SAAO,CAACA,MAAK,IAAI,EAAE;AACrB;;;AC3UA,eAAsB,qBACpB,QACA,OAAuB;AAEvB,QAAM,EAAE,UAAU,cAAc,qBAAqB,UAAS,IAAK;AACnE,QAAM,OAAO,OAAO,YAAW;AAC7B,QAAI,MAAM,QAAQ,QAAQ;AACxB,gBAAU,MAAM,EAAE,MAAM,YAAY,SAAS,QAAQ,SAAS,KAAK,SAAS,SAAS,SAAS,IAAI;AAKlG,UAAI,MAAM,QAAQ,eAAe;AAC/B,eAAO,MAAM,QAAQ,cAAc,gBACjC,UACA,MAAM,YACN,QACA,MAAM,QAAQ,qBAAqB;MAEvC;AAEA,aAAO,OAAO,gBACZ,UACA,MAAM,YACN,QACA,MAAM,QAAQ,qBAAqB;IAEvC;AAGA,QAAI,SAAS,WAAW,KAAK;AAC3B,aAAO;IACT;AAEA,QAAI,MAAM,QAAQ,kBAAkB;AAClC,aAAO;IACT;AAEA,UAAM,cAAc,SAAS,QAAQ,IAAI,cAAc;AACvD,UAAM,YAAY,aAAa,MAAM,GAAG,EAAE,CAAC,GAAG,KAAI;AAClD,UAAM,SAAS,WAAW,SAAS,kBAAkB,KAAK,WAAW,SAAS,OAAO;AACrF,QAAI,QAAQ;AACV,YAAM,gBAAgB,SAAS,QAAQ,IAAI,gBAAgB;AAC3D,UAAI,kBAAkB,KAAK;AAEzB,eAAO;MACT;AAEA,YAAMC,QAAO,MAAM,SAAS,KAAI;AAChC,aAAO,aAAaA,OAAW,QAAQ;IACzC;AAEA,UAAM,OAAO,MAAM,SAAS,KAAI;AAChC,WAAO;EACT,GAAE;AACF,YAAU,MAAM,EAAE,MAChB,IAAI,YAAY,qBAChB,qBAAqB;IACnB;IACA,KAAK,SAAS;IACd,QAAQ,SAAS;IACjB;IACA,YAAY,KAAK,IAAG,IAAK;GAC1B,CAAC;AAEJ,SAAO;AACT;AAOM,SAAU,aAAgB,OAAU,UAAkB;AAC1D,MAAI,CAAC,SAAS,OAAO,UAAU,YAAY,MAAM,QAAQ,KAAK,GAAG;AAC/D,WAAO;EACT;AAEA,SAAO,OAAO,eAAe,OAAO,eAAe;IACjD,OAAO,SAAS,QAAQ,IAAI,cAAc;IAC1C,YAAY;GACb;AACH;;;;ACnFM,IAAO,aAAP,MAAO,oBAAsB,QAAyB;EAI1D,YACE,QACQ,iBACAC,iBAGgC,sBAAoB;AAE5D,UAAM,CAAC,YAAW;AAIhB,cAAQ,IAAW;IACrB,CAAC;AAXO,SAAA,kBAAA;AACA,SAAA,gBAAAA;AALV,uBAAA,IAAA,MAAA,MAAA;AAgBE,2BAAA,MAAI,oBAAW,QAAM,GAAA;EACvB;EAEA,YAAeC,YAAkD;AAC/D,WAAO,IAAI,YAAW,uBAAA,MAAI,oBAAA,GAAA,GAAU,KAAK,iBAAiB,OAAO,QAAQ,UACvE,aAAaA,WAAU,MAAM,KAAK,cAAc,QAAQ,KAAK,GAAG,KAAK,GAAG,MAAM,QAAQ,CAAC;EAE3F;;;;;;;;;;;;EAaA,aAAU;AACR,WAAO,KAAK,gBAAgB,KAAK,CAAC,MAAM,EAAE,QAAQ;EACpD;;;;;;;;;;;;;EAcA,MAAM,eAAY;AAChB,UAAM,CAAC,MAAM,QAAQ,IAAI,MAAM,QAAQ,IAAI,CAAC,KAAK,MAAK,GAAI,KAAK,WAAU,CAAE,CAAC;AAC5E,WAAO,EAAE,MAAM,UAAU,YAAY,SAAS,QAAQ,IAAI,cAAc,EAAC;EAC3E;EAEQ,QAAK;AACX,QAAI,CAAC,KAAK,eAAe;AACvB,WAAK,gBAAgB,KAAK,gBAAgB,KAAK,CAAC,SAC9C,KAAK,cAAc,uBAAA,MAAI,oBAAA,GAAA,GAAU,IAAI,CAAC;IAE1C;AACA,WAAO,KAAK;EACd;EAES,KACP,aACA,YAAmF;AAEnF,WAAO,KAAK,MAAK,EAAG,KAAK,aAAa,UAAU;EAClD;EAES,MACP,YAAiF;AAEjF,WAAO,KAAK,MAAK,EAAG,MAAM,UAAU;EACtC;EAES,QAAQ,WAA2C;AAC1D,WAAO,KAAK,MAAK,EAAG,QAAQ,SAAS;EACvC;;;;;;ACvFI,IAAgB,eAAhB,MAA4B;EAOhC,YAAY,QAAgB,UAAoB,MAAe,SAA4B;AAN3F,yBAAA,IAAA,MAAA,MAAA;AAOE,2BAAA,MAAI,sBAAW,QAAM,GAAA;AACrB,SAAK,UAAU;AACf,SAAK,WAAW;AAChB,SAAK,OAAO;EACd;EAMA,cAAW;AACT,UAAM,QAAQ,KAAK,kBAAiB;AACpC,QAAI,CAAC,MAAM;AAAQ,aAAO;AAC1B,WAAO,KAAK,uBAAsB,KAAM;EAC1C;EAEA,MAAM,cAAW;AACf,UAAM,cAAc,KAAK,uBAAsB;AAC/C,QAAI,CAAC,aAAa;AAChB,YAAM,IAAI,YACR,uFAAuF;IAE3F;AAEA,WAAO,MAAM,uBAAA,MAAI,sBAAA,GAAA,EAAS,eAAe,KAAK,aAAoB,WAAW;EAC/E;EAEA,OAAO,YAAS;AACd,QAAI,OAAa;AACjB,UAAM;AACN,WAAO,KAAK,YAAW,GAAI;AACzB,aAAO,MAAM,KAAK,YAAW;AAC7B,YAAM;IACR;EACF;EAEA,SAAO,uBAAA,oBAAA,QAAA,GAAC,OAAO,cAAa,IAAC;AAC3B,qBAAiB,QAAQ,KAAK,UAAS,GAAI;AACzC,iBAAW,QAAQ,KAAK,kBAAiB,GAAI;AAC3C,cAAM;MACR;IACF;EACF;;AAYI,IAAO,cAAP,cAII,WAAqB;EAG7B,YACE,QACA,SACAC,OAA4E;AAE5E,UACE,QACA,SACA,OAAOC,SAAQ,UACb,IAAID,MACFC,SACA,MAAM,UACN,MAAM,qBAAqBA,SAAQ,KAAK,GACxC,MAAM,OAAO,CACc;EAEnC;;;;;;;;EASA,QAAQ,OAAO,aAAa,IAAC;AAC3B,UAAM,OAAO,MAAM;AACnB,qBAAiB,QAAQ,MAAM;AAC7B,YAAM;IACR;EACF;;AAYI,IAAO,OAAP,cAA0B,aAAkB;EAKhD,YAAY,QAAgB,UAAoB,MAA0B,SAA4B;AACpG,UAAM,QAAQ,UAAU,MAAM,OAAO;AAErC,SAAK,OAAO,KAAK,QAAQ,CAAA;AACzB,SAAK,SAAS,KAAK;EACrB;EAEA,oBAAiB;AACf,WAAO,KAAK,QAAQ,CAAA;EACtB;EAEA,yBAAsB;AACpB,WAAO;EACT;;AAeI,IAAO,aAAP,cACI,aAAkB;EAO1B,YACE,QACA,UACA,MACA,SAA4B;AAE5B,UAAM,QAAQ,UAAU,MAAM,OAAO;AAErC,SAAK,OAAO,KAAK,QAAQ,CAAA;AACzB,SAAK,WAAW,KAAK,YAAY;EACnC;EAEA,oBAAiB;AACf,WAAO,KAAK,QAAQ,CAAA;EACtB;EAES,cAAW;AAClB,QAAI,KAAK,aAAa,OAAO;AAC3B,aAAO;IACT;AAEA,WAAO,MAAM,YAAW;EAC1B;EAEA,yBAAsB;AACpB,UAAM,OAAO,KAAK,kBAAiB;AACnC,UAAM,KAAK,KAAK,KAAK,SAAS,CAAC,GAAG;AAClC,QAAI,CAAC,IAAI;AACP,aAAO;IACT;AAEA,WAAO;MACL,GAAG,KAAK;MACR,OAAO;QACL,GAAG,SAAS,KAAK,QAAQ,KAAK;QAC9B,OAAO;;;EAGb;;AAiBI,IAAO,yBAAP,cACI,aAAkB;EAS1B,YACE,QACA,UACA,MACA,SAA4B;AAE5B,UAAM,QAAQ,UAAU,MAAM,OAAO;AAErC,SAAK,OAAO,KAAK,QAAQ,CAAA;AACzB,SAAK,WAAW,KAAK,YAAY;AACjC,SAAK,UAAU,KAAK,WAAW;EACjC;EAEA,oBAAiB;AACf,WAAO,KAAK,QAAQ,CAAA;EACtB;EAES,cAAW;AAClB,QAAI,KAAK,aAAa,OAAO;AAC3B,aAAO;IACT;AAEA,WAAO,MAAM,YAAW;EAC1B;EAEA,yBAAsB;AACpB,UAAM,SAAS,KAAK;AACpB,QAAI,CAAC,QAAQ;AACX,aAAO;IACT;AAEA,WAAO;MACL,GAAG,KAAK;MACR,OAAO;QACL,GAAG,SAAS,KAAK,QAAQ,KAAK;QAC9B,OAAO;;;EAGb;;;;AC9PK,IAAM,mBAAmB,MAAK;AACnC,MAAI,OAAO,SAAS,aAAa;AAC/B,UAAM,EAAE,SAAAC,SAAO,IAAK;AACpB,UAAM,YACJ,OAAOA,UAAS,UAAU,SAAS,YAAY,SAASA,SAAQ,SAAS,KAAK,MAAM,GAAG,CAAC,IAAI;AAC9F,UAAM,IAAI,MACR,4EACG,YACC,+FACA,GAAG;EAEX;AACF;AAiBM,SAAU,SACd,UACA,UACA,SAAyB;AAEzB,mBAAgB;AAChB,SAAO,IAAI,KAAK,UAAiB,YAAY,gBAAgB,OAAO;AACtE;AAEM,SAAU,QAAQ,OAAU;AAChC,UAEK,OAAO,UAAU,YAChB,UAAU,SACR,UAAU,SAAS,MAAM,QAAQ,OAAO,MAAM,IAAI,KACjD,SAAS,SAAS,MAAM,OAAO,OAAO,MAAM,GAAG,KAC/C,cAAc,SAAS,MAAM,YAAY,OAAO,MAAM,QAAQ,KAC9D,UAAU,SAAS,MAAM,QAAQ,OAAO,MAAM,IAAI,MACvD,IAEC,MAAM,OAAO,EACb,IAAG,KAAM;AAEhB;AAEO,IAAM,kBAAkB,CAAC,UAC9B,SAAS,QAAQ,OAAO,UAAU,YAAY,OAAO,MAAM,OAAO,aAAa,MAAM;AAMhF,IAAM,mCAAmC,OAC9C,MACAC,WAC2B;AAC3B,MAAI,CAAC,mBAAmB,KAAK,IAAI;AAAG,WAAO;AAE3C,SAAO,EAAE,GAAG,MAAM,MAAM,MAAM,WAAW,KAAK,MAAMA,MAAK,EAAC;AAC5D;AAIO,IAAM,8BAA8B,OACzC,MACAA,WAC2B;AAC3B,SAAO,EAAE,GAAG,MAAM,MAAM,MAAM,WAAW,KAAK,MAAMA,MAAK,EAAC;AAC5D;AAEA,IAAM,sBAAsC,oBAAI,QAAO;AAQvD,SAAS,iBAAiB,aAA2B;AACnD,QAAMA,SAAe,OAAO,gBAAgB,aAAa,cAAe,YAAoB;AAC5F,QAAM,SAAS,oBAAoB,IAAIA,MAAK;AAC5C,MAAI;AAAQ,WAAO;AACnB,QAAMC,YAAW,YAAW;AAC1B,QAAI;AACF,YAAM,gBACJ,cAAcD,SACZA,OAAM,YACL,MAAMA,OAAM,QAAQ,GAAG;AAC5B,YAAM,OAAO,IAAI,SAAQ;AACzB,UAAI,KAAK,SAAQ,MAAQ,MAAM,IAAI,cAAc,IAAI,EAAE,KAAI,GAAK;AAC9D,eAAO;MACT;AACA,aAAO;IACT,QAAQ;AAEN,aAAO;IACT;EACF,GAAE;AACF,sBAAoB,IAAIA,QAAOC,QAAO;AACtC,SAAOA;AACT;AAEO,IAAM,aAAa,OACxB,MACAD,WACqB;AACrB,MAAI,CAAE,MAAM,iBAAiBA,MAAK,GAAI;AACpC,UAAM,IAAI,UACR,mGAAmG;EAEvG;AACA,QAAM,OAAO,IAAI,SAAQ;AACzB,QAAM,QAAQ,IAAI,OAAO,QAAQ,QAAQ,CAAA,CAAE,EAAE,IAAI,CAAC,CAAC,KAAK,KAAK,MAAM,aAAa,MAAM,KAAK,KAAK,CAAC,CAAC;AAClG,SAAO;AACT;AAIA,IAAM,cAAc,CAAC,UAAmB,iBAAiB,QAAQ,UAAU;AAE3E,IAAM,eAAe,CAAC,UACpB,OAAO,UAAU,YACjB,UAAU,SACT,iBAAiB,YAAY,gBAAgB,KAAK,KAAK,YAAY,KAAK;AAE3E,IAAM,qBAAqB,CAAC,UAA2B;AACrD,MAAI,aAAa,KAAK;AAAG,WAAO;AAChC,MAAI,MAAM,QAAQ,KAAK;AAAG,WAAO,MAAM,KAAK,kBAAkB;AAC9D,MAAI,SAAS,OAAO,UAAU,UAAU;AACtC,eAAW,KAAK,OAAO;AACrB,UAAI,mBAAoB,MAAc,CAAC,CAAC;AAAG,eAAO;IACpD;EACF;AACA,SAAO;AACT;AAEA,IAAM,eAAe,OAAO,MAAgB,KAAa,UAAiC;AACxF,MAAI,UAAU;AAAW;AACzB,MAAI,SAAS,MAAM;AACjB,UAAM,IAAI,UACR,sBAAsB,GAAG,6DAA6D;EAE1F;AAGA,MAAI,OAAO,UAAU,YAAY,OAAO,UAAU,YAAY,OAAO,UAAU,WAAW;AACxF,SAAK,OAAO,KAAK,OAAO,KAAK,CAAC;EAChC,WAAW,iBAAiB,UAAU;AACpC,SAAK,OAAO,KAAK,SAAS,CAAC,MAAM,MAAM,KAAI,CAAE,GAAG,QAAQ,KAAK,CAAC,CAAC;EACjE,WAAW,gBAAgB,KAAK,GAAG;AACjC,SAAK,OAAO,KAAK,SAAS,CAAC,MAAM,IAAI,SAAS,mBAAmB,KAAK,CAAC,EAAE,KAAI,CAAE,GAAG,QAAQ,KAAK,CAAC,CAAC;EACnG,WAAW,YAAY,KAAK,GAAG;AAC7B,SAAK,OAAO,KAAK,OAAO,QAAQ,KAAK,CAAC;EACxC,WAAW,MAAM,QAAQ,KAAK,GAAG;AAC/B,UAAM,QAAQ,IAAI,MAAM,IAAI,CAAC,UAAU,aAAa,MAAM,MAAM,MAAM,KAAK,CAAC,CAAC;EAC/E,WAAW,OAAO,UAAU,UAAU;AACpC,UAAM,QAAQ,IACZ,OAAO,QAAQ,KAAK,EAAE,IAAI,CAAC,CAAC,MAAM,IAAI,MAAM,aAAa,MAAM,GAAG,GAAG,IAAI,IAAI,KAAK,IAAI,CAAC,CAAC;EAE5F,OAAO;AACL,UAAM,IAAI,UACR,wGAAwG,KAAK,UAAU;EAE3H;AACF;;;AClKA,IAAM,aAAa,CAAC,UAClB,SAAS,QACT,OAAO,UAAU,YACjB,OAAO,MAAM,SAAS,YACtB,OAAO,MAAM,SAAS,YACtB,OAAO,MAAM,SAAS,cACtB,OAAO,MAAM,UAAU,cACvB,OAAO,MAAM,gBAAgB;AAe/B,IAAM,aAAa,CAAC,UAClB,SAAS,QACT,OAAO,UAAU,YACjB,OAAO,MAAM,SAAS,YACtB,OAAO,MAAM,iBAAiB,YAC9B,WAAW,KAAK;AAUlB,IAAM,iBAAiB,CAAC,UACtB,SAAS,QACT,OAAO,UAAU,YACjB,OAAO,MAAM,QAAQ,YACrB,OAAO,MAAM,SAAS;AAiBxB,eAAsB,OACpB,OACA,MACA,SAAqC;AAErC,mBAAgB;AAGhB,UAAQ,MAAM;AAGd,MAAI,WAAW,KAAK,GAAG;AACrB,QAAI,iBAAiB,MAAM;AACzB,aAAO;IACT;AACA,WAAO,SAAS,CAAC,MAAM,MAAM,YAAW,CAAE,GAAG,MAAM,IAAI;EACzD;AAEA,MAAI,eAAe,KAAK,GAAG;AACzB,UAAM,OAAO,MAAM,MAAM,KAAI;AAC7B,aAAA,OAAS,IAAI,IAAI,MAAM,GAAG,EAAE,SAAS,MAAM,OAAO,EAAE,IAAG;AAEvD,WAAO,SAAS,MAAM,SAAS,IAAI,GAAG,MAAM,OAAO;EACrD;AAEA,QAAM,QAAQ,MAAM,SAAS,KAAK;AAElC,WAAA,OAAS,QAAQ,KAAK;AAEtB,MAAI,CAAC,SAAS,MAAM;AAClB,UAAM,OAAO,MAAM,KAAK,CAAC,SAAS,OAAO,SAAS,YAAY,UAAU,QAAQ,KAAK,IAAI;AACzF,QAAI,OAAO,SAAS,UAAU;AAC5B,gBAAU,EAAE,GAAG,SAAS,KAAI;IAC9B;EACF;AAEA,SAAO,SAAS,OAAO,MAAM,OAAO;AACtC;AAEA,eAAe,SAAS,OAAiD;AACvE,MAAI,QAAyB,CAAA;AAC7B,MACE,OAAO,UAAU,YACjB,YAAY,OAAO,KAAK;EACxB,iBAAiB,aACjB;AACA,UAAM,KAAK,KAAK;EAClB,WAAW,WAAW,KAAK,GAAG;AAC5B,UAAM,KAAK,iBAAiB,OAAO,QAAQ,MAAM,MAAM,YAAW,CAAE;EACtE,WACE,gBAAgB,KAAK,GACrB;AACA,qBAAiB,SAAS,OAAO;AAC/B,YAAM,KAAK,GAAI,MAAM,SAAS,KAAqB,CAAE;IACvD;EACF,OAAO;AACL,UAAM,cAAc,OAAO,aAAa;AACxC,UAAM,IAAI,MACR,yBAAyB,OAAO,KAAK,GACnC,cAAc,kBAAkB,WAAW,KAAK,EAClD,GAAG,cAAc,KAAK,CAAC,EAAE;EAE7B;AAEA,SAAO;AACT;AAEA,SAAS,cAAc,OAAc;AACnC,MAAI,OAAO,UAAU,YAAY,UAAU;AAAM,WAAO;AACxD,QAAM,QAAQ,OAAO,oBAAoB,KAAK;AAC9C,SAAO,aAAa,MAAM,IAAI,CAAC,MAAM,IAAI,CAAC,GAAG,EAAE,KAAK,IAAI,CAAC;AAC3D;;;ACrJM,IAAgB,cAAhB,MAA2B;EAG/B,YAAY,QAAc;AACxB,SAAK,UAAU;EACjB;;;;ACCI,SAAU,cAAcE,MAAW;AACvC,SAAOA,KAAI,QAAQ,oCAAoC,kBAAkB;AAC3E;AAEA,IAAM,QAAwB,OAAO,OAAuB,uBAAO,OAAO,IAAI,CAAC;AAExE,IAAM,wBAAwB,CAAC,cAAc,kBAClD,SAASC,MAAK,YAA+B,QAA0B;AAErE,MAAI,QAAQ,WAAW;AAAG,WAAO,QAAQ,CAAC;AAE1C,MAAI,WAAW;AACf,QAAM,kBAAkB,CAAA;AACxB,QAAMA,QAAO,QAAQ,OAAO,CAAC,eAAe,cAAc,UAAS;AACjE,QAAI,OAAO,KAAK,YAAY,GAAG;AAC7B,iBAAW;IACb;AACA,UAAM,QAAQ,OAAO,KAAK;AAC1B,QAAI,WAAW,WAAW,qBAAqB,aAAa,KAAK,KAAK;AACtE,QACE,UAAU,OAAO,WAChB,SAAS,QACP,OAAO,UAAU;IAEhB,MAAM,aACJ,OAAO,eAAe,OAAO,eAAgB,MAAc,kBAAkB,KAAK,KAAK,KAAK,GACxF,WACV;AACA,gBAAU,QAAQ;AAClB,sBAAgB,KAAK;QACnB,OAAO,cAAc,SAAS,aAAa;QAC3C,QAAQ,QAAQ;QAChB,OAAO,iBAAiB,OAAO,UAAU,SACtC,KAAK,KAAK,EACV,MAAM,GAAG,EAAE,CAAC;OAChB;IACH;AACA,WAAO,gBAAgB,gBAAgB,UAAU,OAAO,SAAS,KAAK;EACxE,GAAG,EAAE;AAEL,QAAM,WAAWA,MAAK,MAAM,QAAQ,CAAC,EAAE,CAAC;AACxC,QAAM,wBAAwB,WAAA,uCAAA,IAAoC;AAClE,MAAI;AAGJ,UAAQ,QAAQ,sBAAsB,KAAK,QAAQ,OAAO,MAAM;AAC9D,oBAAgB,KAAK;MACnB,OAAO,MAAM;MACb,QAAQ,MAAM,CAAC,EAAE;MACjB,OAAO,UAAU,MAAM,CAAC,CAAC;KAC1B;EACH;AAEA,kBAAgB,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AAEhD,MAAI,gBAAgB,SAAS,GAAG;AAC9B,QAAI,UAAU;AACd,UAAM,YAAY,gBAAgB,OAAO,CAAC,KAAK,YAAW;AACxD,YAAM,SAAS,IAAI,OAAO,QAAQ,QAAQ,OAAO;AACjD,YAAM,SAAS,IAAI,OAAO,QAAQ,MAAM;AACxC,gBAAU,QAAQ,QAAQ,QAAQ;AAClC,aAAO,MAAM,SAAS;IACxB,GAAG,EAAE;AAEL,UAAM,IAAI,YACR;EAA0D,gBACvD,IAAI,CAAC,MAAM,EAAE,KAAK,EAClB,KAAK,IAAI,CAAC;EAAKA,KAAI;EAAK,SAAS,EAAE;EAE1C;AAEA,SAAOA;AACT;AAKK,IAAM,OAAuB,sBAAsB,aAAa;;;AC9EjE,IAAO,WAAP,cAAwB,YAAW;;;;;;;;;;;;;;;EAevC,KACE,cACA,QAA8C,CAAA,GAC9C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAClB,yBAAyB,YAAY,aACrC,YACA,EAAE,OAAO,GAAG,QAAO,CAAE;EAEzB;;;;ACNI,SAAU,6BAA6BC,OAAc;AACzD,SAAOA,UAAS,UAAa,cAAcA,SAAQA,MAAK,aAAa;AACvE;AAYM,SAAU,4BACd,iBACA,QAAoC;AAEpC,QAAM,MAAM,EAAE,GAAG,gBAAe;AAEhC,SAAO,iBAAiB,KAAK;IAC3B,QAAQ;MACN,OAAO;MACP,YAAY;;IAEd,WAAW;MACT,OAAO;MACP,YAAY;;GAEf;AAED,SAAO;AACT;AA6BM,SAAU,6BACd,iBAAoB;AAEpB,SAAO,kBAAkB,QAAQ,MAAM;AACzC;AAmDM,SAAU,mBAAmBC,OAAS;AAC1C,SAAOA,QAAO,QAAQ,MAAM;AAC9B;AAEM,SAAU,yBAGd,YAA4B,QAAc;AAC1C,MAAI,CAAC,UAAU,CAAC,sBAAsB,MAAM,GAAG;AAC7C,WAAO;MACL,GAAG;MACH,SAAS,WAAW,QAAQ,IAAI,CAAC,WAAU;AACzC,0DAAkD,OAAO,QAAQ,UAAU;AAE3E,eAAO;UACL,GAAG;UACH,SAAS;YACP,GAAG,OAAO;YACV,QAAQ;YACR,GAAI,OAAO,QAAQ,aACjB;cACE,YAAY,OAAO,QAAQ;gBAE7B;;;MAGR,CAAC;;EAEL;AAEA,SAAO,oBAAoB,YAAY,MAAM;AAC/C;AAEM,SAAU,oBAGd,YAA4B,QAAc;AAC1C,QAAM,UAAwC,WAAW,QAAQ,IAAI,CAAC,WAAiC;AACrG,QAAI,OAAO,kBAAkB,UAAU;AACrC,YAAM,IAAI,wBAAuB;IACnC;AAEA,QAAI,OAAO,kBAAkB,kBAAkB;AAC7C,YAAM,IAAI,+BAA8B;IAC1C;AAEA,sDAAkD,OAAO,QAAQ,UAAU;AAE3E,WAAO;MACL,GAAG;MACH,SAAS;QACP,GAAG,OAAO;QACV,GAAI,OAAO,QAAQ,aACjB;UACE,YACE,OAAO,QAAQ,YAAY,IAAI,CAAC,aAAa,cAAc,QAAQ,QAAQ,CAAC,KAAK;YAErF;QACF,QACE,OAAO,QAAQ,WAAW,CAAC,OAAO,QAAQ,UACxC,oBAAoB,QAAQ,OAAO,QAAQ,OAAO,IAClD;;;EAGV,CAAC;AAED,SAAO,EAAE,GAAG,YAAY,QAAO;AACjC;AAEA,SAAS,oBAGP,QAAgB,SAAe;AAC/B,MAAI,OAAO,iBAAiB,SAAS,eAAe;AAClD,WAAO;EACT;AAEA,MAAI,OAAO,iBAAiB,SAAS,eAAe;AAClD,QAAI,eAAe,OAAO,iBAAiB;AACzC,YAAM,kBAAkB,OAAO;AAE/B,aAAO,gBAAgB,UAAU,OAAO;IAC1C;AAEA,WAAO,KAAK,MAAM,OAAO;EAC3B;AAEA,SAAO;AACT;AAEA,SAAS,cACP,QACA,UAA+C;AAE/C,QAAM,YAAY,OAAO,OAAO,KAC9B,CAACC,eACC,6BAA6BA,UAAS,KAAKA,WAAU,UAAU,SAAS,SAAS,SAAS,IAAI;AAElG,SAAO;IACL,GAAG;IACH,UAAU;MACR,GAAG,SAAS;MACZ,kBACE,mBAAmB,SAAS,IAAI,UAAU,UAAU,SAAS,SAAS,SAAS,IAC7E,WAAW,SAAS,SAAS,KAAK,MAAM,SAAS,SAAS,SAAS,IACnE;;;AAGV;AAEM,SAAU,oBACd,QACA,UAA+C;AAE/C,MAAI,CAAC,UAAU,EAAE,WAAW,WAAW,CAAC,OAAO,OAAO;AACpD,WAAO;EACT;AAEA,QAAM,YAAY,OAAO,OAAO,KAC9B,CAACA,eACC,6BAA6BA,UAAS,KAAKA,WAAU,UAAU,SAAS,SAAS,SAAS,IAAI;AAElG,SACE,6BAA6B,SAAS,MACrC,mBAAmB,SAAS,KAAK,WAAW,SAAS,UAAU;AAEpE;AAEM,SAAU,sBAAsB,QAAqC;AACzE,MAAI,6BAA6B,OAAO,eAAe,GAAG;AACxD,WAAO;EACT;AAEA,SACE,OAAO,OAAO,KACZ,CAAC,MAAM,mBAAmB,CAAC,KAAM,EAAE,SAAS,cAAc,EAAE,SAAS,WAAW,IAAK,KAClF;AAET;AAEM,SAAU,kDACd,WAA8C;AAE9C,aAAW,YAAY,aAAa,CAAA,GAAI;AACtC,QAAI,SAAS,SAAS,YAAY;AAChC,YAAM,IAAI,YACR,oEAAoE,SAAS,IAAI,IAAI;IAEzF;EACF;AACF;AAEM,SAAU,mBAAmBC,QAA8C;AAC/E,aAAWF,SAAQE,UAAS,CAAA,GAAI;AAC9B,QAAIF,MAAK,SAAS,YAAY;AAC5B,YAAM,IAAI,YACR,2EAA2EA,MAAK,IAAI,IAAI;IAE5F;AAEA,QAAIA,MAAK,SAAS,WAAW,MAAM;AACjC,YAAM,IAAI,YACR,SAASA,MAAK,SAAS,IAAI,4FAA4F;IAE3H;EACF;AACF;;;AChTO,IAAM,qBAAqB,CAChC,YACkD;AAClD,SAAO,SAAS,SAAS;AAC3B;AAEO,IAAM,gBAAgB,CAC3B,YAC6C;AAC7C,SAAO,SAAS,SAAS;AAC3B;;;;;;;;;;;;;;;;ACdM,IAAO,cAAP,MAAkB;EAoBtB,cAAA;;AAnBA,SAAA,aAA8B,IAAI,gBAAe;AAEjD,kCAAA,IAAA,MAAA,MAAA;AACA,yCAAA,IAAA,MAAuC,MAAK;IAAE,CAAC;AAC/C,wCAAA,IAAA,MAAwD,MAAK;IAAE,CAAC;AAEhE,4BAAA,IAAA,MAAA,MAAA;AACA,mCAAA,IAAA,MAAiC,MAAK;IAAE,CAAC;AACzC,kCAAA,IAAA,MAAkD,MAAK;IAAE,CAAC;AAE1D,2BAAA,IAAA,MAEI,CAAA,CAAE;AAEN,uBAAA,IAAA,MAAS,KAAK;AACd,yBAAA,IAAA,MAAW,KAAK;AAChB,yBAAA,IAAA,MAAW,KAAK;AAChB,wCAAA,IAAA,MAA0B,KAAK;AAG7B,2BAAA,MAAI,+BAAqB,IAAI,QAAc,CAAC,SAAS,WAAU;AAC7D,6BAAA,MAAI,sCAA4B,SAAO,GAAA;AACvC,6BAAA,MAAI,qCAA2B,QAAM,GAAA;IACvC,CAAC,GAAC,GAAA;AAEF,2BAAA,MAAI,yBAAe,IAAI,QAAc,CAAC,SAAS,WAAU;AACvD,6BAAA,MAAI,gCAAsB,SAAO,GAAA;AACjC,6BAAA,MAAI,+BAAqB,QAAM,GAAA;IACjC,CAAC,GAAC,GAAA;AAMF,2BAAA,MAAI,+BAAA,GAAA,EAAmB,MAAM,MAAK;IAAE,CAAC;AACrC,2BAAA,MAAI,yBAAA,GAAA,EAAa,MAAM,MAAK;IAAE,CAAC;EACjC;EAEU,KAAoC,UAA4B;AAGxE,eAAW,MAAK;AACd,eAAQ,EAAG,KAAK,MAAK;AACnB,aAAK,WAAU;AACf,aAAK,MAAM,KAAK;MAClB,GAAG,uBAAA,MAAI,wBAAA,KAAA,wBAAA,EAAc,KAAK,IAAI,CAAC;IACjC,GAAG,CAAC;EACN;EAEU,aAAU;AAClB,QAAI,KAAK;AAAO;AAChB,2BAAA,MAAI,sCAAA,GAAA,EAAyB,KAA7B,IAAI;AACJ,SAAK,MAAM,SAAS;EACtB;EAEA,IAAI,QAAK;AACP,WAAO,uBAAA,MAAI,oBAAA,GAAA;EACb;EAEA,IAAI,UAAO;AACT,WAAO,uBAAA,MAAI,sBAAA,GAAA;EACb;EAEA,IAAI,UAAO;AACT,WAAO,uBAAA,MAAI,sBAAA,GAAA;EACb;EAEA,QAAK;AACH,SAAK,WAAW,MAAK;EACvB;;;;;;;;EASA,GAAmC,OAAc,UAA0C;AACzF,UAAM,YACJ,uBAAA,MAAI,wBAAA,GAAA,EAAY,KAAK,MAAM,uBAAA,MAAI,wBAAA,GAAA,EAAY,KAAK,IAAI,CAAA;AACtD,cAAU,KAAK,EAAE,SAAQ,CAAE;AAC3B,WAAO;EACT;;;;;;;;EASA,IAAoC,OAAc,UAA0C;AAC1F,UAAM,YAAY,uBAAA,MAAI,wBAAA,GAAA,EAAY,KAAK;AACvC,QAAI,CAAC;AAAW,aAAO;AACvB,UAAM,QAAQ,UAAU,UAAU,CAAC,MAAM,EAAE,aAAa,QAAQ;AAChE,QAAI,SAAS;AAAG,gBAAU,OAAO,OAAO,CAAC;AACzC,WAAO;EACT;;;;;;EAOA,KAAqC,OAAc,UAA0C;AAC3F,UAAM,YACJ,uBAAA,MAAI,wBAAA,GAAA,EAAY,KAAK,MAAM,uBAAA,MAAI,wBAAA,GAAA,EAAY,KAAK,IAAI,CAAA;AACtD,cAAU,KAAK,EAAE,UAAU,MAAM,KAAI,CAAE;AACvC,WAAO;EACT;;;;;;;;;;;;EAaA,QACE,OAAY;AAMZ,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAU;AACrC,6BAAA,MAAI,qCAA2B,MAAI,GAAA;AACnC,UAAI,UAAU;AAAS,aAAK,KAAK,SAAS,MAAM;AAChD,WAAK,KAAK,OAAO,OAAc;IACjC,CAAC;EACH;EAEA,MAAM,OAAI;AACR,2BAAA,MAAI,qCAA2B,MAAI,GAAA;AACnC,UAAM,uBAAA,MAAI,yBAAA,GAAA;EACZ;EAyBA,MAEE,UACG,MAAwC;AAG3C,QAAI,uBAAA,MAAI,oBAAA,GAAA,GAAS;AACf;IACF;AAEA,QAAI,UAAU,OAAO;AACnB,6BAAA,MAAI,oBAAU,MAAI,GAAA;AAClB,6BAAA,MAAI,gCAAA,GAAA,EAAmB,KAAvB,IAAI;IACN;AAEA,UAAM,YAA2D,uBAAA,MAAI,wBAAA,GAAA,EAAY,KAAK;AACtF,QAAI,WAAW;AACb,6BAAA,MAAI,wBAAA,GAAA,EAAY,KAAK,IAAI,UAAU,OAAO,CAAC,MAAM,CAAC,EAAE,IAAI;AACxD,gBAAU,QAAQ,CAAC,EAAE,SAAQ,MAAY,SAAS,GAAI,IAAY,CAAC;IACrE;AAEA,QAAI,UAAU,SAAS;AACrB,YAAM,QAAQ,KAAK,CAAC;AACpB,UAAI,CAAC,uBAAA,MAAI,qCAAA,GAAA,KAA4B,CAAC,WAAW,QAAQ;AACvD,gBAAQ,OAAO,KAAK;MACtB;AACA,6BAAA,MAAI,qCAAA,GAAA,EAAwB,KAA5B,MAA6B,KAAK;AAClC,6BAAA,MAAI,+BAAA,GAAA,EAAkB,KAAtB,MAAuB,KAAK;AAC5B,WAAK,MAAM,KAAK;AAChB;IACF;AAEA,QAAI,UAAU,SAAS;AAGrB,YAAM,QAAQ,KAAK,CAAC;AACpB,UAAI,CAAC,uBAAA,MAAI,qCAAA,GAAA,KAA4B,CAAC,WAAW,QAAQ;AAOvD,gBAAQ,OAAO,KAAK;MACtB;AACA,6BAAA,MAAI,qCAAA,GAAA,EAAwB,KAA5B,MAA6B,KAAK;AAClC,6BAAA,MAAI,+BAAA,GAAA,EAAkB,KAAtB,MAAuB,KAAK;AAC5B,WAAK,MAAM,KAAK;IAClB;EACF;EAEU,aAAU;EAAU;;qxBA1Ec,OAAc;AACxD,yBAAA,MAAI,sBAAY,MAAI,GAAA;AACpB,MAAI,iBAAiB,SAAS,MAAM,SAAS,cAAc;AACzD,YAAQ,IAAI,kBAAiB;EAC/B;AACA,MAAI,iBAAiB,mBAAmB;AACtC,2BAAA,MAAI,sBAAY,MAAI,GAAA;AACpB,WAAO,KAAK,MAAM,SAAS,KAAK;EAClC;AACA,MAAI,iBAAiB,aAAa;AAChC,WAAO,KAAK,MAAM,SAAS,KAAK;EAClC;AACA,MAAI,iBAAiB,OAAO;AAC1B,UAAM,cAA2B,IAAI,YAAY,MAAM,OAAO;AAE9D,gBAAY,QAAQ;AACpB,WAAO,KAAK,MAAM,SAAS,WAAW;EACxC;AACA,SAAO,KAAK,MAAM,SAAS,IAAI,YAAY,OAAO,KAAK,CAAC,CAAC;AAC3D;;;ACrFI,SAAU,4BACd,IAAO;AAEP,SAAO,OAAQ,GAAW,UAAU;AACtC;;;;;;;;;;;AC1DA,IAAM,+BAA+B;AAM/B,IAAO,+BAAP,cAGI,YAAuB;EAHjC,cAAA;;;AAIY,SAAA,mBAAoD,CAAA;AAC9D,SAAA,WAAyC,CAAA;EAkW3C;EAhWY,mBAER,gBAA6C;AAE7C,SAAK,iBAAiB,KAAK,cAAc;AACzC,SAAK,MAAM,kBAAkB,cAAc;AAC3C,UAAM,UAAU,eAAe,QAAQ,CAAC,GAAG;AAC3C,QAAI;AAAS,WAAK,YAAY,OAAqC;AACnE,WAAO;EACT;EAEU,YAER,SACA,OAAO,MAAI;AAEX,QAAI,EAAE,aAAa;AAAU,cAAQ,UAAU;AAE/C,SAAK,SAAS,KAAK,OAAO;AAE1B,QAAI,MAAM;AACR,WAAK,MAAM,WAAW,OAAO;AAC7B,UAAI,cAAc,OAAO,KAAK,QAAQ,SAAS;AAE7C,aAAK,MAAM,0BAA0B,QAAQ,OAAiB;MAChE,WAAW,mBAAmB,OAAO,KAAK,QAAQ,YAAY;AAC5D,mBAAW,aAAa,QAAQ,YAAY;AAC1C,cAAI,UAAU,SAAS,YAAY;AACjC,iBAAK,MAAM,oBAAoB,UAAU,QAAQ;UACnD;QACF;MACF;IACF;EACF;;;;;EAMA,MAAM,sBAAmB;AACvB,UAAM,KAAK,KAAI;AACf,UAAM,aAAa,KAAK,iBAAiB,KAAK,iBAAiB,SAAS,CAAC;AACzE,QAAI,CAAC;AAAY,YAAM,IAAI,YAAY,iDAAiD;AACxF,WAAO;EACT;;;;;EAUA,MAAM,eAAY;AAChB,UAAM,KAAK,KAAI;AACf,WAAO,uBAAA,MAAI,yCAAA,KAAA,6CAAA,EAAiB,KAArB,IAAI;EACb;;;;;EAuBA,MAAM,eAAY;AAChB,UAAM,KAAK,KAAI;AACf,WAAO,uBAAA,MAAI,yCAAA,KAAA,6CAAA,EAAiB,KAArB,IAAI;EACb;;;;;EAiBA,MAAM,wBAAqB;AACzB,UAAM,KAAK,KAAI;AACf,WAAO,uBAAA,MAAI,yCAAA,KAAA,sDAAA,EAA0B,KAA9B,IAAI;EACb;EAsBA,MAAM,8BAA2B;AAC/B,UAAM,KAAK,KAAI;AACf,WAAO,uBAAA,MAAI,yCAAA,KAAA,4DAAA,EAAgC,KAApC,IAAI;EACb;EAkBA,MAAM,aAAU;AACd,UAAM,KAAK,KAAI;AACf,WAAO,uBAAA,MAAI,yCAAA,KAAA,iDAAA,EAAqB,KAAzB,IAAI;EACb;EAEA,qBAAkB;AAChB,WAAO,CAAC,GAAG,KAAK,gBAAgB;EAClC;EAEmB,aAAU;AAG3B,UAAM,aAAa,KAAK,iBAAiB,KAAK,iBAAiB,SAAS,CAAC;AACzE,QAAI;AAAY,WAAK,MAAM,uBAAuB,UAAU;AAC5D,UAAM,eAAe,uBAAA,MAAI,yCAAA,KAAA,6CAAA,EAAiB,KAArB,IAAI;AACzB,QAAI;AAAc,WAAK,MAAM,gBAAgB,YAAY;AACzD,UAAM,eAAe,uBAAA,MAAI,yCAAA,KAAA,6CAAA,EAAiB,KAArB,IAAI;AACzB,QAAI;AAAc,WAAK,MAAM,gBAAgB,YAAY;AAEzD,UAAM,oBAAoB,uBAAA,MAAI,yCAAA,KAAA,sDAAA,EAA0B,KAA9B,IAAI;AAC9B,QAAI;AAAmB,WAAK,MAAM,yBAAyB,iBAAiB;AAE5E,UAAM,0BAA0B,uBAAA,MAAI,yCAAA,KAAA,4DAAA,EAAgC,KAApC,IAAI;AACpC,QAAI,2BAA2B;AAAM,WAAK,MAAM,+BAA+B,uBAAuB;AAEtG,QAAI,KAAK,iBAAiB,KAAK,CAAC,MAAM,EAAE,KAAK,GAAG;AAC9C,WAAK,MAAM,cAAc,uBAAA,MAAI,yCAAA,KAAA,iDAAA,EAAqB,KAAzB,IAAI,CAAuB;IACtD;EACF;EAUU,MAAM,sBACd,QACA,QACA,SAAwB;AAExB,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AACA,2BAAA,MAAI,yCAAA,KAAA,4CAAA,EAAgB,KAApB,MAAqB,MAAM;AAE3B,UAAM,iBAAiB,MAAM,OAAO,KAAK,YAAY,OACnD,EAAE,GAAG,QAAQ,QAAQ,MAAK,GAC1B,EAAE,GAAG,SAAS,QAAQ,KAAK,WAAW,OAAM,CAAE;AAEhD,SAAK,WAAU;AACf,WAAO,KAAK,mBAAmB,oBAAoB,gBAAgB,MAAM,CAAC;EAC5E;EAEU,MAAM,mBACd,QACA,QACA,SAAwB;AAExB,eAAW,WAAW,OAAO,UAAU;AACrC,WAAK,YAAY,SAAS,KAAK;IACjC;AACA,WAAO,MAAM,KAAK,sBAAsB,QAAQ,QAAQ,OAAO;EACjE;EAEU,MAAM,UACd,QACA,QAGA,SAAuB;AAEvB,UAAM,OAAO;AACb,UAAM,EAAE,cAAc,QAAQ,QAAQ,GAAG,WAAU,IAAK;AACxD,UAAM,uBACJ,OAAO,gBAAgB,YAAY,YAAY,SAAS,cAAc,aAAa,UAAU;AAC/F,UAAM,EAAE,qBAAqB,6BAA4B,IAAK,WAAW,CAAA;AAGzE,UAAM,aAAa,OAAO,MAAM,IAAI,CAACG,UAAmC;AACtE,UAAI,mBAAmBA,KAAI,GAAG;AAC5B,YAAI,CAACA,MAAK,WAAW;AACnB,gBAAM,IAAI,YAAY,uEAAuE;QAC/F;AAEA,eAAO;UACL,MAAM;UACN,UAAU;YACR,UAAUA,MAAK;YACf,MAAMA,MAAK,SAAS;YACpB,aAAaA,MAAK,SAAS,eAAe;YAC1C,YAAYA,MAAK,SAAS;YAC1B,OAAOA,MAAK;YACZ,QAAQ;;;MAGd;AAEA,aAAOA;IACT,CAAC;AAED,UAAM,kBAAyD,CAAA;AAC/D,eAAW,KAAK,YAAY;AAC1B,UAAI,EAAE,SAAS,YAAY;AACzB,wBAAgB,EAAE,SAAS,QAAQ,EAAE,SAAS,SAAS,IAAI,IAAI,EAAE;MACnE;IACF;AAEA,UAAMC,SACJ,WAAW,SACT,WAAW,IAAI,CAAC,MACd,EAAE,SAAS,aACT;MACE,MAAM;MACN,UAAU;QACR,MAAM,EAAE,SAAS,QAAQ,EAAE,SAAS,SAAS;QAC7C,YAAY,EAAE,SAAS;QACvB,aAAa,EAAE,SAAS;QACxB,QAAQ,EAAE,SAAS;;QAGtB,CAAmC,IAEvC;AAEL,eAAW,WAAW,OAAO,UAAU;AACrC,WAAK,YAAY,SAAS,KAAK;IACjC;AAEA,aAAS,IAAI,GAAG,IAAI,oBAAoB,EAAE,GAAG;AAC3C,YAAM,iBAAiC,MAAM,KAAK,sBAChD,QACA;QACE,GAAG;QACH;QACA,OAAAA;QACA,UAAU,CAAC,GAAG,KAAK,QAAQ;SAE7B,OAAO;AAET,YAAM,UAAU,eAAe,QAAQ,CAAC,GAAG;AAC3C,UAAI,CAAC,SAAS;AACZ,cAAM,IAAI,YAAY,4CAA4C;MACpE;AACA,UAAI,CAAC,QAAQ,YAAY,QAAQ;AAC/B;MACF;AAEA,iBAAW,aAAa,QAAQ,YAAY;AAC1C,YAAI,UAAU,SAAS;AAAY;AACnC,cAAM,eAAe,UAAU;AAC/B,cAAM,EAAE,MAAM,WAAW,KAAI,IAAK,UAAU;AAC5C,cAAM,KAAK,gBAAgB,IAAI;AAE/B,YAAI,CAAC,IAAI;AACP,gBAAMC,WAAU,sBAAsB,KAAK,UAAU,IAAI,CAAC,4BAA4B,OAAO,KAC3F,eAAe,EAEd,IAAI,CAACC,UAAS,KAAK,UAAUA,KAAI,CAAC,EAClC,KAAK,IAAI,CAAC;AAEb,eAAK,YAAY,EAAE,MAAM,cAAc,SAAAD,SAAO,CAAE;AAChD;QACF,WAAW,wBAAwB,yBAAyB,MAAM;AAChE,gBAAMA,WAAU,sBAAsB,KAAK,UAAU,IAAI,CAAC,KAAK,KAAK,UAClE,oBAAoB,CACrB;AAED,eAAK,YAAY,EAAE,MAAM,cAAc,SAAAA,SAAO,CAAE;AAChD;QACF;AAEA,YAAI;AACJ,YAAI;AACF,mBAAS,4BAA4B,EAAE,IAAI,MAAM,GAAG,MAAM,IAAI,IAAI;QACpE,SAAS,OAAO;AACd,gBAAMA,WAAU,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AACrE,eAAK,YAAY,EAAE,MAAM,cAAc,SAAAA,SAAO,CAAE;AAChD;QACF;AAGA,cAAM,aAAa,MAAM,GAAG,SAAS,QAAQ,IAAI;AACjD,cAAM,UAAU,uBAAA,MAAI,yCAAA,KAAA,yDAAA,EAA6B,KAAjC,MAAkC,UAAU;AAC5D,aAAK,YAAY,EAAE,MAAM,cAAc,QAAO,CAAE;AAEhD,YAAI,sBAAsB;AACxB;QACF;MACF;IACF;AAEA;EACF;;;AAxSE,SAAO,uBAAA,MAAI,yCAAA,KAAA,6CAAA,EAAiB,KAArB,IAAI,EAAoB,WAAW;AAC5C,GAAC,gDAAA,SAAAE,iDAAA;AAYC,MAAI,IAAI,KAAK,SAAS;AACtB,SAAO,MAAM,GAAG;AACd,UAAM,UAAU,KAAK,SAAS,CAAC;AAC/B,QAAI,mBAAmB,OAAO,GAAG;AAE/B,YAAM,MAA4C;QAChD,GAAG;QACH,SAAU,QAAkC,WAAW;QACvD,SAAU,QAAkC,WAAW;;AAEzD,aAAO;IACT;EACF;AACA,QAAM,IAAI,YAAY,4EAA4E;AACpG,GAAC,yDAAA,SAAAC,0DAAA;AAYC,WAAS,IAAI,KAAK,SAAS,SAAS,GAAG,KAAK,GAAG,KAAK;AAClD,UAAM,UAAU,KAAK,SAAS,CAAC;AAC/B,QAAI,mBAAmB,OAAO,KAAK,SAAS,YAAY,QAAQ;AAC9D,aAAO,QAAQ,WAAW,OAAO,CAAC,MAAM,EAAE,SAAS,UAAU,EAAE,GAAG,EAAE,GAAG;IACzE;EACF;AAEA;AACF,GAAC,+DAAA,SAAAC,gEAAA;AAYC,WAAS,IAAI,KAAK,SAAS,SAAS,GAAG,KAAK,GAAG,KAAK;AAClD,UAAM,UAAU,KAAK,SAAS,CAAC;AAC/B,QACE,cAAc,OAAO,KACrB,QAAQ,WAAW,QACnB,OAAO,QAAQ,YAAY,YAC3B,KAAK,SAAS,KACZ,CAAC,MACC,EAAE,SAAS,eACX,EAAE,YAAY,KAAK,CAAC,MAAM,EAAE,SAAS,cAAc,EAAE,OAAO,QAAQ,YAAY,CAAC,GAErF;AACA,aAAO,QAAQ;IACjB;EACF;AAEA;AACF,GAAC,oDAAA,SAAAC,qDAAA;AAQC,QAAM,QAAyB;IAC7B,mBAAmB;IACnB,eAAe;IACf,cAAc;;AAEhB,aAAW,EAAE,MAAK,KAAM,KAAK,kBAAkB;AAC7C,QAAI,OAAO;AACT,YAAM,qBAAqB,MAAM;AACjC,YAAM,iBAAiB,MAAM;AAC7B,YAAM,gBAAgB,MAAM;IAC9B;EACF;AACA,SAAO;AACT,GAAC,+CAAA,SAAAC,8CAgCe,QAAkC;AAChD,MAAI,OAAO,KAAK,QAAQ,OAAO,IAAI,GAAG;AACpC,UAAM,IAAI,YACR,8HAA8H;EAElI;AACF,GAAC,4DAAA,SAAAC,2DAmK4B,YAAmB;AAC9C,SACE,OAAO,eAAe,WAAW,aAC/B,eAAe,SAAY,cAC3B,KAAK,UAAU,UAAU;AAE/B;;;AC5WI,IAAO,uBAAP,MAAO,8BAA6C,6BAGzD;EACC,OAAO,SACL,QACA,QACA,SAAuB;AAEvB,UAAM,SAAS,IAAI,sBAAoB;AACvC,UAAM,OAAO;MACX,GAAG;MACH,SAAS,EAAE,GAAG,SAAS,SAAS,6BAA6B,WAAU;;AAEzE,WAAO,KAAK,MAAM,OAAO,UAAU,QAAQ,QAAQ,IAAI,CAAC;AACxD,WAAO;EACT;EAES,YAEP,SACA,OAAgB,MAAI;AAEpB,UAAM,YAAY,SAAS,IAAI;AAC/B,QAAI,mBAAmB,OAAO,KAAK,QAAQ,SAAS;AAClD,WAAK,MAAM,WAAW,QAAQ,OAAiB;IACjD;EACF;;;;ACpDF,IAAM,MAAM;AACZ,IAAM,MAAM;AACZ,IAAM,MAAM;AACZ,IAAM,MAAM;AACZ,IAAM,OAAO;AACb,IAAM,OAAO;AACb,IAAM,MAAM;AACZ,IAAM,WAAW;AACjB,IAAM,iBAAiB;AAEvB,IAAM,MAAM,WAAW;AACvB,IAAM,UAAU,OAAO,OAAO,MAAM;AACpC,IAAM,OAAO,MAAM,MAAM;AACzB,IAAM,aAAa,MAAM;AACzB,IAAM,MAAM,OAAO;AAEnB,IAAM,QAAQ;EACZ;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;AAIF,IAAM,cAAN,cAA0B,MAAK;;AAE/B,IAAM,gBAAN,cAA4B,MAAK;;AAUjC,SAAS,UAAU,YAAoB,eAAuB,MAAM,KAAG;AACrE,MAAI,OAAO,eAAe,UAAU;AAClC,UAAM,IAAI,UAAU,sBAAsB,OAAO,UAAU,EAAE;EAC/D;AACA,MAAI,CAAC,WAAW,KAAI,GAAI;AACtB,UAAM,IAAI,MAAM,GAAG,UAAU,WAAW;EAC1C;AACA,SAAO,WAAW,WAAW,KAAI,GAAI,YAAY;AACnD;AAEA,IAAM,aAAa,CAAC,YAAoB,UAAiB;AACvD,QAAM,SAAS,WAAW;AAC1B,MAAI,QAAQ;AAEZ,QAAM,kBAAkB,CAAC,QAAe;AACtC,UAAM,IAAI,YAAY,GAAG,GAAG,gBAAgB,KAAK,EAAE;EACrD;AAEA,QAAM,sBAAsB,CAAC,QAAe;AAC1C,UAAM,IAAI,cAAc,GAAG,GAAG,gBAAgB,KAAK,EAAE;EACvD;AAEA,QAAM,WAAsB,MAAK;AAC/B,cAAS;AACT,QAAI,SAAS;AAAQ,sBAAgB,yBAAyB;AAC9D,QAAI,WAAW,KAAK,MAAM;AAAK,aAAO,SAAQ;AAC9C,QAAI,WAAW,KAAK,MAAM;AAAK,aAAO,SAAQ;AAC9C,QAAI,WAAW,KAAK,MAAM;AAAK,aAAO,SAAQ;AAC9C,QACE,WAAW,UAAU,OAAO,QAAQ,CAAC,MAAM,UAC1C,MAAM,OAAO,SAAS,SAAS,QAAQ,KAAK,OAAO,WAAW,WAAW,UAAU,KAAK,CAAC,GAC1F;AACA,eAAS;AACT,aAAO;IACT;AACA,QACE,WAAW,UAAU,OAAO,QAAQ,CAAC,MAAM,UAC1C,MAAM,OAAO,SAAS,SAAS,QAAQ,KAAK,OAAO,WAAW,WAAW,UAAU,KAAK,CAAC,GAC1F;AACA,eAAS;AACT,aAAO;IACT;AACA,QACE,WAAW,UAAU,OAAO,QAAQ,CAAC,MAAM,WAC1C,MAAM,OAAO,SAAS,SAAS,QAAQ,KAAK,QAAQ,WAAW,WAAW,UAAU,KAAK,CAAC,GAC3F;AACA,eAAS;AACT,aAAO;IACT;AACA,QACE,WAAW,UAAU,OAAO,QAAQ,CAAC,MAAM,cAC1C,MAAM,WAAW,SAAS,SAAS,QAAQ,KAAK,WAAW,WAAW,WAAW,UAAU,KAAK,CAAC,GAClG;AACA,eAAS;AACT,aAAO;IACT;AACA,QACE,WAAW,UAAU,OAAO,QAAQ,CAAC,MAAM,eAC1C,MAAM,iBAAiB,SACtB,IAAI,SAAS,SACb,SAAS,QAAQ,KACjB,YAAY,WAAW,WAAW,UAAU,KAAK,CAAC,GACpD;AACA,eAAS;AACT,aAAO;IACT;AACA,QACE,WAAW,UAAU,OAAO,QAAQ,CAAC,MAAM,SAC1C,MAAM,MAAM,SAAS,SAAS,QAAQ,KAAK,MAAM,WAAW,WAAW,UAAU,KAAK,CAAC,GACxF;AACA,eAAS;AACT,aAAO;IACT;AACA,WAAO,SAAQ;EACjB;AAEA,QAAM,WAAyB,MAAK;AAClC,UAAM,QAAQ;AACd,QAAIC,UAAS;AACb;AACA,WAAO,QAAQ,WAAW,WAAW,KAAK,MAAM,OAAQA,WAAU,WAAW,QAAQ,CAAC,MAAM,OAAQ;AAClG,MAAAA,UAAS,WAAW,KAAK,MAAM,OAAO,CAACA,UAAS;AAChD;IACF;AACA,QAAI,WAAW,OAAO,KAAK,KAAK,KAAK;AACnC,UAAI;AACF,eAAO,KAAK,MAAM,WAAW,UAAU,OAAO,EAAE,QAAQ,OAAOA,OAAM,CAAC,CAAC;MACzE,SAAS,GAAG;AACV,4BAAoB,OAAO,CAAC,CAAC;MAC/B;IACF,WAAW,MAAM,MAAM,OAAO;AAC5B,UAAI;AACF,eAAO,KAAK,MAAM,WAAW,UAAU,OAAO,QAAQ,OAAOA,OAAM,CAAC,IAAI,GAAG;MAC7E,SAAS,GAAG;AAEV,eAAO,KAAK,MAAM,WAAW,UAAU,OAAO,WAAW,YAAY,IAAI,CAAC,IAAI,GAAG;MACnF;IACF;AACA,oBAAgB,6BAA6B;EAC/C;AAEA,QAAM,WAAW,MAAK;AACpB;AACA,cAAS;AACT,UAAM,MAA2B,CAAA;AACjC,QAAI;AACF,aAAO,WAAW,KAAK,MAAM,KAAK;AAChC,kBAAS;AACT,YAAI,SAAS,UAAU,MAAM,MAAM;AAAO,iBAAO;AACjD,cAAM,MAAM,SAAQ;AACpB,kBAAS;AACT;AACA,YAAI;AACF,gBAAM,QAAQ,SAAQ;AACtB,iBAAO,eAAe,KAAK,KAAK,EAAE,OAAO,UAAU,MAAM,YAAY,MAAM,cAAc,KAAI,CAAE;QACjG,SAAS,GAAG;AACV,cAAI,MAAM,MAAM;AAAO,mBAAO;;AACzB,kBAAM;QACb;AACA,kBAAS;AACT,YAAI,WAAW,KAAK,MAAM;AAAK;MACjC;IACF,SAAS,GAAG;AACV,UAAI,MAAM,MAAM;AAAO,eAAO;;AACzB,wBAAgB,+BAA+B;IACtD;AACA;AACA,WAAO;EACT;AAEA,QAAM,WAAW,MAAK;AACpB;AACA,UAAM,MAAM,CAAA;AACZ,QAAI;AACF,aAAO,WAAW,KAAK,MAAM,KAAK;AAChC,YAAI,KAAK,SAAQ,CAAE;AACnB,kBAAS;AACT,YAAI,WAAW,KAAK,MAAM,KAAK;AAC7B;QACF;MACF;IACF,SAAS,GAAG;AACV,UAAI,MAAM,MAAM,OAAO;AACrB,eAAO;MACT;AACA,sBAAgB,8BAA8B;IAChD;AACA;AACA,WAAO;EACT;AAEA,QAAM,WAAW,MAAK;AACpB,QAAI,UAAU,GAAG;AACf,UAAI,eAAe,OAAO,MAAM,MAAM;AAAO,wBAAgB,sBAAsB;AACnF,UAAI;AACF,eAAO,KAAK,MAAM,UAAU;MAC9B,SAAS,GAAG;AACV,YAAI,MAAM,MAAM,OAAO;AACrB,cAAI;AACF,gBAAI,QAAQ,WAAW,WAAW,SAAS,CAAC;AAC1C,qBAAO,KAAK,MAAM,WAAW,UAAU,GAAG,WAAW,YAAY,GAAG,CAAC,CAAC;AACxE,mBAAO,KAAK,MAAM,WAAW,UAAU,GAAG,WAAW,YAAY,GAAG,CAAC,CAAC;UACxE,SAASC,IAAG;UAAC;QACf;AACA,4BAAoB,OAAO,CAAC,CAAC;MAC/B;IACF;AAEA,UAAM,QAAQ;AAEd,QAAI,WAAW,KAAK,MAAM;AAAK;AAC/B,WAAO,WAAW,KAAK,KAAK,CAAC,MAAM,SAAS,WAAW,KAAK,CAAE;AAAG;AAEjE,QAAI,SAAS,UAAU,EAAE,MAAM,MAAM;AAAQ,sBAAgB,6BAA6B;AAE1F,QAAI;AACF,aAAO,KAAK,MAAM,WAAW,UAAU,OAAO,KAAK,CAAC;IACtD,SAAS,GAAG;AACV,UAAI,WAAW,UAAU,OAAO,KAAK,MAAM,OAAO,MAAM,MAAM;AAC5D,wBAAgB,sBAAsB;AACxC,UAAI;AACF,eAAO,KAAK,MAAM,WAAW,UAAU,OAAO,WAAW,YAAY,GAAG,CAAC,CAAC;MAC5E,SAASA,IAAG;AACV,4BAAoB,OAAOA,EAAC,CAAC;MAC/B;IACF;EACF;AAEA,QAAM,YAAY,MAAK;AACrB,WAAO,QAAQ,UAAU,SAAU,SAAS,WAAW,KAAK,CAAE,GAAG;AAC/D;IACF;EACF;AAEA,SAAO,SAAQ;AACjB;AAGA,IAAM,eAAe,CAAC,UAAkB,UAAU,OAAO,MAAM,MAAM,MAAM,GAAG;;;;;;;;;;;;;;;ACpHxE,IAAO,uBAAP,MAAO,8BACH,6BAA0E;EAOlF,YAAY,QAAyC;AACnD,UAAK;;AALP,iCAAA,IAAA,MAAA,MAAA;AACA,4CAAA,IAAA,MAAA,MAAA;AACA,wDAAA,IAAA,MAAA,MAAA;AAIE,2BAAA,MAAI,8BAAW,QAAM,GAAA;AACrB,2BAAA,MAAI,yCAAsB,CAAA,GAAE,GAAA;EAC9B;EAEA,IAAI,gCAA6B;AAC/B,WAAO,uBAAA,MAAI,qDAAA,GAAA;EACb;;;;;;;;EASA,OAAO,mBAAmB,QAAsB;AAC9C,UAAM,SAAS,IAAI,sBAAqB,IAAI;AAC5C,WAAO,KAAK,MAAM,OAAO,oBAAoB,MAAM,CAAC;AACpD,WAAO;EACT;EAEA,OAAO,qBACL,QACA,QACA,SAAwB;AAExB,UAAM,SAAS,IAAI,sBAA8B,MAA6C;AAC9F,WAAO,KAAK,MACV,OAAO,mBACL,QACA,EAAE,GAAG,QAAQ,QAAQ,KAAI,GACzB,EAAE,GAAG,SAAS,SAAS,EAAE,GAAG,SAAS,SAAS,6BAA6B,SAAQ,EAAE,CAAE,CACxF;AAEH,WAAO;EACT;EAoMmB,MAAM,sBACvB,QACA,QACA,SAAwB;AAExB,UAAM;AACN,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AACA,2BAAA,MAAI,iCAAA,KAAA,kCAAA,EAAc,KAAlB,IAAI;AAEJ,UAAM,SAAS,MAAM,OAAO,KAAK,YAAY,OAC3C,EAAE,GAAG,QAAQ,QAAQ,KAAI,GACzB,EAAE,GAAG,SAAS,QAAQ,KAAK,WAAW,OAAM,CAAE;AAEhD,SAAK,WAAU;AACf,qBAAiB,SAAS,QAAQ;AAChC,6BAAA,MAAI,iCAAA,KAAA,8BAAA,EAAU,KAAd,MAAe,KAAK;IACtB;AACA,QAAI,OAAO,WAAW,QAAQ,SAAS;AACrC,YAAM,IAAI,kBAAiB;IAC7B;AACA,WAAO,KAAK,mBAAmB,uBAAA,MAAI,iCAAA,KAAA,gCAAA,EAAY,KAAhB,IAAI,CAAc;EACnD;EAEU,MAAM,oBACd,gBACA,SAAwB;AAExB,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AACA,2BAAA,MAAI,iCAAA,KAAA,kCAAA,EAAc,KAAlB,IAAI;AACJ,SAAK,WAAU;AACf,UAAM,SAAS,OAAO,mBAAwC,gBAAgB,KAAK,UAAU;AAC7F,QAAI;AACJ,qBAAiB,SAAS,QAAQ;AAChC,UAAI,UAAU,WAAW,MAAM,IAAI;AAEjC,aAAK,mBAAmB,uBAAA,MAAI,iCAAA,KAAA,gCAAA,EAAY,KAAhB,IAAI,CAAc;MAC5C;AAEA,6BAAA,MAAI,iCAAA,KAAA,8BAAA,EAAU,KAAd,MAAe,KAAK;AACpB,eAAS,MAAM;IACjB;AACA,QAAI,OAAO,WAAW,QAAQ,SAAS;AACrC,YAAM,IAAI,kBAAiB;IAC7B;AACA,WAAO,KAAK,mBAAmB,uBAAA,MAAI,iCAAA,KAAA,gCAAA,EAAY,KAAhB,IAAI,CAAc;EACnD;EAuHA,EAAA,+BAAA,oBAAA,QAAA,GAAA,0CAAA,oBAAA,QAAA,GAAA,sDAAA,oBAAA,QAAA,GAAA,kCAAA,oBAAA,QAAA,GAAA,qCAAA,SAAAC,sCAAA;AA7WE,QAAI,KAAK;AAAO;AAChB,2BAAA,MAAI,qDAAkC,QAAS,GAAA;EACjD,GAAC,4CAAA,SAAAC,2CAEoB,QAAqC;AACxD,QAAI,QAAQ,uBAAA,MAAI,yCAAA,GAAA,EAAoB,OAAO,KAAK;AAChD,QAAI,OAAO;AACT,aAAO;IACT;AAEA,YAAQ;MACN,cAAc;MACd,cAAc;MACd,uBAAuB;MACvB,uBAAuB;MACvB,iBAAiB,oBAAI,IAAG;MACxB,yBAAyB;;AAE3B,2BAAA,MAAI,yCAAA,GAAA,EAAoB,OAAO,KAAK,IAAI;AACxC,WAAO;EACT,GAAC,iCAAA,SAAAC,gCAE8C,OAA0B;AACvE,QAAI,KAAK;AAAO;AAEhB,UAAM,aAAa,uBAAA,MAAI,iCAAA,KAAA,8CAAA,EAA0B,KAA9B,MAA+B,KAAK;AACvD,SAAK,MAAM,SAAS,OAAO,UAAU;AAErC,eAAW,UAAU,MAAM,SAAS;AAClC,YAAM,iBAAiB,WAAW,QAAQ,OAAO,KAAK;AAEtD,UACE,OAAO,MAAM,WAAW,QACxB,eAAe,SAAS,SAAS,eACjC,eAAe,SAAS,SACxB;AACA,aAAK,MAAM,WAAW,OAAO,MAAM,SAAS,eAAe,QAAQ,OAAO;AAC1E,aAAK,MAAM,iBAAiB;UAC1B,OAAO,OAAO,MAAM;UACpB,UAAU,eAAe,QAAQ;UACjC,QAAQ,eAAe,QAAQ;SAChC;MACH;AAEA,UACE,OAAO,MAAM,WAAW,QACxB,eAAe,SAAS,SAAS,eACjC,eAAe,SAAS,SACxB;AACA,aAAK,MAAM,iBAAiB;UAC1B,OAAO,OAAO,MAAM;UACpB,UAAU,eAAe,QAAQ;SAClC;MACH;AAEA,UAAI,OAAO,UAAU,WAAW,QAAQ,eAAe,SAAS,SAAS,aAAa;AACpF,aAAK,MAAM,0BAA0B;UACnC,SAAS,OAAO,UAAU;UAC1B,UAAU,eAAe,UAAU,WAAW,CAAA;SAC/C;MACH;AAEA,UAAI,OAAO,UAAU,WAAW,QAAQ,eAAe,SAAS,SAAS,aAAa;AACpF,aAAK,MAAM,0BAA0B;UACnC,SAAS,OAAO,UAAU;UAC1B,UAAU,eAAe,UAAU,WAAW,CAAA;SAC/C;MACH;AAEA,YAAM,QAAQ,uBAAA,MAAI,iCAAA,KAAA,yCAAA,EAAqB,KAAzB,MAA0B,cAAc;AAEtD,UAAI,eAAe,eAAe;AAChC,+BAAA,MAAI,iCAAA,KAAA,2CAAA,EAAuB,KAA3B,MAA4B,cAAc;AAE1C,YAAI,MAAM,2BAA2B,MAAM;AACzC,iCAAA,MAAI,iCAAA,KAAA,2CAAA,EAAuB,KAA3B,MAA4B,gBAAgB,MAAM,uBAAuB;QAC3E;MACF;AAEA,iBAAW,YAAY,OAAO,MAAM,cAAc,CAAA,GAAI;AACpD,YAAI,MAAM,4BAA4B,SAAS,OAAO;AACpD,iCAAA,MAAI,iCAAA,KAAA,2CAAA,EAAuB,KAA3B,MAA4B,cAAc;AAG1C,cAAI,MAAM,2BAA2B,MAAM;AACzC,mCAAA,MAAI,iCAAA,KAAA,2CAAA,EAAuB,KAA3B,MAA4B,gBAAgB,MAAM,uBAAuB;UAC3E;QACF;AAEA,cAAM,0BAA0B,SAAS;MAC3C;AAEA,iBAAW,iBAAiB,OAAO,MAAM,cAAc,CAAA,GAAI;AACzD,cAAM,mBAAmB,eAAe,QAAQ,aAAa,cAAc,KAAK;AAChF,YAAI,CAAC,kBAAkB,MAAM;AAC3B;QACF;AAEA,YAAI,kBAAkB,SAAS,YAAY;AACzC,eAAK,MAAM,uCAAuC;YAChD,MAAM,iBAAiB,UAAU;YACjC,OAAO,cAAc;YACrB,WAAW,iBAAiB,SAAS;YACrC,kBAAkB,iBAAiB,SAAS;YAC5C,iBAAiB,cAAc,UAAU,aAAa;WACvD;QACH,OAAO;AACL,sBAAY,kBAAkB,IAAI;QACpC;MACF;IACF;EACF,GAAC,8CAAA,SAAAC,6CAEsB,gBAA+C,eAAqB;AACzF,UAAM,QAAQ,uBAAA,MAAI,iCAAA,KAAA,yCAAA,EAAqB,KAAzB,MAA0B,cAAc;AACtD,QAAI,MAAM,gBAAgB,IAAI,aAAa,GAAG;AAE5C;IACF;AAEA,UAAM,mBAAmB,eAAe,QAAQ,aAAa,aAAa;AAC1E,QAAI,CAAC,kBAAkB;AACrB,YAAM,IAAI,MAAM,uBAAuB;IACzC;AACA,QAAI,CAAC,iBAAiB,MAAM;AAC1B,YAAM,IAAI,MAAM,mCAAmC;IACrD;AAEA,QAAI,iBAAiB,SAAS,YAAY;AACxC,YAAM,YAAY,uBAAA,MAAI,8BAAA,GAAA,GAAU,OAAO,KACrC,CAACC,UAAS,6BAA6BA,KAAI,KAAKA,MAAK,SAAS,SAAS,iBAAiB,SAAS,IAAI;AAGvG,WAAK,MAAM,sCAAsC;QAC/C,MAAM,iBAAiB,SAAS;QAChC,OAAO;QACP,WAAW,iBAAiB,SAAS;QACrC,kBACE,mBAAmB,SAAS,IAAI,UAAU,UAAU,iBAAiB,SAAS,SAAS,IACrF,WAAW,SAAS,SAAS,KAAK,MAAM,iBAAiB,SAAS,SAAS,IAC3E;OACL;IACH,OAAO;AACL,kBAAY,iBAAiB,IAAI;IACnC;EACF,GAAC,8CAAA,SAAAC,6CAEsB,gBAA6C;AAClE,UAAM,QAAQ,uBAAA,MAAI,iCAAA,KAAA,yCAAA,EAAqB,KAAzB,MAA0B,cAAc;AAEtD,QAAI,eAAe,QAAQ,WAAW,CAAC,MAAM,cAAc;AACzD,YAAM,eAAe;AAErB,YAAM,iBAAiB,uBAAA,MAAI,iCAAA,KAAA,oDAAA,EAAgC,KAApC,IAAI;AAE3B,WAAK,MAAM,gBAAgB;QACzB,SAAS,eAAe,QAAQ;QAChC,QAAQ,iBAAiB,eAAe,UAAU,eAAe,QAAQ,OAAO,IAAK;OACtF;IACH;AAEA,QAAI,eAAe,QAAQ,WAAW,CAAC,MAAM,cAAc;AACzD,YAAM,eAAe;AAErB,WAAK,MAAM,gBAAgB,EAAE,SAAS,eAAe,QAAQ,QAAO,CAAE;IACxE;AAEA,QAAI,eAAe,UAAU,WAAW,CAAC,MAAM,uBAAuB;AACpE,YAAM,wBAAwB;AAE9B,WAAK,MAAM,yBAAyB,EAAE,SAAS,eAAe,SAAS,QAAO,CAAE;IAClF;AAEA,QAAI,eAAe,UAAU,WAAW,CAAC,MAAM,uBAAuB;AACpE,YAAM,wBAAwB;AAE9B,WAAK,MAAM,yBAAyB,EAAE,SAAS,eAAe,SAAS,QAAO,CAAE;IAClF;EACF,GAAC,mCAAA,SAAAC,oCAAA;AAGC,QAAI,KAAK,OAAO;AACd,YAAM,IAAI,YAAY,yCAAyC;IACjE;AACA,UAAM,WAAW,uBAAA,MAAI,qDAAA,GAAA;AACrB,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,YAAY,0CAA0C;IAClE;AACA,2BAAA,MAAI,qDAAkC,QAAS,GAAA;AAC/C,2BAAA,MAAI,yCAAsB,CAAA,GAAE,GAAA;AAC5B,WAAO,uBAAuB,UAAU,uBAAA,MAAI,8BAAA,GAAA,CAAQ;EACtD,GAAC,uDAAA,SAAAC,wDAAA;AA0DC,UAAM,iBAAiB,uBAAA,MAAI,8BAAA,GAAA,GAAU;AACrC,QAAI,6BAAsC,cAAc,GAAG;AACzD,aAAO;IACT;AAEA,WAAO;EACT,GAAC,iDAAA,SAAAC,gDAEyB,OAA0B;;AAClD,QAAI,WAAW,uBAAA,MAAI,qDAAA,GAAA;AACnB,UAAM,EAAE,SAAS,GAAG,KAAI,IAAK;AAC7B,QAAI,CAAC,UAAU;AACb,iBAAW,uBAAA,MAAI,qDAAkC;QAC/C,GAAG;QACH,SAAS,CAAA;SACV,GAAA;IACH,OAAO;AACL,aAAO,OAAO,UAAU,IAAI;IAC9B;AAEA,eAAW,EAAE,OAAO,eAAe,OAAO,WAAW,MAAM,GAAG,MAAK,KAAM,MAAM,SAAS;AACtF,UAAI,SAAS,SAAS,QAAQ,KAAK;AACnC,UAAI,CAAC,QAAQ;AACX,iBAAS,SAAS,QAAQ,KAAK,IAAI,EAAE,eAAe,OAAO,SAAS,CAAA,GAAI,UAAU,GAAG,MAAK;MAC5F;AAEA,UAAI,UAAU;AACZ,YAAI,CAAC,OAAO,UAAU;AACpB,iBAAO,WAAW,OAAO,OAAO,CAAA,GAAI,QAAQ;QAC9C,OAAO;AACL,gBAAM,EAAE,SAAAC,UAAS,SAAAC,UAAS,GAAGC,MAAI,IAAK;AACtC,wBAAcA,KAAI;AAClB,iBAAO,OAAO,OAAO,UAAUA,KAAI;AAEnC,cAAIF,UAAS;AACX,aAAAG,MAAA,OAAO,UAAS,YAAOA,IAAP,UAAY,CAAA;AAC5B,mBAAO,SAAS,QAAQ,KAAK,GAAGH,QAAO;UACzC;AAEA,cAAIC,UAAS;AACX,aAAA,KAAA,OAAO,UAAS,YAAO,GAAP,UAAY,CAAA;AAC5B,mBAAO,SAAS,QAAQ,KAAK,GAAGA,QAAO;UACzC;QACF;MACF;AAEA,UAAI,eAAe;AACjB,eAAO,gBAAgB;AAEvB,YAAI,uBAAA,MAAI,8BAAA,GAAA,KAAY,sBAAsB,uBAAA,MAAI,8BAAA,GAAA,CAAQ,GAAG;AACvD,cAAI,kBAAkB,UAAU;AAC9B,kBAAM,IAAI,wBAAuB;UACnC;AAEA,cAAI,kBAAkB,kBAAkB;AACtC,kBAAM,IAAI,+BAA8B;UAC1C;QACF;MACF;AAEA,aAAO,OAAO,QAAQ,KAAK;AAE3B,UAAI,CAAC;AAAO;AAEZ,YAAM,EAAE,SAAS,SAAS,eAAe,MAAM,YAAY,GAAGC,MAAI,IAAK;AACvE,oBAAcA,KAAI;AAClB,aAAO,OAAO,OAAO,SAASA,KAAI;AAElC,UAAI,SAAS;AACX,eAAO,QAAQ,WAAW,OAAO,QAAQ,WAAW,MAAM;MAC5D;AAEA,UAAI;AAAM,eAAO,QAAQ,OAAO;AAChC,UAAI,eAAe;AACjB,YAAI,CAAC,OAAO,QAAQ,eAAe;AACjC,iBAAO,QAAQ,gBAAgB;QACjC,OAAO;AACL,cAAI,cAAc;AAAM,mBAAO,QAAQ,cAAc,OAAO,cAAc;AAC1E,cAAI,cAAc,WAAW;AAC3B,aAAA,KAAA,OAAO,QAAQ,eAAc,cAAS,GAAT,YAAc;AAC3C,mBAAO,QAAQ,cAAc,aAAa,cAAc;UAC1D;QACF;MACF;AACA,UAAI,SAAS;AACX,eAAO,QAAQ,WAAW,OAAO,QAAQ,WAAW,MAAM;AAE1D,YAAI,CAAC,OAAO,QAAQ,WAAW,uBAAA,MAAI,iCAAA,KAAA,oDAAA,EAAgC,KAApC,IAAI,GAAoC;AACrE,iBAAO,QAAQ,SAAS,aAAa,OAAO,QAAQ,OAAO;QAC7D;MACF;AAEA,UAAI,YAAY;AACd,YAAI,CAAC,OAAO,QAAQ;AAAY,iBAAO,QAAQ,aAAa,CAAA;AAE5D,mBAAW,EAAE,OAAAE,QAAO,IAAI,MAAM,UAAU,IAAI,GAAGF,MAAI,KAAM,YAAY;AACnE,gBAAM,aAAY,KAAC,OAAO,QAAQ,YAAWE,MAAK,MAAA,GAALA,MAAK,IAChD,CAAA;AACF,iBAAO,OAAO,WAAWF,KAAI;AAC7B,cAAI;AAAI,sBAAU,KAAK;AACvB,cAAI;AAAM,sBAAU,OAAO;AAC3B,cAAI;AAAI,sBAAU,aAAV,UAAU,WAAa,EAAE,MAAM,GAAG,QAAQ,IAAI,WAAW,GAAE;AACnE,cAAI,IAAI;AAAM,sBAAU,SAAU,OAAO,GAAG;AAC5C,cAAI,IAAI,WAAW;AACjB,sBAAU,SAAU,aAAa,GAAG;AAEpC,gBAAI,oBAAoB,uBAAA,MAAI,8BAAA,GAAA,GAAU,SAAS,GAAG;AAChD,wBAAU,SAAU,mBAAmB,aAAa,UAAU,SAAU,SAAS;YACnF;UACF;QACF;MACF;IACF;AACA,WAAO;EACT,GAEC,OAAO,cAAa,IAAC;AACpB,UAAM,YAAmC,CAAA;AACzC,UAAM,YAGA,CAAA;AACN,QAAI,OAAO;AAEX,SAAK,GAAG,SAAS,CAAC,UAAS;AACzB,YAAM,SAAS,UAAU,MAAK;AAC9B,UAAI,QAAQ;AACV,eAAO,QAAQ,KAAK;MACtB,OAAO;AACL,kBAAU,KAAK,KAAK;MACtB;IACF,CAAC;AAED,SAAK,GAAG,OAAO,MAAK;AAClB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,QAAQ,MAAS;MAC1B;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,SAAK,GAAG,SAAS,CAAC,QAAO;AACvB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,OAAO,GAAG;MACnB;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,SAAK,GAAG,SAAS,CAAC,QAAO;AACvB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,OAAO,GAAG;MACnB;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,WAAO;MACL,MAAM,YAAyD;AAC7D,YAAI,CAAC,UAAU,QAAQ;AACrB,cAAI,MAAM;AACR,mBAAO,EAAE,OAAO,QAAW,MAAM,KAAI;UACvC;AACA,iBAAO,IAAI,QAAyC,CAAC,SAAS,WAC5D,UAAU,KAAK,EAAE,SAAS,OAAM,CAAE,CAAC,EACnC,KAAK,CAACG,WAAWA,SAAQ,EAAE,OAAOA,QAAO,MAAM,MAAK,IAAK,EAAE,OAAO,QAAW,MAAM,KAAI,CAAG;QAC9F;AACA,cAAM,QAAQ,UAAU,MAAK;AAC7B,eAAO,EAAE,OAAO,OAAO,MAAM,MAAK;MACpC;MACA,QAAQ,YAAW;AACjB,aAAK,MAAK;AACV,eAAO,EAAE,OAAO,QAAW,MAAM,KAAI;MACvC;;EAEJ;EAEA,mBAAgB;AACd,UAAM,SAAS,IAAI,OAAO,KAAK,OAAO,aAAa,EAAE,KAAK,IAAI,GAAG,KAAK,UAAU;AAChF,WAAO,OAAO,iBAAgB;EAChC;;AAGF,SAAS,uBACP,UACA,QAAyC;AAEzC,QAAM,EAAE,IAAI,SAAS,SAAS,OAAO,oBAAoB,GAAG,KAAI,IAAK;AACrE,QAAM,aAA6B;IACjC,GAAG;IACH;IACA,SAAS,QAAQ,IACf,CAAC,EAAE,SAAS,eAAe,OAAO,UAAU,GAAG,WAAU,MAA6B;AACpF,UAAI,CAAC,eAAe;AAClB,cAAM,IAAI,YAAY,oCAAoC,KAAK,EAAE;MACnE;AAEA,YAAM,EAAE,UAAU,MAAM,eAAe,YAAY,GAAG,YAAW,IAAK;AACtE,YAAM,OAAO,QAAQ;AACrB,UAAI,CAAC,MAAM;AACT,cAAM,IAAI,YAAY,2BAA2B,KAAK,EAAE;MAC1D;AAEA,UAAI,eAAe;AACjB,cAAM,EAAE,WAAW,MAAM,KAAI,IAAK;AAClC,YAAI,QAAQ,MAAM;AAChB,gBAAM,IAAI,YAAY,8CAA8C,KAAK,EAAE;QAC7E;AAEA,YAAI,CAAC,MAAM;AACT,gBAAM,IAAI,YAAY,yCAAyC,KAAK,EAAE;QACxE;AAEA,eAAO;UACL,GAAG;UACH,SAAS;YACP;YACA,eAAe,EAAE,WAAW,MAAM,KAAI;YACtC;YACA,SAAS,QAAQ,WAAW;;UAE9B;UACA;UACA;;MAEJ;AAEA,UAAI,YAAY;AACd,eAAO;UACL,GAAG;UACH;UACA;UACA;UACA,SAAS;YACP,GAAG;YACH;YACA;YACA,SAAS,QAAQ,WAAW;YAC5B,YAAY,WAAW,IAAI,CAAC,WAAW,MAAK;AAC1C,oBAAM,EAAE,UAAU,IAAI,MAAM,IAAAC,KAAI,GAAG,SAAQ,IAAK;AAChD,oBAAM,EAAE,WAAW,MAAM,MAAM,GAAG,OAAM,IAAK,MAAM,CAAA;AACnD,kBAAIA,OAAM,MAAM;AACd,sBAAM,IAAI,YAAY,mBAAmB,KAAK,gBAAgB,CAAC;EAAS,IAAI,QAAQ,CAAC,EAAE;cACzF;AACA,kBAAI,QAAQ,MAAM;AAChB,sBAAM,IAAI,YAAY,mBAAmB,KAAK,gBAAgB,CAAC;EAAW,IAAI,QAAQ,CAAC,EAAE;cAC3F;AACA,kBAAI,QAAQ,MAAM;AAChB,sBAAM,IAAI,YACR,mBAAmB,KAAK,gBAAgB,CAAC;EAAoB,IAAI,QAAQ,CAAC,EAAE;cAEhF;AACA,kBAAI,QAAQ,MAAM;AAChB,sBAAM,IAAI,YACR,mBAAmB,KAAK,gBAAgB,CAAC;EAAyB,IAAI,QAAQ,CAAC,EAAE;cAErF;AAEA,qBAAO,EAAE,GAAG,UAAU,IAAAA,KAAI,MAAM,UAAU,EAAE,GAAG,QAAQ,MAAM,WAAW,KAAI,EAAE;YAChF,CAAC;;;MAGP;AACA,aAAO;QACL,GAAG;QACH,SAAS,EAAE,GAAG,aAAa,SAAS,MAAM,SAAS,QAAQ,WAAW,KAAI;QAC1E;QACA;QACA;;IAEJ,CAAC;IAEH;IACA;IACA,QAAQ;IACR,GAAI,qBAAqB,EAAE,mBAAkB,IAAK,CAAA;;AAGpD,SAAO,yBAAyB,YAAY,MAAM;AACpD;AAEA,SAAS,IAAI,GAAU;AACrB,SAAO,KAAK,UAAU,CAAC;AACzB;AA+JA,SAAS,cAA4B,KAAqB;AACxD;AACF;AAEA,SAAS,YAAY,IAAS;AAAG;;;ACh1B3B,IAAO,gCAAP,MAAO,uCACH,qBAA6B;EAGrC,OAAgB,mBAAmB,QAAsB;AACvD,UAAM,SAAS,IAAI,+BAA8B,IAAI;AACrD,WAAO,KAAK,MAAM,OAAO,oBAAoB,MAAM,CAAC;AACpD,WAAO;EACT;EAEA,OAAO,SACL,QACA,QACA,SAAuB;AAEvB,UAAM,SAAS,IAAI;;MAEjB;IAAM;AAER,UAAM,OAAO;MACX,GAAG;MACH,SAAS,EAAE,GAAG,SAAS,SAAS,6BAA6B,WAAU;;AAEzE,WAAO,KAAK,MAAM,OAAO,UAAU,QAAQ,QAAQ,IAAI,CAAC;AACxD,WAAO;EACT;;;;AC1BI,IAAO,cAAP,cAA2B,YAAW;EAA5C,cAAA;;AACE,SAAA,WAAiC,IAAgB,SAAS,KAAK,OAAO;EAuLxE;EA5IE,OACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,qBAAqB,EAAE,MAAM,GAAG,SAAS,QAAQ,KAAK,UAAU,MAAK,CAAE;EAGlG;;;;;;;;;;;EAYA,SAAS,cAAsB,SAAwB;AACrD,WAAO,KAAK,QAAQ,IAAI,yBAAyB,YAAY,IAAI,OAAO;EAC1E;;;;;;;;;;;;;;EAeA,OACE,cACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,yBAAyB,YAAY,IAAI,EAAE,MAAM,GAAG,QAAO,CAAE;EACxF;;;;;;;;;;;;;EAcA,KACE,QAAqD,CAAA,GACrD,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,qBAAqB,YAA4B,EAAE,OAAO,GAAG,QAAO,CAAE;EACvG;;;;;;;;;;;EAYA,OAAO,cAAsB,SAAwB;AACnD,WAAO,KAAK,QAAQ,OAAO,yBAAyB,YAAY,IAAI,OAAO;EAC7E;EAEA,MACE,MACA,SAAwB;AAExB,uBAAmB,KAAK,KAAK;AAE7B,WAAO,KAAK,QAAQ,KAAK,YACtB,OAAO,MAAM;MACZ,GAAG;MACH,SAAS;QACP,GAAG,SAAS;QACZ,6BAA6B;;KAEhC,EACA,YAAY,CAAC,eAAe,oBAAoB,YAAY,IAAI,CAAC;EACtE;EAqBA,SAIE,MACA,SAAuB;AAEvB,QAAI,KAAK,QAAQ;AACf,aAAO,8BAA8B,SACnC,KAAK,SACL,MACA,OAAO;IAEX;AAEA,WAAO,qBAAqB,SAAS,KAAK,SAAS,MAA6C,OAAO;EACzG;;;;EAKA,OACE,MACA,SAAwB;AAExB,WAAO,qBAAqB,qBAAqB,KAAK,SAAS,MAAM,OAAO;EAC9E;;AAsvDF,YAAY,WAAW;;;AC/4DjB,IAAO,OAAP,cAAoB,YAAW;EAArC,cAAA;;AACE,SAAA,cAA0C,IAAmB,YAAY,KAAK,OAAO;EACvF;;AAIA,KAAK,cAAc;;;AC7CnB,IAAM,+BAA+C,uBAAO,8BAA8B;AAgB1F,UAAU,eAAe,SAAoB;AAC3C,MAAI,CAAC;AAAS;AAEd,MAAI,gCAAgC,SAAS;AAC3C,UAAM,EAAE,QAAQ,MAAK,IAAK;AAC1B,WAAO,OAAO,QAAO;AACrB,eAAW,QAAQ,OAAO;AACxB,YAAM,CAAC,MAAM,IAAI;IACnB;AACA;EACF;AAEA,MAAI,cAAc;AAClB,MAAI;AACJ,MAAI,mBAAmB,SAAS;AAC9B,WAAO,QAAQ,QAAO;EACxB,WAAW,gBAAgB,OAAO,GAAG;AACnC,WAAO;EACT,OAAO;AACL,kBAAc;AACd,WAAO,OAAO,QAAQ,WAAW,CAAA,CAAE;EACrC;AACA,WAAS,OAAO,MAAM;AACpB,UAAM,OAAO,IAAI,CAAC;AAClB,QAAI,OAAO,SAAS;AAAU,YAAM,IAAI,UAAU,qCAAqC;AACvF,UAAM,SAAS,gBAAgB,IAAI,CAAC,CAAC,IAAI,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;AACzD,QAAI,WAAW;AACf,eAAW,SAAS,QAAQ;AAC1B,UAAI,UAAU;AAAW;AAIzB,UAAI,eAAe,CAAC,UAAU;AAC5B,mBAAW;AACX,cAAM,CAAC,MAAM,IAAI;MACnB;AACA,YAAM,CAAC,MAAM,KAAK;IACpB;EACF;AACF;AAEO,IAAM,eAAe,CAAC,eAA8C;AACzE,QAAM,gBAAgB,IAAI,QAAO;AACjC,QAAM,cAAc,oBAAI,IAAG;AAC3B,aAAW,WAAW,YAAY;AAChC,UAAM,cAAc,oBAAI,IAAG;AAC3B,eAAW,CAAC,MAAM,KAAK,KAAK,eAAe,OAAO,GAAG;AACnD,YAAM,YAAY,KAAK,YAAW;AAClC,UAAI,CAAC,YAAY,IAAI,SAAS,GAAG;AAC/B,sBAAc,OAAO,IAAI;AACzB,oBAAY,IAAI,SAAS;MAC3B;AACA,UAAI,UAAU,MAAM;AAClB,sBAAc,OAAO,IAAI;AACzB,oBAAY,IAAI,SAAS;MAC3B,OAAO;AACL,sBAAc,OAAO,MAAM,KAAK;AAChC,oBAAY,OAAO,SAAS;MAC9B;IACF;EACF;AACA,SAAO,EAAE,CAAC,4BAA4B,GAAG,MAAM,QAAQ,eAAe,OAAO,YAAW;AAC1F;;;ACpFM,IAAO,SAAP,cAAsB,YAAW;;;;;;;;;;;;;;;;;;EAkBrC,OAAO,MAA0B,SAAwB;AACvD,WAAO,KAAK,QAAQ,KAAK,iBAAiB;MACxC;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,2BAA0B,GAAI,SAAS,OAAO,CAAC;MAChF,kBAAkB;KACnB;EACH;;;;ACrBI,IAAO,iBAAP,cAA8B,YAAW;EAqC7C,OACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAClB,yBACA,4BACE;MACE;MACA,GAAG;MACH,QAAQ,KAAK,UAAU;MACvB,YAAY,EAAE,OAAO,KAAK,MAAK;OAEjC,KAAK,OAAO,CACb;EAEL;;;;ACtDI,IAAO,eAAP,cAA4B,YAAW;EAsB3C,OACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAClB,uBACA,4BAA4B,EAAE,MAAM,GAAG,SAAS,YAAY,EAAE,OAAO,KAAK,MAAK,EAAE,GAAI,KAAK,OAAO,CAAC;EAEtG;;;;ACPI,IAAO,QAAP,cAAqB,YAAW;EAAtC,cAAA;;AACE,SAAA,iBAAmD,IAAsB,eAAe,KAAK,OAAO;AACpG,SAAA,eAA6C,IAAoB,aAAa,KAAK,OAAO;AAC1F,SAAA,SAA2B,IAAc,OAAO,KAAK,OAAO;EAC9D;;AAkBA,MAAM,iBAAiB;AACvB,MAAM,eAAe;AACrB,MAAM,SAAS;;;AC/CT,IAAO,UAAP,cAAuB,YAAW;;;;EAItC,OAAO,MAAyB,SAAwB;AACtD,WAAO,KAAK,QAAQ,KAAK,YAAY,EAAE,MAAM,GAAG,QAAO,CAAE;EAC3D;;;;EAKA,SAAS,SAAiB,SAAwB;AAChD,WAAO,KAAK,QAAQ,IAAI,gBAAgB,OAAO,IAAI,OAAO;EAC5D;;;;EAKA,KACE,QAA4C,CAAA,GAC5C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,YAAY,YAAmB,EAAE,OAAO,GAAG,QAAO,CAAE;EACrF;;;;;;EAOA,OAAO,SAAiB,SAAwB;AAC9C,WAAO,KAAK,QAAQ,KAAK,gBAAgB,OAAO,WAAW,OAAO;EACpE;;;;AC3BI,IAAO,aAAP,cAA0B,YAAW;;;;;;EAMzC,OAAO,MAA6B,SAAwB;AAC1D,WAAO,KAAK,QAAQ,KAAK,eAAe;MACtC;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,SAAS,aAAqB,SAAwB;AACpD,WAAO,KAAK,QAAQ,IAAI,mBAAmB,WAAW,IAAI;MACxD,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OAAO,aAAqB,MAA6B,SAAwB;AAC/E,WAAO,KAAK,QAAQ,KAAK,mBAAmB,WAAW,IAAI;MACzD;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,KACE,QAAgD,CAAA,GAChD,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,eAAe,YAAuB;MACnE;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OAAO,aAAqB,SAAwB;AAClD,WAAO,KAAK,QAAQ,OAAO,mBAAmB,WAAW,IAAI;MAC3D,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;ACzEI,IAAO,WAAP,cAAwB,YAAW;;;;;;;;;;;;;;;;EAgBvC,OAAO,MAA2B,SAAwB;AACxD,WAAO,KAAK,QAAQ,KAAK,sBAAsB;MAC7C;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;ACtBI,IAAO,wBAAP,cAAqC,YAAW;;;;;;;;;;;;;;;;EAgBpD,OAAO,MAAwC,SAAwB;AACrE,WAAO,KAAK,QAAQ,KAAK,oCAAoC;MAC3D;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;ACPI,IAAO,WAAP,cAAwB,YAAW;EAAzC,cAAA;;AACE,SAAA,WAAiC,IAAgB,SAAS,KAAK,OAAO;AACtE,SAAA,wBACE,IAA6B,sBAAsB,KAAK,OAAO;EACnE;;AA8qFA,SAAS,WAAW;AACpB,SAAS,wBAAwB;;;AChsF3B,IAAOC,YAAP,cAAwB,YAAW;;;;;;;;;;;;;EAavC,OAAO,MAA2B,SAAwB;AACxD,WAAO,KAAK,QAAQ,KAAK,qBAAqB;MAC5C;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,kBAAiB,GAAI,SAAS,OAAO,CAAC;KAC/E;EACH;;;;;;;;;;;;EAaA,OAAO,WAAmB,SAAwB;AAChD,WAAO,KAAK,QAAQ,KAAK,yBAAyB,SAAS,WAAW;MACpE,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,kBAAiB,GAAI,SAAS,OAAO,CAAC;KAC/E;EACH;;;;AChCI,IAAO,UAAP,cAAuB,YAAW;;;;;;;;;;EAUtC,SAAS,UAAkB,SAAwB;AACjD,WAAO,KAAK,QAAQ,IAAI,wBAAwB,QAAQ,IAAI;MAC1D,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,kBAAiB,GAAI,SAAS,OAAO,CAAC;KAC/E;EACH;;;;;;;;;;;;EAaA,KACE,QAA6C,CAAA,GAC7C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,oBAAoB,wBAAuC;MACxF;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,kBAAiB,GAAI,SAAS,OAAO,CAAC;KAC/E;EACH;;;;;;;;;;;EAYA,OAAO,UAAkB,SAAwB;AAC/C,WAAO,KAAK,QAAQ,OAAO,wBAAwB,QAAQ,IAAI;MAC7D,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,kBAAiB,GAAI,SAAS,OAAO,CAAC;KAC/E;EACH;;;;;;;;;;;;;;EAeA,UACE,UACA,QAAkD,CAAA,GAClD,SAAwB;AAUxB,WAAO,KAAK,QAAQ,WAClB,wBAAwB,QAAQ,UAChC,wBAQA,EAAE,OAAO,GAAG,SAAS,SAAS,aAAa,CAAC,EAAE,eAAe,kBAAiB,GAAI,SAAS,OAAO,CAAC,EAAC,CAAE;EAE1G;;;;AC3EI,IAAO,UAAP,cAAuB,YAAW;EAAxC,cAAA;;AACE,SAAA,WAAiC,IAAgBC,UAAS,KAAK,OAAO;AACtE,SAAA,UAA8B,IAAe,QAAQ,KAAK,OAAO;EACnE;;AAyCA,QAAQ,WAAWA;AACnB,QAAQ,UAAU;;;AChEZ,IAAOC,YAAP,cAAwB,YAAW;;;;;;EAMvC,OAAO,UAAkB,MAA2B,SAAwB;AAC1E,WAAO,KAAK,QAAQ,KAAK,gBAAgB,QAAQ,aAAa;MAC5D;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,SAAS,WAAmB,QAA+B,SAAwB;AACjF,UAAM,EAAE,UAAS,IAAK;AACtB,WAAO,KAAK,QAAQ,IAAI,gBAAgB,SAAS,aAAa,SAAS,IAAI;MACzE,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OAAO,WAAmB,QAA6B,SAAwB;AAC7E,UAAM,EAAE,WAAW,GAAG,KAAI,IAAK;AAC/B,WAAO,KAAK,QAAQ,KAAK,gBAAgB,SAAS,aAAa,SAAS,IAAI;MAC1E;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,KACE,UACA,QAA8C,CAAA,GAC9C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,gBAAgB,QAAQ,aAAa,YAAqB;MACvF;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OACE,WACA,QACA,SAAwB;AAExB,UAAM,EAAE,UAAS,IAAK;AACtB,WAAO,KAAK,QAAQ,OAAO,gBAAgB,SAAS,aAAa,SAAS,IAAI;MAC5E,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;ACzEI,IAAO,QAAP,cAAqB,YAAW;;;;;;EAMpC,SAAS,QAAgB,QAA4B,SAAwB;AAC3E,UAAM,EAAE,WAAW,QAAQ,GAAG,MAAK,IAAK;AACxC,WAAO,KAAK,QAAQ,IAAI,gBAAgB,SAAS,SAAS,MAAM,UAAU,MAAM,IAAI;MAClF;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,KAAK,OAAe,QAAwB,SAAwB;AAClE,UAAM,EAAE,WAAW,GAAG,MAAK,IAAK;AAChC,WAAO,KAAK,QAAQ,WAAW,gBAAgB,SAAS,SAAS,KAAK,UAAU,YAAqB;MACnG;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;ACKK,IAAM,iBAAiB,CAAC,cAAoC;AACjE,MAAI,OAAO,WAAW,aAAa;AAEjC,UAAM,MAAM,OAAO,KAAK,WAAW,QAAQ;AAC3C,WAAO,MAAM,KACX,IAAI,aAAa,IAAI,QAAQ,IAAI,YAAY,IAAI,SAAS,aAAa,iBAAiB,CAAC;EAE7F,OAAO;AAEL,UAAM,YAAY,KAAK,SAAS;AAChC,UAAM,MAAM,UAAU;AACtB,UAAM,QAAQ,IAAI,WAAW,GAAG;AAChC,aAAS,IAAI,GAAG,IAAI,KAAK,KAAK;AAC5B,YAAM,CAAC,IAAI,UAAU,WAAW,CAAC;IACnC;AACA,WAAO,MAAM,KAAK,IAAI,aAAa,MAAM,MAAM,CAAC;EAClD;AACF;;;ACtDO,IAAM,UAAU,CAAC,QAAmC;AACzD,MAAI,OAAQ,WAAmB,YAAY,aAAa;AACtD,WAAQ,WAAmB,QAAQ,MAAM,GAAG,GAAG,KAAI,KAAM;EAC3D;AACA,MAAI,OAAQ,WAAmB,SAAS,aAAa;AACnD,WAAQ,WAAmB,KAAK,KAAK,MAAM,GAAG,GAAG,KAAI;EACvD;AACA,SAAO;AACT;;;;;;;;;;;;;;;;;;;;;;;;;;ACuDM,IAAO,kBAAP,cACI,YAAkC;EAD5C,cAAA;;;AAKE,4BAAA,IAAA,MAAkC,CAAA,CAAE;AAIpC,sCAAA,IAAA,MAAoD,CAAA,CAAE;AACtD,sCAAA,IAAA,MAA+C,CAAA,CAAE;AACjD,qCAAA,IAAA,MAAA,MAAA;AACA,8BAAA,IAAA,MAAA,MAAA;AACA,yCAAA,IAAA,MAAA,MAAA;AACA,oCAAA,IAAA,MAAA,MAAA;AACA,0CAAA,IAAA,MAAA,MAAA;AACA,qCAAA,IAAA,MAAA,MAAA;AAGA,kCAAA,IAAA,MAAA,MAAA;AACA,wCAAA,IAAA,MAAA,MAAA;AACA,4CAAA,IAAA,MAAA,MAAA;EA0qBF;EAxqBE,EAAA,0BAAA,oBAAA,QAAA,GAAA,oCAAA,oBAAA,QAAA,GAAA,oCAAA,oBAAA,QAAA,GAAA,mCAAA,oBAAA,QAAA,GAAA,4BAAA,oBAAA,QAAA,GAAA,uCAAA,oBAAA,QAAA,GAAA,kCAAA,oBAAA,QAAA,GAAA,wCAAA,oBAAA,QAAA,GAAA,mCAAA,oBAAA,QAAA,GAAA,gCAAA,oBAAA,QAAA,GAAA,sCAAA,oBAAA,QAAA,GAAA,0CAAA,oBAAA,QAAA,GAAA,6BAAA,oBAAA,QAAA,GAAC,OAAO,cAAa,IAAC;AACpB,UAAM,YAAoC,CAAA;AAC1C,UAAM,YAGA,CAAA;AACN,QAAI,OAAO;AAGX,SAAK,GAAG,SAAS,CAAC,UAAS;AACzB,YAAM,SAAS,UAAU,MAAK;AAC9B,UAAI,QAAQ;AACV,eAAO,QAAQ,KAAK;MACtB,OAAO;AACL,kBAAU,KAAK,KAAK;MACtB;IACF,CAAC;AAED,SAAK,GAAG,OAAO,MAAK;AAClB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,QAAQ,MAAS;MAC1B;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,SAAK,GAAG,SAAS,CAAC,QAAO;AACvB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,OAAO,GAAG;MACnB;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,SAAK,GAAG,SAAS,CAAC,QAAO;AACvB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,OAAO,GAAG;MACnB;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,WAAO;MACL,MAAM,YAA0D;AAC9D,YAAI,CAAC,UAAU,QAAQ;AACrB,cAAI,MAAM;AACR,mBAAO,EAAE,OAAO,QAAW,MAAM,KAAI;UACvC;AACA,iBAAO,IAAI,QAA0C,CAAC,SAAS,WAC7D,UAAU,KAAK,EAAE,SAAS,OAAM,CAAE,CAAC,EACnC,KAAK,CAACC,WAAWA,SAAQ,EAAE,OAAOA,QAAO,MAAM,MAAK,IAAK,EAAE,OAAO,QAAW,MAAM,KAAI,CAAG;QAC9F;AACA,cAAM,QAAQ,UAAU,MAAK;AAC7B,eAAO,EAAE,OAAO,OAAO,MAAM,MAAK;MACpC;MACA,QAAQ,YAAW;AACjB,aAAK,MAAK;AACV,eAAO,EAAE,OAAO,QAAW,MAAM,KAAI;MACvC;;EAEJ;EAEA,OAAO,mBAAmB,QAAsB;AAC9C,UAAM,SAAS,IAAI,GAAe;AAClC,WAAO,KAAK,MAAM,OAAO,oBAAoB,MAAM,CAAC;AACpD,WAAO;EACT;EAEU,MAAM,oBACd,gBACA,SAAwB;AAExB,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AACA,SAAK,WAAU;AACf,UAAM,SAAS,OAAO,mBAAyC,gBAAgB,KAAK,UAAU;AAC9F,qBAAiB,SAAS,QAAQ;AAChC,6BAAA,MAAI,4BAAA,KAAA,yBAAA,EAAU,KAAd,MAAe,KAAK;IACtB;AACA,QAAI,OAAO,WAAW,QAAQ,SAAS;AACrC,YAAM,IAAI,kBAAiB;IAC7B;AACA,WAAO,KAAK,QAAQ,uBAAA,MAAI,4BAAA,KAAA,2BAAA,EAAY,KAAhB,IAAI,CAAc;EACxC;EAEA,mBAAgB;AACd,UAAM,SAAS,IAAI,OAAO,KAAK,OAAO,aAAa,EAAE,KAAK,IAAI,GAAG,KAAK,UAAU;AAChF,WAAO,OAAO,iBAAgB;EAChC;EAEA,OAAO,0BACL,OACA,MACA,QACA,SAAmC;AAEnC,UAAM,SAAS,IAAI,GAAe;AAClC,WAAO,KAAK,MACV,OAAO,wBAAwB,OAAO,MAAM,QAAQ;MAClD,GAAG;MACH,SAAS,EAAE,GAAG,SAAS,SAAS,6BAA6B,SAAQ;KACtE,CAAC;AAEJ,WAAO;EACT;EAEU,MAAM,2BACd,KACA,OACA,QACA,SAAwB;AAExB,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AAEA,UAAM,OAA4C,EAAE,GAAG,QAAQ,QAAQ,KAAI;AAC3E,UAAM,SAAS,MAAM,IAAI,kBAAkB,OAAO,MAAM;MACtD,GAAG;MACH,QAAQ,KAAK,WAAW;KACzB;AAED,SAAK,WAAU;AAEf,qBAAiB,SAAS,QAAQ;AAChC,6BAAA,MAAI,4BAAA,KAAA,yBAAA,EAAU,KAAd,MAAe,KAAK;IACtB;AACA,QAAI,OAAO,WAAW,QAAQ,SAAS;AACrC,YAAM,IAAI,kBAAiB;IAC7B;AAEA,WAAO,KAAK,QAAQ,uBAAA,MAAI,4BAAA,KAAA,2BAAA,EAAY,KAAhB,IAAI,CAAc;EACxC;EAEA,OAAO,4BACL,QACA,QACA,SAAwB;AAExB,UAAM,SAAS,IAAI,GAAe;AAClC,WAAO,KAAK,MACV,OAAO,uBAAuB,QAAQ,QAAQ;MAC5C,GAAG;MACH,SAAS,EAAE,GAAG,SAAS,SAAS,6BAA6B,SAAQ;KACtE,CAAC;AAEJ,WAAO;EACT;EAEA,OAAO,sBACL,UACA,MACA,QACA,SAAwB;AAExB,UAAM,SAAS,IAAI,GAAe;AAClC,WAAO,KAAK,MACV,OAAO,oBAAoB,UAAU,MAAM,QAAQ;MACjD,GAAG;MACH,SAAS,EAAE,GAAG,SAAS,SAAS,6BAA6B,SAAQ;KACtE,CAAC;AAEJ,WAAO;EACT;EAEA,eAAY;AACV,WAAO,uBAAA,MAAI,+BAAA,GAAA;EACb;EAEA,aAAU;AACR,WAAO,uBAAA,MAAI,qCAAA,GAAA;EACb;EAEA,yBAAsB;AACpB,WAAO,uBAAA,MAAI,kCAAA,GAAA;EACb;EAEA,yBAAsB;AACpB,WAAO,uBAAA,MAAI,yCAAA,GAAA;EACb;EAEA,MAAM,gBAAa;AACjB,UAAM,KAAK,KAAI;AAEf,WAAO,OAAO,OAAO,uBAAA,MAAI,mCAAA,GAAA,CAAkB;EAC7C;EAEA,MAAM,gBAAa;AACjB,UAAM,KAAK,KAAI;AAEf,WAAO,OAAO,OAAO,uBAAA,MAAI,mCAAA,GAAA,CAAkB;EAC7C;EAEA,MAAM,WAAQ;AACZ,UAAM,KAAK,KAAI;AACf,QAAI,CAAC,uBAAA,MAAI,2BAAA,GAAA;AAAY,YAAM,MAAM,6BAA6B;AAE9D,WAAO,uBAAA,MAAI,2BAAA,GAAA;EACb;EAEU,MAAM,6BACd,QACA,QACA,SAAwB;AAExB,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AAEA,UAAM,OAAiC,EAAE,GAAG,QAAQ,QAAQ,KAAI;AAChE,UAAM,SAAS,MAAM,OAAO,aAAa,MAAM,EAAE,GAAG,SAAS,QAAQ,KAAK,WAAW,OAAM,CAAE;AAE7F,SAAK,WAAU;AAEf,qBAAiB,SAAS,QAAQ;AAChC,6BAAA,MAAI,4BAAA,KAAA,yBAAA,EAAU,KAAd,MAAe,KAAK;IACtB;AACA,QAAI,OAAO,WAAW,QAAQ,SAAS;AACrC,YAAM,IAAI,kBAAiB;IAC7B;AAEA,WAAO,KAAK,QAAQ,uBAAA,MAAI,4BAAA,KAAA,2BAAA,EAAY,KAAhB,IAAI,CAAc;EACxC;EAEU,MAAM,uBACd,KACA,UACA,QACA,SAAwB;AAExB,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AAEA,UAAM,OAAiC,EAAE,GAAG,QAAQ,QAAQ,KAAI;AAChE,UAAM,SAAS,MAAM,IAAI,OAAO,UAAU,MAAM,EAAE,GAAG,SAAS,QAAQ,KAAK,WAAW,OAAM,CAAE;AAE9F,SAAK,WAAU;AAEf,qBAAiB,SAAS,QAAQ;AAChC,6BAAA,MAAI,4BAAA,KAAA,yBAAA,EAAU,KAAd,MAAe,KAAK;IACtB;AACA,QAAI,OAAO,WAAW,QAAQ,SAAS;AACrC,YAAM,IAAI,kBAAiB;IAC7B;AAEA,WAAO,KAAK,QAAQ,uBAAA,MAAI,4BAAA,KAAA,2BAAA,EAAY,KAAhB,IAAI,CAAc;EACxC;EAgTA,OAAO,gBAAgB,KAA0B,OAA0B;AACzE,eAAW,CAAC,KAAK,UAAU,KAAK,OAAO,QAAQ,KAAK,GAAG;AACrD,UAAI,CAAC,IAAI,eAAe,GAAG,GAAG;AAC5B,YAAI,GAAG,IAAI;AACX;MACF;AAEA,UAAI,WAAW,IAAI,GAAG;AACtB,UAAI,aAAa,QAAQ,aAAa,QAAW;AAC/C,YAAI,GAAG,IAAI;AACX;MACF;AAGA,UAAI,QAAQ,WAAW,QAAQ,QAAQ;AACrC,YAAI,GAAG,IAAI;AACX;MACF;AAGA,UAAI,OAAO,aAAa,YAAY,OAAO,eAAe,UAAU;AAClE,oBAAY;MACd,WAAW,OAAO,aAAa,YAAY,OAAO,eAAe,UAAU;AACzE,oBAAY;MACd,WAAW,MAAM,QAAQ,KAAK,MAAM,UAAU,GAAG;AAC/C,mBAAW,KAAK,gBAAgB,UAAiC,UAAiC;MACpG,WAAW,MAAM,QAAQ,QAAQ,KAAK,MAAM,QAAQ,UAAU,GAAG;AAC/D,YAAI,SAAS,MAAM,CAAC,MAAM,OAAO,MAAM,YAAY,OAAO,MAAM,QAAQ,GAAG;AACzE,mBAAS,KAAK,GAAG,UAAU;AAC3B;QACF;AAEA,mBAAW,cAAc,YAAY;AACnC,cAAI,CAAC,MAAM,UAAU,GAAG;AACtB,kBAAM,IAAI,MAAM,uDAAuD,UAAU,EAAE;UACrF;AAEA,gBAAM,QAAQ,WAAW,OAAO;AAChC,cAAI,SAAS,MAAM;AACjB,oBAAQ,MAAM,UAAU;AACxB,kBAAM,IAAI,MAAM,wDAAwD;UAC1E;AAEA,cAAI,OAAO,UAAU,UAAU;AAC7B,kBAAM,IAAI,MAAM,wEAAwE,KAAK,EAAE;UACjG;AAEA,gBAAM,WAAW,SAAS,KAAK;AAC/B,cAAI,YAAY,MAAM;AACpB,qBAAS,KAAK,UAAU;UAC1B,OAAO;AACL,qBAAS,KAAK,IAAI,KAAK,gBAAgB,UAAU,UAAU;UAC7D;QACF;AACA;MACF,OAAO;AACL,cAAM,MAAM,0BAA0B,GAAG,iBAAiB,UAAU,eAAe,QAAQ,EAAE;MAC/F;AACA,UAAI,GAAG,IAAI;IACb;AAEA,WAAO;EACT;EA6BU,QAAQ,KAAQ;AACxB,WAAO;EACT;EAEU,MAAM,uBACd,QACA,QACA,SAAwB;AAExB,WAAO,MAAM,KAAK,6BAA6B,QAAQ,QAAQ,OAAO;EACxE;EAEU,MAAM,oBACd,UACA,MACA,QACA,SAAwB;AAExB,WAAO,MAAM,KAAK,uBAAuB,MAAM,UAAU,QAAQ,OAAO;EAC1E;EAEU,MAAM,wBACd,OACA,MACA,QACA,SAAwB;AAExB,WAAO,MAAM,KAAK,2BAA2B,MAAM,OAAO,QAAQ,OAAO;EAC3E;;sFAraU,OAA2B;AACnC,MAAI,KAAK;AAAO;AAEhB,yBAAA,MAAI,+BAAiB,OAAK,GAAA;AAE1B,yBAAA,MAAI,4BAAA,KAAA,4BAAA,EAAa,KAAjB,MAAkB,KAAK;AAEvB,UAAQ,MAAM,OAAO;IACnB,KAAK;AAEH;IAEF,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,6BAAA,MAAI,4BAAA,KAAA,0BAAA,EAAW,KAAf,MAAgB,KAAK;AACrB;IAEF,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,6BAAA,MAAI,4BAAA,KAAA,8BAAA,EAAe,KAAnB,MAAoB,KAAK;AACzB;IAEF,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,6BAAA,MAAI,4BAAA,KAAA,8BAAA,EAAe,KAAnB,MAAoB,KAAK;AACzB;IAEF,KAAK;AAEH,YAAM,IAAI,MACR,qFAAqF;IAEzF;AACE,MAAAC,aAAY,KAAK;EACrB;AACF,GAAC,8BAAA,SAAAC,+BAAA;AAGC,MAAI,KAAK,OAAO;AACd,UAAM,IAAI,YAAY,yCAAyC;EACjE;AAEA,MAAI,CAAC,uBAAA,MAAI,2BAAA,GAAA;AAAY,UAAM,MAAM,iCAAiC;AAElE,SAAO,uBAAA,MAAI,2BAAA,GAAA;AACb,GAAC,iCAAA,SAAAC,gCAEqC,OAAyB;AAC7D,QAAM,CAAC,oBAAoB,UAAU,IAAI,uBAAA,MAAI,4BAAA,KAAA,kCAAA,EAAmB,KAAvB,MAAwB,OAAO,uBAAA,MAAI,kCAAA,GAAA,CAAiB;AAC7F,yBAAA,MAAI,kCAAoB,oBAAkB,GAAA;AAC1C,yBAAA,MAAI,mCAAA,GAAA,EAAmB,mBAAmB,EAAE,IAAI;AAEhD,aAAW,WAAW,YAAY;AAChC,UAAM,kBAAkB,mBAAmB,QAAQ,QAAQ,KAAK;AAChE,QAAI,iBAAiB,QAAQ,QAAQ;AACnC,WAAK,MAAM,eAAe,gBAAgB,IAAI;IAChD;EACF;AAEA,UAAQ,MAAM,OAAO;IACnB,KAAK;AACH,WAAK,MAAM,kBAAkB,MAAM,IAAI;AACvC;IAEF,KAAK;AACH;IAEF,KAAK;AACH,WAAK,MAAM,gBAAgB,MAAM,KAAK,OAAO,kBAAkB;AAE/D,UAAI,MAAM,KAAK,MAAM,SAAS;AAC5B,mBAAW,WAAW,MAAM,KAAK,MAAM,SAAS;AAE9C,cAAI,QAAQ,QAAQ,UAAU,QAAQ,MAAM;AAC1C,gBAAI,YAAY,QAAQ;AACxB,gBAAI,WAAW,mBAAmB,QAAQ,QAAQ,KAAK;AACvD,gBAAI,YAAY,SAAS,QAAQ,QAAQ;AACvC,mBAAK,MAAM,aAAa,WAAW,SAAS,IAAI;YAClD,OAAO;AACL,oBAAM,MAAM,qEAAqE;YACnF;UACF;AAEA,cAAI,QAAQ,SAAS,uBAAA,MAAI,sCAAA,GAAA,GAAuB;AAE9C,gBAAI,uBAAA,MAAI,iCAAA,GAAA,GAAkB;AACxB,sBAAQ,uBAAA,MAAI,iCAAA,GAAA,EAAiB,MAAM;gBACjC,KAAK;AACH,uBAAK,MAAM,YAAY,uBAAA,MAAI,iCAAA,GAAA,EAAiB,MAAM,uBAAA,MAAI,kCAAA,GAAA,CAAiB;AACvE;gBACF,KAAK;AACH,uBAAK,MAAM,iBAAiB,uBAAA,MAAI,iCAAA,GAAA,EAAiB,YAAY,uBAAA,MAAI,kCAAA,GAAA,CAAiB;AAClF;cACJ;YACF;AAEA,mCAAA,MAAI,sCAAwB,QAAQ,OAAK,GAAA;UAC3C;AAEA,iCAAA,MAAI,iCAAmB,mBAAmB,QAAQ,QAAQ,KAAK,GAAC,GAAA;QAClE;MACF;AAEA;IAEF,KAAK;IACL,KAAK;AAEH,UAAI,uBAAA,MAAI,sCAAA,GAAA,MAA0B,QAAW;AAC3C,cAAM,iBAAiB,MAAM,KAAK,QAAQ,uBAAA,MAAI,sCAAA,GAAA,CAAqB;AACnE,YAAI,gBAAgB;AAClB,kBAAQ,eAAe,MAAM;YAC3B,KAAK;AACH,mBAAK,MAAM,iBAAiB,eAAe,YAAY,uBAAA,MAAI,kCAAA,GAAA,CAAiB;AAC5E;YACF,KAAK;AACH,mBAAK,MAAM,YAAY,eAAe,MAAM,uBAAA,MAAI,kCAAA,GAAA,CAAiB;AACjE;UACJ;QACF;MACF;AAEA,UAAI,uBAAA,MAAI,kCAAA,GAAA,GAAmB;AACzB,aAAK,MAAM,eAAe,MAAM,IAAI;MACtC;AAEA,6BAAA,MAAI,kCAAoB,QAAS,GAAA;EACrC;AACF,GAAC,iCAAA,SAAAC,gCAEqC,OAAyB;AAC7D,QAAM,qBAAqB,uBAAA,MAAI,4BAAA,KAAA,kCAAA,EAAmB,KAAvB,MAAwB,KAAK;AACxD,yBAAA,MAAI,yCAA2B,oBAAkB,GAAA;AAEjD,UAAQ,MAAM,OAAO;IACnB,KAAK;AACH,WAAK,MAAM,kBAAkB,MAAM,IAAI;AACvC;IACF,KAAK;AACH,YAAM,QAAQ,MAAM,KAAK;AACzB,UACE,MAAM,gBACN,MAAM,aAAa,QAAQ,gBAC3B,MAAM,aAAa,cACnB,mBAAmB,aAAa,QAAQ,cACxC;AACA,mBAAW,YAAY,MAAM,aAAa,YAAY;AACpD,cAAI,SAAS,SAAS,uBAAA,MAAI,uCAAA,GAAA,GAAwB;AAChD,iBAAK,MACH,iBACA,UACA,mBAAmB,aAAa,WAAW,SAAS,KAAK,CAAa;UAE1E,OAAO;AACL,gBAAI,uBAAA,MAAI,kCAAA,GAAA,GAAmB;AACzB,mBAAK,MAAM,gBAAgB,uBAAA,MAAI,kCAAA,GAAA,CAAiB;YAClD;AAEA,mCAAA,MAAI,uCAAyB,SAAS,OAAK,GAAA;AAC3C,mCAAA,MAAI,kCAAoB,mBAAmB,aAAa,WAAW,SAAS,KAAK,GAAC,GAAA;AAClF,gBAAI,uBAAA,MAAI,kCAAA,GAAA;AAAmB,mBAAK,MAAM,mBAAmB,uBAAA,MAAI,kCAAA,GAAA,CAAiB;UAChF;QACF;MACF;AAEA,WAAK,MAAM,gBAAgB,MAAM,KAAK,OAAO,kBAAkB;AAC/D;IACF,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,6BAAA,MAAI,yCAA2B,QAAS,GAAA;AACxC,YAAM,UAAU,MAAM,KAAK;AAC3B,UAAI,QAAQ,QAAQ,cAAc;AAChC,YAAI,uBAAA,MAAI,kCAAA,GAAA,GAAmB;AACzB,eAAK,MAAM,gBAAgB,uBAAA,MAAI,kCAAA,GAAA,CAA6B;AAC5D,iCAAA,MAAI,kCAAoB,QAAS,GAAA;QACnC;MACF;AACA,WAAK,MAAM,eAAe,MAAM,MAAM,kBAAkB;AACxD;IACF,KAAK;AACH;EACJ;AACF,GAAC,+BAAA,SAAAC,8BAEmC,OAA2B;AAC7D,yBAAA,MAAI,yBAAA,GAAA,EAAS,KAAK,KAAK;AACvB,OAAK,MAAM,SAAS,KAAK;AAC3B,GAAC,qCAAA,SAAAC,oCAEkB,OAAyB;AAC1C,UAAQ,MAAM,OAAO;IACnB,KAAK;AACH,6BAAA,MAAI,mCAAA,GAAA,EAAmB,MAAM,KAAK,EAAE,IAAI,MAAM;AAC9C,aAAO,MAAM;IAEf,KAAK;AACH,UAAI,WAAW,uBAAA,MAAI,mCAAA,GAAA,EAAmB,MAAM,KAAK,EAAE;AACnD,UAAI,CAAC,UAAU;AACb,cAAM,MAAM,uDAAuD;MACrE;AAEA,UAAI,OAAO,MAAM;AAEjB,UAAI,KAAK,OAAO;AACd,cAAM,cAAc,GAAgB,gBAAgB,UAAU,KAAK,KAAK;AACxE,+BAAA,MAAI,mCAAA,GAAA,EAAmB,MAAM,KAAK,EAAE,IAAI;MAC1C;AAEA,aAAO,uBAAA,MAAI,mCAAA,GAAA,EAAmB,MAAM,KAAK,EAAE;IAE7C,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,6BAAA,MAAI,mCAAA,GAAA,EAAmB,MAAM,KAAK,EAAE,IAAI,MAAM;AAC9C;EACJ;AAEA,MAAI,uBAAA,MAAI,mCAAA,GAAA,EAAmB,MAAM,KAAK,EAAE;AAAG,WAAO,uBAAA,MAAI,mCAAA,GAAA,EAAmB,MAAM,KAAK,EAAE;AACtF,QAAM,IAAI,MAAM,uBAAuB;AACzC,GAAC,qCAAA,SAAAC,oCAGC,OACA,UAA6B;AAE7B,MAAI,aAAoC,CAAA;AAExC,UAAQ,MAAM,OAAO;IACnB,KAAK;AAEH,aAAO,CAAC,MAAM,MAAM,UAAU;IAEhC,KAAK;AACH,UAAI,CAAC,UAAU;AACb,cAAM,MACJ,wFAAwF;MAE5F;AAEA,UAAI,OAAO,MAAM;AAGjB,UAAI,KAAK,MAAM,SAAS;AACtB,mBAAW,kBAAkB,KAAK,MAAM,SAAS;AAC/C,cAAI,eAAe,SAAS,SAAS,SAAS;AAC5C,gBAAI,iBAAiB,SAAS,QAAQ,eAAe,KAAK;AAC1D,qBAAS,QAAQ,eAAe,KAAK,IAAI,uBAAA,MAAI,4BAAA,KAAA,kCAAA,EAAmB,KAAvB,MACvC,gBACA,cAAc;UAElB,OAAO;AACL,qBAAS,QAAQ,eAAe,KAAK,IAAI;AAEzC,uBAAW,KAAK,cAAc;UAChC;QACF;MACF;AAEA,aAAO,CAAC,UAAU,UAAU;IAE9B,KAAK;IACL,KAAK;IACL,KAAK;AAEH,UAAI,UAAU;AACZ,eAAO,CAAC,UAAU,UAAU;MAC9B,OAAO;AACL,cAAM,MAAM,yDAAyD;MACvE;EACJ;AACA,QAAM,MAAM,yCAAyC;AACvD,GAAC,qCAAA,SAAAC,oCAGC,gBACA,gBAA0C;AAE1C,SAAO,GAAgB,gBAAgB,gBAA+C,cAAc;AAGtG,GAAC,6BAAA,SAAAC,4BAkEiC,OAAqB;AACrD,yBAAA,MAAI,qCAAuB,MAAM,MAAI,GAAA;AAErC,UAAQ,MAAM,OAAO;IACnB,KAAK;AACH;IACF,KAAK;AACH;IACF,KAAK;AACH;IACF,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,6BAAA,MAAI,2BAAa,MAAM,MAAI,GAAA;AAC3B,UAAI,uBAAA,MAAI,kCAAA,GAAA,GAAmB;AACzB,aAAK,MAAM,gBAAgB,uBAAA,MAAI,kCAAA,GAAA,CAAiB;AAChD,+BAAA,MAAI,kCAAoB,QAAS,GAAA;MACnC;AACA;IACF,KAAK;AACH;EACJ;AACF;AAiCF,SAASR,aAAY,IAAS;AAAG;;;AC3tB3B,IAAO,OAAP,cAAoB,YAAW;EAArC,cAAA;;AACE,SAAA,QAAwB,IAAa,MAAM,KAAK,OAAO;EAqPzD;EAnOE,OACE,UACA,QACA,SAAwB;AAExB,UAAM,EAAE,SAAS,GAAG,KAAI,IAAK;AAC7B,WAAO,KAAK,QAAQ,KAAK,gBAAgB,QAAQ,SAAS;MACxD,OAAO,EAAE,QAAO;MAChB;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;MAC5E,QAAQ,OAAO,UAAU;MACzB,uBAAuB;KACxB;EACH;;;;;;EAOA,SAAS,OAAe,QAA2B,SAAwB;AACzE,UAAM,EAAE,UAAS,IAAK;AACtB,WAAO,KAAK,QAAQ,IAAI,gBAAgB,SAAS,SAAS,KAAK,IAAI;MACjE,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OAAO,OAAe,QAAyB,SAAwB;AACrE,UAAM,EAAE,WAAW,GAAG,KAAI,IAAK;AAC/B,WAAO,KAAK,QAAQ,KAAK,gBAAgB,SAAS,SAAS,KAAK,IAAI;MAClE;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,KACE,UACA,QAA0C,CAAA,GAC1C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,gBAAgB,QAAQ,SAAS,YAAiB;MAC/E;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OAAO,OAAe,QAAyB,SAAwB;AACrE,UAAM,EAAE,UAAS,IAAK;AACtB,WAAO,KAAK,QAAQ,KAAK,gBAAgB,SAAS,SAAS,KAAK,WAAW;MACzE,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,MAAM,cACJ,UACA,MACA,SAAsD;AAEtD,UAAM,MAAM,MAAM,KAAK,OAAO,UAAU,MAAM,OAAO;AACrD,WAAO,MAAM,KAAK,KAAK,IAAI,IAAI,EAAE,WAAW,SAAQ,GAAI,OAAO;EACjE;;;;;;EAOA,gBACE,UACA,MACA,SAAwB;AAExB,WAAO,gBAAgB,sBAAsB,UAAU,KAAK,QAAQ,KAAK,QAAQ,MAAM,MAAM,OAAO;EACtG;;;;;;EAOA,MAAM,KACJ,OACA,QACA,SAAsD;AAEtD,UAAM,UAAU,aAAa;MAC3B,SAAS;MACT;QACE,2BAA2B;QAC3B,oCAAoC,SAAS,gBAAgB,SAAQ,KAAM;;KAE9E;AAED,WAAO,MAAM;AACX,YAAM,EAAE,MAAM,KAAK,SAAQ,IAAK,MAAM,KAAK,SAAS,OAAO,QAAQ;QACjE,GAAG;QACH,SAAS,EAAE,GAAG,SAAS,SAAS,GAAG,QAAO;OAC3C,EAAE,aAAY;AAEf,cAAQ,IAAI,QAAQ;;QAElB,KAAK;QACL,KAAK;QACL,KAAK;AACH,cAAI,gBAAgB;AAEpB,cAAI,SAAS,gBAAgB;AAC3B,4BAAgB,QAAQ;UAC1B,OAAO;AACL,kBAAM,iBAAiB,SAAS,QAAQ,IAAI,sBAAsB;AAClE,gBAAI,gBAAgB;AAClB,oBAAM,mBAAmB,SAAS,cAAc;AAChD,kBAAI,CAAC,MAAM,gBAAgB,GAAG;AAC5B,gCAAgB;cAClB;YACF;UACF;AACA,gBAAM,MAAM,aAAa;AACzB;;QAEF,KAAK;QACL,KAAK;QACL,KAAK;QACL,KAAK;QACL,KAAK;QACL,KAAK;AACH,iBAAO;MACX;IACF;EACF;;;;EAKA,OAAO,UAAkB,MAAiC,SAAwB;AAChF,WAAO,gBAAgB,sBAAsB,UAAU,KAAK,QAAQ,KAAK,QAAQ,MAAM,MAAM,OAAO;EACtG;EAyBA,kBACE,OACA,QACA,SAAwB;AAExB,UAAM,EAAE,WAAW,GAAG,KAAI,IAAK;AAC/B,WAAO,KAAK,QAAQ,KAAK,gBAAgB,SAAS,SAAS,KAAK,wBAAwB;MACtF;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;MAC5E,QAAQ,OAAO,UAAU;MACzB,uBAAuB;KACxB;EACH;;;;;;EAOA,MAAM,yBACJ,OACA,QACA,SAAsD;AAEtD,UAAM,MAAM,MAAM,KAAK,kBAAkB,OAAO,QAAQ,OAAO;AAC/D,WAAO,MAAM,KAAK,KAAK,IAAI,IAAI,QAAQ,OAAO;EAChD;;;;;;EAOA,wBACE,OACA,QACA,SAAwB;AAExB,WAAO,gBAAgB,0BAA0B,OAAO,KAAK,QAAQ,KAAK,QAAQ,MAAM,QAAQ,OAAO;EACzG;;AAsuBF,KAAK,QAAQ;;;AC57BP,IAAOS,WAAP,cAAuB,YAAW;EAAxC,cAAA;;AACE,SAAA,OAAqB,IAAY,KAAK,KAAK,OAAO;AAClD,SAAA,WAAiC,IAAgBC,UAAS,KAAK,OAAO;EAkGxE;;;;;;EA3FE,OAAO,OAA8C,CAAA,GAAI,SAAwB;AAC/E,WAAO,KAAK,QAAQ,KAAK,YAAY;MACnC;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,SAAS,UAAkB,SAAwB;AACjD,WAAO,KAAK,QAAQ,IAAI,gBAAgB,QAAQ,IAAI;MAClD,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OAAO,UAAkB,MAA0B,SAAwB;AACzE,WAAO,KAAK,QAAQ,KAAK,gBAAgB,QAAQ,IAAI;MACnD;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;EAOA,OAAO,UAAkB,SAAwB;AAC/C,WAAO,KAAK,QAAQ,OAAO,gBAAgB,QAAQ,IAAI;MACrD,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;EAgBA,aACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,iBAAiB;MACxC;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;MAC5E,QAAQ,KAAK,UAAU;MACvB,uBAAuB;KACxB;EACH;;;;;;EAOA,MAAM,iBACJ,MACA,SAAsD;AAEtD,UAAM,MAAM,MAAM,KAAK,aAAa,MAAM,OAAO;AACjD,WAAO,MAAM,KAAK,KAAK,KAAK,IAAI,IAAI,EAAE,WAAW,IAAI,UAAS,GAAI,OAAO;EAC3E;;;;EAKA,mBAAmB,MAA0C,SAAwB;AACnF,WAAO,gBAAgB,4BAA4B,MAAM,KAAK,QAAQ,KAAK,SAAS,OAAO;EAC7F;;AAmnCFD,SAAQ,OAAO;AACfA,SAAQ,WAAWC;;;ACrsCb,IAAO,OAAP,cAAoB,YAAW;EAArC,cAAA;;AACE,SAAA,WAAiC,IAAgB,SAAS,KAAK,OAAO;AACtE,SAAA,UAA8B,IAAe,QAAQ,KAAK,OAAO;AACjE,SAAA,aAAuC,IAAkB,WAAW,KAAK,OAAO;AAChF,SAAA,UAA8B,IAAeC,SAAQ,KAAK,OAAO;EACnE;;AAEA,KAAK,WAAW;AAChB,KAAK,UAAU;AACf,KAAK,aAAa;AAClB,KAAK,UAAUA;;;AChGT,IAAOC,eAAP,cAA2B,YAAW;EAqB1C,OACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,gBAAgB,EAAE,MAAM,GAAG,SAAS,QAAQ,KAAK,UAAU,MAAK,CAAE;EAG7F;;;;AC7BI,IAAO,UAAP,cAAuB,YAAW;;;;EAItC,SAAS,QAAgB,QAA+B,SAAwB;AAC9E,UAAM,EAAE,aAAY,IAAK;AACzB,WAAO,KAAK,QAAQ,IAAI,mBAAmB,YAAY,UAAU,MAAM,YAAY;MACjF,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,qBAAoB,GAAI,SAAS,OAAO,CAAC;MAC1E,kBAAkB;KACnB;EACH;;;;ACNI,IAAO,QAAP,cAAqB,YAAW;EAAtC,cAAA;;AACE,SAAA,UAA8B,IAAe,QAAQ,KAAK,OAAO;EAuDnE;;;;;;;EA/CE,OACE,aACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAClB,mBAAmB,WAAW,UAC9B,iCAAiC,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EAExE;;;;EAKA,SACE,QACA,QACA,SAAwB;AAExB,UAAM,EAAE,aAAY,IAAK;AACzB,WAAO,KAAK,QAAQ,IAAI,mBAAmB,YAAY,UAAU,MAAM,IAAI,OAAO;EACpF;;;;EAKA,KACE,aACA,QAA2C,CAAA,GAC3C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,mBAAmB,WAAW,UAAU,YAA8B;MACnG;MACA,GAAG;KACJ;EACH;;;;EAKA,OAAO,QAAgB,QAA0B,SAAwB;AACvE,UAAM,EAAE,aAAY,IAAK;AACzB,WAAO,KAAK,QAAQ,OAAO,mBAAmB,YAAY,UAAU,MAAM,IAAI;MAC5E,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,MAAK,GAAI,SAAS,OAAO,CAAC;KAC5D;EACH;;AAgJF,MAAM,UAAU;;;AC9LV,IAAO,aAAP,cAA0B,YAAW;EAA3C,cAAA;;AACE,SAAA,QAAwB,IAAa,MAAM,KAAK,OAAO;EAmCzD;;;;EA9BE,OAAO,MAA6B,SAAwB;AAC1D,WAAO,KAAK,QAAQ,KAAK,eAAe,EAAE,MAAM,GAAG,QAAO,CAAE;EAC9D;;;;EAKA,SAAS,aAAqB,SAAwB;AACpD,WAAO,KAAK,QAAQ,IAAI,mBAAmB,WAAW,IAAI,OAAO;EACnE;;;;EAKA,KACE,QAAgD,CAAA,GAChD,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,eAAe,YAAmC,EAAE,OAAO,GAAG,QAAO,CAAE;EACxG;;;;EAKA,OAAO,aAAqB,SAAwB;AAClD,WAAO,KAAK,QAAQ,OAAO,mBAAmB,WAAW,IAAI;MAC3D,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,MAAK,GAAI,SAAS,OAAO,CAAC;KAC5D;EACH;;AA0TF,WAAW,QAAQ;;;ACrWb,IAAO,QAAP,cAAqB,YAAW;;;;EAIpC,OACE,gBACA,QACA,SAAwB;AAExB,UAAM,EAAE,SAAS,GAAG,KAAI,IAAK;AAC7B,WAAO,KAAK,QAAQ,KAAK,sBAAsB,cAAc,UAAU;MACrE,OAAO,EAAE,QAAO;MAChB;MACA,GAAG;KACJ;EACH;;;;EAKA,SACE,QACA,QACA,SAAwB;AAExB,UAAM,EAAE,iBAAiB,GAAG,MAAK,IAAK;AACtC,WAAO,KAAK,QAAQ,IAAI,sBAAsB,eAAe,UAAU,MAAM,IAAI,EAAE,OAAO,GAAG,QAAO,CAAE;EACxG;;;;EAKA,KACE,gBACA,QAA2C,CAAA,GAC3C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAClB,sBAAsB,cAAc,UACpC,wBACA,EAAE,OAAO,GAAG,QAAO,CAAE;EAEzB;;;;EAKA,OACE,QACA,QACA,SAAwB;AAExB,UAAM,EAAE,gBAAe,IAAK;AAC5B,WAAO,KAAK,QAAQ,OAAO,sBAAsB,eAAe,UAAU,MAAM,IAAI,OAAO;EAC7F;;;;AChDI,IAAO,gBAAP,cAA6B,YAAW;EAA9C,cAAA;;AACE,SAAA,QAAwB,IAAa,MAAM,KAAK,OAAO;EAoCzD;;;;EA/BE,OACE,OAAoD,CAAA,GACpD,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,kBAAkB,EAAE,MAAM,GAAG,QAAO,CAAE;EACjE;;;;EAKA,SAAS,gBAAwB,SAAwB;AACvD,WAAO,KAAK,QAAQ,IAAI,sBAAsB,cAAc,IAAI,OAAO;EACzE;;;;EAKA,OACE,gBACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,sBAAsB,cAAc,IAAI,EAAE,MAAM,GAAG,QAAO,CAAE;EACvF;;;;EAKA,OAAO,gBAAwB,SAAwB;AACrD,WAAO,KAAK,QAAQ,OAAO,sBAAsB,cAAc,IAAI,OAAO;EAC5E;;AA8LF,cAAc,QAAQ;;;AC/OhB,IAAO,aAAP,cAA0B,YAAW;;;;;;;;;;;;;EAazC,OAAO,MAA6B,SAAwB;AAC1D,UAAM,gCAAgC,CAAC,CAAC,KAAK;AAG7C,QAAI,kBACF,gCAAgC,KAAK,kBAAkB;AAEzD,QAAI,+BAA+B;AACjC,gBAAU,KAAK,OAAO,EAAE,MAAM,4CAA4C,KAAK,eAAe;IAChG;AAEA,UAAM,WAAgD,KAAK,QAAQ,KAAK,eAAe;MACrF,MAAM;QACJ,GAAG;QACH;;MAEF,GAAG;KACJ;AAGD,QAAI,+BAA+B;AACjC,aAAO;IACT;AAMA,cAAU,KAAK,OAAO,EAAE,MAAM,mDAAmD;AAEjF,WAAQ,SAAiD,YAAY,CAACC,cAAY;AAChF,UAAIA,aAAYA,UAAS,MAAM;AAC7B,QAAAA,UAAS,KAAK,QAAQ,CAAC,uBAAsB;AAC3C,gBAAM,qBAAqB,mBAAmB;AAC9C,6BAAmB,YAAY,eAAe,kBAAkB;QAClE,CAAC;MACH;AAEA,aAAOA;IACT,CAAC;EACH;;;;ACnDI,IAAO,cAAP,cAA2B,YAAW;;;;EAI1C,SACE,cACA,QACA,SAAwB;AAExB,UAAM,EAAE,SAAS,OAAM,IAAK;AAC5B,WAAO,KAAK,QAAQ,IAAI,cAAc,OAAO,SAAS,MAAM,iBAAiB,YAAY,IAAI,OAAO;EACtG;;;;EAKA,KACE,OACA,QACA,SAAwB;AAExB,UAAM,EAAE,SAAS,GAAG,MAAK,IAAK;AAC9B,WAAO,KAAK,QAAQ,WAClB,cAAc,OAAO,SAAS,KAAK,iBACnC,YACA,EAAE,OAAO,GAAG,QAAO,CAAE;EAEzB;;;;ACfI,IAAOC,QAAP,cAAoB,YAAW;EAArC,cAAA;;AACE,SAAA,cAA0C,IAAmB,YAAY,KAAK,OAAO;EAoDvF;;;;;;EA7CE,OAAO,QAAgB,MAAuB,SAAwB;AACpE,WAAO,KAAK,QAAQ,KAAK,cAAc,MAAM,SAAS,EAAE,MAAM,GAAG,QAAO,CAAE;EAC5E;;;;EAKA,SACE,OACA,QACA,SAAwB;AAExB,UAAM,EAAE,QAAO,IAAK;AACpB,WAAO,KAAK,QAAQ,IAAI,cAAc,OAAO,SAAS,KAAK,IAAI,OAAO;EACxE;;;;EAKA,KACE,QACA,QAA0C,CAAA,GAC1C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,cAAc,MAAM,SAAS,YAA6B;MACvF;MACA,GAAG;KACJ;EACH;;;;EAKA,OAAO,OAAe,QAAyB,SAAwB;AACrE,UAAM,EAAE,QAAO,IAAK;AACpB,WAAO,KAAK,QAAQ,OAAO,cAAc,OAAO,SAAS,KAAK,IAAI,OAAO;EAC3E;;;;EAKA,OAAO,OAAe,QAAyB,SAAwB;AACrE,UAAM,EAAE,QAAO,IAAK;AACpB,WAAO,KAAK,QAAQ,KAAK,cAAc,OAAO,SAAS,KAAK,IAAI,OAAO;EACzE;;AA+sFFA,MAAK,cAAc;;;AC3vFb,IAAO,QAAP,cAAqB,YAAW;EAAtC,cAAA;;AACE,SAAA,OAAqB,IAAYC,MAAK,KAAK,OAAO;EA4CpD;;;;;;;;;EAlCE,OAAO,MAAwB,SAAwB;AACrD,WAAO,KAAK,QAAQ,KAAK,UAAU,EAAE,MAAM,GAAG,QAAO,CAAE;EACzD;;;;EAKA,SAAS,QAAgB,SAAwB;AAC/C,WAAO,KAAK,QAAQ,IAAI,cAAc,MAAM,IAAI,OAAO;EACzD;;;;EAKA,OAAO,QAAgB,MAAwB,SAAwB;AACrE,WAAO,KAAK,QAAQ,KAAK,cAAc,MAAM,IAAI,EAAE,MAAM,GAAG,QAAO,CAAE;EACvE;;;;EAKA,KACE,QAA2C,CAAA,GAC3C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,UAAU,YAA8B,EAAE,OAAO,GAAG,QAAO,CAAE;EAC9F;;;;EAKA,OAAO,QAAgB,SAAwB;AAC7C,WAAO,KAAK,QAAQ,OAAO,cAAc,MAAM,IAAI,OAAO;EAC5D;;AAszBF,MAAM,OAAOA;;;ACl3BP,IAAOC,SAAP,cAAqB,YAAW;;;;;;;;;;;;;;;;;;;;;;;EAuBpC,OAAO,MAAwB,SAAwB;AACrD,WAAO,KAAK,QAAQ,KAAK,UAAU,4BAA4B,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EACpG;;;;EAKA,SAAS,QAAgB,SAAwB;AAC/C,WAAO,KAAK,QAAQ,IAAI,cAAc,MAAM,IAAI,OAAO;EACzD;;;;EAKA,KACE,QAA2C,CAAA,GAC3C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,UAAU,YAAwB,EAAE,OAAO,GAAG,QAAO,CAAE;EACxF;;;;EAKA,OAAO,QAAgB,SAAwB;AAC7C,WAAO,KAAK,QAAQ,OAAO,cAAc,MAAM,IAAI,OAAO;EAC5D;;;;EAKA,QAAQ,QAAgB,SAAwB;AAC9C,WAAO,KAAK,QAAQ,IAAI,cAAc,MAAM,YAAY;MACtD,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,qBAAoB,GAAI,SAAS,OAAO,CAAC;MAC1E,kBAAkB;KACnB;EACH;;;;EAKA,MAAM,kBACJ,IACA,EAAE,eAAe,KAAM,UAAU,KAAK,KAAK,IAAI,IAAkD,CAAA,GAAE;AAEnG,UAAM,kBAAkB,oBAAI,IAAI,CAAC,aAAa,SAAS,SAAS,CAAC;AAEjE,UAAM,QAAQ,KAAK,IAAG;AACtB,QAAIC,QAAO,MAAM,KAAK,SAAS,EAAE;AAEjC,WAAO,CAACA,MAAK,UAAU,CAAC,gBAAgB,IAAIA,MAAK,MAAM,GAAG;AACxD,YAAM,MAAM,YAAY;AAExB,MAAAA,QAAO,MAAM,KAAK,SAAS,EAAE;AAC7B,UAAI,KAAK,IAAG,IAAK,QAAQ,SAAS;AAChC,cAAM,IAAI,0BAA0B;UAClC,SAAS,iCAAiC,EAAE,+BAA+B,OAAO;SACnF;MACH;IACF;AAEA,WAAOA;EACT;;;;AC9FI,IAAO,UAAP,cAAuB,YAAW;;;;ACElC,IAAO,UAAP,cAAuB,YAAW;;;;;;;;;;;;;;;;;;EAkBtC,IAAI,MAAuB,SAAwB;AACjD,WAAO,KAAK,QAAQ,KAAK,kCAAkC,EAAE,MAAM,GAAG,QAAO,CAAE;EACjF;;;;;;;;;;;;;;;;;;EAmBA,SAAS,MAA4B,SAAwB;AAC3D,WAAO,KAAK,QAAQ,KAAK,uCAAuC,EAAE,MAAM,GAAG,QAAO,CAAE;EACtF;;;;ACpCI,IAAO,QAAP,cAAqB,YAAW;EAAtC,cAAA;;AACE,SAAA,UAA8B,IAAe,QAAQ,KAAK,OAAO;EACnE;;AAEA,MAAM,UAAU;;;ACRV,IAAO,cAAP,cAA2B,YAAW;;;;;;;;;;;;;;;;;;EAkB1C,OACE,0BACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAClB,gCAAgC,wBAAwB,gBACxD,MACA,EAAE,MAAM,QAAQ,QAAQ,GAAG,QAAO,CAAE;EAExC;;;;;;;;;;;;;;;EAgBA,SACE,0BACA,QAAqD,CAAA,GACrD,SAAwB;AAExB,WAAO,KAAK,QAAQ,IAAI,gCAAgC,wBAAwB,gBAAgB;MAC9F;MACA,GAAG;KACJ;EACH;;;;;;;;;;;;;;;;;;;EAoBA,OACE,cACA,QACA,SAAwB;AAExB,UAAM,EAAE,4BAA2B,IAAK;AACxC,WAAO,KAAK,QAAQ,OAClB,gCAAgC,2BAA2B,gBAAgB,YAAY,IACvF,OAAO;EAEX;;;;AC5EI,IAAO,cAAP,cAA2B,YAAW;EAA5C,cAAA;;AACE,SAAA,cAA0C,IAAmB,YAAY,KAAK,OAAO;EACvF;;AAEA,YAAY,cAAc;;;ACZpB,IAAOC,eAAP,cAA2B,YAAW;;;;;;;;;;;;;;EAc1C,KACE,iBACA,QAAiD,CAAA,GACjD,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAClB,yBAAyB,eAAe,gBACxC,YACA,EAAE,OAAO,GAAG,QAAO,CAAE;EAEzB;;;;ACdI,IAAO,OAAP,cAAoB,YAAW;EAArC,cAAA;;AACE,SAAA,cAA0C,IAAmBC,aAAY,KAAK,OAAO;EA2HvF;;;;;;;;;;;;;;;;;;EAxGE,OAAO,MAAuB,SAAwB;AACpD,WAAO,KAAK,QAAQ,KAAK,qBAAqB,EAAE,MAAM,GAAG,QAAO,CAAE;EACpE;;;;;;;;;;;;;EAcA,SAAS,iBAAyB,SAAwB;AACxD,WAAO,KAAK,QAAQ,IAAI,yBAAyB,eAAe,IAAI,OAAO;EAC7E;;;;;;;;;;;;EAaA,KACE,QAA0C,CAAA,GAC1C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,qBAAqB,YAA2B,EAAE,OAAO,GAAG,QAAO,CAAE;EACtG;;;;;;;;;;;EAYA,OAAO,iBAAyB,SAAwB;AACtD,WAAO,KAAK,QAAQ,KAAK,yBAAyB,eAAe,WAAW,OAAO;EACrF;;;;;;;;;;;;;;EAeA,WACE,iBACA,QAAgD,CAAA,GAChD,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAClB,yBAAyB,eAAe,WACxC,YACA,EAAE,OAAO,GAAG,QAAO,CAAE;EAEzB;;;;;;;;;;;EAYA,MAAM,iBAAyB,SAAwB;AACrD,WAAO,KAAK,QAAQ,KAAK,yBAAyB,eAAe,UAAU,OAAO;EACpF;;;;;;;;;;;EAYA,OAAO,iBAAyB,SAAwB;AACtD,WAAO,KAAK,QAAQ,KAAK,yBAAyB,eAAe,WAAW,OAAO;EACrF;;AA2eF,KAAK,cAAcA;;;ACvlBb,IAAO,aAAP,cAA0B,YAAW;EAA3C,cAAA;;AACE,SAAA,UAA8B,IAAe,QAAQ,KAAK,OAAO;AACjE,SAAA,OAAqB,IAAY,KAAK,KAAK,OAAO;AAClD,SAAA,cAA0C,IAAmB,YAAY,KAAK,OAAO;AACrF,SAAA,QAAwB,IAAa,MAAM,KAAK,OAAO;EACzD;;AAEA,WAAW,UAAU;AACrB,WAAW,OAAO;AAClB,WAAW,cAAc;AACzB,WAAW,QAAQ;;;ACnCb,IAAO,eAAP,cAA4B,YAAW;;;;ACQvC,IAAOC,WAAP,cAAuB,YAAW;EAAxC,cAAA;;AACE,SAAA,eAA6C,IAAoB,aAAa,KAAK,OAAO;EAC5F;;AAEAA,SAAQ,eAAe;;;ACTjB,IAAO,SAAP,cAAsB,YAAW;;;;;;;;;;;EAWrC,gBAAgB,MAAkC,SAAwB;AACxE,WAAO,KAAK,QAAQ,KAClB,sBACA,4BAA4B,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EAEnE;EAqBA,KACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAClB,iBACA,4BAA4B,EAAE,MAAM,GAAG,SAAS,QAAQ,KAAK,UAAU,MAAK,GAAI,KAAK,OAAO,CAAC;EAEjG;EAsBA,SACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,uBAAuB,EAAE,MAAM,GAAG,SAAS,QAAQ,KAAK,UAAU,MAAK,CAAE;EAGpG;;;;AC5EI,IAAO,SAAP,cAAsB,YAAW;;;;;EAKrC,SAAS,OAAe,SAAwB;AAC9C,WAAO,KAAK,QAAQ,IAAI,eAAe,KAAK,IAAI,OAAO;EACzD;;;;;EAMA,KAAK,SAAwB;AAC3B,WAAO,KAAK,QAAQ,WAAW,WAAW,MAAa,OAAO;EAChE;;;;;EAMA,OAAO,OAAe,SAAwB;AAC5C,WAAO,KAAK,QAAQ,OAAO,eAAe,KAAK,IAAI,OAAO;EAC5D;;;;ACzBI,IAAO,cAAP,cAA2B,YAAW;;;;;EAK1C,OAAO,MAA8B,SAAwB;AAC3D,WAAO,KAAK,QAAQ,KAAK,gBAAgB,EAAE,MAAM,GAAG,QAAO,CAAE;EAC/D;;;;ACHI,IAAO,QAAP,cAAqB,YAAW;;;;;;;;;;;;EAYpC,OAAO,QAAgB,MAAwB,SAAwB;AACrE,WAAO,KAAK,QAAQ,KAAK,uBAAuB,MAAM,WAAW;MAC/D;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,MAAK,GAAI,SAAS,OAAO,CAAC;KAC5D;EACH;;;;;;;;;EAUA,OAAO,QAAgB,SAAwB;AAC7C,WAAO,KAAK,QAAQ,KAAK,uBAAuB,MAAM,WAAW;MAC/D,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,MAAK,GAAI,SAAS,OAAO,CAAC;KAC5D;EACH;;;;;;;;;;;EAYA,MAAM,QAAgB,MAAuB,SAAwB;AACnE,WAAO,KAAK,QAAQ,KAAK,uBAAuB,MAAM,UAAU;MAC9D;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,MAAK,GAAI,SAAS,OAAO,CAAC;KAC5D;EACH;;;;;;;;;EAUA,OACE,QACA,OAA4C,CAAA,GAC5C,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,uBAAuB,MAAM,WAAW;MAC/D;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,MAAK,GAAI,SAAS,OAAO,CAAC;KAC5D;EACH;;;;ACxEI,IAAO,gBAAP,cAA6B,YAAW;;;;;;;;;;;;;;;;;;;;;;;;EAwB5C,OAAO,MAAgC,SAAwB;AAC7D,WAAO,KAAK,QAAQ,KAAK,4BAA4B,EAAE,MAAM,GAAG,QAAO,CAAE;EAC3E;;;;AChBI,IAAOC,YAAP,cAAwB,YAAW;EAAzC,cAAA;;AACE,SAAA,gBAAgD,IAAqB,cAAc,KAAK,OAAO;AAC/F,SAAA,QAAwB,IAAa,MAAM,KAAK,OAAO;EACzD;;AA6lJAA,UAAS,gBAAgB;AACzBA,UAAS,QAAQ;;;ACxlJX,SAAU,mBAGd,UAAoB,QAAc;AAClC,MAAI,CAAC,UAAU,CAACC,uBAAsB,MAAM,GAAG;AAC7C,WAAO;MACL,GAAG;MACH,eAAe;MACf,QAAQ,SAAS,OAAO,IAAI,CAAC,SAAQ;AACnC,YAAI,KAAK,SAAS,iBAAiB;AACjC,iBAAO;YACL,GAAG;YACH,kBAAkB;;QAEtB;AAEA,YAAI,KAAK,SAAS,WAAW;AAC3B,iBAAO;YACL,GAAG;YACH,SAAS,KAAK,QAAQ,IAAI,CAAC,aAAa;cACtC,GAAG;cACH,QAAQ;cACR;;QAEN,OAAO;AACL,iBAAO;QACT;MACF,CAAC;;EAEL;AAEA,SAAO,cAAc,UAAU,MAAM;AACvC;AAEM,SAAU,cAGd,UAAoB,QAAc;AAClC,QAAM,SAAmD,SAAS,OAAO,IACvE,CAAC,SAA2C;AAC1C,QAAI,KAAK,SAAS,iBAAiB;AACjC,aAAO;QACL,GAAG;QACH,kBAAkBC,eAAc,QAAQ,IAAI;;IAEhD;AACA,QAAI,KAAK,SAAS,WAAW;AAC3B,YAAM,UAAyC,KAAK,QAAQ,IAAI,CAACC,aAAW;AAC1E,YAAIA,SAAQ,SAAS,eAAe;AAClC,iBAAO;YACL,GAAGA;YACH,QAAQ,gBAAgB,QAAQA,SAAQ,IAAI;;QAEhD;AAEA,eAAOA;MACT,CAAC;AAED,aAAO;QACL,GAAG;QACH;;IAEJ;AAEA,WAAO;EACT,CAAC;AAGH,QAAM,SAAyD,OAAO,OAAO,CAAA,GAAI,UAAU,EAAE,OAAM,CAAE;AACrG,MAAI,CAAC,OAAO,yBAAyB,UAAU,aAAa,GAAG;AAC7D,kBAAc,MAAM;EACtB;AAEA,SAAO,eAAe,QAAQ,iBAAiB;IAC7C,YAAY;IACZ,MAAG;AACD,iBAAWC,WAAU,OAAO,QAAQ;AAClC,YAAIA,QAAO,SAAS,WAAW;AAC7B;QACF;AAEA,mBAAW,WAAWA,QAAO,SAAS;AACpC,cAAI,QAAQ,SAAS,iBAAiB,QAAQ,WAAW,MAAM;AAC7D,mBAAO,QAAQ;UACjB;QACF;MACF;AAEA,aAAO;IACT;GACD;AAED,SAAO;AACT;AAEA,SAAS,gBAGP,QAAgB,SAAe;AAC/B,MAAI,OAAO,MAAM,QAAQ,SAAS,eAAe;AAC/C,WAAO;EACT;AAEA,MAAI,eAAe,OAAO,MAAM,QAAQ;AACtC,UAAM,cAAc,OAAO,MAAM;AACjC,WAAO,YAAY,UAAU,OAAO;EACtC;AAEA,SAAO,KAAK,MAAM,OAAO;AAC3B;AAEM,SAAUH,uBAAsB,QAAqC;AACzE,MAAI,6BAA6B,OAAO,MAAM,MAAM,GAAG;AACrD,WAAO;EACT;AAEA,SAAO;AACT;AAkDM,SAAUI,oBAAmBC,OAAS;AAC1C,SAAOA,QAAO,QAAQ,MAAM;AAC9B;AAEA,SAAS,mBAAmB,aAA0B,MAAY;AAChE,SAAO,YAAY,KAAK,CAACA,UAASA,MAAK,SAAS,cAAcA,MAAK,SAAS,IAAI;AAGlF;AAEA,SAASC,eACP,QACA,UAAkC;AAElC,QAAM,YAAY,mBAAmB,OAAO,SAAS,CAAA,GAAI,SAAS,IAAI;AAEtE,SAAO;IACL,GAAG;IACH,GAAG;IACH,kBACEF,oBAAmB,SAAS,IAAI,UAAU,UAAU,SAAS,SAAS,IACpE,WAAW,SAAS,KAAK,MAAM,SAAS,SAAS,IACjD;;AAER;AA8BM,SAAU,cAAc,KAAa;AACzC,QAAM,QAAkB,CAAA;AACxB,aAAW,UAAU,IAAI,QAAQ;AAC/B,QAAI,OAAO,SAAS,WAAW;AAC7B;IACF;AAEA,eAAW,WAAW,OAAO,SAAS;AACpC,UAAI,QAAQ,SAAS,eAAe;AAClC,cAAM,KAAK,QAAQ,IAAI;MACzB;IACF;EACF;AAEA,MAAI,cAAc,MAAM,KAAK,EAAE;AACjC;;;;;;;;;;;ACzMM,IAAO,iBAAP,MAAO,wBACH,YAA2B;EAOnC,YAAY,QAAsC;AAChD,UAAK;;AALP,2BAAA,IAAA,MAAA,MAAA;AACA,4CAAA,IAAA,MAAA,MAAA;AACA,kCAAA,IAAA,MAAA,MAAA;AAIE,2BAAA,MAAI,wBAAW,QAAM,GAAA;EACvB;EAEA,OAAO,eACL,QACA,QACA,SAAwB;AAExB,UAAM,SAAS,IAAI,gBAAwB,MAAuC;AAClF,WAAO,KAAK,MACV,OAAO,0BAA0B,QAAQ,QAAQ;MAC/C,GAAG;MACH,SAAS,EAAE,GAAG,SAAS,SAAS,6BAA6B,SAAQ;KACtE,CAAC;AAEJ,WAAO;EACT;EA2EU,MAAM,0BACd,QACA,QACA,SAAwB;AAExB,UAAM,SAAS,SAAS;AACxB,QAAI,QAAQ;AACV,UAAI,OAAO;AAAS,aAAK,WAAW,MAAK;AACzC,aAAO,iBAAiB,SAAS,MAAM,KAAK,WAAW,MAAK,CAAE;IAChE;AACA,2BAAA,MAAI,2BAAA,KAAA,4BAAA,EAAc,KAAlB,IAAI;AAEJ,QAAI;AACJ,QAAI,iBAAgC;AACpC,QAAI,iBAAiB,QAAQ;AAC3B,eAAS,MAAM,OAAO,UAAU,SAC9B,OAAO,aACP,EAAE,QAAQ,KAAI,GACd,EAAE,GAAG,SAAS,QAAQ,KAAK,WAAW,QAAQ,QAAQ,KAAI,CAAE;AAE9D,uBAAiB,OAAO,kBAAkB;IAC5C,OAAO;AACL,eAAS,MAAM,OAAO,UAAU,OAC9B,EAAE,GAAG,QAAQ,QAAQ,KAAI,GACzB,EAAE,GAAG,SAAS,QAAQ,KAAK,WAAW,OAAM,CAAE;IAElD;AAEA,SAAK,WAAU;AACf,qBAAiB,SAAS,QAAQ;AAChC,6BAAA,MAAI,2BAAA,KAAA,wBAAA,EAAU,KAAd,MAAe,OAAO,cAAc;IACtC;AACA,QAAI,OAAO,WAAW,QAAQ,SAAS;AACrC,YAAM,IAAI,kBAAiB;IAC7B;AACA,WAAO,uBAAA,MAAI,2BAAA,KAAA,0BAAA,EAAY,KAAhB,IAAI;EACb;EAyFA,EAAA,yBAAA,oBAAA,QAAA,GAAA,0CAAA,oBAAA,QAAA,GAAA,gCAAA,oBAAA,QAAA,GAAA,4BAAA,oBAAA,QAAA,GAAA,+BAAA,SAAAG,gCAAA;AArME,QAAI,KAAK;AAAO;AAChB,2BAAA,MAAI,yCAA4B,QAAS,GAAA;EAC3C,GAAC,2BAAA,SAAAC,0BAEwC,OAA4B,gBAA6B;AAChG,QAAI,KAAK;AAAO;AAEhB,UAAM,YAAY,CAAC,MAAcC,WAAsD;AACrF,UAAI,kBAAkB,QAAQA,OAAM,kBAAkB,gBAAgB;AACpE,aAAK,MAAM,MAAaA,MAAK;MAC/B;IACF;AAEA,UAAM,WAAW,uBAAA,MAAI,2BAAA,KAAA,kCAAA,EAAoB,KAAxB,MAAyB,KAAK;AAC/C,cAAU,SAAS,KAAK;AAExB,YAAQ,MAAM,MAAM;MAClB,KAAK,8BAA8B;AACjC,cAAM,SAAS,SAAS,OAAO,MAAM,YAAY;AACjD,YAAI,CAAC,QAAQ;AACX,gBAAM,IAAI,YAAY,2BAA2B,MAAM,YAAY,EAAE;QACvE;AACA,YAAI,OAAO,SAAS,WAAW;AAC7B,gBAAM,UAAU,OAAO,QAAQ,MAAM,aAAa;AAClD,cAAI,CAAC,SAAS;AACZ,kBAAM,IAAI,YAAY,4BAA4B,MAAM,aAAa,EAAE;UACzE;AACA,cAAI,QAAQ,SAAS,eAAe;AAClC,kBAAM,IAAI,YAAY,6CAA6C,QAAQ,IAAI,EAAE;UACnF;AAEA,oBAAU,8BAA8B;YACtC,GAAG;YACH,UAAU,QAAQ;WACnB;QACH;AACA;MACF;MACA,KAAK,0CAA0C;AAC7C,cAAM,SAAS,SAAS,OAAO,MAAM,YAAY;AACjD,YAAI,CAAC,QAAQ;AACX,gBAAM,IAAI,YAAY,2BAA2B,MAAM,YAAY,EAAE;QACvE;AACA,YAAI,OAAO,SAAS,iBAAiB;AACnC,oBAAU,0CAA0C;YAClD,GAAG;YACH,UAAU,OAAO;WAClB;QACH;AACA;MACF;MACA;AACE,kBAAU,MAAM,MAAM,KAAK;AAC3B;IACJ;EACF,GAAC,6BAAA,SAAAC,8BAAA;AAGC,QAAI,KAAK,OAAO;AACd,YAAM,IAAI,YAAY,yCAAyC;IACjE;AACA,UAAM,WAAW,uBAAA,MAAI,yCAAA,GAAA;AACrB,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,YAAY,0CAA0C;IAClE;AACA,2BAAA,MAAI,yCAA4B,QAAS,GAAA;AACzC,UAAM,iBAAiB,iBAA0B,UAAU,uBAAA,MAAI,wBAAA,GAAA,CAAQ;AACvE,2BAAA,MAAI,+BAAkB,gBAAc,GAAA;AAEpC,WAAO;EACT,GAAC,qCAAA,SAAAC,oCAwCmB,OAA0B;AAC5C,QAAI,WAAW,uBAAA,MAAI,yCAAA,GAAA;AACnB,QAAI,CAAC,UAAU;AACb,UAAI,MAAM,SAAS,oBAAoB;AACrC,cAAM,IAAI,YACR,6EAA6E,MAAM,IAAI,EAAE;MAE7F;AACA,iBAAW,uBAAA,MAAI,yCAA4B,MAAM,UAAQ,GAAA;AACzD,aAAO;IACT;AAEA,YAAQ,MAAM,MAAM;MAClB,KAAK,8BAA8B;AACjC,iBAAS,OAAO,KAAK,MAAM,IAAI;AAC/B;MACF;MACA,KAAK,+BAA+B;AAClC,cAAM,SAAS,SAAS,OAAO,MAAM,YAAY;AACjD,YAAI,CAAC,QAAQ;AACX,gBAAM,IAAI,YAAY,2BAA2B,MAAM,YAAY,EAAE;QACvE;AACA,cAAM,OAAO,OAAO;AACpB,cAAM,OAAO,MAAM;AACnB,YAAI,SAAS,aAAa,KAAK,SAAS,kBAAkB;AACxD,iBAAO,QAAQ,KAAK,IAAI;QAC1B,WAAW,SAAS,eAAe,KAAK,SAAS,kBAAkB;AACjE,cAAI,CAAC,OAAO,SAAS;AACnB,mBAAO,UAAU,CAAA;UACnB;AACA,iBAAO,QAAQ,KAAK,IAAI;QAC1B;AACA;MACF;MACA,KAAK,8BAA8B;AACjC,cAAM,SAAS,SAAS,OAAO,MAAM,YAAY;AACjD,YAAI,CAAC,QAAQ;AACX,gBAAM,IAAI,YAAY,2BAA2B,MAAM,YAAY,EAAE;QACvE;AACA,YAAI,OAAO,SAAS,WAAW;AAC7B,gBAAM,UAAU,OAAO,QAAQ,MAAM,aAAa;AAClD,cAAI,CAAC,SAAS;AACZ,kBAAM,IAAI,YAAY,4BAA4B,MAAM,aAAa,EAAE;UACzE;AACA,cAAI,QAAQ,SAAS,eAAe;AAClC,kBAAM,IAAI,YAAY,6CAA6C,QAAQ,IAAI,EAAE;UACnF;AACA,kBAAQ,QAAQ,MAAM;QACxB;AACA;MACF;MACA,KAAK,0CAA0C;AAC7C,cAAM,SAAS,SAAS,OAAO,MAAM,YAAY;AACjD,YAAI,CAAC,QAAQ;AACX,gBAAM,IAAI,YAAY,2BAA2B,MAAM,YAAY,EAAE;QACvE;AACA,YAAI,OAAO,SAAS,iBAAiB;AACnC,iBAAO,aAAa,MAAM;QAC5B;AACA;MACF;MACA,KAAK,iCAAiC;AACpC,cAAM,SAAS,SAAS,OAAO,MAAM,YAAY;AACjD,YAAI,CAAC,QAAQ;AACX,gBAAM,IAAI,YAAY,2BAA2B,MAAM,YAAY,EAAE;QACvE;AACA,YAAI,OAAO,SAAS,aAAa;AAC/B,gBAAM,UAAU,OAAO,UAAU,MAAM,aAAa;AACpD,cAAI,CAAC,SAAS;AACZ,kBAAM,IAAI,YAAY,4BAA4B,MAAM,aAAa,EAAE;UACzE;AACA,cAAI,QAAQ,SAAS,kBAAkB;AACrC,kBAAM,IAAI,YAAY,gDAAgD,QAAQ,IAAI,EAAE;UACtF;AACA,kBAAQ,QAAQ,MAAM;QACxB;AACA;MACF;MACA,KAAK,sBAAsB;AACzB,+BAAA,MAAI,yCAA4B,MAAM,UAAQ,GAAA;AAC9C;MACF;IACF;AAEA,WAAO;EACT,GAEC,OAAO,cAAa,IAAC;AACpB,UAAM,YAAmC,CAAA;AACzC,UAAM,YAGA,CAAA;AACN,QAAI,OAAO;AAEX,SAAK,GAAG,SAAS,CAAC,UAAS;AACzB,YAAM,SAAS,UAAU,MAAK;AAC9B,UAAI,QAAQ;AACV,eAAO,QAAQ,KAAK;MACtB,OAAO;AACL,kBAAU,KAAK,KAAK;MACtB;IACF,CAAC;AAED,SAAK,GAAG,OAAO,MAAK;AAClB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,QAAQ,MAAS;MAC1B;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,SAAK,GAAG,SAAS,CAAC,QAAO;AACvB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,OAAO,GAAG;MACnB;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,SAAK,GAAG,SAAS,CAAC,QAAO;AACvB,aAAO;AACP,iBAAW,UAAU,WAAW;AAC9B,eAAO,OAAO,GAAG;MACnB;AACA,gBAAU,SAAS;IACrB,CAAC;AAED,WAAO;MACL,MAAM,YAAyD;AAC7D,YAAI,CAAC,UAAU,QAAQ;AACrB,cAAI,MAAM;AACR,mBAAO,EAAE,OAAO,QAAW,MAAM,KAAI;UACvC;AACA,iBAAO,IAAI,QAAyC,CAAC,SAAS,WAC5D,UAAU,KAAK,EAAE,SAAS,OAAM,CAAE,CAAC,EACnC,KAAK,CAACF,WAAWA,SAAQ,EAAE,OAAOA,QAAO,MAAM,MAAK,IAAK,EAAE,OAAO,QAAW,MAAM,KAAI,CAAG;QAC9F;AACA,cAAM,QAAQ,UAAU,MAAK;AAC7B,eAAO,EAAE,OAAO,OAAO,MAAM,MAAK;MACpC;MACA,QAAQ,YAAW;AACjB,aAAK,MAAK;AACV,eAAO,EAAE,OAAO,QAAW,MAAM,KAAI;MACvC;;EAEJ;;;;;EAMA,MAAM,gBAAa;AACjB,UAAM,KAAK,KAAI;AACf,UAAM,WAAW,uBAAA,MAAI,+BAAA,GAAA;AACrB,QAAI,CAAC;AAAU,YAAM,IAAI,YAAY,iDAAiD;AACtF,WAAO;EACT;;AAGF,SAAS,iBACP,UACA,QAAsC;AAEtC,SAAO,mBAAmB,UAAU,MAAM;AAC5C;;;ACtWM,IAAO,aAAP,cAA0B,YAAW;;;;;;;;;;;;;;EAczC,KACE,YACA,QAAgD,CAAA,GAChD,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAClB,kBAAkB,UAAU,gBAC5B,YACA,EAAE,OAAO,GAAG,QAAO,CAAE;EAEzB;;;;ACzBI,IAAO,cAAP,cAA2B,YAAW;;;;;;;;;;;;EAY1C,MACE,OAAiD,CAAA,GACjD,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,2BAA2B,EAAE,MAAM,GAAG,QAAO,CAAE;EAC1E;;;;ACwCI,IAAO,YAAP,cAAyB,YAAW;EAA1C,cAAA;;AACE,SAAA,aAAuC,IAAkB,WAAW,KAAK,OAAO;AAChF,SAAA,cAA0C,IAAmB,YAAY,KAAK,OAAO;EAiKvF;EApIE,OACE,MACA,SAAwB;AAExB,WACE,KAAK,QAAQ,KAAK,cAAc,EAAE,MAAM,GAAG,SAAS,QAAQ,KAAK,UAAU,MAAK,CAAE,EAGlF,YAAY,CAAC,QAAO;AACpB,UAAI,YAAY,OAAO,IAAI,WAAW,YAAY;AAChD,sBAAc,GAAe;MAC/B;AAEA,aAAO;IACT,CAAC;EACH;EA2BA,SACE,YACA,QAA4C,CAAA,GAC5C,SAAwB;AAExB,WACE,KAAK,QAAQ,IAAI,kBAAkB,UAAU,IAAI;MAC/C;MACA,GAAG;MACH,QAAQ,OAAO,UAAU;KAC1B,EACD,YAAY,CAAC,QAAO;AACpB,UAAI,YAAY,OAAO,IAAI,WAAW,YAAY;AAChD,sBAAc,GAAe;MAC/B;AAEA,aAAO;IACT,CAAC;EACH;;;;;;;;;;;EAYA,OAAO,YAAoB,SAAwB;AACjD,WAAO,KAAK,QAAQ,OAAO,kBAAkB,UAAU,IAAI;MACzD,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,MAAK,GAAI,SAAS,OAAO,CAAC;KAC5D;EACH;EAEA,MACE,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,UACjB,OAAO,MAAM,OAAO,EACpB,YAAY,CAAC,aAAa,cAAc,UAAsB,IAAI,CAAC;EACxE;;;;EAKA,OACE,MACA,SAAwB;AAExB,WAAO,eAAe,eAAwB,KAAK,SAAS,MAAM,OAAO;EAC3E;;;;;;;;;;;;;EAcA,OAAO,YAAoB,SAAwB;AACjD,WAAO,KAAK,QAAQ,KAAK,kBAAkB,UAAU,WAAW,OAAO;EACzE;;;;;;;;;;;;;;;;EAiBA,QAAQ,MAA6B,SAAwB;AAC3D,WAAO,KAAK,QAAQ,KAAK,sBAAsB,EAAE,MAAM,GAAG,QAAO,CAAE;EACrE;;AAm6NF,UAAU,aAAa;AACvB,UAAU,cAAc;;;AC/nOlB,IAAOG,WAAP,cAAuB,YAAW;;;;EAItC,SAAS,SAAiB,SAAwB;AAChD,WAAO,KAAK,QAAQ,IAAI,eAAe,OAAO,YAAY;MACxD,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,qBAAoB,GAAI,SAAS,OAAO,CAAC;MAC1E,kBAAkB;KACnB;EACH;;;;ACVI,IAAOC,WAAP,cAAuB,YAAW;;;;EAItC,SAAS,SAAiB,QAA+B,SAAwB;AAC/E,UAAM,EAAE,SAAQ,IAAK;AACrB,WAAO,KAAK,QAAQ,IAAI,eAAe,QAAQ,aAAa,OAAO,YAAY;MAC7E,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,qBAAoB,GAAI,SAAS,OAAO,CAAC;MAC1E,kBAAkB;KACnB;EACH;;;;ACPI,IAAO,WAAP,cAAwB,YAAW;EAAzC,cAAA;;AACE,SAAA,UAA8B,IAAeC,SAAQ,KAAK,OAAO;EAqDnE;;;;EAhDE,OACE,SACA,OAA+C,CAAA,GAC/C,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAClB,eAAe,OAAO,aACtB,iCAAiC,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EAExE;;;;EAKA,SACE,SACA,QACA,SAAwB;AAExB,UAAM,EAAE,SAAQ,IAAK;AACrB,WAAO,KAAK,QAAQ,IAAI,eAAe,QAAQ,aAAa,OAAO,IAAI,OAAO;EAChF;;;;EAKA,KACE,SACA,QAA8C,CAAA,GAC9C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,eAAe,OAAO,aAAa,YAA0B;MAC1F;MACA,GAAG;KACJ;EACH;;;;EAKA,OACE,SACA,QACA,SAAwB;AAExB,UAAM,EAAE,SAAQ,IAAK;AACrB,WAAO,KAAK,QAAQ,OAAO,eAAe,QAAQ,aAAa,OAAO,IAAI,OAAO;EACnF;;AAmHF,SAAS,UAAUA;;;AC5Jb,IAAO,SAAP,cAAsB,YAAW;EAAvC,cAAA;;AACE,SAAA,UAA8B,IAAeC,SAAQ,KAAK,OAAO;AACjE,SAAA,WAAiC,IAAgB,SAAS,KAAK,OAAO;EAuCxE;;;;EAlCE,OAAO,OAA6C,CAAA,GAAI,SAAwB;AAC9E,WAAO,KAAK,QAAQ,KAAK,WAAW,iCAAiC,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EAC1G;;;;EAKA,SAAS,SAAiB,SAAwB;AAChD,WAAO,KAAK,QAAQ,IAAI,eAAe,OAAO,IAAI,OAAO;EAC3D;;;;EAKA,OAAO,SAAiB,MAAyB,SAAwB;AACvE,WAAO,KAAK,QAAQ,KAAK,eAAe,OAAO,IAAI,EAAE,MAAM,GAAG,QAAO,CAAE;EACzE;;;;EAKA,KACE,QAA4C,CAAA,GAC5C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,WAAW,YAAmB,EAAE,OAAO,GAAG,QAAO,CAAE;EACpF;;;;EAKA,OAAO,SAAiB,SAAwB;AAC9C,WAAO,KAAK,QAAQ,OAAO,eAAe,OAAO,IAAI,OAAO;EAC9D;;AAmGF,OAAO,UAAUA;AACjB,OAAO,WAAW;;;AC3JZ,IAAO,QAAP,cAAqB,YAAW;;;;;;;;;;;;;;EAcpC,OAAO,UAAkB,MAAwB,SAAwB;AACvE,WAAO,KAAK,QAAQ,KAClB,gBAAgB,QAAQ,UACxB,4BAA4B,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EAEnE;;;;AClBI,IAAO,UAAP,cAAuB,YAAW;EAAxC,cAAA;;AACE,SAAA,QAAwB,IAAa,MAAM,KAAK,OAAO;EA0DzD;;;;;;;;;;;;;;;;;;;;;;;;EAjCE,OAAO,MAA0B,SAAwB;AACvD,WAAO,KAAK,QAAQ,KAAK,YAAY,EAAE,MAAM,GAAG,QAAO,CAAE;EAC3D;;;;;;EAOA,OAAO,UAAkB,SAAwB;AAC/C,WAAO,KAAK,QAAQ,KAAK,gBAAgB,QAAQ,WAAW,OAAO;EACrE;;;;;;;;;;;;;;;;;;EAmBA,SAAS,UAAkB,MAA4B,SAAwB;AAC7E,WAAO,KAAK,QAAQ,KAAK,gBAAgB,QAAQ,aAAa,EAAE,MAAM,GAAG,QAAO,CAAE;EACpF;;AA0HF,QAAQ,QAAQ;;;AC3LT,IAAM,sBAAsB,OAAU,aAAwC;AACnF,QAAM,UAAU,MAAM,QAAQ,WAAW,QAAQ;AACjD,QAAM,WAAW,QAAQ,OAAO,CAAC,WAA4C,OAAO,WAAW,UAAU;AACzG,MAAI,SAAS,QAAQ;AACnB,eAAW,UAAU,UAAU;AAC7B,cAAQ,MAAM,OAAO,MAAM;IAC7B;AAEA,UAAM,IAAI,MAAM,GAAG,SAAS,MAAM,2CAA2C;EAC/E;AAGA,QAAM,SAAc,CAAA;AACpB,aAAW,UAAU,SAAS;AAC5B,QAAI,OAAO,WAAW,aAAa;AACjC,aAAO,KAAK,OAAO,KAAK;IAC1B;EACF;AACA,SAAO;AACT;;;ACPM,IAAO,cAAP,cAA2B,YAAW;;;;EAI1C,OACE,eACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,sBAAsB,aAAa,iBAAiB;MAC3E;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,SACE,SACA,QACA,SAAwB;AAExB,UAAM,EAAE,gBAAe,IAAK;AAC5B,WAAO,KAAK,QAAQ,IAAI,sBAAsB,eAAe,iBAAiB,OAAO,IAAI;MACvF,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;EAMA,OACE,SACA,QACA,SAAwB;AAExB,UAAM,EAAE,gBAAe,IAAK;AAC5B,WAAO,KAAK,QAAQ,KAAK,sBAAsB,eAAe,iBAAiB,OAAO,WAAW;MAC/F,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,MAAM,cACJ,eACA,MACA,SAAsD;AAEtD,UAAM,QAAQ,MAAM,KAAK,OAAO,eAAe,IAAI;AACnD,WAAO,MAAM,KAAK,KAAK,eAAe,MAAM,IAAI,OAAO;EACzD;;;;EAKA,UACE,SACA,QACA,SAAwB;AAExB,UAAM,EAAE,iBAAiB,GAAG,MAAK,IAAK;AACtC,WAAO,KAAK,QAAQ,WAClB,sBAAsB,eAAe,iBAAiB,OAAO,UAC7D,YACA,EAAE,OAAO,GAAG,SAAS,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC,EAAC,CAAE;EAExG;;;;;;;EAQA,MAAM,KACJ,eACA,SACA,SAAsD;AAEtD,UAAM,UAAU,aAAa;MAC3B,SAAS;MACT;QACE,2BAA2B;QAC3B,oCAAoC,SAAS,gBAAgB,SAAQ,KAAM;;KAE9E;AAED,WAAO,MAAM;AACX,YAAM,EAAE,MAAM,OAAO,SAAQ,IAAK,MAAM,KAAK,SAC3C,SACA,EAAE,iBAAiB,cAAa,GAChC;QACE,GAAG;QACH;OACD,EACD,aAAY;AAEd,cAAQ,MAAM,QAAQ;QACpB,KAAK;AACH,cAAI,gBAAgB;AAEpB,cAAI,SAAS,gBAAgB;AAC3B,4BAAgB,QAAQ;UAC1B,OAAO;AACL,kBAAM,iBAAiB,SAAS,QAAQ,IAAI,sBAAsB;AAClE,gBAAI,gBAAgB;AAClB,oBAAM,mBAAmB,SAAS,cAAc;AAChD,kBAAI,CAAC,MAAM,gBAAgB,GAAG;AAC5B,gCAAgB;cAClB;YACF;UACF;AACA,gBAAM,MAAM,aAAa;AACzB;QACF,KAAK;QACL,KAAK;QACL,KAAK;AACH,iBAAO;MACX;IACF;EACF;;;;;;EAOA,MAAM,cACJ,eACA,EAAE,OAAO,UAAU,CAAA,EAAE,GACrB,SAA+E;AAE/E,QAAI,SAAS,QAAQ,MAAM,UAAU,GAAG;AACtC,YAAM,IAAI,MACR,gHAAgH;IAEpH;AAEA,UAAM,wBAAwB,SAAS,kBAAkB;AAGzD,UAAM,mBAAmB,KAAK,IAAI,uBAAuB,MAAM,MAAM;AAErE,UAAM,SAAS,KAAK;AACpB,UAAM,eAAe,MAAM,OAAM;AACjC,UAAM,aAAuB,CAAC,GAAG,OAAO;AAIxC,mBAAe,aAAa,UAAsC;AAChE,eAAS,QAAQ,UAAU;AACzB,cAAM,UAAU,MAAM,OAAO,MAAM,OAAO,EAAE,MAAM,MAAM,SAAS,aAAY,GAAI,OAAO;AACxF,mBAAW,KAAK,QAAQ,EAAE;MAC5B;IACF;AAGA,UAAM,UAAU,MAAM,gBAAgB,EAAE,KAAK,YAAY,EAAE,IAAI,YAAY;AAG3E,UAAM,oBAAoB,OAAO;AAEjC,WAAO,MAAM,KAAK,cAAc,eAAe;MAC7C,UAAU;KACX;EACH;;;;AC/KI,IAAOC,SAAP,cAAqB,YAAW;;;;;;EAMpC,OACE,eACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,sBAAsB,aAAa,UAAU;MACpE;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,SACE,QACA,QACA,SAAwB;AAExB,UAAM,EAAE,gBAAe,IAAK;AAC5B,WAAO,KAAK,QAAQ,IAAI,sBAAsB,eAAe,UAAU,MAAM,IAAI;MAC/E,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,OAAO,QAAgB,QAA0B,SAAwB;AACvE,UAAM,EAAE,iBAAiB,GAAG,KAAI,IAAK;AACrC,WAAO,KAAK,QAAQ,KAAK,sBAAsB,eAAe,UAAU,MAAM,IAAI;MAChF;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,KACE,eACA,QAA2C,CAAA,GAC3C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,sBAAsB,aAAa,UAAU,YAA6B;MACvG;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;;;EAQA,OACE,QACA,QACA,SAAwB;AAExB,UAAM,EAAE,gBAAe,IAAK;AAC5B,WAAO,KAAK,QAAQ,OAAO,sBAAsB,eAAe,UAAU,MAAM,IAAI;MAClF,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,MAAM,cACJ,eACA,MACA,SAAsD;AAEtD,UAAMC,QAAO,MAAM,KAAK,OAAO,eAAe,MAAM,OAAO;AAC3D,WAAO,MAAM,KAAK,KAAK,eAAeA,MAAK,IAAI,OAAO;EACxD;;;;;;;EAOA,MAAM,KACJ,eACA,QACA,SAAsD;AAEtD,UAAM,UAAU,aAAa;MAC3B,SAAS;MACT;QACE,2BAA2B;QAC3B,oCAAoC,SAAS,gBAAgB,SAAQ,KAAM;;KAE9E;AAED,WAAO,MAAM;AACX,YAAM,eAAe,MAAM,KAAK,SAC9B,QACA;QACE,iBAAiB;SAEnB,EAAE,GAAG,SAAS,QAAO,CAAE,EACvB,aAAY;AAEd,YAAMA,QAAO,aAAa;AAE1B,cAAQA,MAAK,QAAQ;QACnB,KAAK;AACH,cAAI,gBAAgB;AAEpB,cAAI,SAAS,gBAAgB;AAC3B,4BAAgB,QAAQ;UAC1B,OAAO;AACL,kBAAM,iBAAiB,aAAa,SAAS,QAAQ,IAAI,sBAAsB;AAC/E,gBAAI,gBAAgB;AAClB,oBAAM,mBAAmB,SAAS,cAAc;AAChD,kBAAI,CAAC,MAAM,gBAAgB,GAAG;AAC5B,gCAAgB;cAClB;YACF;UACF;AACA,gBAAM,MAAM,aAAa;AACzB;QACF,KAAK;QACL,KAAK;AACH,iBAAOA;MACX;IACF;EACF;;;;;;;EAOA,MAAM,OAAO,eAAuBA,OAAkB,SAAwB;AAC5E,UAAM,WAAW,MAAM,KAAK,QAAQ,MAAM,OAAO,EAAE,MAAMA,OAAM,SAAS,aAAY,GAAI,OAAO;AAC/F,WAAO,KAAK,OAAO,eAAe,EAAE,SAAS,SAAS,GAAE,GAAI,OAAO;EACrE;;;;EAIA,MAAM,cACJ,eACAA,OACA,SAAsD;AAEtD,UAAM,WAAW,MAAM,KAAK,OAAO,eAAeA,OAAM,OAAO;AAC/D,WAAO,MAAM,KAAK,KAAK,eAAe,SAAS,IAAI,OAAO;EAC5D;;;;EAKA,QACE,QACA,QACA,SAAwB;AAExB,UAAM,EAAE,gBAAe,IAAK;AAC5B,WAAO,KAAK,QAAQ,WAClB,sBAAsB,eAAe,UAAU,MAAM,YACrD,MACA,EAAE,GAAG,SAAS,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC,EAAC,CAAE;EAEjG;;;;AC5JI,IAAO,eAAP,cAA4B,YAAW;EAA7C,cAAA;;AACE,SAAA,QAAwB,IAAaC,OAAM,KAAK,OAAO;AACvD,SAAA,cAA0C,IAAmB,YAAY,KAAK,OAAO;EAkFvF;;;;EA7EE,OAAO,MAA+B,SAAwB;AAC5D,WAAO,KAAK,QAAQ,KAAK,kBAAkB;MACzC;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,SAAS,eAAuB,SAAwB;AACtD,WAAO,KAAK,QAAQ,IAAI,sBAAsB,aAAa,IAAI;MAC7D,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,OACE,eACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,KAAK,sBAAsB,aAAa,IAAI;MAC9D;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,KACE,QAAkD,CAAA,GAClD,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,kBAAkB,YAAyB;MACxE;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;EAKA,OAAO,eAAuB,SAAwB;AACpD,WAAO,KAAK,QAAQ,OAAO,sBAAsB,aAAa,IAAI;MAChE,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EACH;;;;;EAMA,OACE,eACA,MACA,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAClB,sBAAsB,aAAa,WACnC,MACA;MACE;MACA,QAAQ;MACR,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,eAAe,gBAAe,GAAI,SAAS,OAAO,CAAC;KAC7E;EAEL;;AA8YF,aAAa,QAAQA;AACrB,aAAa,cAAc;;;ACzfrB,IAAO,SAAP,cAAsB,YAAW;;;;EAIrC,OAAO,MAAyB,SAAwB;AACtD,WAAO,KAAK,QAAQ,KAAK,WAAW,iCAAiC,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EAC1G;;;;EAKA,SAAS,SAAiB,SAAwB;AAChD,WAAO,KAAK,QAAQ,IAAI,eAAe,OAAO,IAAI,OAAO;EAC3D;;;;EAKA,KACE,QAA4C,CAAA,GAC5C,SAAwB;AAExB,WAAO,KAAK,QAAQ,WAAW,WAAW,wBAA+B,EAAE,OAAO,GAAG,QAAO,CAAE;EAChG;;;;EAKA,OAAO,SAAiB,SAAwB;AAC9C,WAAO,KAAK,QAAQ,OAAO,eAAe,OAAO,IAAI,OAAO;EAC9D;;;;;;EAOA,gBACE,SACA,QAAuD,CAAA,GACvD,SAAwB;AAExB,WAAO,KAAK,QAAQ,IAAI,eAAe,OAAO,YAAY;MACxD;MACA,GAAG;MACH,SAAS,aAAa,CAAC,EAAE,QAAQ,qBAAoB,GAAI,SAAS,OAAO,CAAC;MAC1E,kBAAkB;KACnB;EACH;;;;EAKA,MAAM,SAAiB,MAAwB,SAAwB;AACrE,WAAO,KAAK,QAAQ,KAClB,eAAe,OAAO,UACtB,iCAAiC,EAAE,MAAM,GAAG,QAAO,GAAI,KAAK,OAAO,CAAC;EAExE;;;;;;;AC/DI,IAAO,WAAP,cAAwB,YAAW;EAAzC,cAAA;;;EAqIA;;;;EAjIE,MAAM,OACJ,SACA,SACA,SAAoC,KAAK,QAAQ,eACjD,YAAoB,KAAG;AAEvB,UAAM,KAAK,gBAAgB,SAAS,SAAS,QAAQ,SAAS;AAE9D,WAAO,KAAK,MAAM,OAAO;EAC3B;;;;;;;;;;;EAYA,MAAM,gBACJ,SACA,SACA,SAAoC,KAAK,QAAQ,eACjD,YAAoB,KAAG;AAEvB,QACE,OAAO,WAAW,eAClB,OAAO,OAAO,OAAO,cAAc,cACnC,OAAO,OAAO,OAAO,WAAW,YAChC;AACA,YAAM,IAAI,MAAM,sFAAsF;IACxG;AAEA,2BAAA,MAAI,qBAAA,KAAA,wBAAA,EAAgB,KAApB,MAAqB,MAAM;AAE3B,UAAM,aAAa,aAAa,CAAC,OAAO,CAAC,EAAE;AAC3C,UAAM,kBAAkB,uBAAA,MAAI,qBAAA,KAAA,2BAAA,EAAmB,KAAvB,MAAwB,YAAY,mBAAmB;AAC/E,UAAM,YAAY,uBAAA,MAAI,qBAAA,KAAA,2BAAA,EAAmB,KAAvB,MAAwB,YAAY,mBAAmB;AACzE,UAAM,YAAY,uBAAA,MAAI,qBAAA,KAAA,2BAAA,EAAmB,KAAvB,MAAwB,YAAY,YAAY;AAGlE,UAAM,mBAAmB,SAAS,WAAW,EAAE;AAC/C,QAAI,MAAM,gBAAgB,GAAG;AAC3B,YAAM,IAAI,6BAA6B,kCAAkC;IAC3E;AAEA,UAAM,aAAa,KAAK,MAAM,KAAK,IAAG,IAAK,GAAI;AAE/C,QAAI,aAAa,mBAAmB,WAAW;AAC7C,YAAM,IAAI,6BAA6B,8BAA8B;IACvE;AAEA,QAAI,mBAAmB,aAAa,WAAW;AAC7C,YAAM,IAAI,6BAA6B,8BAA8B;IACvE;AAKA,UAAM,aAAa,gBAChB,MAAM,GAAG,EACT,IAAI,CAAC,SAAU,KAAK,WAAW,KAAK,IAAI,KAAK,UAAU,CAAC,IAAI,IAAK;AAGpE,UAAM,gBACJ,OAAO,WAAW,QAAQ,IACxB,OAAO,KAAK,OAAO,QAAQ,UAAU,EAAE,GAAG,QAAQ,IAClD,OAAO,KAAK,QAAQ,OAAO;AAG/B,UAAM,gBAAgB,YAAY,GAAG,SAAS,IAAI,SAAS,IAAI,OAAO,KAAK,GAAG,SAAS,IAAI,OAAO;AAGlG,UAAM,MAAM,MAAM,OAAO,OAAO,UAC9B,OACA,eACA,EAAE,MAAM,QAAQ,MAAM,UAAS,GAC/B,OACA,CAAC,QAAQ,CAAC;AAIZ,eAAW,aAAa,YAAY;AAClC,UAAI;AACF,cAAM,iBAAiB,OAAO,KAAK,WAAW,QAAQ;AACtD,cAAM,UAAU,MAAM,OAAO,OAAO,OAClC,QACA,KACA,gBACA,IAAI,YAAW,EAAG,OAAO,aAAa,CAAC;AAGzC,YAAI,SAAS;AACX;QACF;MACF,QAAQ;AAEN;MACF;IACF;AAEA,UAAM,IAAI,6BACR,mEAAmE;EAEvE;;mHAEgB,QAAiC;AAC/C,MAAI,OAAO,WAAW,YAAY,OAAO,WAAW,GAAG;AACrD,UAAM,IAAI,MACR,mKAAmK;EAEvK;AACF,GAAC,8BAAA,SAAAC,6BAEkB,SAAkB,MAAY;AAC/C,MAAI,CAAC,SAAS;AACZ,UAAM,IAAI,MAAM,sBAAsB;EACxC;AAEA,QAAM,QAAQ,QAAQ,IAAI,IAAI;AAE9B,MAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,UAAM,IAAI,MAAM,4BAA4B,IAAI,EAAE;EACpD;AAEA,SAAO;AACT;;;;;;;AC4MI,IAAO,SAAP,MAAa;;;;;;;;;;;;;;;;;EAkCjB,YAAY,EACV,UAAU,QAAQ,iBAAiB,GACnC,SAAS,QAAQ,gBAAgB,GACjC,eAAe,QAAQ,eAAe,KAAK,MAC3C,UAAU,QAAQ,mBAAmB,KAAK,MAC1C,gBAAgB,QAAQ,uBAAuB,KAAK,MACpD,GAAG,KAAI,IACU,CAAA,GAAE;;AA3BrB,oBAAA,IAAA,MAAA,MAAA;AAkqBA,SAAA,cAA+B,IAAQC,aAAY,IAAI;AACvD,SAAA,OAAiB,IAAQ,KAAK,IAAI;AAClC,SAAA,aAA6B,IAAQ,WAAW,IAAI;AACpD,SAAA,QAAmB,IAAQC,OAAM,IAAI;AACrC,SAAA,SAAqB,IAAQ,OAAO,IAAI;AACxC,SAAA,QAAmB,IAAQ,MAAM,IAAI;AACrC,SAAA,cAA+B,IAAQ,YAAY,IAAI;AACvD,SAAA,SAAqB,IAAQ,OAAO,IAAI;AACxC,SAAA,aAA6B,IAAQ,WAAW,IAAI;AACpD,SAAA,UAAuB,IAAQC,SAAQ,IAAI;AAC3C,SAAA,eAAiC,IAAQ,aAAa,IAAI;AAC1D,SAAA,WAAyB,IAAQ,SAAS,IAAI;AAC9C,SAAA,OAAiB,IAAQ,KAAK,IAAI;AAClC,SAAA,UAAuB,IAAQ,QAAQ,IAAI;AAC3C,SAAA,UAAuB,IAAQ,QAAQ,IAAI;AAC3C,SAAA,YAA2B,IAAQ,UAAU,IAAI;AACjD,SAAA,WAAyB,IAAQC,UAAS,IAAI;AAC9C,SAAA,gBAAmC,IAAQ,cAAc,IAAI;AAC7D,SAAA,QAAmB,IAAQ,MAAM,IAAI;AACrC,SAAA,aAA6B,IAAQ,WAAW,IAAI;AACpD,SAAA,SAAqB,IAAQ,OAAO,IAAI;AACxC,SAAA,SAAqB,IAAQ,OAAO,IAAI;AA3pBtC,QAAI,WAAW,QAAW;AACxB,YAAM,IAAW,YACf,iGAAiG;IAErG;AAEA,UAAM,UAAyB;MAC7B;MACA;MACA;MACA;MACA,GAAG;MACH,SAAS,WAAW;;AAGtB,QAAI,CAAC,QAAQ,2BAA2B,mBAAkB,GAAI;AAC5D,YAAM,IAAW,YACf,obAAob;IAExb;AAEA,SAAK,UAAU,QAAQ;AACvB,SAAK,UAAU,QAAQ,WAAWC,IAAO;AACzC,SAAK,SAAS,QAAQ,UAAU;AAChC,UAAM,kBAAkB;AAExB,SAAK,WAAW;AAChB,SAAK,WACH,cAAc,QAAQ,UAAU,0BAA0B,IAAI,KAC9D,cAAc,QAAQ,YAAY,GAAG,6BAA6B,IAAI,KACtE;AACF,SAAK,eAAe,QAAQ;AAC5B,SAAK,aAAa,QAAQ,cAAc;AACxC,SAAK,QAAQ,QAAQ,SAAe,gBAAe;AACnD,2BAAA,MAAI,iBAAiB,iBAAe,GAAA;AAEpC,SAAK,WAAW;AAEhB,SAAK,SAAS,OAAO,WAAW,WAAW,SAAS;AACpD,SAAK,eAAe;AACpB,SAAK,UAAU;AACf,SAAK,gBAAgB;EACvB;;;;EAKA,YAAY,SAA+B;AACzC,UAAM,SAAS,IAAK,KAAK,YAAiE;MACxF,GAAG,KAAK;MACR,SAAS,KAAK;MACd,YAAY,KAAK;MACjB,SAAS,KAAK;MACd,QAAQ,KAAK;MACb,UAAU,KAAK;MACf,OAAO,KAAK;MACZ,cAAc,KAAK;MACnB,QAAQ,KAAK;MACb,cAAc,KAAK;MACnB,SAAS,KAAK;MACd,eAAe,KAAK;MACpB,GAAG;KACJ;AACD,WAAO;EACT;EASU,eAAY;AACpB,WAAO,KAAK,SAAS;EACvB;EAEU,gBAAgB,EAAE,QAAQ,MAAK,GAAmB;AAC1D;EACF;EAEU,MAAM,YAAY,MAAyB;AACnD,WAAO,aAAa,CAAC,EAAE,eAAe,UAAU,KAAK,MAAM,GAAE,CAAE,CAAC;EAClE;EAEU,eAAe,OAA8B;AACrD,WAAU,UAAU,OAAO,EAAE,aAAa,WAAU,CAAE;EACxD;EAEQ,eAAY;AAClB,WAAO,GAAG,KAAK,YAAY,IAAI,OAAO,OAAO;EAC/C;EAEU,wBAAqB;AAC7B,WAAO,wBAAwB,MAAK,CAAE;EACxC;EAEU,gBACR,QACA,OACA,SACA,SAAgB;AAEhB,WAAc,SAAS,SAAS,QAAQ,OAAO,SAAS,OAAO;EACjE;EAEA,MAAM,cAAW;AACf,UAAM,SAAS,KAAK,SAAS;AAC7B,QAAI,OAAO,WAAW;AAAY,aAAO;AAEzC,QAAI;AACJ,QAAI;AACF,cAAQ,MAAM,OAAM;IACtB,SAAS,KAAU;AACjB,UAAI,eAAsB;AAAa,cAAM;AAC7C,YAAM,IAAW;QACf,+CAA+C,IAAI,OAAO;;QAE1D,EAAE,OAAO,IAAG;MAAE;IAElB;AAEA,QAAI,OAAO,UAAU,YAAY,CAAC,OAAO;AACvC,YAAM,IAAW,YACf,0EAA0E,KAAK,EAAE;IAErF;AACA,SAAK,SAAS;AACd,WAAO;EACT;EAEA,SACEC,OACA,OACA,gBAAmC;AAEnC,UAAM,UAAW,CAAC,uBAAA,MAAI,mBAAA,KAAA,yBAAA,EAAmB,KAAvB,IAAI,KAAyB,kBAAmB,KAAK;AACvE,UAAMC,OACJ,cAAcD,KAAI,IAChB,IAAI,IAAIA,KAAI,IACZ,IAAI,IAAI,WAAW,QAAQ,SAAS,GAAG,KAAKA,MAAK,WAAW,GAAG,IAAIA,MAAK,MAAM,CAAC,IAAIA,MAAK;AAE5F,UAAM,eAAe,KAAK,aAAY;AACtC,QAAI,CAAC,WAAW,YAAY,GAAG;AAC7B,cAAQ,EAAE,GAAG,cAAc,GAAG,MAAK;IACrC;AAEA,QAAI,OAAO,UAAU,YAAY,SAAS,CAAC,MAAM,QAAQ,KAAK,GAAG;AAC/D,MAAAC,KAAI,SAAS,KAAK,eAAe,KAAgC;IACnE;AAEA,WAAOA,KAAI,SAAQ;EACrB;;;;EAKU,MAAM,eAAe,SAA4B;AACzD,UAAM,KAAK,YAAW;EACxB;;;;;;;EAQU,MAAM,eACd,SACA,EAAE,KAAAA,MAAK,QAAO,GAAiD;EAC/C;EAElB,IAASD,OAAc,MAAqC;AAC1D,WAAO,KAAK,cAAc,OAAOA,OAAM,IAAI;EAC7C;EAEA,KAAUA,OAAc,MAAqC;AAC3D,WAAO,KAAK,cAAc,QAAQA,OAAM,IAAI;EAC9C;EAEA,MAAWA,OAAc,MAAqC;AAC5D,WAAO,KAAK,cAAc,SAASA,OAAM,IAAI;EAC/C;EAEA,IAASA,OAAc,MAAqC;AAC1D,WAAO,KAAK,cAAc,OAAOA,OAAM,IAAI;EAC7C;EAEA,OAAYA,OAAc,MAAqC;AAC7D,WAAO,KAAK,cAAc,UAAUA,OAAM,IAAI;EAChD;EAEQ,cACN,QACAA,OACA,MAAqC;AAErC,WAAO,KAAK,QACV,QAAQ,QAAQ,IAAI,EAAE,KAAK,CAACE,UAAQ;AAClC,aAAO,EAAE,QAAQ,MAAAF,OAAM,GAAGE,MAAI;IAChC,CAAC,CAAC;EAEN;EAEA,QACE,SACA,mBAAkC,MAAI;AAEtC,WAAO,IAAI,WAAW,MAAM,KAAK,YAAY,SAAS,kBAAkB,MAAS,CAAC;EACpF;EAEQ,MAAM,YACZ,cACA,kBACA,qBAAuC;AAEvC,UAAM,UAAU,MAAM;AACtB,UAAM,aAAa,QAAQ,cAAc,KAAK;AAC9C,QAAI,oBAAoB,MAAM;AAC5B,yBAAmB;IACrB;AAEA,UAAM,KAAK,eAAe,OAAO;AAEjC,UAAM,EAAE,KAAK,KAAAD,MAAK,QAAO,IAAK,MAAM,KAAK,aAAa,SAAS;MAC7D,YAAY,aAAa;KAC1B;AAED,UAAM,KAAK,eAAe,KAAK,EAAE,KAAAA,MAAK,QAAO,CAAE;AAG/C,UAAM,eAAe,UAAW,KAAK,OAAM,KAAM,KAAK,MAAO,GAAG,SAAS,EAAE,EAAE,SAAS,GAAG,GAAG;AAC5F,UAAM,cAAc,wBAAwB,SAAY,KAAK,cAAc,mBAAmB;AAC9F,UAAM,YAAY,KAAK,IAAG;AAE1B,cAAU,IAAI,EAAE,MACd,IAAI,YAAY,qBAChB,qBAAqB;MACnB;MACA,QAAQ,QAAQ;MAChB,KAAAA;MACA;MACA,SAAS,IAAI;KACd,CAAC;AAGJ,QAAI,QAAQ,QAAQ,SAAS;AAC3B,YAAM,IAAW,kBAAiB;IACpC;AAEA,UAAM,aAAa,IAAI,gBAAe;AACtC,UAAM,WAAW,MAAM,KAAK,iBAAiBA,MAAK,KAAK,SAAS,UAAU,EAAE,MAAM,WAAW;AAC7F,UAAM,cAAc,KAAK,IAAG;AAE5B,QAAI,oBAAoB,WAAW,OAAO;AACxC,YAAM,eAAe,aAAa,gBAAgB;AAClD,UAAI,QAAQ,QAAQ,SAAS;AAC3B,cAAM,IAAW,kBAAiB;MACpC;AAKA,YAAM,YACJ,aAAa,QAAQ,KACrB,eAAe,KAAK,OAAO,QAAQ,KAAK,WAAW,WAAW,OAAO,SAAS,KAAK,IAAI,GAAG;AAC5F,UAAI,kBAAkB;AACpB,kBAAU,IAAI,EAAE,KACd,IAAI,YAAY,gBAAgB,YAAY,cAAc,QAAQ,MAAM,YAAY,EAAE;AAExF,kBAAU,IAAI,EAAE,MACd,IAAI,YAAY,gBAAgB,YAAY,cAAc,QAAQ,KAAK,YAAY,KACnF,qBAAqB;UACnB;UACA,KAAAA;UACA,YAAY,cAAc;UAC1B,SAAS,SAAS;SACnB,CAAC;AAEJ,eAAO,KAAK,aAAa,SAAS,kBAAkB,uBAAuB,YAAY;MACzF;AACA,gBAAU,IAAI,EAAE,KACd,IAAI,YAAY,gBAAgB,YAAY,cAAc,QAAQ,gCAAgC;AAEpG,gBAAU,IAAI,EAAE,MACd,IAAI,YAAY,gBAAgB,YAAY,cAAc,QAAQ,kCAClE,qBAAqB;QACnB;QACA,KAAAA;QACA,YAAY,cAAc;QAC1B,SAAS,SAAS;OACnB,CAAC;AAEJ,UAAI,WAAW;AACb,cAAM,IAAW,0BAAyB;MAC5C;AACA,YAAM,IAAW,mBAAmB,EAAE,OAAO,SAAQ,CAAE;IACzD;AAEA,UAAM,iBAAiB,CAAC,GAAG,SAAS,QAAQ,QAAO,CAAE,EAClD,OAAO,CAAC,CAAC,IAAI,MAAM,SAAS,cAAc,EAC1C,IAAI,CAAC,CAAC,MAAM,KAAK,MAAM,OAAO,OAAO,OAAO,KAAK,UAAU,KAAK,CAAC,EACjE,KAAK,EAAE;AACV,UAAM,eAAe,IAAI,YAAY,GAAG,WAAW,GAAG,cAAc,KAAK,IAAI,MAAM,IAAIA,IAAG,IACxF,SAAS,KAAK,cAAc,QAC9B,gBAAgB,SAAS,MAAM,OAAO,cAAc,SAAS;AAE7D,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,cAAc,MAAM,KAAK,YAAY,QAAQ;AACnD,UAAI,oBAAoB,aAAa;AACnC,cAAME,gBAAe,aAAa,gBAAgB;AAGlD,cAAY,qBAAqB,SAAS,IAAI;AAC9C,kBAAU,IAAI,EAAE,KAAK,GAAG,YAAY,MAAMA,aAAY,EAAE;AACxD,kBAAU,IAAI,EAAE,MACd,IAAI,YAAY,qBAAqBA,aAAY,KACjD,qBAAqB;UACnB;UACA,KAAK,SAAS;UACd,QAAQ,SAAS;UACjB,SAAS,SAAS;UAClB,YAAY,cAAc;SAC3B,CAAC;AAEJ,eAAO,KAAK,aACV,SACA,kBACA,uBAAuB,cACvB,SAAS,OAAO;MAEpB;AAEA,YAAM,eAAe,cAAc,gCAAgC;AAEnE,gBAAU,IAAI,EAAE,KAAK,GAAG,YAAY,MAAM,YAAY,EAAE;AAExD,YAAM,UAAU,MAAM,SAAS,KAAI,EAAG,MAAM,CAACC,SAAa,YAAYA,IAAG,EAAE,OAAO;AAClF,YAAM,UAAU,SAAS,OAAO;AAChC,YAAM,aAAa,UAAU,SAAY;AAEzC,gBAAU,IAAI,EAAE,MACd,IAAI,YAAY,qBAAqB,YAAY,KACjD,qBAAqB;QACnB;QACA,KAAK,SAAS;QACd,QAAQ,SAAS;QACjB,SAAS,SAAS;QAClB,SAAS;QACT,YAAY,KAAK,IAAG,IAAK;OAC1B,CAAC;AAGJ,YAAM,MAAM,KAAK,gBAAgB,SAAS,QAAQ,SAAS,YAAY,SAAS,OAAO;AACvF,YAAM;IACR;AAEA,cAAU,IAAI,EAAE,KAAK,YAAY;AACjC,cAAU,IAAI,EAAE,MACd,IAAI,YAAY,oBAChB,qBAAqB;MACnB;MACA,KAAK,SAAS;MACd,QAAQ,SAAS;MACjB,SAAS,SAAS;MAClB,YAAY,cAAc;KAC3B,CAAC;AAGJ,WAAO,EAAE,UAAU,SAAS,YAAY,cAAc,qBAAqB,UAAS;EACtF;EAEA,WACEJ,OACAK,OACA,MAAqC;AAErC,WAAO,KAAK,eACVA,OACA,QAAQ,UAAU,OAChB,KAAK,KAAK,CAACH,WAAU,EAAE,QAAQ,OAAO,MAAAF,OAAM,GAAGE,MAAI,EAAG,IACtD,EAAE,QAAQ,OAAO,MAAAF,OAAM,GAAG,KAAI,CAAE;EAEtC;EAEA,eAIEK,OACA,SAA4C;AAE5C,UAAM,UAAU,KAAK,YAAY,SAAS,MAAM,MAAS;AACzD,WAAO,IAAe,YAA6B,MAAuB,SAASA,KAAI;EACzF;EAEA,MAAM,iBACJJ,MACA,MACA,IACA,YAA2B;AAE3B,UAAM,EAAE,QAAQ,QAAQ,GAAG,QAAO,IAAK,QAAQ,CAAA;AAC/C,UAAM,QAAQ,KAAK,WAAW,UAAU;AACxC,QAAI;AAAQ,aAAO,iBAAiB,SAAS,OAAO,EAAE,MAAM,KAAI,CAAE;AAElE,UAAM,UAAU,WAAW,OAAO,EAAE;AAEpC,UAAM,iBACF,WAAmB,kBAAkB,QAAQ,gBAAiB,WAAmB,kBAClF,OAAO,QAAQ,SAAS,YAAY,QAAQ,SAAS,QAAQ,OAAO,iBAAiB,QAAQ;AAEhG,UAAM,eAA4B;MAChC,QAAQ,WAAW;MACnB,GAAI,iBAAiB,EAAE,QAAQ,OAAM,IAAK,CAAA;MAC1C,QAAQ;MACR,GAAG;;AAEL,QAAI,QAAQ;AAGV,mBAAa,SAAS,OAAO,YAAW;IAC1C;AAEA,QAAI;AAEF,aAAO,MAAM,KAAK,MAAM,KAAK,QAAWA,MAAK,YAAY;IAC3D;AACE,mBAAa,OAAO;IACtB;EACF;EAEQ,MAAM,YAAY,UAAkB;AAE1C,UAAM,oBAAoB,SAAS,QAAQ,IAAI,gBAAgB;AAG/D,QAAI,sBAAsB;AAAQ,aAAO;AACzC,QAAI,sBAAsB;AAAS,aAAO;AAG1C,QAAI,SAAS,WAAW;AAAK,aAAO;AAGpC,QAAI,SAAS,WAAW;AAAK,aAAO;AAGpC,QAAI,SAAS,WAAW;AAAK,aAAO;AAGpC,QAAI,SAAS,UAAU;AAAK,aAAO;AAEnC,WAAO;EACT;EAEQ,MAAM,aACZ,SACA,kBACA,cACA,iBAAqC;AAErC,QAAI;AAGJ,UAAM,yBAAyB,iBAAiB,IAAI,gBAAgB;AACpE,QAAI,wBAAwB;AAC1B,YAAM,YAAY,WAAW,sBAAsB;AACnD,UAAI,CAAC,OAAO,MAAM,SAAS,GAAG;AAC5B,wBAAgB;MAClB;IACF;AAGA,UAAM,mBAAmB,iBAAiB,IAAI,aAAa;AAC3D,QAAI,oBAAoB,CAAC,eAAe;AACtC,YAAM,iBAAiB,WAAW,gBAAgB;AAClD,UAAI,CAAC,OAAO,MAAM,cAAc,GAAG;AACjC,wBAAgB,iBAAiB;MACnC,OAAO;AACL,wBAAgB,KAAK,MAAM,gBAAgB,IAAI,KAAK,IAAG;MACzD;IACF;AAIA,QAAI,EAAE,iBAAiB,KAAK,iBAAiB,gBAAgB,KAAK,MAAO;AACvE,YAAM,aAAa,QAAQ,cAAc,KAAK;AAC9C,sBAAgB,KAAK,mCAAmC,kBAAkB,UAAU;IACtF;AACA,UAAM,MAAM,aAAa;AAEzB,WAAO,KAAK,YAAY,SAAS,mBAAmB,GAAG,YAAY;EACrE;EAEQ,mCAAmC,kBAA0B,YAAkB;AACrF,UAAM,oBAAoB;AAC1B,UAAM,gBAAgB;AAEtB,UAAM,aAAa,aAAa;AAGhC,UAAM,eAAe,KAAK,IAAI,oBAAoB,KAAK,IAAI,GAAG,UAAU,GAAG,aAAa;AAGxF,UAAM,SAAS,IAAI,KAAK,OAAM,IAAK;AAEnC,WAAO,eAAe,SAAS;EACjC;EAEA,MAAM,aACJ,cACA,EAAE,aAAa,EAAC,IAA8B,CAAA,GAAE;AAEhD,UAAM,UAAU,EAAE,GAAG,aAAY;AACjC,UAAM,EAAE,QAAQ,MAAAD,OAAM,OAAO,eAAc,IAAK;AAEhD,UAAMC,OAAM,KAAK,SAASD,OAAO,OAAkC,cAAc;AACjF,QAAI,aAAa;AAAS,8BAAwB,WAAW,QAAQ,OAAO;AAC5E,YAAQ,UAAU,QAAQ,WAAW,KAAK;AAC1C,UAAM,EAAE,aAAa,KAAI,IAAK,KAAK,UAAU,EAAE,QAAO,CAAE;AACxD,UAAM,aAAa,MAAM,KAAK,aAAa,EAAE,SAAS,cAAc,QAAQ,aAAa,WAAU,CAAE;AAErG,UAAM,MAA4B;MAChC;MACA,SAAS;MACT,GAAI,QAAQ,UAAU,EAAE,QAAQ,QAAQ,OAAM;MAC9C,GAAK,WAAmB,kBACtB,gBAAiB,WAAmB,kBAAkB,EAAE,QAAQ,OAAM;MACxE,GAAI,QAAQ,EAAE,KAAI;MAClB,GAAK,KAAK,gBAAwB,CAAA;MAClC,GAAK,QAAQ,gBAAwB,CAAA;;AAGvC,WAAO,EAAE,KAAK,KAAAC,MAAK,SAAS,QAAQ,QAAO;EAC7C;EAEQ,MAAM,aAAa,EACzB,SACA,QACA,aACA,WAAU,GAMX;AACC,QAAI,qBAAkC,CAAA;AACtC,QAAI,KAAK,qBAAqB,WAAW,OAAO;AAC9C,UAAI,CAAC,QAAQ;AAAgB,gBAAQ,iBAAiB,KAAK,sBAAqB;AAChF,yBAAmB,KAAK,iBAAiB,IAAI,QAAQ;IACvD;AAEA,UAAM,UAAU,aAAa;MAC3B;MACA;QACE,QAAQ;QACR,cAAc,KAAK,aAAY;QAC/B,2BAA2B,OAAO,UAAU;QAC5C,GAAI,QAAQ,UAAU,EAAE,uBAAuB,OAAO,KAAK,MAAM,QAAQ,UAAU,GAAI,CAAC,EAAC,IAAK,CAAA;QAC9F,GAAG,mBAAkB;QACrB,uBAAuB,KAAK;QAC5B,kBAAkB,KAAK;;MAEzB,MAAM,KAAK,YAAY,OAAO;MAC9B,KAAK,SAAS;MACd;MACA,QAAQ;KACT;AAED,SAAK,gBAAgB,OAAO;AAE5B,WAAO,QAAQ;EACjB;EAEQ,WAAW,YAA2B;AAG5C,WAAO,MAAM,WAAW,MAAK;EAC/B;EAEQ,UAAU,EAAE,SAAS,EAAE,MAAM,SAAS,WAAU,EAAE,GAAoC;AAI5F,QAAI,CAAC,MAAM;AACT,aAAO,EAAE,aAAa,QAAW,MAAM,OAAS;IAClD;AACA,UAAM,UAAU,aAAa,CAAC,UAAU,CAAC;AACzC;;MAEE,YAAY,OAAO,IAAI,KACvB,gBAAgB,eAChB,gBAAgB,YACf,OAAO,SAAS;MAEf,QAAQ,OAAO,IAAI,cAAc;MAEjC,WAAmB,QAAQ,gBAAiB,WAAmB;MAEjE,gBAAgB;MAEhB,gBAAgB;MAEd,WAAmB,kBAAkB,gBAAiB,WAAmB;MAC3E;AACA,aAAO,EAAE,aAAa,QAAW,KAAsB;IACzD,WACE,OAAO,SAAS,aACf,OAAO,iBAAiB,QACtB,OAAO,YAAY,QAAQ,UAAU,QAAQ,OAAO,KAAK,SAAS,aACrE;AACA,aAAO,EAAE,aAAa,QAAW,MAAY,mBAAmB,IAAiC,EAAC;IACpG,WACE,OAAO,SAAS,YAChB,QAAQ,OAAO,IAAI,cAAc,MAAM,qCACvC;AACA,aAAO;QACL,aAAa,EAAE,gBAAgB,oCAAmC;QAClE,MAAM,KAAK,eAAe,IAA+B;;IAE7D,OAAO;AACL,aAAO,uBAAA,MAAI,iBAAA,GAAA,EAAS,KAAb,MAAc,EAAE,MAAM,QAAO,CAAE;IACxC;EACF;;;AA1iBE,SAAO,KAAK,YAAY;AAC1B;AA2iBO,OAAA,SAASF;AACT,OAAA,kBAAkB;AAElB,OAAA,cAAqB;AACrB,OAAA,WAAkB;AAClB,OAAA,qBAA4B;AAC5B,OAAA,4BAAmC;AACnC,OAAA,oBAA2B;AAC3B,OAAA,gBAAuB;AACvB,OAAA,gBAAuB;AACvB,OAAA,iBAAwB;AACxB,OAAA,kBAAyB;AACzB,OAAA,sBAA6B;AAC7B,OAAA,sBAA6B;AAC7B,OAAA,wBAA+B;AAC/B,OAAA,2BAAkC;AAClC,OAAA,+BAAsC;AAEtC,OAAA,SAAiB;AA0B1B,OAAO,cAAcJ;AACrB,OAAO,OAAO;AACd,OAAO,aAAa;AACpB,OAAO,QAAQC;AACf,OAAO,SAAS;AAChB,OAAO,QAAQ;AACf,OAAO,cAAc;AACrB,OAAO,SAAS;AAChB,OAAO,aAAa;AACpB,OAAO,UAAUC;AACjB,OAAO,eAAe;AACtB,OAAO,WAAW;AAClB,OAAO,OAAO;AACd,OAAO,UAAU;AACjB,OAAO,UAAU;AACjB,OAAO,YAAY;AACnB,OAAO,WAAWC;AAClB,OAAO,gBAAgB;AACvB,OAAO,QAAQ;AACf,OAAO,aAAa;AACpB,OAAO,SAAS;AAChB,OAAO,SAAS;;;AC5gCV,IAAO,cAAP,cAA2B,OAAM;;;;;;;;;;;;;;;;;;EAqBrC,YAAY,EACV,UAAU,QAAQ,iBAAiB,GACnC,SAAS,QAAQ,sBAAsB,GACvC,aAAa,QAAQ,oBAAoB,GACzC,UACA,YACA,sBACA,yBACA,GAAG,KAAI,IACe,CAAA,GAAE;AACxB,QAAI,CAAC,YAAY;AACf,YAAM,IAAW,YACf,8MAA8M;IAElN;AAEA,QAAI,OAAO,yBAAyB,YAAY;AAC9C,gCAA0B;IAC5B;AAEA,QAAI,CAAC,wBAAwB,CAAC,QAAQ;AACpC,YAAM,IAAW,YACf,sIAAsI;IAE1I;AAEA,QAAI,wBAAwB,QAAQ;AAClC,YAAM,IAAW,YACf,6GAA6G;IAEjH;AAEA,SAAK,eAAe,EAAE,GAAG,KAAK,cAAc,eAAe,WAAU;AAErE,QAAI,CAAC,SAAS;AACZ,UAAI,CAAC,UAAU;AACb,mBAAW,QAAQ,IAAI,uBAAuB;MAChD;AAEA,UAAI,CAAC,UAAU;AACb,cAAM,IAAW,YACf,gHAAgH;MAEpH;AAEA,gBAAU,GAAG,QAAQ;IACvB,OAAO;AACL,UAAI,UAAU;AACZ,cAAM,IAAW,YAAY,6CAA6C;MAC5E;IACF;AAEA,UAAM;MACJ,QAAQ,wBAAwB;MAChC;MACA,GAAG;MACH,GAAI,4BAA4B,SAAY,EAAE,wBAAuB,IAAK,CAAA;KAC3E;AA5EH,SAAA,aAAqB;AA8EnB,SAAK,aAAa;AAClB,SAAK,iBAAiB;EACxB;EAES,MAAM,aACb,SACA,QAAiC,CAAA,GAAE;AAEnC,QAAI,uBAAuB,IAAI,QAAQ,IAAI,KAAK,QAAQ,WAAW,UAAU,QAAQ,SAAS,QAAW;AACvG,UAAI,CAAC,MAAM,QAAQ,IAAI,GAAG;AACxB,cAAM,IAAI,MAAM,uCAAuC;MACzD;AACA,YAAM,QAAQ,KAAK,kBAAkB,QAAQ,KAAK,OAAO,KAAK,QAAQ,aAAa,OAAO;AAC1F,UAAI,UAAU,UAAa,CAAC,KAAK,QAAQ,SAAS,cAAc,GAAG;AACjE,gBAAQ,OAAO,gBAAgB,KAAK,GAAG,QAAQ,IAAI;MACrD;IACF;AACA,WAAO,MAAM,aAAa,SAAS,KAAK;EAC1C;EAEmB,MAAM,YAAY,MAAyB;AAC5D,QAAI,OAAO,KAAK,SAAS,WAAW,UAAU;AAC5C,aAAO,aAAa,CAAC,EAAE,WAAW,KAAK,OAAM,CAAE,CAAC;IAClD;AACA,WAAO,MAAM,YAAY,IAAI;EAC/B;;AAGF,IAAM,yBAAyB,oBAAI,IAAI;EACrC;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;CACD;;;ACzJD,SAAS,8BAA8B,GAAoB;AAEzD,MADiB,OAAO,CAAA,EACX,SAAS,yBAAA,EACpB,QAAO;AAET,MACE,aAAa,KACb,OAAO,EAAE,YAAY,aACpB,EAAE,QAAQ,SAAS,0CAAA,KAClB,EAAE,QAAQ,SAAS,4BAAA,GAErB,QAAO;AAET,SAAO;;AAGT,SAAgB,sBAAsB,GAAY;AAChD,MAAI,CAAC,KAAK,OAAO,MAAM,SACrB,QAAO;AAGT,MAAI;AACJ,MACE,EAAE,YAAY,SAAS,0BAA0B,QACjD,aAAa,KACb,OAAO,EAAE,YAAY,UACrB;AACA,YAAQ,IAAI,MAAM,EAAE,OAAA;AACpB,UAAM,OAAO;aAEb,EAAE,YAAY,SAAS,kBAAkB,QACzC,aAAa,KACb,OAAO,EAAE,YAAY,UACrB;AACA,YAAQ,IAAI,MAAM,EAAE,OAAA;AACpB,UAAM,OAAO;aACJ,8BAA8B,CAAA,EACvC,SAAQ,qBAAqB,UAAU,CAAA;WAEvC,YAAY,KACZ,EAAE,WAAW,OACb,aAAa,KACb,OAAO,EAAE,YAAY,YACrB,EAAE,QAAQ,SAAS,YAAA,EAEnB,SAAQQ,yBAAwB,GAAG,sBAAA;WAC1B,YAAY,KAAK,EAAE,WAAW,IACvC,SAAQA,yBAAwB,GAAG,sBAAA;WAC1B,YAAY,KAAK,EAAE,WAAW,IACvC,SAAQA,yBAAwB,GAAG,kBAAA;WAC1B,YAAY,KAAK,EAAE,WAAW,IACvC,SAAQA,yBAAwB,GAAG,iBAAA;MAEnC,SAAQ;AAEV,SAAO;;;;ACnDT,IAAaC,SAAAA,CAAW,OAAgB,GAAA;AAExC,SAAgB,iBAAiB,OAAgB;AAC/C,MAAI,CAAC,MAAO,QAAO;AACnB,MAAI,OAAO,KAAK,SAAS,EAAA,EAAK,QAAO;AACrC,MAAI,MAAM,WAAW,OAAA,KAAY,CAAC,MAAM,WAAW,YAAA,EAAe,QAAO;AACzE,SAAO;;AAGT,SAAgB,gCAAgC,SAAsB;AACpE,MACE,QAAQ,SAAS,YACjB,QAAQ,SAAS,eACjB,QAAQ,SAAS,eACjB,QAAQ,SAAS,UACjB,QAAQ,SAAS,cACjB,QAAQ,SAAS,OAEjB,SAAQ,KAAK,yBAAyB,QAAQ,IAAA,EAAA;AAGhD,SAAO,QAAQ;;AAGjB,SAAgB,wBACd,OAIoB;AACpB,SAAQ,MAAM,UAAU,YACtB,MAAM,UAAU,QAChB,MAAM,UAAU;;AAGpB,SAAgB,gCACd,OAIQ;AACR,QAAM,WAAY,MAAM,UAAU,YAChC,MAAM,UAAU,QAChB,MAAM,UAAU;AAElB,MAAI,CAAC,SACH,OAAM,IAAI,MACR,oGAAA;AAIJ,SAAO;;AAET,SAAgB,oBACd,SACiC;AACjC,QAAM,OAAO,QAAQ,SAAA;AACrB,UAAQ,MAAR;IACE,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;AACH,UAAI,CAAC,YAAY,WAAW,OAAA,EAC1B,OAAM,IAAI,MAAM,8BAAA;AAClB,aAAO,gCAAgC,OAAA;IAEzC;AACE,YAAM,IAAI,MAAM,yBAAyB,IAAA,EAAA;;;AAI/C,SAAgB,0BAA0B,OAAwB;AAChE,MAAI,MAAM,SAAS,aAAA,EAAgB,QAAO;AAE1C,MAAI,MAAM,SAAS,OAAA,EAAU,QAAO;AACpC,SAAO;;;;ACrDT,SAAgB,YAAYC,SAA8B;AACxD,QAAM,EACJ,8BACA,4BACA,mBACA,qBACA,SACA,sBACA,oBAAA,IACEA;AAEJ,OACG,qBAAqB,yBACtB,uBACA,6BAEA,QAAO,GAAG,mBAAA,IAAuB,4BAAA;AAEnC,OACG,qBAAqB,yBACtB,uBACA,6BAEA,QAAO,GAAG,mBAAA,uBAA0C,4BAAA;AAGtD,MAAI,qBAAqB,sBAAsB;AAC7C,QAAI,CAAC,2BACH,OAAM,IAAI,MACR,qEAAA;AAGJ,QAAI,CAAC,6BACH,OAAM,IAAI,MACR,mFAAA;AAGJ,WAAO,WAAW,0BAAA,wCAAkE,4BAAA;;AAGtF,SAAO;;AAaT,SAAgB,UAAU,SAAsC;AAC9D,SACE,OAAO,YAAY,eACnB,YAAY,QACZ,OAAO,YAAY,YACnB,OAAO,UAAU,SAAS,KAAK,OAAA,MAAa;;AAiChD,SAAgB,iBACd,SACsD;AACtD,QAAM,SAAS,OAAA,MAAW;AAExB,QAAI,UAAU,OAAA,EACZ,QAAO;aAGA,MAAM,QAAQ,OAAA,EACrB,QAAO,IAAI,QAAQ,OAAA;aAInB,OAAO,YAAY,YACnB,YAAY,QACZ,YAAY,WACZ,UAAU,QAAQ,MAAA,EAElB,QAAO,QAAQ;aAGR,OAAO,YAAY,YAAY,YAAY,MAAM;AACxD,YAAM,UAA8B,OAAO,QAAQ,OAAA,EAChD,OAAA,CAAQ,CAAA,EAAG,CAAA,MAAO,OAAO,MAAM,QAAA,EAC/B,IAAA,CAAK,CAAC,GAAG,CAAA,MAAO,CAAC,GAAG,CAAA,CAAY;AACnC,aAAO,IAAI,QAAQ,OAAA;;AAErB,WAAO,IAAI,QAAA;;AAGb,SAAO,OAAO,YAAY,OAAO,QAAA,CAAS;;AAG5C,SAAgB,kBAAkB;AAChC,MAAI,MAAM,OAAA;AACV,MAAI,QAAQ,UAAU,QAAQ,OAC5B,OAAM,IAAI,GAAA,IAAO,QAAQ,OAAA,KAAY,QAAQ,QAAA,KAAa,QAAQ,IAAA;AAEpE,SAAO;;AAKT,SAAgB,wBACd,SACA,UAAU,OACV,UAAU,SACc;AACxB,QAAM,oBAAoB,iBAAiB,OAAA;AAC3C,QAAM,MAAM,gBAAA;AACZ,QAAM,UAAU,cAAc,UAAU,WAAW,EAAA;AACnD,SAAO;IACL,GAAG;IACH,cAAc,kBAAkB,YAAA,IAC5B,GAAG,OAAA,IAAW,OAAA,KAAY,GAAA,IAAO,kBAAkB,YAAA,CAAA,KACnD,GAAG,OAAA,IAAW,OAAA,KAAY,GAAA;;;;;ACqMlC,SAAgB,iBACdC,OACiC;AACjC,SACEA,UAAS,UACT,MAAM,QAASA,MAAiC,YAAA;;AAUpD,SAAgB,mBAAmBA,OAA0C;AAC3E,SACEA,UAAS,UACT,SAAS,WAAWA,KAAA,KACpB,aAAaA,MAAK,eAClB,OAAOA,MAAK,YAAY,YAAY,cACpCA,MAAK,YAAY,QAAA,MAAc;;AAUnC,SAAgB,uBACdA,OAC8B;AAC9B,SACE,CAAC,CAACA,SACF,OAAOA,UAAS,YAChB,UAAUA,SACV,YAAYA,UAEX,mBAAmBA,MAAK,MAAA,KACtBA,MAAK,UAAU,QACd,OAAOA,MAAK,WAAW,YACvB,UAAUA,MAAK,UACf,OAAOA,MAAK,OAAO,SAAS,YAC5B;IAAC;IAAQ;IAAW;IAAU;IAAS;IAAU;IAAU,SACzDA,MAAK,OAAO,IAAA;;AAatB,SAAgB,gBAAgBA,OAA8C;AAC5E,SACE,uBAAuBA,KAAA,KACvB,mBAAmBA,KAAA,KAEnB,iBAAiBA,KAAA;;;;;;;;;;;;AClarB,SAAgB,wBACdC,OACA,QASoB;AAEpB,QAAM,aAAa,OAAO,WAAW,WAAW,SAAY;AAE5D,SAAO;IACL,MAAMA,MAAK;IACX,aAAaA,MAAK;IAClB,YAAY,aAAaA,MAAK,MAAA;IAE9B,GAAI,YAAY,WAAW,SAAY,EAAE,QAAQ,WAAW,OAAA,IAAW,CAAA;;;AAa3E,SAAgB,oBAEdA,OACA,QASgB;AAEhB,QAAM,aAAa,OAAO,WAAW,WAAW,SAAY;AAE5D,MAAI;AACJ,MAAI,gBAAgBA,KAAA,EAClB,WAAU;IACR,MAAM;IACN,UAAU,wBAAwBA,KAAA;;MAGpC,WAAUA;AAGZ,MAAI,YAAY,WAAW,OAExB,SAAQ,SAAiB,SAAS,WAAW;AAGhD,SAAO;;;;ACvFT,IAAI,gBAAgC,YAAY;AAAA,EAC/C,wBAAwB,MAAM;AAAA,EAC9B,4BAA4B,MAAM;AAAA,EAClC,0BAA0B,MAAM;AAAA,EAChC,sBAAsB,MAAM;AAAA,EAC5B,cAAc,MAAM;AAAA,EACpB,mBAAmB,MAAM;AAAA,EACzB,kBAAkB,MAAM;AAAA,EACxB,uBAAuB,MAAM;AAAA,EAC7B,oCAAoC,MAAM;AAAA,EAC1C,yBAAyB,MAAM;AAAA,EAC/B,6BAA6B,MAAM;AAAA,EACnC,wBAAwB,MAAM;AAAA,EAC9B,gCAAgC,MAAM;AAAA,EACtC,mBAAmB,MAAM;AAAA,EACzB,qBAAqB,MAAM;AAAA,EAC3B,oBAAoB,MAAM;AAAA,EAC1B,oBAAoB,MAAM;AAAA,EAC1B,sBAAsB,MAAM;AAAA,EAC5B,yBAAyB,MAAM;AAAA,EAC/B,cAAc,MAAM;AAAA,EACpB,gBAAgB,MAAM;AAAA,EACtB,gBAAgB,MAAM;AAAA,EACtB,iBAAiB,MAAM;AAAA,EACvB,eAAe,MAAM;AAAA,EACrB,eAAe,MAAM;AAAA,EACrB,iBAAiB,MAAM;AAAA,EACvB,aAAa,MAAM;AAAA,EACnB,eAAe,MAAM;AAAA,EACrB,eAAe,MAAM;AACtB,CAAC;;;ACVD,SAAgB,qBACdC,OACA,QAOiC;AACjC,MAAI;AAEJ,MAAI,gBAAgBA,KAAA,EAClB,WAAUC,oBAAmBD,KAAA;MAE7B,WAAUA;AAGZ,MAAI,QAAQ,WAAW,OACrB,SAAQ,SAAS,SAAS,OAAO;AAGnC,SAAO;;AAiDT,SAAS,YAAY,MAA+B;AAClD,SACG,KAAmB,UAAU,UAC9B,MAAM,QAAS,KAAmB,KAAA;;AAMtC,SAAgB,0BAA0B,WAA0B;AAClE,QAAM,QAAQ,CAAC,yBAAyB,EAAA;AACxC,aAAW,KAAK,WAAW;AACzB,QAAI,EAAE,YACJ,OAAM,KAAK,MAAM,EAAE,WAAA,EAAA;AAErB,QAAI,OAAO,KAAK,EAAE,WAAW,cAAc,CAAA,CAAE,EAAE,SAAS,GAAG;AACzD,YAAM,KAAK,QAAQ,EAAE,IAAA,UAAK;AAC1B,YAAM,KAAK,uBAAuB,EAAE,YAAY,CAAA,CAAE;AAClD,YAAM,KAAK,YAAA;UAEX,OAAM,KAAK,QAAQ,EAAE,IAAA,eAAK;AAE5B,UAAM,KAAK,EAAA;;AAEb,QAAM,KAAK,0BAAA;AACX,SAAO,MAAM,KAAK,IAAA;;AAIpB,SAAS,uBAAuB,KAAiB,QAAwB;AACvE,QAAM,QAAkB,CAAA;AACxB,aAAW,CAAC,MAAM,KAAA,KAAU,OAAO,QAAQ,IAAI,cAAc,CAAA,CAAE,GAAG;AAChE,QAAI,MAAM,eAAe,SAAS,EAChC,OAAM,KAAK,MAAM,MAAM,WAAA,EAAA;AAEzB,QAAI,IAAI,UAAU,SAAS,IAAA,EACzB,OAAM,KAAK,GAAG,IAAA,KAAS,WAAW,OAAO,MAAA,CAAO,GAAC;QAEjD,OAAM,KAAK,GAAG,IAAA,MAAU,WAAW,OAAO,MAAA,CAAO,GAAC;;AAGtD,SAAO,MAAM,IAAA,CAAK,SAAS,IAAI,OAAO,MAAA,IAAU,IAAA,EAAM,KAAK,IAAA;;AAI7D,SAAS,WAAW,OAAa,QAAwB;AACvD,MAAI,YAAY,KAAA,EACd,QAAO,MAAM,MAAM,IAAA,CAAK,MAAM,WAAW,GAAG,MAAA,CAAO,EAAE,KAAK,KAAA;AAE5D,UAAQ,MAAM,MAAd;IACE,KAAK;AACH,UAAI,MAAM,KACR,QAAO,MAAM,KAAK,IAAA,CAAK,MAAM,IAAI,CAAA,GAAE,EAAI,KAAK,KAAA;AAE9C,aAAO;IACT,KAAK;AACH,UAAI,MAAM,KACR,QAAO,MAAM,KAAK,IAAA,CAAK,MAAM,GAAG,CAAA,EAAA,EAAK,KAAK,KAAA;AAE5C,aAAO;IACT,KAAK;AACH,UAAI,MAAM,KACR,QAAO,MAAM,KAAK,IAAA,CAAK,MAAM,GAAG,CAAA,EAAA,EAAK,KAAK,KAAA;AAE5C,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;QAAC;QAAK,uBAAuB,OAAO,SAAS,CAAA;QAAI;QAAK,KAAK,IAAA;IACpE,KAAK;AACH,UAAI,MAAM,MACR,QAAO,GAAG,WAAW,MAAM,OAAO,MAAA,CAAO;AAE3C,aAAO;IACT;AACE,aAAO;;;AAqCb,SAAgB,yBACd,YACyD;AACzD,MAAI,CAAC,WACH;WACS,eAAe,SAAS,eAAe,WAChD,QAAO;WACE,eAAe,OACxB,QAAO;WACE,eAAe,OACxB,QAAO;WACE,OAAO,eAAe,SAC/B,QAAO;IACL,MAAM;IACN,UAAU,EACR,MAAM,WAAA;;MAIV,QAAO;;AAIX,SAAgB,cAAcA,OAAiD;AAC7E,SAAO,UAAUA,SAAQA,MAAK,SAAS;;AAkBzC,SAAgB,0BACdA,OAC6C;AAC7C,SACE,OAAOA,UAAS,YAChBA,UAAS,QACT,YAAYA,SACZ,OAAQA,MAA6C,WAAW,YAC/DA,MAA6C,WAAW,QACzD,4BACGA,MAA6C,UAChD,OAAQA,MAA6C,OAClD,2BAA2B,YAC7BA,MAA6C,OAC3C,2BAA2B;;AAIlC,SAAgB,oBACd,aACoC;AACpC,SACE,eAAe,QACf,OAAO,gBAAgB,YACvB,UAAU,eACV,YAAY,SAAS;;AAezB,SAAgB,aAAaA,OAA4C;AACvE,SACE,OAAOA,UAAS,YAChBA,UAAS,QACT,cAAcA,SACd,OAAOA,MAAK,aAAa,YACzBA,MAAK,aAAa,QAClB,gBAAgBA,MAAK,YACrB,OAAOA,MAAK,SAAS,eAAe,YACpCA,MAAK,SAAS,eAAe;;AAIjC,SAAgB,mBACdA,OACoD;AACpD,SACE,UAAUA,SACVA,MAAK,SAAS,YACd,YAAYA,SACZ,OAAOA,MAAK,WAAW,YACvBA,MAAK,WAAW;;AAIpB,SAAgB,oBAEd,aAC4B;AAC5B,MAAI,YAAY,SAAS,mBACvB;AAEF,SAAO;IACL,GAAG;IACH,MAAM;IACN,SAAS,YAAY;IACrB,IAAI,YAAY;IAChB,MAAM,YAAY;IAClB,cAAc;IACd,MAAM,EACJ,OAAO,YAAY,MAAA;;;AAoBzB,SAAgB,kBAEd,aAC8B;AAC9B,MAAI,YAAY,SAAS,gBACvB;AAEF,SAAO;IACL,GAAG;IACH,MAAM;IACN,SAAS,YAAY;IACrB,IAAI,YAAY;IAChB,MAAM;IACN,gBAAgB;IAChB,MAAM,EACJ,QAAQ,YAAY,OAAA;;;AAU1B,SAAgB,mBACd,UAC8B;AAC9B,SACE,OAAO,aAAa,YACpB,aAAa,QACb,UAAU,YACV,SAAS,SAAS,eAClB,oBAAoB,YACpB,SAAS,mBAAmB;;AAIhC,SAAgB,iBACd,UAC4B;AAC5B,SACE,OAAO,aAAa,YACpB,aAAa,QACb,UAAU,YACV,SAAS,SAAS,eAClB,kBAAkB,YAClB,SAAS,iBAAiB;;AAI9B,SAAgB,6BACdA,OACmC;AACnC,QAAM,YAAA,MAAkB;AACtB,QAAI,CAACA,MAAK,OAAO,OACf;AAEF,QAAIA,MAAK,OAAO,OAAO,SAAS,UAC9B,QAAO;MACL,MAAM;MACN,YAAYA,MAAK,OAAO,OAAO,QAAQ;MACvC,QAAQA,MAAK,OAAO,OAAO,QAAQ;;AAGvC,QAAIA,MAAK,OAAO,OAAO,SAAS,OAC9B,QAAO,EACL,MAAM,OAAA;;AAKZ,SAAO;IACL,MAAM;IACN,MAAMA,MAAK,OAAO;IAClB,aAAaA,MAAK,OAAO;IACzB,QAAQ,UAAA;;;AAIZ,SAAgB,2BACdA,OAC4C;AAC5C,QAAM,YAAA,MAAkB;AACtB,QAAI,CAACA,MAAK,OACR;AAEF,QAAIA,MAAK,OAAO,SAAS,UACvB,QAAO;MACL,MAAM;MACN,SAAS;QACP,YAAYA,MAAK,OAAO;QACxB,QAAQA,MAAK,OAAO;;;AAI1B,QAAIA,MAAK,OAAO,SAAS,OACvB,QAAO,EACL,MAAM,OAAA;;AAKZ,SAAO;IACL,MAAM;IACN,QAAQ;MACN,MAAMA,MAAK;MACX,aAAaA,MAAK;MAClB,QAAQ,UAAA;;;;;;AC3cd,IAAAE,oBAAA;AAAA,SAAAA,mBAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+BAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA;AAAA;AAAA,kBAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA,gBAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,eAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA;AAAA;AAAA;;;ACAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA,gBAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAAAC;AAAA,EAAA;AAAA;AAAA;;;ACAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEO,IAAM,iBAAoC,aAAa,kBAAkB,CAAC,MAAM,QAAQ;AAC3F,EAAK,gBAAgB,KAAK,MAAM,GAAG;AACnC,EAAQ,gBAAgB,KAAK,MAAM,GAAG;AAC1C,CAAC;AACM,SAAS,SAAS,QAAQ;AAC7B,SAAY,aAAa,gBAAgB,MAAM;AACnD;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,EAAQ,gBAAgB,KAAK,MAAM,GAAG;AAC1C,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,SAAS,YAAY,MAAM;AAC3C;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,EAAQ,gBAAgB,KAAK,MAAM,GAAG;AAC1C,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,SAAS,YAAY,MAAM;AAC3C;AACO,IAAM,iBAAoC,aAAa,kBAAkB,CAAC,MAAM,QAAQ;AAC3F,EAAK,gBAAgB,KAAK,MAAM,GAAG;AACnC,EAAQ,gBAAgB,KAAK,MAAM,GAAG;AAC1C,CAAC;AACM,SAAS,SAAS,QAAQ;AAC7B,SAAY,aAAa,gBAAgB,MAAM;AACnD;;;AC1BA,IAAM,cAAc,CAAC,MAAM,WAAW;AAClC,YAAU,KAAK,MAAM,MAAM;AAC3B,OAAK,OAAO;AACZ,SAAO,iBAAiB,MAAM;AAAA,IAC1B,QAAQ;AAAA,MACJ,OAAO,CAAC,WAAgB,YAAY,MAAM,MAAM;AAAA;AAAA,IAEpD;AAAA,IACA,SAAS;AAAA,MACL,OAAO,CAAC,WAAgB,aAAa,MAAM,MAAM;AAAA;AAAA,IAErD;AAAA,IACA,UAAU;AAAA,MACN,OAAO,CAAC,UAAU;AACd,aAAK,OAAO,KAAK,KAAK;AACtB,aAAK,UAAU,KAAK,UAAU,KAAK,QAAa,uBAAuB,CAAC;AAAA,MAC5E;AAAA;AAAA,IAEJ;AAAA,IACA,WAAW;AAAA,MACP,OAAO,CAACC,YAAW;AACf,aAAK,OAAO,KAAK,GAAGA,OAAM;AAC1B,aAAK,UAAU,KAAK,UAAU,KAAK,QAAa,uBAAuB,CAAC;AAAA,MAC5E;AAAA;AAAA,IAEJ;AAAA,IACA,SAAS;AAAA,MACL,MAAM;AACF,eAAO,KAAK,OAAO,WAAW;AAAA,MAClC;AAAA;AAAA,IAEJ;AAAA,EACJ,CAAC;AAML;AACO,IAAM,WAAgB,aAAa,YAAY,WAAW;AAC1D,IAAM,eAAoB,aAAa,YAAY,aAAa;AAAA,EACnE,QAAQ;AACZ,CAAC;;;AC3CM,IAAMC,SAA6B,OAAO,YAAY;AACtD,IAAM,aAAkC,YAAY,YAAY;AAChE,IAAM,YAAiC,WAAW,YAAY;AAC9D,IAAM,iBAAsC,gBAAgB,YAAY;AAExE,IAAMC,UAA8B,QAAQ,YAAY;AACxD,IAAM,SAA8B,QAAQ,YAAY;AACxD,IAAM,cAAmC,aAAa,YAAY;AAClE,IAAM,cAAmC,aAAa,YAAY;AAClE,IAAM,aAAkC,YAAY,YAAY;AAChE,IAAM,aAAkC,YAAY,YAAY;AAChE,IAAM,kBAAuC,iBAAiB,YAAY;AAC1E,IAAM,kBAAuC,iBAAiB,YAAY;;;AJP1E,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,SAAO,OAAO,KAAK,WAAW,GAAG;AAAA,IAC7B,YAAY;AAAA,MACR,OAAO,+BAA+B,MAAM,OAAO;AAAA,MACnD,QAAQ,+BAA+B,MAAM,QAAQ;AAAA,IACzD;AAAA,EACJ,CAAC;AACD,OAAK,eAAe,yBAAyB,MAAM,CAAC,CAAC;AACrD,OAAK,MAAM;AACX,OAAK,OAAO,IAAI;AAChB,SAAO,eAAe,MAAM,QAAQ,EAAE,OAAO,IAAI,CAAC;AAElD,OAAK,QAAQ,IAAI,WAAW;AACxB,WAAO,KAAK,MAAM,aAAK,UAAU,KAAK;AAAA,MAClC,QAAQ;AAAA,QACJ,GAAI,IAAI,UAAU,CAAC;AAAA,QACnB,GAAG,OAAO,IAAI,CAAC,OAAO,OAAO,OAAO,aAAa,EAAE,MAAM,EAAE,OAAO,IAAI,KAAK,EAAE,OAAO,SAAS,GAAG,UAAU,CAAC,EAAE,EAAE,IAAI,EAAE;AAAA,MACzH;AAAA,IACJ,CAAC,GAAG;AAAA,MACA,QAAQ;AAAA,IACZ,CAAC;AAAA,EACL;AACA,OAAK,OAAO,KAAK;AACjB,OAAK,QAAQ,CAACC,MAAK,WAAgB,MAAM,MAAMA,MAAK,MAAM;AAC1D,OAAK,QAAQ,MAAM;AACnB,OAAK,YAAY,CAAC,KAAKC,UAAS;AAC5B,QAAI,IAAI,MAAMA,KAAI;AAClB,WAAO;AAAA,EACX;AAEA,OAAK,QAAQ,CAAC,MAAM,WAAiBC,OAAM,MAAM,MAAM,QAAQ,EAAE,QAAQ,KAAK,MAAM,CAAC;AACrF,OAAK,YAAY,CAAC,MAAM,WAAiB,UAAU,MAAM,MAAM,MAAM;AACrE,OAAK,aAAa,OAAO,MAAM,WAAiB,WAAW,MAAM,MAAM,QAAQ,EAAE,QAAQ,KAAK,WAAW,CAAC;AAC1G,OAAK,iBAAiB,OAAO,MAAM,WAAiB,eAAe,MAAM,MAAM,MAAM;AACrF,OAAK,MAAM,KAAK;AAEhB,OAAK,SAAS,CAAC,MAAM,WAAiBC,QAAO,MAAM,MAAM,MAAM;AAC/D,OAAK,SAAS,CAAC,MAAM,WAAiB,OAAO,MAAM,MAAM,MAAM;AAC/D,OAAK,cAAc,OAAO,MAAM,WAAiB,YAAY,MAAM,MAAM,MAAM;AAC/E,OAAK,cAAc,OAAO,MAAM,WAAiB,YAAY,MAAM,MAAM,MAAM;AAC/E,OAAK,aAAa,CAAC,MAAM,WAAiB,WAAW,MAAM,MAAM,MAAM;AACvE,OAAK,aAAa,CAAC,MAAM,WAAiB,WAAW,MAAM,MAAM,MAAM;AACvE,OAAK,kBAAkB,OAAO,MAAM,WAAiB,gBAAgB,MAAM,MAAM,MAAM;AACvF,OAAK,kBAAkB,OAAO,MAAM,WAAiB,gBAAgB,MAAM,MAAM,MAAM;AAEvF,OAAK,SAAS,CAACC,QAAO,WAAW,KAAK,MAAM,OAAOA,QAAO,MAAM,CAAC;AACjE,OAAK,cAAc,CAAC,eAAe,KAAK,MAAM,YAAY,UAAU,CAAC;AACrE,OAAK,YAAY,CAAC,OAAO,KAAK,MAAa,WAAU,EAAE,CAAC;AAExD,OAAK,WAAW,MAAM,SAAS,IAAI;AACnC,OAAK,gBAAgB,MAAM,cAAc,IAAI;AAC7C,OAAK,WAAW,MAAM,SAAS,IAAI;AACnC,OAAK,UAAU,MAAM,SAAS,SAAS,IAAI,CAAC;AAC5C,OAAK,cAAc,CAAC,WAAW,YAAY,MAAM,MAAM;AACvD,OAAK,QAAQ,MAAM,MAAM,IAAI;AAC7B,OAAK,KAAK,CAAC,QAAQ,MAAM,CAAC,MAAM,GAAG,CAAC;AACpC,OAAK,MAAM,CAAC,QAAQ,aAAa,MAAM,GAAG;AAC1C,OAAK,YAAY,CAAC,OAAO,KAAK,MAAM,UAAU,EAAE,CAAC;AACjD,OAAK,UAAU,CAACJ,SAAQ,SAAS,MAAMA,IAAG;AAC1C,OAAK,WAAW,CAACA,SAAQ,SAAS,MAAMA,IAAG;AAE3C,OAAK,QAAQ,CAAC,WAAW,OAAO,MAAM,MAAM;AAC5C,OAAK,OAAO,CAAC,WAAW,KAAK,MAAM,MAAM;AACzC,OAAK,WAAW,MAAM,SAAS,IAAI;AAEnC,OAAK,WAAW,CAAC,gBAAgB;AAC7B,UAAM,KAAK,KAAK,MAAM;AACtB,IAAK,eAAe,IAAI,IAAI,EAAE,YAAY,CAAC;AAC3C,WAAO;AAAA,EACX;AACA,SAAO,eAAe,MAAM,eAAe;AAAA,IACvC,MAAM;AACF,aAAY,eAAe,IAAI,IAAI,GAAG;AAAA,IAC1C;AAAA,IACA,cAAc;AAAA,EAClB,CAAC;AACD,OAAK,OAAO,IAAI,SAAS;AACrB,QAAI,KAAK,WAAW,GAAG;AACnB,aAAY,eAAe,IAAI,IAAI;AAAA,IACvC;AACA,UAAM,KAAK,KAAK,MAAM;AACtB,IAAK,eAAe,IAAI,IAAI,KAAK,CAAC,CAAC;AACnC,WAAO;AAAA,EACX;AAEA,OAAK,aAAa,MAAM,KAAK,UAAU,MAAS,EAAE;AAClD,OAAK,aAAa,MAAM,KAAK,UAAU,IAAI,EAAE;AAC7C,OAAK,QAAQ,CAAC,OAAO,GAAG,IAAI;AAC5B,SAAO;AACX,CAAC;AAEM,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKK,OAAM,WAAsB,gBAAgB,MAAM,KAAKA,OAAM,MAAM;AACvG,QAAM,MAAM,KAAK,KAAK;AACtB,OAAK,SAAS,IAAI,UAAU;AAC5B,OAAK,YAAY,IAAI,WAAW;AAChC,OAAK,YAAY,IAAI,WAAW;AAEhC,OAAK,QAAQ,IAAI,SAAS,KAAK,MAAa,OAAM,GAAG,IAAI,CAAC;AAC1D,OAAK,WAAW,IAAI,SAAS,KAAK,MAAa,UAAS,GAAG,IAAI,CAAC;AAChE,OAAK,aAAa,IAAI,SAAS,KAAK,MAAa,YAAW,GAAG,IAAI,CAAC;AACpE,OAAK,WAAW,IAAI,SAAS,KAAK,MAAa,UAAS,GAAG,IAAI,CAAC;AAChE,OAAK,MAAM,IAAI,SAAS,KAAK,MAAa,WAAU,GAAG,IAAI,CAAC;AAC5D,OAAK,MAAM,IAAI,SAAS,KAAK,MAAa,WAAU,GAAG,IAAI,CAAC;AAC5D,OAAK,SAAS,IAAI,SAAS,KAAK,MAAa,QAAO,GAAG,IAAI,CAAC;AAC5D,OAAK,WAAW,IAAI,SAAS,KAAK,MAAa,WAAU,GAAG,GAAG,IAAI,CAAC;AACpE,OAAK,YAAY,CAAC,WAAW,KAAK,MAAa,WAAU,MAAM,CAAC;AAChE,OAAK,YAAY,CAAC,WAAW,KAAK,MAAa,WAAU,MAAM,CAAC;AAEhE,OAAK,OAAO,MAAM,KAAK,MAAa,MAAK,CAAC;AAC1C,OAAK,YAAY,IAAI,SAAS,KAAK,MAAa,WAAU,GAAG,IAAI,CAAC;AAClE,OAAK,cAAc,MAAM,KAAK,MAAa,aAAY,CAAC;AACxD,OAAK,cAAc,MAAM,KAAK,MAAa,aAAY,CAAC;AACxD,OAAK,UAAU,MAAM,KAAK,MAAa,SAAQ,CAAC;AACpD,CAAC;AACM,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,aAAW,KAAK,MAAM,GAAG;AACzB,OAAK,QAAQ,CAAC,WAAW,KAAK,MAAW,OAAO,UAAU,MAAM,CAAC;AACjE,OAAK,MAAM,CAAC,WAAW,KAAK,MAAW,KAAK,QAAQ,MAAM,CAAC;AAC3D,OAAK,MAAM,CAAC,WAAW,KAAK,MAAW,KAAK,QAAQ,MAAM,CAAC;AAC3D,OAAK,QAAQ,CAAC,WAAW,KAAK,MAAW,OAAO,UAAU,MAAM,CAAC;AACjE,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAC9D,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAC9D,OAAK,SAAS,CAAC,WAAW,KAAK,MAAW,QAAQ,SAAS,MAAM,CAAC;AAClE,OAAK,SAAS,CAAC,WAAW,KAAK,MAAW,QAAQ,SAAS,MAAM,CAAC;AAClE,OAAK,SAAS,CAAC,WAAW,KAAK,MAAW,QAAQ,SAAS,MAAM,CAAC;AAClE,OAAK,SAAS,CAAC,WAAW,KAAK,MAAW,QAAQ,WAAW,MAAM,CAAC;AACpE,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAC9D,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAC9D,OAAK,QAAQ,CAAC,WAAW,KAAK,MAAW,OAAO,UAAU,MAAM,CAAC;AACjE,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAC9D,OAAK,SAAS,CAAC,WAAW,KAAK,MAAW,QAAQ,WAAW,MAAM,CAAC;AACpE,OAAK,YAAY,CAAC,WAAW,KAAK,MAAW,WAAW,cAAc,MAAM,CAAC;AAC7E,OAAK,MAAM,CAAC,WAAW,KAAK,MAAW,KAAK,QAAQ,MAAM,CAAC;AAC3D,OAAK,QAAQ,CAAC,WAAW,KAAK,MAAW,OAAO,UAAU,MAAM,CAAC;AACjE,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAC9D,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAC9D,OAAK,SAAS,CAAC,WAAW,KAAK,MAAW,QAAQ,WAAW,MAAM,CAAC;AACpE,OAAK,SAAS,CAAC,WAAW,KAAK,MAAW,QAAQ,WAAW,MAAM,CAAC;AACpE,OAAK,OAAO,CAAC,WAAW,KAAK,MAAW,MAAM,SAAS,MAAM,CAAC;AAE9D,OAAK,WAAW,CAAC,WAAW,KAAK,MAAU,SAAS,MAAM,CAAC;AAC3D,OAAK,OAAO,CAAC,WAAW,KAAK,MAAU,KAAK,MAAM,CAAC;AACnD,OAAK,OAAO,CAAC,WAAW,KAAK,MAAU,KAAK,MAAM,CAAC;AACnD,OAAK,WAAW,CAAC,WAAW,KAAK,MAAU,SAAS,MAAM,CAAC;AAC/D,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,kBAAqC,aAAa,mBAAmB,CAAC,MAAM,QAAQ;AAC7F,EAAK,iBAAiB,KAAK,MAAM,GAAG;AACpC,aAAW,KAAK,MAAM,GAAG;AAC7B,CAAC;AACM,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAE/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,MAAM,QAAQ;AAC1B,SAAY,OAAO,UAAU,MAAM;AACvC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAE7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAE7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,SAAS,MAAM;AACvC;AAEO,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,SAAS,MAAM;AACvC;AAEO,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,SAAS,MAAM;AACvC;AACO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAE3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,IAAI,QAAQ;AACxB,SAAY,KAAK,QAAQ,MAAM;AACnC;AACO,SAAS,QAAQ,QAAQ;AAC5B,SAAY,KAAK,QAAQ;AAAA,IACrB,UAAU;AAAA,IACV,UAAe,gBAAQ;AAAA,IACvB,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAE/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,MAAM,QAAQ;AAC1B,SAAY,OAAO,UAAU,MAAM;AACvC;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AAEjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAE7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAE/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,MAAM,QAAQ;AAC1B,SAAY,OAAO,UAAU,MAAM;AACvC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAE7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAE3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,IAAI,QAAQ;AACxB,SAAY,KAAK,QAAQ,MAAM;AACnC;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAE/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,MAAM,QAAQ;AAC1B,SAAY,OAAO,UAAU,MAAM;AACvC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAE7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAE3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,IAAI,QAAQ;AACxB,SAAY,KAAK,QAAQ,MAAM;AACnC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAE7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AAEjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,eAAkC,aAAa,gBAAgB,CAAC,MAAM,QAAQ;AAEvF,EAAK,cAAc,KAAK,MAAM,GAAG;AACjC,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,UAAU,QAAQ;AAC9B,SAAY,WAAW,cAAc,MAAM;AAC/C;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAE7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAE3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,IAAI,QAAQ;AACxB,SAAY,KAAK,QAAQ,MAAM;AACnC;AACO,IAAM,wBAA2C,aAAa,yBAAyB,CAAC,MAAM,QAAQ;AAEzG,EAAK,uBAAuB,KAAK,MAAM,GAAG;AAC1C,kBAAgB,KAAK,MAAM,GAAG;AAClC,CAAC;AACM,SAAS,aAAa,QAAQ,WAAW,UAAU,CAAC,GAAG;AAC1D,SAAY,cAAc,uBAAuB,QAAQ,WAAW,OAAO;AAC/E;AACO,SAAS,SAAS,SAAS;AAC9B,SAAY,cAAc,uBAAuB,YAAiB,gBAAQ,UAAU,OAAO;AAC/F;AACO,SAAS,IAAI,SAAS;AACzB,SAAY,cAAc,uBAAuB,OAAY,gBAAQ,KAAK,OAAO;AACrF;AACO,SAAS,KAAK,KAAK,QAAQ;AAC9B,QAAM,MAAM,QAAQ,OAAO;AAC3B,QAAM,SAAS,GAAG,GAAG,IAAI,GAAG;AAC5B,QAAM,QAAa,gBAAQ,MAAM;AACjC,MAAI,CAAC;AACD,UAAM,IAAI,MAAM,6BAA6B,MAAM,EAAE;AACzD,SAAY,cAAc,uBAAuB,QAAQ,OAAO,MAAM;AAC1E;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,gBAAgB,MAAM,KAAKA,OAAM,MAAM;AACvG,OAAK,KAAK,CAAC,OAAO,WAAW,KAAK,MAAa,IAAG,OAAO,MAAM,CAAC;AAChE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,KAAK,CAAC,OAAO,WAAW,KAAK,MAAa,IAAG,OAAO,MAAM,CAAC;AAChE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,WAAW,KAAK,MAAM,IAAI,MAAM,CAAC;AAC7C,OAAK,OAAO,CAAC,WAAW,KAAK,MAAM,IAAI,MAAM,CAAC;AAC9C,OAAK,WAAW,CAAC,WAAW,KAAK,MAAa,IAAG,GAAG,MAAM,CAAC;AAC3D,OAAK,cAAc,CAAC,WAAW,KAAK,MAAa,KAAI,GAAG,MAAM,CAAC;AAC/D,OAAK,WAAW,CAAC,WAAW,KAAK,MAAa,IAAG,GAAG,MAAM,CAAC;AAC3D,OAAK,cAAc,CAAC,WAAW,KAAK,MAAa,KAAI,GAAG,MAAM,CAAC;AAC/D,OAAK,aAAa,CAAC,OAAO,WAAW,KAAK,MAAa,YAAW,OAAO,MAAM,CAAC;AAChF,OAAK,OAAO,CAAC,OAAO,WAAW,KAAK,MAAa,YAAW,OAAO,MAAM,CAAC;AAE1E,OAAK,SAAS,MAAM;AACpB,QAAM,MAAM,KAAK,KAAK;AACtB,OAAK,WACD,KAAK,IAAI,IAAI,WAAW,OAAO,mBAAmB,IAAI,oBAAoB,OAAO,iBAAiB,KAAK;AAC3G,OAAK,WACD,KAAK,IAAI,IAAI,WAAW,OAAO,mBAAmB,IAAI,oBAAoB,OAAO,iBAAiB,KAAK;AAC3G,OAAK,SAAS,IAAI,UAAU,IAAI,SAAS,KAAK,KAAK,OAAO,cAAc,IAAI,cAAc,GAAG;AAC7F,OAAK,WAAW;AAChB,OAAK,SAAS,IAAI,UAAU;AAChC,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,kBAAqC,aAAa,mBAAmB,CAAC,MAAM,QAAQ;AAC7F,EAAK,iBAAiB,KAAK,MAAM,GAAG;AACpC,YAAU,KAAK,MAAM,GAAG;AAC5B,CAAC;AACM,SAAS,IAAI,QAAQ;AACxB,SAAY,KAAK,iBAAiB,MAAM;AAC5C;AACO,SAAS,QAAQ,QAAQ;AAC5B,SAAY,SAAS,iBAAiB,MAAM;AAChD;AACO,SAAS,QAAQ,QAAQ;AAC5B,SAAY,SAAS,iBAAiB,MAAM;AAChD;AACO,SAAS,MAAM,QAAQ;AAC1B,SAAY,OAAO,iBAAiB,MAAM;AAC9C;AACO,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,iBAAiB,MAAM;AAC/C;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,iBAAiB,MAAM,KAAKA,OAAM,MAAM;AAC5G,CAAC;AACM,SAAS,QAAQ,QAAQ;AAC5B,SAAY,SAAS,YAAY,MAAM;AAC3C;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,gBAAgB,MAAM,KAAKA,OAAM,MAAM;AACvG,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,KAAK,CAAC,OAAO,WAAW,KAAK,MAAa,IAAG,OAAO,MAAM,CAAC;AAChE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,KAAK,CAAC,OAAO,WAAW,KAAK,MAAa,IAAG,OAAO,MAAM,CAAC;AAChE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,WAAW,CAAC,WAAW,KAAK,MAAa,IAAG,OAAO,CAAC,GAAG,MAAM,CAAC;AACnE,OAAK,WAAW,CAAC,WAAW,KAAK,MAAa,IAAG,OAAO,CAAC,GAAG,MAAM,CAAC;AACnE,OAAK,cAAc,CAAC,WAAW,KAAK,MAAa,KAAI,OAAO,CAAC,GAAG,MAAM,CAAC;AACvE,OAAK,cAAc,CAAC,WAAW,KAAK,MAAa,KAAI,OAAO,CAAC,GAAG,MAAM,CAAC;AACvE,OAAK,aAAa,CAAC,OAAO,WAAW,KAAK,MAAa,YAAW,OAAO,MAAM,CAAC;AAChF,QAAM,MAAM,KAAK,KAAK;AACtB,OAAK,WAAW,IAAI,WAAW;AAC/B,OAAK,WAAW,IAAI,WAAW;AAC/B,OAAK,SAAS,IAAI,UAAU;AAChC,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,kBAAqC,aAAa,mBAAmB,CAAC,MAAM,QAAQ;AAC7F,EAAK,iBAAiB,KAAK,MAAM,GAAG;AACpC,YAAU,KAAK,MAAM,GAAG;AAC5B,CAAC;AAEM,SAAS,MAAM,QAAQ;AAC1B,SAAY,OAAO,iBAAiB,MAAM;AAC9C;AAEO,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,iBAAiB,MAAM;AAC/C;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,gBAAgB,MAAM,KAAKA,OAAM,MAAM;AAC3G,CAAC;AACM,SAAS,OAAO,QAAQ;AAC3B,SAAY,QAAQ,WAAW,MAAM;AACzC;AACO,IAAM,eAAkC,aAAa,gBAAgB,CAAC,MAAM,QAAQ;AACvF,EAAK,cAAc,KAAK,MAAM,GAAG;AACjC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,mBAAmB,MAAM,KAAKA,OAAM,MAAM;AAC9G,CAAC;AACD,SAASC,YAAW,QAAQ;AACxB,SAAY,WAAW,cAAc,MAAM;AAC/C;AAEO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKC,OAAM,WAAsB,cAAc,MAAM,KAAKA,OAAM,MAAM;AACzG,CAAC;AACD,SAASC,OAAM,QAAQ;AACnB,SAAY,MAAM,SAAS,MAAM;AACrC;AAEO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAC3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKC,OAAM,WAAsB,aAAa,MAAM,KAAKA,OAAM,MAAM;AACxG,CAAC;AACM,SAAS,MAAM;AAClB,SAAY,KAAK,MAAM;AAC3B;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,iBAAiB,MAAM,KAAKA,OAAM,MAAM;AAC5G,CAAC;AACM,SAAS,UAAU;AACtB,SAAY,SAAS,UAAU;AACnC;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAC/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,eAAe,MAAM,KAAKA,OAAM,MAAM;AAC1G,CAAC;AACM,SAAS,MAAM,QAAQ;AAC1B,SAAY,OAAO,UAAU,MAAM;AACvC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,cAAc,MAAM,KAAKA,OAAM,MAAM;AACzG,CAAC;AACD,SAASC,OAAM,QAAQ;AACnB,SAAY,MAAM,SAAS,MAAM;AACrC;AAEO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKC,OAAM,WAAsB,cAAc,MAAM,KAAKA,OAAM,MAAM;AACrG,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,OAAO,WAAW,KAAK,MAAa,KAAI,OAAO,MAAM,CAAC;AAClE,QAAM,IAAI,KAAK,KAAK;AACpB,OAAK,UAAU,EAAE,UAAU,IAAI,KAAK,EAAE,OAAO,IAAI;AACjD,OAAK,UAAU,EAAE,UAAU,IAAI,KAAK,EAAE,OAAO,IAAI;AACrD,CAAC;AACM,SAASC,MAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAC/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKD,OAAM,WAAsB,eAAe,MAAM,KAAKA,OAAM,MAAM;AACtG,OAAK,UAAU,IAAI;AACnB,OAAK,MAAM,CAAC,WAAW,WAAW,KAAK,MAAa,WAAU,WAAW,MAAM,CAAC;AAChF,OAAK,WAAW,CAAC,WAAW,KAAK,MAAa,WAAU,GAAG,MAAM,CAAC;AAClE,OAAK,MAAM,CAAC,WAAW,WAAW,KAAK,MAAa,WAAU,WAAW,MAAM,CAAC;AAChF,OAAK,SAAS,CAAC,KAAK,WAAW,KAAK,MAAa,QAAO,KAAK,MAAM,CAAC;AACpE,OAAK,SAAS,MAAM,KAAK;AAC7B,CAAC;AACM,SAAS,MAAM,SAAS,QAAQ;AACnC,SAAY,OAAO,UAAU,SAAS,MAAM;AAChD;AAEO,SAAS,MAAM,QAAQ;AAC1B,QAAM,QAAQ,OAAO,KAAK,IAAI;AAC9B,SAAO,MAAM,OAAO,KAAK,KAAK,CAAC;AACnC;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,cAAc,KAAK,MAAM,GAAG;AACjC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,gBAAgB,MAAM,KAAKA,OAAM,MAAM;AACvG,eAAK,WAAW,MAAM,SAAS,MAAM;AACjC,WAAO,IAAI;AAAA,EACf,CAAC;AACD,OAAK,QAAQ,MAAM,MAAM,OAAO,KAAK,KAAK,KAAK,IAAI,KAAK,CAAC;AACzD,OAAK,WAAW,CAAC,aAAa,KAAK,MAAM,EAAE,GAAG,KAAK,KAAK,KAAK,SAAmB,CAAC;AACjF,OAAK,cAAc,MAAM,KAAK,MAAM,EAAE,GAAG,KAAK,KAAK,KAAK,UAAU,QAAQ,EAAE,CAAC;AAC7E,OAAK,QAAQ,MAAM,KAAK,MAAM,EAAE,GAAG,KAAK,KAAK,KAAK,UAAU,QAAQ,EAAE,CAAC;AACvE,OAAK,SAAS,MAAM,KAAK,MAAM,EAAE,GAAG,KAAK,KAAK,KAAK,UAAU,MAAM,EAAE,CAAC;AACtE,OAAK,QAAQ,MAAM,KAAK,MAAM,EAAE,GAAG,KAAK,KAAK,KAAK,UAAU,OAAU,CAAC;AACvE,OAAK,SAAS,CAAC,aAAa;AACxB,WAAO,aAAK,OAAO,MAAM,QAAQ;AAAA,EACrC;AACA,OAAK,aAAa,CAAC,aAAa;AAC5B,WAAO,aAAK,WAAW,MAAM,QAAQ;AAAA,EACzC;AACA,OAAK,QAAQ,CAAC,UAAU,aAAK,MAAM,MAAM,KAAK;AAC9C,OAAK,OAAO,CAAC,SAAS,aAAK,KAAK,MAAM,IAAI;AAC1C,OAAK,OAAO,CAAC,SAAS,aAAK,KAAK,MAAM,IAAI;AAC1C,OAAK,UAAU,IAAI,SAAS,aAAK,QAAQ,aAAa,MAAM,KAAK,CAAC,CAAC;AACnE,OAAK,WAAW,IAAI,SAAS,aAAK,SAAS,gBAAgB,MAAM,KAAK,CAAC,CAAC;AAC5E,CAAC;AACM,SAAS,OAAO,OAAO,QAAQ;AAClC,QAAM,MAAM;AAAA,IACR,MAAM;AAAA,IACN,OAAO,SAAS,CAAC;AAAA,IACjB,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC;AACA,SAAO,IAAI,UAAU,GAAG;AAC5B;AAEO,SAAS,aAAa,OAAO,QAAQ;AACxC,SAAO,IAAI,UAAU;AAAA,IACjB,MAAM;AAAA,IACN;AAAA,IACA,UAAU,MAAM;AAAA,IAChB,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AAEO,SAAS,YAAY,OAAO,QAAQ;AACvC,SAAO,IAAI,UAAU;AAAA,IACjB,MAAM;AAAA,IACN;AAAA,IACA,UAAU,QAAQ;AAAA,IAClB,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAC/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,eAAe,MAAM,KAAKA,OAAM,MAAM;AACtG,OAAK,UAAU,IAAI;AACvB,CAAC;AACM,SAAS,MAAM,SAAS,QAAQ;AACnC,SAAO,IAAI,SAAS;AAAA,IAChB,MAAM;AAAA,IACN;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAC3E,WAAS,KAAK,MAAM,GAAG;AACvB,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,eAAe,MAAM,KAAKA,OAAM,MAAM;AACtG,OAAK,UAAU,IAAI;AACvB,CAAC;AAIM,SAAS,IAAI,SAAS,QAAQ;AACjC,SAAO,IAAI,OAAO;AAAA,IACd,MAAM;AAAA,IACN;AAAA,IACA,WAAW;AAAA,IACX,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,wBAA2C,aAAa,yBAAyB,CAAC,MAAM,QAAQ;AACzG,WAAS,KAAK,MAAM,GAAG;AACvB,EAAK,uBAAuB,KAAK,MAAM,GAAG;AAC9C,CAAC;AACM,SAAS,mBAAmB,eAAe,SAAS,QAAQ;AAE/D,SAAO,IAAI,sBAAsB;AAAA,IAC7B,MAAM;AAAA,IACN;AAAA,IACA;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,kBAAqC,aAAa,mBAAmB,CAAC,MAAM,QAAQ;AAC7F,EAAK,iBAAiB,KAAK,MAAM,GAAG;AACpC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,sBAAsB,MAAM,KAAKA,OAAM,MAAM;AACjH,CAAC;AACM,SAAS,aAAa,MAAM,OAAO;AACtC,SAAO,IAAI,gBAAgB;AAAA,IACvB,MAAM;AAAA,IACN;AAAA,IACA;AAAA,EACJ,CAAC;AACL;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAC/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,eAAe,MAAM,KAAKA,OAAM,MAAM;AACtG,OAAK,OAAO,CAAC,SAAS,KAAK,MAAM;AAAA,IAC7B,GAAG,KAAK,KAAK;AAAA,IACb;AAAA,EACJ,CAAC;AACL,CAAC;AACM,SAAS,MAAM,OAAO,eAAe,SAAS;AACjD,QAAM,UAAU,yBAA8B;AAC9C,QAAM,SAAS,UAAU,UAAU;AACnC,QAAM,OAAO,UAAU,gBAAgB;AACvC,SAAO,IAAI,SAAS;AAAA,IAChB,MAAM;AAAA,IACN;AAAA,IACA;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,gBAAgB,MAAM,KAAKA,OAAM,MAAM;AACvG,OAAK,UAAU,IAAI;AACnB,OAAK,YAAY,IAAI;AACzB,CAAC;AACM,SAAS,OAAO,SAAS,WAAW,QAAQ;AAC/C,SAAO,IAAI,UAAU;AAAA,IACjB,MAAM;AAAA,IACN;AAAA,IACA;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AAEO,SAAS,cAAc,SAAS,WAAW,QAAQ;AACtD,QAAM,IAAS,MAAM,OAAO;AAC5B,IAAE,KAAK,SAAS;AAChB,SAAO,IAAI,UAAU;AAAA,IACjB,MAAM;AAAA,IACN,SAAS;AAAA,IACT;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,SAAS,YAAY,SAAS,WAAW,QAAQ;AACpD,SAAO,IAAI,UAAU;AAAA,IACjB,MAAM;AAAA,IACN;AAAA,IACA;AAAA,IACA,MAAM;AAAA,IACN,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAC3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,aAAa,MAAM,KAAKA,OAAM,MAAM;AACpG,OAAK,UAAU,IAAI;AACnB,OAAK,YAAY,IAAI;AACrB,OAAK,MAAM,IAAI,SAAS,KAAK,MAAW,SAAS,GAAG,IAAI,CAAC;AACzD,OAAK,WAAW,CAAC,WAAW,KAAK,MAAW,SAAS,GAAG,MAAM,CAAC;AAC/D,OAAK,MAAM,IAAI,SAAS,KAAK,MAAW,SAAS,GAAG,IAAI,CAAC;AACzD,OAAK,OAAO,IAAI,SAAS,KAAK,MAAW,MAAM,GAAG,IAAI,CAAC;AAC3D,CAAC;AACM,SAAS,IAAI,SAAS,WAAW,QAAQ;AAC5C,SAAO,IAAI,OAAO;AAAA,IACd,MAAM;AAAA,IACN;AAAA,IACA;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAC3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,aAAa,MAAM,KAAKA,OAAM,MAAM;AACpG,OAAK,MAAM,IAAI,SAAS,KAAK,MAAW,SAAS,GAAG,IAAI,CAAC;AACzD,OAAK,WAAW,CAAC,WAAW,KAAK,MAAW,SAAS,GAAG,MAAM,CAAC;AAC/D,OAAK,MAAM,IAAI,SAAS,KAAK,MAAW,SAAS,GAAG,IAAI,CAAC;AACzD,OAAK,OAAO,IAAI,SAAS,KAAK,MAAW,MAAM,GAAG,IAAI,CAAC;AAC3D,CAAC;AACM,SAAS,IAAI,WAAW,QAAQ;AACnC,SAAO,IAAI,OAAO;AAAA,IACd,MAAM;AAAA,IACN;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,cAAc,MAAM,KAAKA,OAAM,MAAM;AACrG,OAAK,OAAO,IAAI;AAChB,OAAK,UAAU,OAAO,OAAO,IAAI,OAAO;AACxC,QAAM,OAAO,IAAI,IAAI,OAAO,KAAK,IAAI,OAAO,CAAC;AAC7C,OAAK,UAAU,CAAC,QAAQ,WAAW;AAC/B,UAAM,aAAa,CAAC;AACpB,eAAW,SAAS,QAAQ;AACxB,UAAI,KAAK,IAAI,KAAK,GAAG;AACjB,mBAAW,KAAK,IAAI,IAAI,QAAQ,KAAK;AAAA,MACzC;AAEI,cAAM,IAAI,MAAM,OAAO,KAAK,oBAAoB;AAAA,IACxD;AACA,WAAO,IAAI,QAAQ;AAAA,MACf,GAAG;AAAA,MACH,QAAQ,CAAC;AAAA,MACT,GAAG,aAAK,gBAAgB,MAAM;AAAA,MAC9B,SAAS;AAAA,IACb,CAAC;AAAA,EACL;AACA,OAAK,UAAU,CAAC,QAAQ,WAAW;AAC/B,UAAM,aAAa,EAAE,GAAG,IAAI,QAAQ;AACpC,eAAW,SAAS,QAAQ;AACxB,UAAI,KAAK,IAAI,KAAK,GAAG;AACjB,eAAO,WAAW,KAAK;AAAA,MAC3B;AAEI,cAAM,IAAI,MAAM,OAAO,KAAK,oBAAoB;AAAA,IACxD;AACA,WAAO,IAAI,QAAQ;AAAA,MACf,GAAG;AAAA,MACH,QAAQ,CAAC;AAAA,MACT,GAAG,aAAK,gBAAgB,MAAM;AAAA,MAC9B,SAAS;AAAA,IACb,CAAC;AAAA,EACL;AACJ,CAAC;AACD,SAAS,MAAM,QAAQ,QAAQ;AAC3B,QAAM,UAAU,MAAM,QAAQ,MAAM,IAAI,OAAO,YAAY,OAAO,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC,IAAI;AACxF,SAAO,IAAI,QAAQ;AAAA,IACf,MAAM;AAAA,IACN;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AASO,SAAS,WAAW,SAAS,QAAQ;AACxC,SAAO,IAAI,QAAQ;AAAA,IACf,MAAM;AAAA,IACN;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKE,OAAM,WAAsB,iBAAiB,MAAM,KAAKA,OAAM,MAAM;AACxG,OAAK,SAAS,IAAI,IAAI,IAAI,MAAM;AAChC,SAAO,eAAe,MAAM,SAAS;AAAA,IACjC,MAAM;AACF,UAAI,IAAI,OAAO,SAAS,GAAG;AACvB,cAAM,IAAI,MAAM,4EAA4E;AAAA,MAChG;AACA,aAAO,IAAI,OAAO,CAAC;AAAA,IACvB;AAAA,EACJ,CAAC;AACL,CAAC;AACM,SAAS,QAAQ,OAAO,QAAQ;AACnC,SAAO,IAAI,WAAW;AAAA,IAClB,MAAM;AAAA,IACN,QAAQ,MAAM,QAAQ,KAAK,IAAI,QAAQ,CAAC,KAAK;AAAA,IAC7C,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,cAAc,MAAM,KAAKA,OAAM,MAAM;AACrG,OAAK,MAAM,CAAC,MAAM,WAAW,KAAK,MAAW,SAAS,MAAM,MAAM,CAAC;AACnE,OAAK,MAAM,CAAC,MAAM,WAAW,KAAK,MAAW,SAAS,MAAM,MAAM,CAAC;AACnE,OAAK,OAAO,CAAC,OAAO,WAAW,KAAK,MAAW,MAAM,MAAM,QAAQ,KAAK,IAAI,QAAQ,CAAC,KAAK,GAAG,MAAM,CAAC;AACxG,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAY,MAAM,SAAS,MAAM;AACrC;AACO,IAAM,eAAkC,aAAa,gBAAgB,CAAC,MAAM,QAAQ;AACvF,EAAK,cAAc,KAAK,MAAM,GAAG;AACjC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,mBAAmB,MAAM,KAAKA,OAAM,MAAM;AAC1G,OAAK,KAAK,QAAQ,CAAC,SAAS,SAAS;AACjC,QAAI,KAAK,cAAc,YAAY;AAC/B,YAAM,IAAS,gBAAgB,KAAK,YAAY,IAAI;AAAA,IACxD;AACA,YAAQ,WAAW,CAAC,UAAU;AAC1B,UAAI,OAAO,UAAU,UAAU;AAC3B,gBAAQ,OAAO,KAAK,aAAK,MAAM,OAAO,QAAQ,OAAO,GAAG,CAAC;AAAA,MAC7D,OACK;AAED,cAAM,SAAS;AACf,YAAI,OAAO;AACP,iBAAO,WAAW;AACtB,eAAO,SAAS,OAAO,OAAO;AAC9B,eAAO,UAAU,OAAO,QAAQ,QAAQ;AACxC,eAAO,SAAS,OAAO,OAAO;AAE9B,gBAAQ,OAAO,KAAK,aAAK,MAAM,MAAM,CAAC;AAAA,MAC1C;AAAA,IACJ;AACA,UAAM,SAAS,IAAI,UAAU,QAAQ,OAAO,OAAO;AACnD,QAAI,kBAAkB,SAAS;AAC3B,aAAO,OAAO,KAAK,CAACC,YAAW;AAC3B,gBAAQ,QAAQA;AAChB,eAAO;AAAA,MACX,CAAC;AAAA,IACL;AACA,YAAQ,QAAQ;AAChB,WAAO;AAAA,EACX;AACJ,CAAC;AACM,SAAS,UAAU,IAAI;AAC1B,SAAO,IAAI,aAAa;AAAA,IACpB,MAAM;AAAA,IACN,WAAW;AAAA,EACf,CAAC;AACL;AACO,IAAM,cAAiC,aAAa,eAAe,CAAC,MAAM,QAAQ;AACrF,EAAK,aAAa,KAAK,MAAM,GAAG;AAChC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKD,OAAM,WAAsB,kBAAkB,MAAM,KAAKA,OAAM,MAAM;AACzG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,SAAS,WAAW;AAChC,SAAO,IAAI,YAAY;AAAA,IACnB,MAAM;AAAA,IACN;AAAA,EACJ,CAAC;AACL;AACO,IAAM,mBAAsC,aAAa,oBAAoB,CAAC,MAAM,QAAQ;AAC/F,EAAK,kBAAkB,KAAK,MAAM,GAAG;AACrC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,kBAAkB,MAAM,KAAKA,OAAM,MAAM;AACzG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,cAAc,WAAW;AACrC,SAAO,IAAI,iBAAiB;AAAA,IACxB,MAAM;AAAA,IACN;AAAA,EACJ,CAAC;AACL;AACO,IAAM,cAAiC,aAAa,eAAe,CAAC,MAAM,QAAQ;AACrF,EAAK,aAAa,KAAK,MAAM,GAAG;AAChC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,kBAAkB,MAAM,KAAKA,OAAM,MAAM;AACzG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,SAAS,WAAW;AAChC,SAAO,IAAI,YAAY;AAAA,IACnB,MAAM;AAAA,IACN;AAAA,EACJ,CAAC;AACL;AAEO,SAAS,QAAQ,WAAW;AAC/B,SAAO,SAAS,SAAS,SAAS,CAAC;AACvC;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,iBAAiB,MAAM,KAAKA,OAAM,MAAM;AACxG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AAClC,OAAK,gBAAgB,KAAK;AAC9B,CAAC;AACM,SAAS,SAAS,WAAW,cAAc;AAC9C,SAAO,IAAI,WAAW;AAAA,IAClB,MAAM;AAAA,IACN;AAAA,IACA,IAAI,eAAe;AACf,aAAO,OAAO,iBAAiB,aAAa,aAAa,IAAI,aAAK,aAAa,YAAY;AAAA,IAC/F;AAAA,EACJ,CAAC;AACL;AACO,IAAM,cAAiC,aAAa,eAAe,CAAC,MAAM,QAAQ;AACrF,EAAK,aAAa,KAAK,MAAM,GAAG;AAChC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,kBAAkB,MAAM,KAAKA,OAAM,MAAM;AACzG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,SAAS,WAAW,cAAc;AAC9C,SAAO,IAAI,YAAY;AAAA,IACnB,MAAM;AAAA,IACN;AAAA,IACA,IAAI,eAAe;AACf,aAAO,OAAO,iBAAiB,aAAa,aAAa,IAAI,aAAK,aAAa,YAAY;AAAA,IAC/F;AAAA,EACJ,CAAC;AACL;AACO,IAAM,iBAAoC,aAAa,kBAAkB,CAAC,MAAM,QAAQ;AAC3F,EAAK,gBAAgB,KAAK,MAAM,GAAG;AACnC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,qBAAqB,MAAM,KAAKA,OAAM,MAAM;AAC5G,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,YAAY,WAAW,QAAQ;AAC3C,SAAO,IAAI,eAAe;AAAA,IACtB,MAAM;AAAA,IACN;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,iBAAiB,MAAM,KAAKA,OAAM,MAAM;AACxG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,QAAQ,WAAW;AAC/B,SAAO,IAAI,WAAW;AAAA,IAClB,MAAM;AAAA,IACN;AAAA,EACJ,CAAC;AACL;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAC/E,EAAK,UAAU,KAAK,MAAM,GAAG;AAC7B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,eAAe,MAAM,KAAKA,OAAM,MAAM;AACtG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AAClC,OAAK,cAAc,KAAK;AAC5B,CAAC;AACD,SAAS,OAAO,WAAW,YAAY;AACnC,SAAO,IAAI,SAAS;AAAA,IAChB,MAAM;AAAA,IACN;AAAA,IACA,YAAa,OAAO,eAAe,aAAa,aAAa,MAAM;AAAA,EACvE,CAAC;AACL;AAEO,IAAM,SAA4B,aAAa,UAAU,CAAC,MAAM,QAAQ;AAC3E,EAAK,QAAQ,KAAK,MAAM,GAAG;AAC3B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKE,OAAM,WAAsB,aAAa,MAAM,KAAKA,OAAM,MAAM;AACxG,CAAC;AACM,SAAS,IAAI,QAAQ;AACxB,SAAY,KAAK,QAAQ,MAAM;AACnC;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,cAAc,MAAM,KAAKA,OAAM,MAAM;AACrG,OAAK,KAAK,IAAI;AACd,OAAK,MAAM,IAAI;AACnB,CAAC;AACM,SAAS,KAAK,KAAK,KAAK;AAC3B,SAAO,IAAI,QAAQ;AAAA,IACf,MAAM;AAAA,IACN,IAAI;AAAA,IACJ;AAAA;AAAA,EAEJ,CAAC;AACL;AACO,IAAM,WAA8B,aAAa,YAAY,CAAC,MAAM,QAAQ;AAC/E,UAAQ,KAAK,MAAM,GAAG;AACtB,EAAK,UAAU,KAAK,MAAM,GAAG;AACjC,CAAC;AACM,SAAS,MAAM,KAAK,KAAK,QAAQ;AACpC,SAAO,IAAI,SAAS;AAAA,IAChB,MAAM;AAAA,IACN,IAAI;AAAA,IACJ;AAAA,IACA,WAAW,OAAO;AAAA,IAClB,kBAAkB,OAAO;AAAA,EAC7B,CAAC;AACL;AACO,IAAM,cAAiC,aAAa,eAAe,CAAC,MAAM,QAAQ;AACrF,EAAK,aAAa,KAAK,MAAM,GAAG;AAChC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,kBAAkB,MAAM,KAAKA,OAAM,MAAM;AACzG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,SAAS,WAAW;AAChC,SAAO,IAAI,YAAY;AAAA,IACnB,MAAM;AAAA,IACN;AAAA,EACJ,CAAC;AACL;AACO,IAAM,qBAAwC,aAAa,sBAAsB,CAAC,MAAM,QAAQ;AACnG,EAAK,oBAAoB,KAAK,MAAM,GAAG;AACvC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,yBAAyB,MAAM,KAAKA,OAAM,MAAM;AACpH,CAAC;AACM,SAAS,gBAAgB,OAAO,QAAQ;AAC3C,SAAO,IAAI,mBAAmB;AAAA,IAC1B,MAAM;AAAA,IACN;AAAA,IACA,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACL;AACO,IAAM,UAA6B,aAAa,WAAW,CAAC,MAAM,QAAQ;AAC7E,EAAK,SAAS,KAAK,MAAM,GAAG;AAC5B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,cAAc,MAAM,KAAKA,OAAM,MAAM;AACrG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI,OAAO;AAC7C,CAAC;AACM,SAAS,KAAK,QAAQ;AACzB,SAAO,IAAI,QAAQ;AAAA,IACf,MAAM;AAAA,IACN;AAAA,EACJ,CAAC;AACL;AACO,IAAM,aAAgC,aAAa,cAAc,CAAC,MAAM,QAAQ;AACnF,EAAK,YAAY,KAAK,MAAM,GAAG;AAC/B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,iBAAiB,MAAM,KAAKA,OAAM,MAAM;AACxG,OAAK,SAAS,MAAM,KAAK,KAAK,IAAI;AACtC,CAAC;AACM,SAAS,QAAQ,WAAW;AAC/B,SAAO,IAAI,WAAW;AAAA,IAClB,MAAM;AAAA,IACN;AAAA,EACJ,CAAC;AACL;AACO,IAAM,cAAiC,aAAa,eAAe,CAAC,MAAM,QAAQ;AACrF,EAAK,aAAa,KAAK,MAAM,GAAG;AAChC,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKA,OAAM,WAAsB,kBAAkB,MAAM,KAAKA,OAAM,MAAM;AAC7G,CAAC;AACM,SAAS,UAAU,QAAQ;AAC9B,SAAO,IAAI,YAAY;AAAA,IACnB,MAAM;AAAA,IACN,OAAO,MAAM,QAAQ,QAAQ,KAAK,IAAI,MAAM,QAAQ,KAAK,IAAK,QAAQ,SAAS,MAAM,QAAQ,CAAC;AAAA,IAC9F,QAAQ,QAAQ,UAAU,QAAQ;AAAA,EACtC,CAAC;AACL;AAEO,IAAM,YAA+B,aAAa,aAAa,CAAC,MAAM,QAAQ;AACjF,EAAK,WAAW,KAAK,MAAM,GAAG;AAC9B,UAAQ,KAAK,MAAM,GAAG;AACtB,OAAK,KAAK,oBAAoB,CAAC,KAAKC,OAAM,WAAsB,gBAAgB,MAAM,KAAKA,OAAM,MAAM;AAC3G,CAAC;AAEM,SAAS,MAAM,IAAI;AACtB,QAAM,KAAK,IAAS,UAAU;AAAA,IAC1B,OAAO;AAAA;AAAA,EAEX,CAAC;AACD,KAAG,KAAK,QAAQ;AAChB,SAAO;AACX;AACO,SAAS,OAAO,IAAI,SAAS;AAChC,SAAY,QAAQ,WAAW,OAAO,MAAM,OAAO,OAAO;AAC9D;AACO,SAAS,OAAO,IAAI,UAAU,CAAC,GAAG;AACrC,SAAY,QAAQ,WAAW,IAAI,OAAO;AAC9C;AAEO,SAAS,YAAY,IAAI;AAC5B,SAAY,aAAa,EAAE;AAC/B;AAEO,IAAMC,YAAgB;AACtB,IAAMC,QAAY;AACzB,SAAS,YAAY,KAAK,SAAS,CAAC,GAAG;AACnC,QAAM,OAAO,IAAI,UAAU;AAAA,IACvB,MAAM;AAAA,IACN,OAAO;AAAA,IACP,IAAI,CAAC,SAAS,gBAAgB;AAAA,IAC9B,OAAO;AAAA,IACP,GAAG,aAAK,gBAAgB,MAAM;AAAA,EAClC,CAAC;AACD,OAAK,KAAK,IAAI,QAAQ;AAEtB,OAAK,KAAK,QAAQ,CAAC,YAAY;AAC3B,QAAI,EAAE,QAAQ,iBAAiB,MAAM;AACjC,cAAQ,OAAO,KAAK;AAAA,QAChB,MAAM;AAAA,QACN,UAAU,IAAI;AAAA,QACd,OAAO,QAAQ;AAAA,QACf;AAAA,QACA,MAAM,CAAC,GAAI,KAAK,KAAK,IAAI,QAAQ,CAAC,CAAE;AAAA,MACxC,CAAC;AAAA,IACL;AAAA,EACJ;AACA,SAAO;AACX;AAGO,IAAM,aAAa,IAAI,SAAc,YAAY;AAAA,EACpD,OAAO;AAAA,EACP,SAAS;AAAA,EACT,QAAQ;AACZ,GAAG,GAAG,IAAI;AACH,SAAS,KAAK,QAAQ;AACzB,QAAM,aAAa,KAAK,MAAM;AAC1B,WAAO,MAAM,CAAC,OAAO,MAAM,GAAG,OAAO,GAAG,QAAQ,GAAGC,OAAM,GAAG,MAAM,UAAU,GAAG,OAAO,OAAO,GAAG,UAAU,CAAC,CAAC;AAAA,EAChH,CAAC;AACD,SAAO;AACX;AAGO,SAAS,WAAW,IAAI,QAAQ;AACnC,SAAO,KAAK,UAAU,EAAE,GAAG,MAAM;AACrC;;;AKjoCO,IAAM,eAAe;AAAA,EACxB,cAAc;AAAA,EACd,SAAS;AAAA,EACT,WAAW;AAAA,EACX,gBAAgB;AAAA,EAChB,iBAAiB;AAAA,EACjB,mBAAmB;AAAA,EACnB,eAAe;AAAA,EACf,aAAa;AAAA,EACb,iBAAiB;AAAA,EACjB,eAAe;AAAA,EACf,QAAQ;AACZ;AAGO,SAAS,YAAYC,MAAK;AAC7B,EAAK,OAAO;AAAA,IACR,aAAaA;AAAA,EACjB,CAAC;AACL;AAEO,SAAS,cAAc;AAC1B,SAAY,OAAO,EAAE;AACzB;AAEO,IAAIC;AACV,0BAAUA,wBAAuB;AAClC,GAAGA,2BAA0BA,yBAAwB,CAAC,EAAE;;;ACzBxD,IAAM,IAAI;AAAA,EACN,GAAG;AAAA,EACH,GAAG;AAAA,EACH,KAAK;AACT;AAEA,IAAM,kBAAkB,oBAAI,IAAI;AAAA;AAAA,EAE5B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EAEA;AAAA,EACA;AACJ,CAAC;AACD,SAAS,cAAc,QAAQ,eAAe;AAC1C,QAAM,UAAU,OAAO;AACvB,MAAI,YAAY,gDAAgD;AAC5D,WAAO;AAAA,EACX;AACA,MAAI,YAAY,2CAA2C;AACvD,WAAO;AAAA,EACX;AACA,MAAI,YAAY,2CAA2C;AACvD,WAAO;AAAA,EACX;AAEA,SAAO,iBAAiB;AAC5B;AACA,SAAS,WAAW,KAAK,KAAK;AAC1B,MAAI,CAAC,IAAI,WAAW,GAAG,GAAG;AACtB,UAAM,IAAI,MAAM,qEAAqE;AAAA,EACzF;AACA,QAAMC,QAAO,IAAI,MAAM,CAAC,EAAE,MAAM,GAAG,EAAE,OAAO,OAAO;AAEnD,MAAIA,MAAK,WAAW,GAAG;AACnB,WAAO,IAAI;AAAA,EACf;AACA,QAAM,UAAU,IAAI,YAAY,kBAAkB,UAAU;AAC5D,MAAIA,MAAK,CAAC,MAAM,SAAS;AACrB,UAAM,MAAMA,MAAK,CAAC;AAClB,QAAI,CAAC,OAAO,CAAC,IAAI,KAAK,GAAG,GAAG;AACxB,YAAM,IAAI,MAAM,wBAAwB,GAAG,EAAE;AAAA,IACjD;AACA,WAAO,IAAI,KAAK,GAAG;AAAA,EACvB;AACA,QAAM,IAAI,MAAM,wBAAwB,GAAG,EAAE;AACjD;AACA,SAAS,kBAAkB,QAAQ,KAAK;AAEpC,MAAI,OAAO,QAAQ,QAAW;AAE1B,QAAI,OAAO,OAAO,QAAQ,YAAY,OAAO,KAAK,OAAO,GAAG,EAAE,WAAW,GAAG;AACxE,aAAO,EAAE,MAAM;AAAA,IACnB;AACA,UAAM,IAAI,MAAM,4DAA4D;AAAA,EAChF;AACA,MAAI,OAAO,qBAAqB,QAAW;AACvC,UAAM,IAAI,MAAM,mCAAmC;AAAA,EACvD;AACA,MAAI,OAAO,0BAA0B,QAAW;AAC5C,UAAM,IAAI,MAAM,wCAAwC;AAAA,EAC5D;AACA,MAAI,OAAO,OAAO,UAAa,OAAO,SAAS,UAAa,OAAO,SAAS,QAAW;AACnF,UAAM,IAAI,MAAM,sDAAsD;AAAA,EAC1E;AACA,MAAI,OAAO,qBAAqB,UAAa,OAAO,sBAAsB,QAAW;AACjF,UAAM,IAAI,MAAM,0DAA0D;AAAA,EAC9E;AAEA,MAAI,OAAO,MAAM;AACb,UAAM,UAAU,OAAO;AACvB,QAAI,IAAI,KAAK,IAAI,OAAO,GAAG;AACvB,aAAO,IAAI,KAAK,IAAI,OAAO;AAAA,IAC/B;AACA,QAAI,IAAI,WAAW,IAAI,OAAO,GAAG;AAE7B,aAAO,EAAE,KAAK,MAAM;AAChB,YAAI,CAAC,IAAI,KAAK,IAAI,OAAO,GAAG;AACxB,gBAAM,IAAI,MAAM,oCAAoC,OAAO,EAAE;AAAA,QACjE;AACA,eAAO,IAAI,KAAK,IAAI,OAAO;AAAA,MAC/B,CAAC;AAAA,IACL;AACA,QAAI,WAAW,IAAI,OAAO;AAC1B,UAAM,WAAW,WAAW,SAAS,GAAG;AACxC,UAAMC,aAAY,cAAc,UAAU,GAAG;AAC7C,QAAI,KAAK,IAAI,SAASA,UAAS;AAC/B,QAAI,WAAW,OAAO,OAAO;AAC7B,WAAOA;AAAA,EACX;AAEA,MAAI,OAAO,SAAS,QAAW;AAC3B,UAAM,aAAa,OAAO;AAE1B,QAAI,IAAI,YAAY,iBAChB,OAAO,aAAa,QACpB,WAAW,WAAW,KACtB,WAAW,CAAC,MAAM,MAAM;AACxB,aAAO,EAAE,KAAK;AAAA,IAClB;AACA,QAAI,WAAW,WAAW,GAAG;AACzB,aAAO,EAAE,MAAM;AAAA,IACnB;AACA,QAAI,WAAW,WAAW,GAAG;AACzB,aAAO,EAAE,QAAQ,WAAW,CAAC,CAAC;AAAA,IAClC;AAEA,QAAI,WAAW,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,GAAG;AAChD,aAAO,EAAE,KAAK,UAAU;AAAA,IAC5B;AAEA,UAAM,iBAAiB,WAAW,IAAI,CAAC,MAAM,EAAE,QAAQ,CAAC,CAAC;AACzD,QAAI,eAAe,SAAS,GAAG;AAC3B,aAAO,eAAe,CAAC;AAAA,IAC3B;AACA,WAAO,EAAE,MAAM,CAAC,eAAe,CAAC,GAAG,eAAe,CAAC,GAAG,GAAG,eAAe,MAAM,CAAC,CAAC,CAAC;AAAA,EACrF;AAEA,MAAI,OAAO,UAAU,QAAW;AAC5B,WAAO,EAAE,QAAQ,OAAO,KAAK;AAAA,EACjC;AAEA,QAAM,OAAO,OAAO;AACpB,MAAI,MAAM,QAAQ,IAAI,GAAG;AAErB,UAAM,cAAc,KAAK,IAAI,CAAC,MAAM;AAChC,YAAM,aAAa,EAAE,GAAG,QAAQ,MAAM,EAAE;AACxC,aAAO,kBAAkB,YAAY,GAAG;AAAA,IAC5C,CAAC;AACD,QAAI,YAAY,WAAW,GAAG;AAC1B,aAAO,EAAE,MAAM;AAAA,IACnB;AACA,QAAI,YAAY,WAAW,GAAG;AAC1B,aAAO,YAAY,CAAC;AAAA,IACxB;AACA,WAAO,EAAE,MAAM,WAAW;AAAA,EAC9B;AACA,MAAI,CAAC,MAAM;AAEP,WAAO,EAAE,IAAI;AAAA,EACjB;AACA,MAAI;AACJ,UAAQ,MAAM;AAAA,IACV,KAAK,UAAU;AACX,UAAI,eAAe,EAAE,OAAO;AAE5B,UAAI,OAAO,QAAQ;AACf,cAAM,SAAS,OAAO;AAEtB,YAAI,WAAW,SAAS;AACpB,yBAAe,aAAa,MAAM,EAAE,MAAM,CAAC;AAAA,QAC/C,WACS,WAAW,SAAS,WAAW,iBAAiB;AACrD,yBAAe,aAAa,MAAM,EAAE,IAAI,CAAC;AAAA,QAC7C,WACS,WAAW,UAAU,WAAW,QAAQ;AAC7C,yBAAe,aAAa,MAAM,EAAE,KAAK,CAAC;AAAA,QAC9C,WACS,WAAW,aAAa;AAC7B,yBAAe,aAAa,MAAM,EAAE,IAAI,SAAS,CAAC;AAAA,QACtD,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,IAAI,KAAK,CAAC;AAAA,QAClD,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,IAAI,KAAK,CAAC;AAAA,QAClD,WACS,WAAW,YAAY;AAC5B,yBAAe,aAAa,MAAM,EAAE,IAAI,SAAS,CAAC;AAAA,QACtD,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,KAAK,CAAC;AAAA,QAC9C,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,KAAK,CAAC;AAAA,QAC9C,WACS,WAAW,OAAO;AACvB,yBAAe,aAAa,MAAM,EAAE,IAAI,CAAC;AAAA,QAC7C,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,OAAO,CAAC;AAAA,QAChD,WACS,WAAW,WAAW;AAC3B,yBAAe,aAAa,MAAM,EAAE,OAAO,CAAC;AAAA,QAChD,WACS,WAAW,UAAU;AAC1B,yBAAe,aAAa,MAAM,EAAE,OAAO,CAAC;AAAA,QAChD,WACS,WAAW,aAAa;AAC7B,yBAAe,aAAa,MAAM,EAAE,UAAU,CAAC;AAAA,QACnD,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,KAAK,CAAC;AAAA,QAC9C,WACS,WAAW,OAAO;AACvB,yBAAe,aAAa,MAAM,EAAE,IAAI,CAAC;AAAA,QAC7C,WACS,WAAW,SAAS;AACzB,yBAAe,aAAa,MAAM,EAAE,MAAM,CAAC;AAAA,QAC/C,WACS,WAAW,UAAU;AAC1B,yBAAe,aAAa,MAAM,EAAE,OAAO,CAAC;AAAA,QAChD,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,KAAK,CAAC;AAAA,QAC9C,WACS,WAAW,SAAS;AACzB,yBAAe,aAAa,MAAM,EAAE,MAAM,CAAC;AAAA,QAC/C,WACS,WAAW,QAAQ;AACxB,yBAAe,aAAa,MAAM,EAAE,KAAK,CAAC;AAAA,QAC9C,WACS,WAAW,OAAO;AACvB,yBAAe,aAAa,MAAM,EAAE,IAAI,CAAC;AAAA,QAC7C,WACS,WAAW,SAAS;AACzB,yBAAe,aAAa,MAAM,EAAE,MAAM,CAAC;AAAA,QAC/C;AAAA,MAGJ;AAEA,UAAI,OAAO,OAAO,cAAc,UAAU;AACtC,uBAAe,aAAa,IAAI,OAAO,SAAS;AAAA,MACpD;AACA,UAAI,OAAO,OAAO,cAAc,UAAU;AACtC,uBAAe,aAAa,IAAI,OAAO,SAAS;AAAA,MACpD;AACA,UAAI,OAAO,SAAS;AAEhB,uBAAe,aAAa,MAAM,IAAI,OAAO,OAAO,OAAO,CAAC;AAAA,MAChE;AACA,kBAAY;AACZ;AAAA,IACJ;AAAA,IACA,KAAK;AAAA,IACL,KAAK,WAAW;AACZ,UAAI,eAAe,SAAS,YAAY,EAAE,OAAO,EAAE,IAAI,IAAI,EAAE,OAAO;AAEpE,UAAI,OAAO,OAAO,YAAY,UAAU;AACpC,uBAAe,aAAa,IAAI,OAAO,OAAO;AAAA,MAClD;AACA,UAAI,OAAO,OAAO,YAAY,UAAU;AACpC,uBAAe,aAAa,IAAI,OAAO,OAAO;AAAA,MAClD;AACA,UAAI,OAAO,OAAO,qBAAqB,UAAU;AAC7C,uBAAe,aAAa,GAAG,OAAO,gBAAgB;AAAA,MAC1D,WACS,OAAO,qBAAqB,QAAQ,OAAO,OAAO,YAAY,UAAU;AAC7E,uBAAe,aAAa,GAAG,OAAO,OAAO;AAAA,MACjD;AACA,UAAI,OAAO,OAAO,qBAAqB,UAAU;AAC7C,uBAAe,aAAa,GAAG,OAAO,gBAAgB;AAAA,MAC1D,WACS,OAAO,qBAAqB,QAAQ,OAAO,OAAO,YAAY,UAAU;AAC7E,uBAAe,aAAa,GAAG,OAAO,OAAO;AAAA,MACjD;AACA,UAAI,OAAO,OAAO,eAAe,UAAU;AACvC,uBAAe,aAAa,WAAW,OAAO,UAAU;AAAA,MAC5D;AACA,kBAAY;AACZ;AAAA,IACJ;AAAA,IACA,KAAK,WAAW;AACZ,kBAAY,EAAE,QAAQ;AACtB;AAAA,IACJ;AAAA,IACA,KAAK,QAAQ;AACT,kBAAY,EAAE,KAAK;AACnB;AAAA,IACJ;AAAA,IACA,KAAK,UAAU;AACX,YAAM,QAAQ,CAAC;AACf,YAAM,aAAa,OAAO,cAAc,CAAC;AACzC,YAAM,cAAc,IAAI,IAAI,OAAO,YAAY,CAAC,CAAC;AAEjD,iBAAW,CAAC,KAAK,UAAU,KAAK,OAAO,QAAQ,UAAU,GAAG;AACxD,cAAM,gBAAgB,cAAc,YAAY,GAAG;AAEnD,cAAM,GAAG,IAAI,YAAY,IAAI,GAAG,IAAI,gBAAgB,cAAc,SAAS;AAAA,MAC/E;AAEA,UAAI,OAAO,eAAe;AACtB,cAAM,YAAY,cAAc,OAAO,eAAe,GAAG;AACzD,cAAM,cAAc,OAAO,wBAAwB,OAAO,OAAO,yBAAyB,WACpF,cAAc,OAAO,sBAAsB,GAAG,IAC9C,EAAE,IAAI;AAEZ,YAAI,OAAO,KAAK,KAAK,EAAE,WAAW,GAAG;AACjC,sBAAY,EAAE,OAAO,WAAW,WAAW;AAC3C;AAAA,QACJ;AAEA,cAAMC,gBAAe,EAAE,OAAO,KAAK,EAAE,YAAY;AACjD,cAAM,eAAe,EAAE,YAAY,WAAW,WAAW;AACzD,oBAAY,EAAE,aAAaA,eAAc,YAAY;AACrD;AAAA,MACJ;AAEA,UAAI,OAAO,mBAAmB;AAG1B,cAAM,eAAe,OAAO;AAC5B,cAAM,cAAc,OAAO,KAAK,YAAY;AAC5C,cAAM,eAAe,CAAC;AACtB,mBAAW,WAAW,aAAa;AAC/B,gBAAM,eAAe,cAAc,aAAa,OAAO,GAAG,GAAG;AAC7D,gBAAM,YAAY,EAAE,OAAO,EAAE,MAAM,IAAI,OAAO,OAAO,CAAC;AACtD,uBAAa,KAAK,EAAE,YAAY,WAAW,YAAY,CAAC;AAAA,QAC5D;AAEA,cAAM,qBAAqB,CAAC;AAC5B,YAAI,OAAO,KAAK,KAAK,EAAE,SAAS,GAAG;AAE/B,6BAAmB,KAAK,EAAE,OAAO,KAAK,EAAE,YAAY,CAAC;AAAA,QACzD;AACA,2BAAmB,KAAK,GAAG,YAAY;AACvC,YAAI,mBAAmB,WAAW,GAAG;AACjC,sBAAY,EAAE,OAAO,CAAC,CAAC,EAAE,YAAY;AAAA,QACzC,WACS,mBAAmB,WAAW,GAAG;AACtC,sBAAY,mBAAmB,CAAC;AAAA,QACpC,OACK;AAED,cAAI,SAAS,EAAE,aAAa,mBAAmB,CAAC,GAAG,mBAAmB,CAAC,CAAC;AACxE,mBAAS,IAAI,GAAG,IAAI,mBAAmB,QAAQ,KAAK;AAChD,qBAAS,EAAE,aAAa,QAAQ,mBAAmB,CAAC,CAAC;AAAA,UACzD;AACA,sBAAY;AAAA,QAChB;AACA;AAAA,MACJ;AAIA,YAAM,eAAe,EAAE,OAAO,KAAK;AACnC,UAAI,OAAO,yBAAyB,OAAO;AAEvC,oBAAY,aAAa,OAAO;AAAA,MACpC,WACS,OAAO,OAAO,yBAAyB,UAAU;AAEtD,oBAAY,aAAa,SAAS,cAAc,OAAO,sBAAsB,GAAG,CAAC;AAAA,MACrF,OACK;AAED,oBAAY,aAAa,YAAY;AAAA,MACzC;AACA;AAAA,IACJ;AAAA,IACA,KAAK,SAAS;AAIV,YAAM,cAAc,OAAO;AAC3B,YAAM,QAAQ,OAAO;AACrB,UAAI,eAAe,MAAM,QAAQ,WAAW,GAAG;AAE3C,cAAM,aAAa,YAAY,IAAI,CAAC,SAAS,cAAc,MAAM,GAAG,CAAC;AACrE,cAAM,OAAO,SAAS,OAAO,UAAU,YAAY,CAAC,MAAM,QAAQ,KAAK,IACjE,cAAc,OAAO,GAAG,IACxB;AACN,YAAI,MAAM;AACN,sBAAY,EAAE,MAAM,UAAU,EAAE,KAAK,IAAI;AAAA,QAC7C,OACK;AACD,sBAAY,EAAE,MAAM,UAAU;AAAA,QAClC;AAEA,YAAI,OAAO,OAAO,aAAa,UAAU;AACrC,sBAAY,UAAU,MAAM,EAAE,UAAU,OAAO,QAAQ,CAAC;AAAA,QAC5D;AACA,YAAI,OAAO,OAAO,aAAa,UAAU;AACrC,sBAAY,UAAU,MAAM,EAAE,UAAU,OAAO,QAAQ,CAAC;AAAA,QAC5D;AAAA,MACJ,WACS,MAAM,QAAQ,KAAK,GAAG;AAE3B,cAAM,aAAa,MAAM,IAAI,CAAC,SAAS,cAAc,MAAM,GAAG,CAAC;AAC/D,cAAM,OAAO,OAAO,mBAAmB,OAAO,OAAO,oBAAoB,WACnE,cAAc,OAAO,iBAAiB,GAAG,IACzC;AACN,YAAI,MAAM;AACN,sBAAY,EAAE,MAAM,UAAU,EAAE,KAAK,IAAI;AAAA,QAC7C,OACK;AACD,sBAAY,EAAE,MAAM,UAAU;AAAA,QAClC;AAEA,YAAI,OAAO,OAAO,aAAa,UAAU;AACrC,sBAAY,UAAU,MAAM,EAAE,UAAU,OAAO,QAAQ,CAAC;AAAA,QAC5D;AACA,YAAI,OAAO,OAAO,aAAa,UAAU;AACrC,sBAAY,UAAU,MAAM,EAAE,UAAU,OAAO,QAAQ,CAAC;AAAA,QAC5D;AAAA,MACJ,WACS,UAAU,QAAW;AAE1B,cAAM,UAAU,cAAc,OAAO,GAAG;AACxC,YAAI,cAAc,EAAE,MAAM,OAAO;AAEjC,YAAI,OAAO,OAAO,aAAa,UAAU;AACrC,wBAAc,YAAY,IAAI,OAAO,QAAQ;AAAA,QACjD;AACA,YAAI,OAAO,OAAO,aAAa,UAAU;AACrC,wBAAc,YAAY,IAAI,OAAO,QAAQ;AAAA,QACjD;AACA,oBAAY;AAAA,MAChB,OACK;AAED,oBAAY,EAAE,MAAM,EAAE,IAAI,CAAC;AAAA,MAC/B;AACA;AAAA,IACJ;AAAA,IACA;AACI,YAAM,IAAI,MAAM,qBAAqB,IAAI,EAAE;AAAA,EACnD;AAEA,MAAI,OAAO,aAAa;AACpB,gBAAY,UAAU,SAAS,OAAO,WAAW;AAAA,EACrD;AACA,MAAI,OAAO,YAAY,QAAW;AAC9B,gBAAY,UAAU,QAAQ,OAAO,OAAO;AAAA,EAChD;AACA,SAAO;AACX;AACA,SAAS,cAAc,QAAQ,KAAK;AAChC,MAAI,OAAO,WAAW,WAAW;AAC7B,WAAO,SAAS,EAAE,IAAI,IAAI,EAAE,MAAM;AAAA,EACtC;AAEA,MAAI,aAAa,kBAAkB,QAAQ,GAAG;AAC9C,QAAM,kBAAkB,OAAO,QAAQ,OAAO,SAAS,UAAa,OAAO,UAAU;AAGrF,MAAI,OAAO,SAAS,MAAM,QAAQ,OAAO,KAAK,GAAG;AAC7C,UAAM,UAAU,OAAO,MAAM,IAAI,CAAC,MAAM,cAAc,GAAG,GAAG,CAAC;AAC7D,UAAM,aAAa,EAAE,MAAM,OAAO;AAClC,iBAAa,kBAAkB,EAAE,aAAa,YAAY,UAAU,IAAI;AAAA,EAC5E;AAEA,MAAI,OAAO,SAAS,MAAM,QAAQ,OAAO,KAAK,GAAG;AAC7C,UAAM,UAAU,OAAO,MAAM,IAAI,CAAC,MAAM,cAAc,GAAG,GAAG,CAAC;AAC7D,UAAM,aAAa,EAAE,IAAI,OAAO;AAChC,iBAAa,kBAAkB,EAAE,aAAa,YAAY,UAAU,IAAI;AAAA,EAC5E;AAEA,MAAI,OAAO,SAAS,MAAM,QAAQ,OAAO,KAAK,GAAG;AAC7C,QAAI,OAAO,MAAM,WAAW,GAAG;AAC3B,mBAAa,kBAAkB,aAAa,EAAE,IAAI;AAAA,IACtD,OACK;AACD,UAAI,SAAS,kBAAkB,aAAa,cAAc,OAAO,MAAM,CAAC,GAAG,GAAG;AAC9E,YAAM,WAAW,kBAAkB,IAAI;AACvC,eAAS,IAAI,UAAU,IAAI,OAAO,MAAM,QAAQ,KAAK;AACjD,iBAAS,EAAE,aAAa,QAAQ,cAAc,OAAO,MAAM,CAAC,GAAG,GAAG,CAAC;AAAA,MACvE;AACA,mBAAa;AAAA,IACjB;AAAA,EACJ;AAEA,MAAI,OAAO,aAAa,QAAQ,IAAI,YAAY,eAAe;AAC3D,iBAAa,EAAE,SAAS,UAAU;AAAA,EACtC;AAEA,MAAI,OAAO,aAAa,MAAM;AAC1B,iBAAa,EAAE,SAAS,UAAU;AAAA,EACtC;AAEA,QAAM,YAAY,CAAC;AAEnB,QAAM,mBAAmB,CAAC,OAAO,MAAM,YAAY,WAAW,eAAe,eAAe,gBAAgB;AAC5G,aAAW,OAAO,kBAAkB;AAChC,QAAI,OAAO,QAAQ;AACf,gBAAU,GAAG,IAAI,OAAO,GAAG;AAAA,IAC/B;AAAA,EACJ;AAEA,QAAM,sBAAsB,CAAC,mBAAmB,oBAAoB,eAAe;AACnF,aAAW,OAAO,qBAAqB;AACnC,QAAI,OAAO,QAAQ;AACf,gBAAU,GAAG,IAAI,OAAO,GAAG;AAAA,IAC/B;AAAA,EACJ;AAEA,aAAW,OAAO,OAAO,KAAK,MAAM,GAAG;AACnC,QAAI,CAAC,gBAAgB,IAAI,GAAG,GAAG;AAC3B,gBAAU,GAAG,IAAI,OAAO,GAAG;AAAA,IAC/B;AAAA,EACJ;AACA,MAAI,OAAO,KAAK,SAAS,EAAE,SAAS,GAAG;AACnC,QAAI,SAAS,IAAI,YAAY,SAAS;AAAA,EAC1C;AACA,SAAO;AACX;AAGO,SAAS,eAAe,QAAQ,QAAQ;AAE3C,MAAI,OAAO,WAAW,WAAW;AAC7B,WAAO,SAAS,EAAE,IAAI,IAAI,EAAE,MAAM;AAAA,EACtC;AACA,QAAM,UAAU,cAAc,QAAQ,QAAQ,aAAa;AAC3D,QAAM,OAAQ,OAAO,SAAS,OAAO,eAAe,CAAC;AACrD,QAAM,MAAM;AAAA,IACR;AAAA,IACA;AAAA,IACA,MAAM,oBAAI,IAAI;AAAA,IACd,YAAY,oBAAI,IAAI;AAAA,IACpB,YAAY;AAAA,IACZ,UAAU,QAAQ,YAAY;AAAA,EAClC;AACA,SAAO,cAAc,QAAQ,GAAG;AACpC;;;ACvkBA;AAAA;AAAA,gBAAAC;AAAA,EAAA,eAAAC;AAAA,EAAA,YAAAC;AAAA,EAAA,cAAAC;AAAA,EAAA,cAAAC;AAAA;AAEO,SAASC,QAAO,QAAQ;AAC3B,SAAY,eAAuB,WAAW,MAAM;AACxD;AACO,SAASC,QAAO,QAAQ;AAC3B,SAAY,eAAuB,WAAW,MAAM;AACxD;AACO,SAASC,SAAQ,QAAQ;AAC5B,SAAY,gBAAwB,YAAY,MAAM;AAC1D;AACO,SAASC,QAAO,QAAQ;AAC3B,SAAY,eAAuB,WAAW,MAAM;AACxD;AACO,SAASC,MAAK,QAAQ;AACzB,SAAY,aAAqB,SAAS,MAAM;AACpD;;;ARPA,OAAO,WAAG,CAAC;;;ASDJ,IAAM,iBAAiB,uBAAO,mDAAmD;AAgCxF,IAAM,iBAA4D;EAChE,MAAM;EACN,cAAc;EACd,gBAAgB;EAChB,cAAc;EACd,cAAc;EACd,aAAa;EACb,kBAAkB;EAClB,0BAA0B;EAC1B,gBAAgB;EAChB,QAAQ;EACR,cAAc;EACd,eAAe;EACf,qBAAqB;EACrB,iBAAiB;EACjB,iBAAiB;EACjB,eAAe;EACf,gBAAgB;EAChB,cAAc;;AAGT,IAAM,oBAAoB,CAC/B,YACE;AAEF,SACE,OAAO,YAAY,WACjB;IACE,GAAG;IACH,UAAU,CAAC,GAAG;IACd,aAAa,CAAA;IACb,MAAM;MAER;IACE,GAAG;IACH,UAAU,CAAC,GAAG;IACd,aAAa,CAAA;IACb,GAAG;;AAEX;;;AC7EO,IAAM,SAAS,CAAC,cAAiD;AACtE,SAAO,UAAU,YAAY,UAAU,OAAO;AAChD;AAEM,SAAUC,YAAW,KAA8B;AACvD,MAAI,CAAC;AAAK,WAAO;AACjB,aAAW,MAAM;AAAK,WAAO;AAC7B,SAAO;AACT;;;ACaO,IAAM,UAAU,CAAC,YAAsD;AAC5E,QAAM,WAAW,kBAAkB,OAAO;AAC1C,QAAM,cACJ,SAAS,SAAS,SAChB,CAAC,GAAG,SAAS,UAAU,SAAS,gBAAgB,SAAS,IAAI,IAC7D,SAAS;AACb,SAAO;IACL,GAAG;IACH;IACA,cAAc;IACd,UAAU,oBAAI,IAAG;IACjB,MAAM,IAAI,IACR,OAAO,QAAQ,SAAS,WAAW,EAAE,IAAI,CAAC,CAAC,MAAM,GAAG,MAAM;MACxD,OAAO,GAAG;MACV;QACE,KAAK,OAAO,GAAG;QACf,MAAM,CAAC,GAAG,SAAS,UAAU,SAAS,gBAAgB,IAAI;;QAE1D,YAAY;;KAEf,CAAC;;AAGR;;;ACvCM,SAAU,gBACd,KACA,KACA,cACA,MAAU;AAEV,MAAI,CAAC,MAAM;AAAe;AAC1B,MAAI,cAAc;AAChB,QAAI,eAAe;MACjB,GAAG,IAAI;MACP,CAAC,GAAG,GAAG;;EAEX;AACF;AAEM,SAAU,0BAKd,KAAgB,KAAU,OAAuB,cAAkC,MAAU;AAC7F,MAAI,GAAG,IAAI;AACX,kBAAgB,KAAK,KAAK,cAAc,IAAI;AAC9C;;;AC5BM,SAAU,cAAW;AACzB,SAAO,CAAA;AACT;;;ACSM,SAAU,cAAc,KAAkB,MAAU;AACxD,QAAM,MAA4B;IAChC,MAAM;;AAER,MAAI,IAAI,MAAM,MAAM,aAAa,sBAAsB,QAAQ;AAC7D,QAAI,QAAQ,SAAS,IAAI,KAAK,MAAM;MAClC,GAAG;MACH,aAAa,CAAC,GAAG,KAAK,aAAa,OAAO;KAC3C;EACH;AAEA,MAAI,IAAI,WAAW;AACjB,8BAA0B,KAAK,YAAY,IAAI,UAAU,OAAO,IAAI,UAAU,SAAS,IAAI;EAC7F;AACA,MAAI,IAAI,WAAW;AACjB,8BAA0B,KAAK,YAAY,IAAI,UAAU,OAAO,IAAI,UAAU,SAAS,IAAI;EAC7F;AACA,MAAI,IAAI,aAAa;AACnB,8BAA0B,KAAK,YAAY,IAAI,YAAY,OAAO,IAAI,YAAY,SAAS,IAAI;AAC/F,8BAA0B,KAAK,YAAY,IAAI,YAAY,OAAO,IAAI,YAAY,SAAS,IAAI;EACjG;AACA,SAAO;AACT;;;ACpBM,SAAU,eAAe,KAAmB,MAAU;AAC1D,QAAM,MAA6B;IACjC,MAAM;IACN,QAAQ;;AAGV,MAAI,CAAC,IAAI;AAAQ,WAAO;AAExB,aAAWC,UAAS,IAAI,QAAQ;AAC9B,YAAQA,OAAM,MAAM;MAClB,KAAK;AACH,YAAI,KAAK,WAAW,eAAe;AACjC,cAAIA,OAAM,WAAW;AACnB,sCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;UAC5E,OAAO;AACL,sCAA0B,KAAK,oBAAoBA,OAAM,OAAOA,OAAM,SAAS,IAAI;UACrF;QACF,OAAO;AACL,cAAI,CAACA,OAAM,WAAW;AACpB,gBAAI,mBAAmB;UACzB;AACA,oCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;QAC5E;AACA;MACF,KAAK;AACH,YAAI,KAAK,WAAW,eAAe;AACjC,cAAIA,OAAM,WAAW;AACnB,sCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;UAC5E,OAAO;AACL,sCAA0B,KAAK,oBAAoBA,OAAM,OAAOA,OAAM,SAAS,IAAI;UACrF;QACF,OAAO;AACL,cAAI,CAACA,OAAM,WAAW;AACpB,gBAAI,mBAAmB;UACzB;AACA,oCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;QAC5E;AACA;MACF,KAAK;AACH,kCAA0B,KAAK,cAAcA,OAAM,OAAOA,OAAM,SAAS,IAAI;AAC7E;IACJ;EACF;AACA,SAAO;AACT;;;ACvDM,SAAU,kBAAe;AAC7B,SAAO;IACL,MAAM;;AAEV;;;ACJM,SAAU,gBAAgB,MAA0B,MAAU;AAClE,SAAO,SAAS,KAAK,KAAK,MAAM,IAAI;AACtC;;;ACFO,IAAM,gBAAgB,CAAC,KAAuB,SAAc;AACjE,SAAO,SAAS,IAAI,UAAU,MAAM,IAAI;AAC1C;;;ACYM,SAAU,aACd,KACA,MACA,sBAAmC;AAEnC,QAAM,WAAW,wBAAwB,KAAK;AAE9C,MAAI,MAAM,QAAQ,QAAQ,GAAG;AAC3B,WAAO;MACL,OAAO,SAAS,IAAI,CAAC,MAAM,MAAM,aAAa,KAAK,MAAM,IAAI,CAAC;;EAElE;AAEA,UAAQ,UAAU;IAChB,KAAK;IACL,KAAK;AACH,aAAO;QACL,MAAM;QACN,QAAQ;;IAEZ,KAAK;AACH,aAAO;QACL,MAAM;QACN,QAAQ;;IAEZ,KAAK;AACH,aAAO,kBAAkB,KAAK,IAAI;EACtC;AACF;AAEA,IAAM,oBAAoB,CAAC,KAAiB,SAAc;AACxD,QAAM,MAA2B;IAC/B,MAAM;IACN,QAAQ;;AAGV,MAAI,KAAK,WAAW,YAAY;AAC9B,WAAO;EACT;AAEA,aAAWC,UAAS,IAAI,QAAQ;AAC9B,YAAQA,OAAM,MAAM;MAClB,KAAK;AACH;UACE;UACA;UACAA,OAAM;;UACNA,OAAM;UACN;QAAI;AAEN;MACF,KAAK;AACH;UACE;UACA;UACAA,OAAM;;UACNA,OAAM;UACN;QAAI;AAEN;IACJ;EACF;AAEA,SAAO;AACT;;;AC9EM,SAAU,gBAAgB,MAAqB,MAAU;AAC7D,SAAO;IACL,GAAG,SAAS,KAAK,UAAU,MAAM,IAAI;IACrC,SAAS,KAAK,aAAY;;AAE9B;;;ACLM,SAAU,gBACd,MACA,MACA,iBAAwB;AAExB,SAAO,KAAK,mBAAmB,UAAU,SAAS,KAAK,OAAO,MAAM,MAAM,eAAe,IAAI,CAAA;AAC/F;;;ACHM,SAAU,aAAa,KAAe;AAC1C,SAAO;IACL,MAAM;IACN,MAAM,CAAC,GAAG,IAAI,MAAM;;AAExB;;;ACFA,IAAM,yBAAyB,CAC7B,SACgC;AAChC,MAAI,UAAU,QAAQ,KAAK,SAAS;AAAU,WAAO;AACrD,SAAO,WAAW;AACpB;AAEM,SAAU,qBACd,KACA,MAAU;AAEV,QAAM,QAAQ;IACZ,SAAS,IAAI,KAAK,MAAM;MACtB,GAAG;MACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG;KAChD;IACD,SAAS,IAAI,MAAM,MAAM;MACvB,GAAG;MACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG;KAChD;IACD,OAAO,CAAC,MAA4B,CAAC,CAAC,CAAC;AAEzC,MAAI,wBACF,KAAK,WAAW,sBAAsB,EAAE,uBAAuB,MAAK,IAAK;AAE3E,QAAM,cAAiC,CAAA;AAEvC,QAAM,QAAQ,CAAC,WAAU;AACvB,QAAI,uBAAuB,MAAM,GAAG;AAClC,kBAAY,KAAK,GAAG,OAAO,KAAK;AAChC,UAAI,OAAO,0BAA0B,QAAW;AAG9C,gCAAwB;MAC1B;IACF,OAAO;AACL,UAAI,eAAgC;AACpC,UAAI,0BAA0B,UAAU,OAAO,yBAAyB,OAAO;AAC7E,cAAM,EAAE,sBAAsB,GAAG,KAAI,IAAK;AAC1C,uBAAe;MACjB,OAAO;AAEL,gCAAwB;MAC1B;AACA,kBAAY,KAAK,YAAY;IAC/B;EACF,CAAC;AACD,SAAO,YAAY,SACf;IACE,OAAO;IACP,GAAG;MAEL;AACN;;;ACnDM,SAAU,gBAAgB,KAAoB,MAAU;AAC5D,QAAM,aAAa,OAAO,IAAI;AAC9B,MACE,eAAe,YACf,eAAe,YACf,eAAe,aACf,eAAe,UACf;AACA,WAAO;MACL,MAAM,MAAM,QAAQ,IAAI,KAAK,IAAI,UAAU;;EAE/C;AAEA,MAAI,KAAK,WAAW,YAAY;AAC9B,WAAO;MACL,MAAM,eAAe,WAAW,YAAY;MAC5C,MAAM,CAAC,IAAI,KAAK;;EAEpB;AAEA,SAAO;IACL,MAAM,eAAe,WAAW,YAAY;IAC5C,OAAO,IAAI;;AAEf;;;AC/BA,IAAI;AAQG,IAAM,cAAc;;;;EAIzB,MAAM;EACN,OAAO;EACP,MAAM;;;;EAIN,OAAO;;;;;;;;;;;;EAYP,OAAO,MAAK;AACV,QAAI,eAAe,QAAW;AAC5B,mBAAa,OAAO,wDAAwD,GAAG;IACjF;AACA,WAAO;EACT;;;;EAIA,MAAM;;;;EAIN,MAAM;;;;EAIN,MAAM;EACN,QAAQ;EACR,QAAQ;;AA+BJ,SAAU,eAAe,KAAmB,MAAU;AAC1D,QAAM,MAA6B;IACjC,MAAM;;AAGR,WAAS,eAAe,OAAa;AACnC,WAAO,KAAK,oBAAoB,WAAW,sBAAsB,KAAK,IAAI;EAC5E;AAEA,MAAI,IAAI,QAAQ;AACd,eAAWC,UAAS,IAAI,QAAQ;AAC9B,cAAQA,OAAM,MAAM;QAClB,KAAK;AACH,oCACE,KACA,aACA,OAAO,IAAI,cAAc,WAAW,KAAK,IAAI,IAAI,WAAWA,OAAM,KAAK,IAAIA,OAAM,OACjFA,OAAM,SACN,IAAI;AAEN;QACF,KAAK;AACH,oCACE,KACA,aACA,OAAO,IAAI,cAAc,WAAW,KAAK,IAAI,IAAI,WAAWA,OAAM,KAAK,IAAIA,OAAM,OACjFA,OAAM,SACN,IAAI;AAGN;QACF,KAAK;AACH,kBAAQ,KAAK,eAAe;YAC1B,KAAK;AACH,wBAAU,KAAK,SAASA,OAAM,SAAS,IAAI;AAC3C;YACF,KAAK;AACH,wBAAU,KAAK,aAAaA,OAAM,SAAS,IAAI;AAC/C;YACF,KAAK;AACH,yBAAW,KAAK,YAAY,OAAOA,OAAM,SAAS,IAAI;AACtD;UACJ;AAEA;QACF,KAAK;AACH,oBAAU,KAAK,OAAOA,OAAM,SAAS,IAAI;AACzC;QACF,KAAK;AACH,oBAAU,KAAK,QAAQA,OAAM,SAAS,IAAI;AAC1C;QACF,KAAK;AACH,qBAAW,KAAKA,OAAM,OAAOA,OAAM,SAAS,IAAI;AAChD;QACF,KAAK;AACH,qBAAW,KAAK,YAAY,MAAMA,OAAM,SAAS,IAAI;AACrD;QACF,KAAK;AACH,qBAAW,KAAK,YAAY,OAAOA,OAAM,SAAS,IAAI;AACtD;QACF,KAAK;AACH,qBAAW,KAAK,OAAO,IAAI,eAAeA,OAAM,KAAK,CAAC,EAAE,GAAGA,OAAM,SAAS,IAAI;AAC9E;QACF,KAAK;AACH,qBAAW,KAAK,OAAO,GAAG,eAAeA,OAAM,KAAK,CAAC,GAAG,GAAGA,OAAM,SAAS,IAAI;AAC9E;QAEF,KAAK;AACH,oBAAU,KAAK,aAAaA,OAAM,SAAS,IAAI;AAC/C;QACF,KAAK;AACH,oBAAU,KAAK,QAAQA,OAAM,SAAS,IAAI;AAC1C;QACF,KAAK;AACH,oBAAU,KAAK,QAAQA,OAAM,SAAS,IAAI;AAC1C;QACF,KAAK;AACH,oBAAU,KAAK,YAAYA,OAAM,SAAS,IAAI;AAC9C;QACF,KAAK;AACH,oCACE,KACA,aACA,OAAO,IAAI,cAAc,WAAW,KAAK,IAAI,IAAI,WAAWA,OAAM,KAAK,IAAIA,OAAM,OACjFA,OAAM,SACN,IAAI;AAEN,oCACE,KACA,aACA,OAAO,IAAI,cAAc,WAAW,KAAK,IAAI,IAAI,WAAWA,OAAM,KAAK,IAAIA,OAAM,OACjFA,OAAM,SACN,IAAI;AAEN;QACF,KAAK,YAAY;AACf,qBAAW,KAAK,OAAO,eAAeA,OAAM,KAAK,CAAC,GAAGA,OAAM,SAAS,IAAI;AACxE;QACF;QACA,KAAK,MAAM;AACT,cAAIA,OAAM,YAAY,MAAM;AAC1B,sBAAU,KAAK,QAAQA,OAAM,SAAS,IAAI;UAC5C;AACA,cAAIA,OAAM,YAAY,MAAM;AAC1B,sBAAU,KAAK,QAAQA,OAAM,SAAS,IAAI;UAC5C;AACA;QACF;QACA,KAAK;AACH,qBAAW,KAAK,YAAY,OAAOA,OAAM,SAAS,IAAI;AACtD;QACF,KAAK,QAAQ;AACX,qBAAW,KAAK,YAAY,MAAMA,OAAM,SAAS,IAAI;AACrD;QACF;QACA,KAAK,UAAU;AACb,kBAAQ,KAAK,gBAAgB;YAC3B,KAAK,iBAAiB;AACpB,wBAAU,KAAK,UAAiBA,OAAM,SAAS,IAAI;AACnD;YACF;YAEA,KAAK,0BAA0B;AAC7B,wCAA0B,KAAK,mBAAmB,UAAUA,OAAM,SAAS,IAAI;AAC/E;YACF;YAEA,KAAK,eAAe;AAClB,yBAAW,KAAK,YAAY,QAAQA,OAAM,SAAS,IAAI;AACvD;YACF;UACF;AACA;QACF;QACA,KAAK,UAAU;AACb,qBAAW,KAAK,YAAY,QAAQA,OAAM,SAAS,IAAI;QACzD;QACA,KAAK;QACL,KAAK;QACL,KAAK;AACH;QACF;AACE,UAAC,kBAAC,MAAY;UAAE,GAAGA,MAAK;MAC5B;IACF;EACF;AAEA,SAAO;AACT;AAEA,IAAM,wBAAwB,CAAC,UAC7B,MAAM,KAAK,KAAK,EACb,IAAI,CAAC,MAAO,cAAc,KAAK,CAAC,IAAI,IAAI,KAAK,CAAC,EAAG,EACjD,KAAK,EAAE;AAEZ,IAAM,YAAY,CAChB,QACA,OACA,SACA,SACE;AACF,MAAI,OAAO,UAAU,OAAO,OAAO,KAAK,CAAC,MAAM,EAAE,MAAM,GAAG;AACxD,QAAI,CAAC,OAAO,OAAO;AACjB,aAAO,QAAQ,CAAA;IACjB;AAEA,QAAI,OAAO,QAAQ;AACjB,aAAO,MAAO,KAAK;QACjB,QAAQ,OAAO;QACf,GAAI,OAAO,gBACT,KAAK,iBAAiB;UACpB,cAAc,EAAE,QAAQ,OAAO,aAAa,OAAM;;OAEvD;AACD,aAAO,OAAO;AACd,UAAI,OAAO,cAAc;AACvB,eAAO,OAAO,aAAa;AAC3B,YAAI,OAAO,KAAK,OAAO,YAAY,EAAE,WAAW,GAAG;AACjD,iBAAO,OAAO;QAChB;MACF;IACF;AAEA,WAAO,MAAO,KAAK;MACjB,QAAQ;MACR,GAAI,WAAW,KAAK,iBAAiB,EAAE,cAAc,EAAE,QAAQ,QAAO,EAAE;KACzE;EACH,OAAO;AACL,8BAA0B,QAAQ,UAAU,OAAO,SAAS,IAAI;EAClE;AACF;AAEA,IAAM,aAAa,CACjB,QACA,OACA,SACA,SACE;AACF,MAAI,OAAO,WAAW,OAAO,OAAO,KAAK,CAAC,MAAM,EAAE,OAAO,GAAG;AAC1D,QAAI,CAAC,OAAO,OAAO;AACjB,aAAO,QAAQ,CAAA;IACjB;AAEA,QAAI,OAAO,SAAS;AAClB,aAAO,MAAO,KAAK;QACjB,SAAS,OAAO;QAChB,GAAI,OAAO,gBACT,KAAK,iBAAiB;UACpB,cAAc,EAAE,SAAS,OAAO,aAAa,QAAO;;OAEzD;AACD,aAAO,OAAO;AACd,UAAI,OAAO,cAAc;AACvB,eAAO,OAAO,aAAa;AAC3B,YAAI,OAAO,KAAK,OAAO,YAAY,EAAE,WAAW,GAAG;AACjD,iBAAO,OAAO;QAChB;MACF;IACF;AAEA,WAAO,MAAO,KAAK;MACjB,SAAS,cAAc,OAAO,IAAI;MAClC,GAAI,WAAW,KAAK,iBAAiB,EAAE,cAAc,EAAE,SAAS,QAAO,EAAE;KAC1E;EACH,OAAO;AACL,8BAA0B,QAAQ,WAAW,cAAc,OAAO,IAAI,GAAG,SAAS,IAAI;EACxF;AACF;AAGA,IAAM,gBAAgB,CAAC,iBAA0C,SAAsB;AACrF,QAAM,QAAQ,OAAO,oBAAoB,aAAa,gBAAe,IAAK;AAC1E,MAAI,CAAC,KAAK,mBAAmB,CAAC,MAAM;AAAO,WAAO,MAAM;AAGxD,QAAM,QAAQ;IACZ,GAAG,MAAM,MAAM,SAAS,GAAG;;IAC3B,GAAG,MAAM,MAAM,SAAS,GAAG;;IAC3B,GAAG,MAAM,MAAM,SAAS,GAAG;;;AAK7B,QAAM,SAAS,MAAM,IAAI,MAAM,OAAO,YAAW,IAAK,MAAM;AAC5D,MAAI,UAAU;AACd,MAAI,YAAY;AAChB,MAAI,cAAc;AAClB,MAAI,cAAc;AAElB,WAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,QAAI,WAAW;AACb,iBAAW,OAAO,CAAC;AACnB,kBAAY;AACZ;IACF;AAEA,QAAI,MAAM,GAAG;AACX,UAAI,aAAa;AACf,YAAI,OAAO,CAAC,EAAE,MAAM,OAAO,GAAG;AAC5B,cAAI,aAAa;AACf,uBAAW,OAAO,CAAC;AACnB,uBAAW,GAAG,OAAO,IAAI,CAAC,CAAC,IAAI,OAAO,CAAC,CAAC,GAAG,YAAW;AACtD,0BAAc;UAChB,WAAW,OAAO,IAAI,CAAC,MAAM,OAAO,OAAO,IAAI,CAAC,GAAG,MAAM,OAAO,GAAG;AACjE,uBAAW,OAAO,CAAC;AACnB,0BAAc;UAChB,OAAO;AACL,uBAAW,GAAG,OAAO,CAAC,CAAC,GAAG,OAAO,CAAC,EAAE,YAAW,CAAE;UACnD;AACA;QACF;MACF,WAAW,OAAO,CAAC,EAAE,MAAM,OAAO,GAAG;AACnC,mBAAW,IAAI,OAAO,CAAC,CAAC,GAAG,OAAO,CAAC,EAAE,YAAW,CAAE;AAClD;MACF;IACF;AAEA,QAAI,MAAM,GAAG;AACX,UAAI,OAAO,CAAC,MAAM,KAAK;AACrB,mBAAW;;AACX;MACF,WAAW,OAAO,CAAC,MAAM,KAAK;AAC5B,mBAAW;;AACX;MACF;IACF;AAEA,QAAI,MAAM,KAAK,OAAO,CAAC,MAAM,KAAK;AAChC,iBAAW,cAAc,GAAG,OAAO,CAAC,CAAC;IAAS,IAAI,OAAO,CAAC,CAAC;;AAC3D;IACF;AAEA,eAAW,OAAO,CAAC;AACnB,QAAI,OAAO,CAAC,MAAM,MAAM;AACtB,kBAAY;IACd,WAAW,eAAe,OAAO,CAAC,MAAM,KAAK;AAC3C,oBAAc;IAChB,WAAW,CAAC,eAAe,OAAO,CAAC,MAAM,KAAK;AAC5C,oBAAc;IAChB;EACF;AAEA,MAAI;AACF,UAAM,YAAY,IAAI,OAAO,OAAO;EACtC,QAAQ;AACN,YAAQ,KACN,sCAAsC,KAAK,YAAY,KACrD,GAAG,CACJ,uEAAuE;AAE1E,WAAO,MAAM;EACf;AAEA,SAAO;AACT;;;AC9XM,SAAU,eACd,KACA,MAAU;AAEV,MAAI,KAAK,WAAW,cAAc,IAAI,SAAS,KAAK,aAAa,sBAAsB,SAAS;AAC9F,WAAO;MACL,MAAM;MACN,UAAU,IAAI,QAAQ,KAAK;MAC3B,YAAY,IAAI,QAAQ,KAAK,OAAO,OAClC,CAAC,KAAsC,SAAiB;QACtD,GAAG;QACH,CAAC,GAAG,GACF,SAAS,IAAI,UAAU,MAAM;UAC3B,GAAG;UACH,aAAa,CAAC,GAAG,KAAK,aAAa,cAAc,GAAG;SACrD,KAAK,CAAA;UAEV,CAAA,CAAE;MAEJ,sBAAsB;;EAE1B;AAEA,QAAM,SAAgC;IACpC,MAAM;IACN,sBACE,SAAS,IAAI,UAAU,MAAM;MAC3B,GAAG;MACH,aAAa,CAAC,GAAG,KAAK,aAAa,sBAAsB;KAC1D,KAAK,CAAA;;AAGV,MAAI,KAAK,WAAW,YAAY;AAC9B,WAAO;EACT;AAEA,MAAI,IAAI,SAAS,KAAK,aAAa,sBAAsB,aAAa,IAAI,QAAQ,KAAK,QAAQ,QAAQ;AACrG,UAAM,UAA8C,OAAO,QACzD,eAAe,IAAI,QAAQ,MAAM,IAAI,CAAC,EACtC,OAAO,CAAC,KAAK,CAAC,KAAK,KAAK,MAAO,QAAQ,SAAS,MAAM,EAAE,GAAG,KAAK,CAAC,GAAG,GAAG,MAAK,GAAK,CAAA,CAAE;AAErF,WAAO;MACL,GAAG;MACH,eAAe;;EAEnB,WAAW,IAAI,SAAS,KAAK,aAAa,sBAAsB,SAAS;AACvE,WAAO;MACL,GAAG;MACH,eAAe;QACb,MAAM,IAAI,QAAQ,KAAK;;;EAG7B;AAEA,SAAO;AACT;;;ACxDM,SAAU,YAAY,KAAgB,MAAU;AACpD,MAAI,KAAK,gBAAgB,UAAU;AACjC,WAAO,eAAe,KAAK,IAAI;EACjC;AAEA,QAAM,OACJ,SAAS,IAAI,QAAQ,MAAM;IACzB,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,SAAS,GAAG;GACzD,KAAK,CAAA;AACR,QAAM,SACJ,SAAS,IAAI,UAAU,MAAM;IAC3B,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,SAAS,GAAG;GACzD,KAAK,CAAA;AACR,SAAO;IACL,MAAM;IACN,UAAU;IACV,OAAO;MACL,MAAM;MACN,OAAO,CAAC,MAAM,MAAM;MACpB,UAAU;MACV,UAAU;;;AAGhB;;;AClCM,SAAU,mBAAmB,KAAqB;AACtD,QAAMC,UAAS,IAAI;AACnB,QAAM,aAAa,OAAO,KAAK,IAAI,MAAM,EAAE,OAAO,CAAC,QAAe;AAChE,WAAO,OAAOA,QAAOA,QAAO,GAAG,CAAE,MAAM;EACzC,CAAC;AAED,QAAM,eAAe,WAAW,IAAI,CAAC,QAAgBA,QAAO,GAAG,CAAE;AAEjE,QAAM,cAAc,MAAM,KAAK,IAAI,IAAI,aAAa,IAAI,CAAC,WAA4B,OAAO,MAAM,CAAC,CAAC;AAEpG,SAAO;IACL,MACE,YAAY,WAAW,IACrB,YAAY,CAAC,MAAM,WACjB,WACA,WACF,CAAC,UAAU,QAAQ;IACvB,MAAM;;AAEV;;;ACtBM,SAAU,gBAAa;AAC3B,SAAO;IACL,KAAK,CAAA;;AAET;;;ACFM,SAAU,aAAa,MAAU;AACrC,SAAO,KAAK,WAAW,aAClB;IACC,MAAM,CAAC,MAAM;IACb,UAAU;MAEZ;IACE,MAAM;;AAEd;;;ACXO,IAAM,oBAAoB;EAC/B,WAAW;EACX,WAAW;EACX,WAAW;EACX,YAAY;EACZ,SAAS;;AAoBL,SAAU,cACd,KACA,MAAU;AAEV,MAAI,KAAK,WAAW;AAAY,WAAO,QAAQ,KAAK,IAAI;AAExD,QAAM,UACJ,IAAI,mBAAmB,MAAM,MAAM,KAAK,IAAI,QAAQ,OAAM,CAAE,IAAI,IAAI;AAGtE,MACE,QAAQ,MAAM,CAAC,MAAM,EAAE,KAAK,YAAY,sBAAsB,CAAC,EAAE,KAAK,UAAU,CAAC,EAAE,KAAK,OAAO,OAAO,GACtG;AAGA,UAAM,QAAQ,QAAQ,OAAO,CAACC,QAA+B,MAAK;AAChE,YAAM,OAAO,kBAAkB,EAAE,KAAK,QAAwB;AAC9D,aAAO,QAAQ,CAACA,OAAM,SAAS,IAAI,IAAI,CAAC,GAAGA,QAAO,IAAI,IAAIA;IAC5D,GAAG,CAAA,CAAE;AAEL,WAAO;MACL,MAAM,MAAM,SAAS,IAAI,QAAQ,MAAM,CAAC;;EAE5C,WAAW,QAAQ,MAAM,CAAC,MAAM,EAAE,KAAK,aAAa,gBAAgB,CAAC,EAAE,WAAW,GAAG;AAGnF,UAAM,QAAQ,QAAQ,OAAO,CAAC,KAA6B,MAA8B;AACvF,YAAM,OAAO,OAAO,EAAE,KAAK;AAC3B,cAAQ,MAAM;QACZ,KAAK;QACL,KAAK;QACL,KAAK;AACH,iBAAO,CAAC,GAAG,KAAK,IAAI;QACtB,KAAK;AACH,iBAAO,CAAC,GAAG,KAAK,SAAkB;QACpC,KAAK;AACH,cAAI,EAAE,KAAK,UAAU;AAAM,mBAAO,CAAC,GAAG,KAAK,MAAe;QAC5D,KAAK;QACL,KAAK;QACL,KAAK;QACL;AACE,iBAAO;MACX;IACF,GAAG,CAAA,CAAE;AAEL,QAAI,MAAM,WAAW,QAAQ,QAAQ;AAGnC,YAAM,cAAc,MAAM,OAAO,CAAC,GAAG,GAAG,MAAM,EAAE,QAAQ,CAAC,MAAM,CAAC;AAChE,aAAO;QACL,MAAM,YAAY,SAAS,IAAI,cAAc,YAAY,CAAC;QAC1D,MAAM,QAAQ,OACZ,CAAC,KAAK,MAAK;AACT,iBAAO,IAAI,SAAS,EAAE,KAAK,KAAK,IAAI,MAAM,CAAC,GAAG,KAAK,EAAE,KAAK,KAAK;QACjE,GACA,CAAA,CAAmD;;IAGzD;EACF,WAAW,QAAQ,MAAM,CAAC,MAAM,EAAE,KAAK,aAAa,SAAS,GAAG;AAC9D,WAAO;MACL,MAAM;MACN,MAAM,QAAQ,OACZ,CAAC,KAAe,MAAM,CAAC,GAAG,KAAK,GAAG,EAAE,KAAK,OAAO,OAAO,CAACC,OAAc,CAAC,IAAI,SAASA,EAAC,CAAC,CAAC,GACvF,CAAA,CAAE;;EAGR;AAEA,SAAO,QAAQ,KAAK,IAAI;AAC1B;AAEA,IAAM,UAAU,CACd,KACA,SACoE;AACpE,QAAM,SAAU,IAAI,mBAAmB,MAAM,MAAM,KAAK,IAAI,QAAQ,OAAM,CAAE,IAAI,IAAI,SACjF,IAAI,CAAC,GAAG,MACP,SAAS,EAAE,MAAM;IACf,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG,CAAC,EAAE;GACnD,CAAC,EAEH,OACC,CAAC,MACC,CAAC,CAAC,MAAM,CAAC,KAAK,gBAAiB,OAAO,MAAM,YAAY,OAAO,KAAK,CAAC,EAAE,SAAS,EAAG;AAGzF,SAAO,MAAM,SAAS,EAAE,MAAK,IAAK;AACpC;;;ACxGM,SAAU,iBAAiB,KAAqB,MAAU;AAC9D,MACE,CAAC,aAAa,aAAa,aAAa,cAAc,SAAS,EAAE,SAAS,IAAI,UAAU,KAAK,QAAQ,MACpG,CAAC,IAAI,UAAU,KAAK,UAAU,CAAC,IAAI,UAAU,KAAK,OAAO,SAC1D;AACA,QAAI,KAAK,WAAW,cAAc,KAAK,qBAAqB,YAAY;AACtE,aAAO;QACL,MAAM,kBAAkB,IAAI,UAAU,KAAK,QAA0C;QACrF,UAAU;;IAEd;AAEA,WAAO;MACL,MAAM,CAAC,kBAAkB,IAAI,UAAU,KAAK,QAA0C,GAAG,MAAM;;EAEnG;AAEA,MAAI,KAAK,WAAW,YAAY;AAC9B,UAAMC,QAAO,SAAS,IAAI,UAAU,MAAM;MACxC,GAAG;MACH,aAAa,CAAC,GAAG,KAAK,WAAW;KAClC;AAED,QAAIA,SAAQ,UAAUA;AAAM,aAAO,EAAE,OAAO,CAACA,KAAI,GAAG,UAAU,KAAI;AAElE,WAAOA,SAAS,EAAE,GAAGA,OAAM,UAAU,KAAI;EAC3C;AAEA,QAAM,OAAO,SAAS,IAAI,UAAU,MAAM;IACxC,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG;GAChD;AAED,SAAO,QAAQ,EAAE,OAAO,CAAC,MAAM,EAAE,MAAM,OAAM,CAAE,EAAC;AAClD;;;AClCM,SAAU,eAAe,KAAmB,MAAU;AAC1D,QAAM,MAA6B;IACjC,MAAM;;AAGR,MAAI,CAAC,IAAI;AAAQ,WAAO;AAExB,aAAWC,UAAS,IAAI,QAAQ;AAC9B,YAAQA,OAAM,MAAM;MAClB,KAAK;AACH,YAAI,OAAO;AACX,wBAAgB,KAAK,QAAQA,OAAM,SAAS,IAAI;AAChD;MACF,KAAK;AACH,YAAI,KAAK,WAAW,eAAe;AACjC,cAAIA,OAAM,WAAW;AACnB,sCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;UAC5E,OAAO;AACL,sCAA0B,KAAK,oBAAoBA,OAAM,OAAOA,OAAM,SAAS,IAAI;UACrF;QACF,OAAO;AACL,cAAI,CAACA,OAAM,WAAW;AACpB,gBAAI,mBAAmB;UACzB;AACA,oCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;QAC5E;AACA;MACF,KAAK;AACH,YAAI,KAAK,WAAW,eAAe;AACjC,cAAIA,OAAM,WAAW;AACnB,sCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;UAC5E,OAAO;AACL,sCAA0B,KAAK,oBAAoBA,OAAM,OAAOA,OAAM,SAAS,IAAI;UACrF;QACF,OAAO;AACL,cAAI,CAACA,OAAM,WAAW;AACpB,gBAAI,mBAAmB;UACzB;AACA,oCAA0B,KAAK,WAAWA,OAAM,OAAOA,OAAM,SAAS,IAAI;QAC5E;AACA;MACF,KAAK;AACH,kCAA0B,KAAK,cAAcA,OAAM,OAAOA,OAAM,SAAS,IAAI;AAC7E;IACJ;EACF;AACA,SAAO;AACT;;;ACzDA,SAAS,2BAA2B,KAAmB,MAAU;AAC/D,MAAI,KAAK,6BAA6B,UAAU;AAC9C,WAAO,IAAI,SAAS,KAAK,aAAa,aAClC,IAAI,gBAAgB,WACpB,SAAS,IAAI,SAAS,MAAM;MAC1B,GAAG;MACH,aAAa,CAAC,GAAG,KAAK,aAAa,sBAAsB;KAC1D,KAAK;EACZ,OAAO;AACL,WAAO,IAAI,SAAS,KAAK,aAAa,aAClC,IAAI,gBAAgB,gBACpB,SAAS,IAAI,SAAS,MAAM;MAC1B,GAAG;MACH,aAAa,CAAC,GAAG,KAAK,aAAa,sBAAsB;KAC1D,KAAK;EACZ;AACF;AASM,SAAU,eAAe,KAAmB,MAAU;AAC1D,QAAM,SAAgC;IACpC,MAAM;IACN,GAAG,OAAO,QAAQ,IAAI,MAAK,CAAE,EAAE,OAC7B,CACE,KAIA,CAAC,UAAU,OAAO,MAChB;AACF,UAAI,YAAY,UAAa,QAAQ,SAAS;AAAW,eAAO;AAChE,YAAM,eAAe,CAAC,GAAG,KAAK,aAAa,cAAc,QAAQ;AACjE,YAAM,YAAY,SAAS,QAAQ,MAAM;QACvC,GAAG;QACH,aAAa;QACb;OACD;AACD,UAAI,cAAc;AAAW,eAAO;AACpC,UACE,KAAK,oBACL,QAAQ,WAAU,KAClB,CAAC,QAAQ,WAAU,KACnB,OAAO,QAAQ,MAAM,iBAAiB,aACtC;AACA,cAAM,IAAI,MACR,kBAAkB,aAAa,KAC7B,GAAG,CACJ,mMAAmM;MAExM;AACA,aAAO;QACL,YAAY;UACV,GAAG,IAAI;UACP,CAAC,QAAQ,GAAG;;QAEd,UACE,QAAQ,WAAU,KAAM,CAAC,KAAK,mBAAmB,IAAI,WAAW,CAAC,GAAG,IAAI,UAAU,QAAQ;;IAEhG,GACA,EAAE,YAAY,CAAA,GAAI,UAAU,CAAA,EAAE,CAAE;IAElC,sBAAsB,2BAA2B,KAAK,IAAI;;AAE5D,MAAI,CAAC,OAAO,SAAU;AAAQ,WAAO,OAAO;AAC5C,SAAO;AACT;;;ACvEO,IAAM,mBAAmB,CAAC,KAAqB,SAA2C;AAC/F,MACE,KAAK,gBACL,KAAK,YAAY,MAAM,GAAG,KAAK,aAAa,MAAM,EAAE,SAAQ,MAAO,KAAK,aAAa,SAAQ,GAC7F;AACA,WAAO,SAAS,IAAI,UAAU,MAAM,EAAE,GAAG,MAAM,aAAa,KAAK,YAAW,CAAE;EAChF;AAEA,QAAM,cAAc,SAAS,IAAI,UAAU,MAAM;IAC/C,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG;GAChD;AAED,SAAO,cACH;IACE,OAAO;MACL;QACE,KAAK,CAAA;;MAEP;;MAGJ,CAAA;AACN;;;ACtBO,IAAM,mBAAmB,CAC9B,KACA,SACsD;AACtD,MAAI,KAAK,iBAAiB,SAAS;AACjC,WAAO,SAAS,IAAI,GAAG,MAAM,IAAI;EACnC,WAAW,KAAK,iBAAiB,UAAU;AACzC,WAAO,SAAS,IAAI,IAAI,MAAM,IAAI;EACpC;AAEA,QAAM,IAAI,SAAS,IAAI,GAAG,MAAM;IAC9B,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG;GAChD;AACD,QAAM,IAAI,SAAS,IAAI,IAAI,MAAM;IAC/B,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,IAAI,MAAM,GAAG;GAC1D;AAED,SAAO;IACL,OAAO,CAAC,GAAG,CAAC,EAAE,OAAO,CAAC,MAA4B,MAAM,MAAS;;AAErE;;;ACvBM,SAAU,gBAAgB,KAAoB,MAAU;AAC5D,SAAO,SAAS,IAAI,KAAK,MAAM,IAAI;AACrC;;;ACQM,SAAU,YAAY,KAAgB,MAAU;AACpD,QAAM,QAAQ,SAAS,IAAI,UAAU,MAAM;IACzC,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,aAAa,OAAO;GAC3C;AAED,QAAM,SAA6B;IACjC,MAAM;IACN,aAAa;IACb;;AAGF,MAAI,IAAI,SAAS;AACf,8BAA0B,QAAQ,YAAY,IAAI,QAAQ,OAAO,IAAI,QAAQ,SAAS,IAAI;EAC5F;AAEA,MAAI,IAAI,SAAS;AACf,8BAA0B,QAAQ,YAAY,IAAI,QAAQ,OAAO,IAAI,QAAQ,SAAS,IAAI;EAC5F;AAEA,SAAO;AACT;;;AClBM,SAAU,cACd,KACA,MAAU;AAEV,MAAI,IAAI,MAAM;AACZ,WAAO;MACL,MAAM;MACN,UAAU,IAAI,MAAM;MACpB,OAAO,IAAI,MACR,IAAI,CAAC,GAAG,MACP,SAAS,EAAE,MAAM;QACf,GAAG;QACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG,CAAC,EAAE;OACnD,CAAC,EAEH,OAAO,CAAC,KAAwB,MAAO,MAAM,SAAY,MAAM,CAAC,GAAG,KAAK,CAAC,GAAI,CAAA,CAAE;MAClF,iBAAiB,SAAS,IAAI,KAAK,MAAM;QACvC,GAAG;QACH,aAAa,CAAC,GAAG,KAAK,aAAa,iBAAiB;OACrD;;EAEL,OAAO;AACL,WAAO;MACL,MAAM;MACN,UAAU,IAAI,MAAM;MACpB,UAAU,IAAI,MAAM;MACpB,OAAO,IAAI,MACR,IAAI,CAAC,GAAG,MACP,SAAS,EAAE,MAAM;QACf,GAAG;QACH,aAAa,CAAC,GAAG,KAAK,aAAa,SAAS,GAAG,CAAC,EAAE;OACnD,CAAC,EAEH,OAAO,CAAC,KAAwB,MAAO,MAAM,SAAY,MAAM,CAAC,GAAG,KAAK,CAAC,GAAI,CAAA,CAAE;;EAEtF;AACF;;;ACjDM,SAAU,oBAAiB;AAC/B,SAAO;IACL,KAAK,CAAA;;AAET;;;ACNM,SAAU,kBAAe;AAC7B,SAAO,CAAA;AACT;;;ACAO,IAAM,mBAAmB,CAAC,KAA0B,SAAc;AACvE,SAAO,SAAS,IAAI,UAAU,MAAM,IAAI;AAC1C;;;ACgEM,SAAU,SACd,KACA,MACA,kBAAkB,OAAK;AAEvB,QAAM,WAAW,KAAK,KAAK,IAAI,GAAG;AAElC,MAAI,KAAK,UAAU;AACjB,UAAM,iBAAiB,KAAK,WAAW,KAAK,MAAM,UAAU,eAAe;AAE3E,QAAI,mBAAmB,gBAAgB;AACrC,aAAO;IACT;EACF;AAEA,MAAI,YAAY,CAAC,iBAAiB;AAChC,UAAM,aAAa,QAAQ,UAAU,IAAI;AAEzC,QAAI,eAAe,QAAW;AAC5B,UAAI,UAAU,YAAY;AACxB,aAAK,SAAS,IAAI,WAAW,IAAI;MACnC;AAEA,aAAO;IACT;EACF;AAEA,QAAM,UAAgB,EAAE,KAAK,MAAM,KAAK,aAAa,YAAY,OAAS;AAE1E,OAAK,KAAK,IAAI,KAAK,OAAO;AAE1B,QAAM,aAAa,aAAa,KAAM,IAAY,UAAU,MAAM,eAAe;AAEjF,MAAI,YAAY;AACd,YAAQ,KAAK,MAAM,UAAU;EAC/B;AAEA,UAAQ,aAAa;AAErB,SAAO;AACT;AAEA,IAAM,UAAU,CACd,MACA,SAMc;AACd,UAAQ,KAAK,cAAc;IACzB,KAAK;AACH,aAAO,EAAE,MAAM,KAAK,KAAK,KAAK,GAAG,EAAC;;;;;;;;;IASpC,KAAK;AACH,YAAM,OAAO,KAAK,KAAK,MAAM,KAAK,SAAS,SAAS,CAAC,EAAE,KAAK,GAAG;AAI/D,UAAI,SAAS,KAAK,QAAQ,KAAK,iBAAiB,iBAAiB;AAC/D,aAAK,YAAY,IAAI,IAAI,KAAK;MAChC;AAEA,aAAO,EAAE,MAAM,CAAC,GAAG,KAAK,UAAU,KAAK,gBAAgB,IAAI,EAAE,KAAK,GAAG,EAAC;IACxE,KAAK;AACH,aAAO,EAAE,MAAM,gBAAgB,KAAK,aAAa,KAAK,IAAI,EAAC;IAC7D,KAAK;IACL,KAAK,QAAQ;AACX,UACE,KAAK,KAAK,SAAS,KAAK,YAAY,UACpC,KAAK,KAAK,MAAM,CAAC,OAAO,UAAU,KAAK,YAAY,KAAK,MAAM,KAAK,GACnE;AACA,gBAAQ,KAAK,mCAAmC,KAAK,YAAY,KAAK,GAAG,CAAC,qBAAqB;AAE/F,eAAO,CAAA;MACT;AAEA,aAAO,KAAK,iBAAiB,SAAS,CAAA,IAAK;IAC7C;EACF;AACF;AAEA,IAAM,kBAAkB,CAAC,OAAiB,UAAmB;AAC3D,MAAI,IAAI;AACR,SAAO,IAAI,MAAM,UAAU,IAAI,MAAM,QAAQ,KAAK;AAChD,QAAI,MAAM,CAAC,MAAM,MAAM,CAAC;AAAG;EAC7B;AACA,SAAO,EAAE,MAAM,SAAS,GAAG,SAAQ,GAAI,GAAG,MAAM,MAAM,CAAC,CAAC,EAAE,KAAK,GAAG;AACpE;AAEA,IAAM,eAAe,CACnB,KACA,UACA,MACA,oBAC+B;AAC/B,UAAQ,UAAU;IAChB,KAAK,sBAAsB;AACzB,aAAO,eAAe,KAAK,IAAI;IACjC,KAAK,sBAAsB;AACzB,aAAO,eAAe,KAAK,IAAI;IACjC,KAAK,sBAAsB;AACzB,aAAO,eAAe,KAAK,IAAI;IACjC,KAAK,sBAAsB;AACzB,aAAO,eAAe,KAAK,IAAI;IACjC,KAAK,sBAAsB;AACzB,aAAO,gBAAe;IACxB,KAAK,sBAAsB;AACzB,aAAO,aAAa,KAAK,IAAI;IAC/B,KAAK,sBAAsB;AACzB,aAAO,kBAAiB;IAC1B,KAAK,sBAAsB;AACzB,aAAO,aAAa,IAAI;IAC1B,KAAK,sBAAsB;AACzB,aAAO,cAAc,KAAK,IAAI;IAChC,KAAK,sBAAsB;IAC3B,KAAK,sBAAsB;AACzB,aAAO,cAAc,KAAK,IAAI;IAChC,KAAK,sBAAsB;AACzB,aAAO,qBAAqB,KAAK,IAAI;IACvC,KAAK,sBAAsB;AACzB,aAAO,cAAc,KAAK,IAAI;IAChC,KAAK,sBAAsB;AACzB,aAAO,eAAe,KAAK,IAAI;IACjC,KAAK,sBAAsB;AACzB,aAAO,gBAAgB,KAAK,IAAI;IAClC,KAAK,sBAAsB;AACzB,aAAO,aAAa,GAAG;IACzB,KAAK,sBAAsB;AACzB,aAAO,mBAAmB,GAAG;IAC/B,KAAK,sBAAsB;AACzB,aAAO,iBAAiB,KAAK,IAAI;IACnC,KAAK,sBAAsB;AACzB,aAAO,iBAAiB,KAAK,IAAI;IACnC,KAAK,sBAAsB;AACzB,aAAO,YAAY,KAAK,IAAI;IAC9B,KAAK,sBAAsB;AACzB,aAAO,YAAY,KAAK,IAAI;IAC9B,KAAK,sBAAsB;AACzB,aAAO,SAAS,IAAI,OAAM,EAAG,MAAM,IAAI;IACzC,KAAK,sBAAsB;AACzB,aAAO,gBAAgB,KAAK,IAAI;IAClC,KAAK,sBAAsB;IAC3B,KAAK,sBAAsB;AACzB,aAAO,cAAa;IACtB,KAAK,sBAAsB;AACzB,aAAO,gBAAgB,KAAK,MAAM,eAAe;IACnD,KAAK,sBAAsB;AACzB,aAAO,YAAW;IACpB,KAAK,sBAAsB;AACzB,aAAO,gBAAe;IACxB,KAAK,sBAAsB;AACzB,aAAO,gBAAgB,KAAK,IAAI;IAClC,KAAK,sBAAsB;AACzB,aAAO,gBAAgB,KAAK,IAAI;IAClC,KAAK,sBAAsB;AACzB,aAAO,iBAAiB,KAAK,IAAI;IACnC,KAAK,sBAAsB;AACzB,aAAO,cAAc,KAAK,IAAI;IAChC,KAAK,sBAAsB;AACzB,aAAO,iBAAiB,KAAK,IAAI;IACnC,KAAK,sBAAsB;IAC3B,KAAK,sBAAsB;IAC3B,KAAK,sBAAsB;AACzB,aAAO;IACT;AACE,aAAQ,kBAAC,MAAa,QAAW,QAAQ;EAC7C;AACF;AAEA,IAAM,UAAU,CAAC,KAAiB,MAAY,eAAgD;AAC5F,MAAI,IAAI,aAAa;AACnB,eAAW,cAAc,IAAI;AAE7B,QAAI,KAAK,qBAAqB;AAC5B,iBAAW,sBAAsB,IAAI;IACvC;EACF;AACA,SAAO;AACT;;;AC3PA,IAAM,kBAAkB,CACtB,QACA,YAQE;AACF,QAAM,OAAO,QAAQ,OAAO;AAE5B,QAAM,OACJ,OAAO,YAAY,WAAW,UAC5B,SAAS,iBAAiB,UAAU,SACpC,SAAS;AAEb,QAAM,OACJ,SACE,OAAO,MACP,SAAS,SAAY,OACnB;IACE,GAAG;IACH,aAAa,CAAC,GAAG,KAAK,UAAU,KAAK,gBAAgB,IAAI;KAG7D,KAAK,KACF,CAAA;AAEP,QAAM,QACJ,OAAO,YAAY,YAAY,QAAQ,SAAS,UAAa,QAAQ,iBAAiB,UACpF,QAAQ,OACR;AAEJ,MAAI,UAAU,QAAW;AACvB,SAAK,QAAQ;EACf;AAEA,QAAM,eAAe,MAAK;AACxB,QAAIC,YAAW,KAAK,WAAW,GAAG;AAChC,aAAO;IACT;AAEA,UAAMC,eAAmC,CAAA;AACzC,UAAM,uBAAuB,oBAAI,IAAG;AAOpC,aAAS,IAAI,GAAG,IAAI,KAAK,KAAK;AAC5B,YAAM,iBAAiB,OAAO,QAAQ,KAAK,WAAW,EAAE,OACtD,CAAC,CAAC,GAAG,MAAM,CAAC,qBAAqB,IAAI,GAAG,CAAC;AAE3C,UAAI,eAAe,WAAW;AAAG;AAEjC,iBAAW,CAAC,KAAKC,OAAM,KAAK,gBAAgB;AAC1C,QAAAD,aAAY,GAAG,IACb,SACE,OAAOC,OAAM,GACb,EAAE,GAAG,MAAM,aAAa,CAAC,GAAG,KAAK,UAAU,KAAK,gBAAgB,GAAG,EAAC,GACpE,IAAI,KACD,CAAA;AACP,6BAAqB,IAAI,GAAG;MAC9B;IACF;AAEA,WAAOD;EACT,GAAE;AAEF,QAAM,WACJ,SAAS,SACP,cACE;IACE,GAAG;IACH,CAAC,KAAK,cAAc,GAAG;MAEzB,OACF,KAAK,iBAAiB,kBACtB;IACE,GAAG;IACH,GAAI,eAAe,KAAK,SAAS,OAC/B;MACE,CAAC,KAAK,cAAc,GAAG;QACrB,GAAG;;;QAGH,GAAI,KAAK,SAAS,OAAO,EAAE,CAAC,IAAI,GAAG,KAAI,IAAK;;QAGhD;MAEJ;IACE,MAAM,CAAC,GAAI,KAAK,iBAAiB,aAAa,CAAA,IAAK,KAAK,UAAW,KAAK,gBAAgB,IAAI,EAAE,KAC5F,GAAG;IAEL,CAAC,KAAK,cAAc,GAAG;MACrB,GAAG;MACH,CAAC,IAAI,GAAG;;;AAIhB,MAAI,KAAK,WAAW,eAAe;AACjC,aAAS,UAAU;EACrB,WAAW,KAAK,WAAW,qBAAqB;AAC9C,aAAS,UAAU;EACrB;AAEA,SAAO;AACT;;;ACnHM,SAAU,mBAAmB,QAAkB;AACnD,MAAI,OAAO,SAAS,UAAU;AAC5B,UAAM,IAAI,MACR,sDAAsD,OAAO,OAAO,IAAI,OAAO,IAAI,MAAM,WAAW,EAAE;EAE1G;AAEA,QAAM,aAAa,gBAAgB,MAAM;AACzC,SAAO,uBAAuB,YAAY,CAAA,GAAI,UAAU;AAC1D;AAEA,SAAS,WAAW,QAA4B;AAC9C,MAAI,OAAO,WAAW,WAAW;AAC/B,WAAO;EACT;AACA,MAAI,OAAO,SAAS,QAAQ;AAC1B,WAAO;EACT;AACA,aAAW,gBAAgB,OAAO,SAAS,CAAA,GAAI;AAC7C,QAAI,WAAW,YAAY,GAAG;AAC5B,aAAO;IACT;EACF;AACA,aAAW,gBAAgB,OAAO,SAAS,CAAA,GAAI;AAC7C,QAAI,WAAW,YAAY,GAAG;AAC5B,aAAO;IACT;EACF;AACA,SAAO;AACT;AAMA,SAAS,uBACP,YACAE,OACA,MAAgB;AAEhB,MAAI,OAAO,eAAe,WAAW;AACnC,UAAM,IAAI,UAAU,gDAAgDA,MAAK,KAAK,GAAG,CAAC,EAAE;EACtF;AAEA,MAAI,CAAC,SAAS,UAAU,GAAG;AACzB,UAAM,IAAI,UAAU,YAAY,KAAK,UAAU,UAAU,CAAC,0BAA0BA,MAAK,KAAK,GAAG,CAAC,EAAE;EACtG;AAGA,QAAM,OAAQ,WAAmB;AACjC,MAAI,SAAS,IAAI,GAAG;AAClB,eAAW,CAAC,SAAS,SAAS,KAAK,OAAO,QAAQ,IAAI,GAAG;AACvD,6BAAuB,WAAyB,CAAC,GAAGA,OAAM,SAAS,OAAO,GAAG,IAAI;IACnF;EACF;AAGA,QAAM,cAAe,WAAmB;AACxC,MAAI,SAAS,WAAW,GAAG;AACzB,eAAW,CAAC,gBAAgB,gBAAgB,KAAK,OAAO,QAAQ,WAAW,GAAG;AAC5E,6BAAuB,kBAAgC,CAAC,GAAGA,OAAM,eAAe,cAAc,GAAG,IAAI;IACvG;EACF;AAGA,QAAM,MAAM,WAAW;AACvB,MAAI,QAAQ,YAAY,EAAE,0BAA0B,aAAa;AAC/D,eAAW,uBAAuB;EACpC;AAEA,QAAM,WAAW,WAAW,YAAY,CAAA;AAGxC,QAAM,aAAa,WAAW;AAC9B,MAAI,SAAS,UAAU,GAAG;AACxB,eAAW,CAAC,KAAK,KAAK,KAAK,OAAO,QAAQ,UAAU,GAAG;AACrD,UAAI,CAAC,WAAW,KAAK,KAAK,CAAC,SAAS,SAAS,GAAG,GAAG;AACjD,cAAM,IAAI,MACR,kBAAkB,CAAC,GAAGA,OAAM,cAAc,GAAG,EAAE,KAC7C,GAAG,CACJ,mMAAmM;MAExM;IACF;AACA,eAAW,WAAW,OAAO,KAAK,UAAU;AAC5C,eAAW,aAAa,OAAO,YAC7B,OAAO,QAAQ,UAAU,EAAE,IAAI,CAAC,CAAC,KAAK,UAAU,MAAM;MACpD;MACA,uBAAuB,YAAY,CAAC,GAAGA,OAAM,cAAc,GAAG,GAAG,IAAI;KACtE,CAAC;EAEN;AAGA,QAAM,QAAQ,WAAW;AACzB,MAAI,SAAS,KAAK,GAAG;AACnB,eAAW,QAAQ,uBAAuB,OAAO,CAAC,GAAGA,OAAM,OAAO,GAAG,IAAI;EAC3E;AAGA,QAAM,QAAQ,WAAW;AACzB,MAAI,MAAM,QAAQ,KAAK,GAAG;AACxB,eAAW,QAAQ,MAAM,IAAI,CAAC,SAAS,MACrC,uBAAuB,SAAS,CAAC,GAAGA,OAAM,SAAS,OAAO,CAAC,CAAC,GAAG,IAAI,CAAC;EAExE;AAGA,QAAM,QAAQ,WAAW;AACzB,MAAI,MAAM,QAAQ,KAAK,GAAG;AACxB,QAAI,MAAM,WAAW,GAAG;AACtB,YAAM,WAAW,uBAAuB,MAAM,CAAC,GAAI,CAAC,GAAGA,OAAM,SAAS,GAAG,GAAG,IAAI;AAChF,aAAO,OAAO,YAAY,QAAQ;AAClC,aAAO,WAAW;IACpB,OAAO;AACL,iBAAW,QAAQ,MAAM,IAAI,CAAC,OAAO,MACnC,uBAAuB,OAAO,CAAC,GAAGA,OAAM,SAAS,OAAO,CAAC,CAAC,GAAG,IAAI,CAAC;IAEtE;EACF;AAGA,MAAI,WAAW,YAAY,MAAM;AAC/B,WAAO,WAAW;EACpB;AAGA,QAAM,MAAO,WAAmB;AAChC,MAAI,OAAO,iBAAiB,YAAY,CAAC,GAAG;AAC1C,QAAI,OAAO,QAAQ,UAAU;AAC3B,YAAM,IAAI,UAAU,8BAA8B,GAAG,UAAUA,MAAK,KAAK,GAAG,CAAC,EAAE;IACjF;AAEA,UAAM,WAAWC,YAAW,MAAM,GAAG;AACrC,QAAI,OAAO,aAAa,WAAW;AACjC,YAAM,IAAI,MAAM,oBAAoB,GAAG,mDAAmD;IAC5F;AACA,QAAI,CAAC,SAAS,QAAQ,GAAG;AACvB,YAAM,IAAI,MACR,oBAAoB,GAAG,sCAAsC,KAAK,UAAU,QAAQ,CAAC,EAAE;IAE3F;AAGA,WAAO,OAAO,YAAY,EAAE,GAAG,UAAU,GAAG,WAAU,CAAE;AACxD,WAAQ,WAAmB;AAI3B,WAAO,uBAAuB,YAAYD,OAAM,IAAI;EACtD;AAEA,SAAO;AACT;AAEA,SAASC,YAAW,MAAkB,KAAW;AAC/C,MAAI,CAAC,IAAI,WAAW,IAAI,GAAG;AACzB,UAAM,IAAI,MAAM,0BAA0B,KAAK,UAAU,GAAG,CAAC,0BAA0B;EACzF;AAEA,QAAM,YAAY,IAAI,MAAM,CAAC,EAAE,MAAM,GAAG;AACxC,MAAI,WAAgB;AAEpB,aAAW,OAAO,WAAW;AAC3B,QAAI,CAAC,SAAS,QAAQ,GAAG;AACvB,YAAM,IAAI,MAAM,gDAAgD,GAAG,MAAM,KAAK,UAAU,QAAQ,CAAC,EAAE;IACrG;AACA,UAAM,QAAQ,SAAS,GAAG;AAC1B,QAAI,UAAU,QAAW;AACvB,YAAM,IAAI,MAAM,OAAO,GAAG,8BAA8B,GAAG,EAAE;IAC/D;AACA,eAAW;EACb;AAEA,SAAO;AACT;AAEA,SAAS,SAAY,KAAmB;AACtC,SAAO,OAAO,QAAQ,YAAY,QAAQ,QAAQ,CAAC,MAAM,QAAQ,GAAG;AACtE;AAEA,SAAS,iBAAiB,KAA0B,GAAS;AAC3D,MAAI,IAAI;AACR,aAAW,KAAK,KAAK;AACnB;AACA,QAAI,IAAI,GAAG;AACT,aAAO;IACT;EACF;AACA,SAAO;AACT;;;AC1KA,SAAS,kBAAkB,QAAoB,SAAyB;AACtE,SAAO,gBAAiB,QAAQ;IAC9B,kBAAkB;IAClB,MAAM,QAAQ;IACd,cAAc;IACd,cAAc;IACd,kBAAkB;GACnB;AACH;AAEA,SAAS,kBAAkB,QAAkB;AAC3C,SAAO,mBACF,aAAa,QAAQ;IACtB,QAAQ;GACT,CAAe;AAEpB;AAEA,SAAS,QAAQ,WAAkC;AACjD,SAAO,UAAU;AACnB;AAuCM,SAAU,kBACd,WACA,MACA,OAA+E;AAE/E,SAAO,4BACL;IACE,MAAM;IACN,aAAa;MACX,GAAG;MACH;MACA,QAAQ;MACR,QAAQ,QAAQ,SAAS,IAAI,kBAAkB,SAAS,IAAI,kBAAkB,WAAW,EAAE,KAAI,CAAE;;KAGrG,CAAC,YAAY,UAAU,MAAM,KAAK,MAAM,OAAO,CAAC,CAAC;AAErD;;;ACtFA,IAAM,oBAAoB;EACxB;EACA;EACA;;AAaF,SAAgB,0BACd,OACA,QACiB;AAIjB,MACE,OAAO,WAAW,eAClB,CAAC,kBAAkB,SAAS,MAAA,EAE5B,OAAM,IAAI,MACR,mBAAmB,MAAA,4BAAkC,kBAAkB,KACrE,IAAA,CACD,EAAA;AAIL,QAAM,0BACJ,CAAC,MAAM,WAAW,OAAA,KAClB,CAAC,MAAM,WAAW,QAAA,KAClB,UAAU;AAKZ,MAAI,2BAA2B,CAAC,OAC9B,QAAO;AAGT,MAAI,CAAC,2BAA2B,WAAW,aACzC,OAAM,IAAI,MACR,2CAA2C,KAAA,yEAAM;AAOrD,SAAQ,UAA8B;;AAIxC,SAASC,6BACP,iBACA,QACA;AACA,QAAM,MAAM,EAAE,GAAG,gBAAA;AAEjB,SAAO,iBAAiB,KAAK;IAC3B,QAAQ;MACN,OAAO;MACP,YAAY;;IAEd,WAAW;MACT,OAAO;MACP,YAAY;;GAEf;AAED,SAAO;;AAGT,SAAgB,yBACd,WACA,MACA,OACA;AACA,MAAI,cAAc,SAAA,EAChB,QAAO,kBAAkB,WAAW,MAAM,KAAA;AAE5C,MAAI,cAAc,SAAA,EAChB,QAAOA,6BACL;IACE,MAAM;IACN,aAAa;MACX,GAAG;MACH;MACA,QAAQ;MACR,QAAQ,aAAa,WAAW;QAC9B,QAAQ;QACR,QAAQ;QACR,SAAS,KAAK;AACZ,cAAI,WAAW,QAAQ;;OAU1B;;KAEJ,CACA,YAAYC,MAAQ,WAAW,KAAK,MAAM,OAAA,CAAQ,CAAC;AAGxD,QAAM,IAAI,MAAM,oCAAA;;AAUlB,SAAgB,uBACd,SACA,UACyB;AAKzB,MACE,YACA,OAAO,aAAa,YACpB,YAAY,YACZ,MAAM,QAAQ,SAAS,MAAA,GACvB;AACA,UAAM,SAAS,SAAS,OACrB,OAAA,CAAQ,UAAU,OAAO,OAAO,WAAW,QAAQ,QAAA,EACnD,IAAA,CACE,WACE;MACC,MAAM;MACN,KAAK,MAAM,UAAU;MACtB;AAEP,WAAO,CAAC;MAAE,MAAM;MAAQ,MAAM;OAAW,GAAG,MAAA;;AAG9C,SAAO;;;;AC/JT,IAAM,WAAyC;EAC7C,qBAAqB;IACnB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,iBAAiB;IACf,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,eAAe;IACb,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,aAAa;IACX,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,eAAe;IACb,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,0BAA0B;IACxB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,qBAAqB;IACnB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,qBAAqB;IACnB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,qBAAqB;IACnB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,qBAAqB;IACnB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,uBAAuB;IACrB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,iBAAiB;IACf,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,oBAAoB;IAClB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,IAAI;IACF,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,WAAW;IACT,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,yBAAyB;IACvB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,uBAAuB;IACrB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,IAAI;IACF,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,0BAA0B;IACxB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,gBAAgB;IACd,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,0BAA0B;IACxB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,iBAAiB;IACf,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,sBAAsB;IACpB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,WAAW;IACT,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,WAAW;IACT,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,UAAU;IACR,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,eAAe;IACb,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,SAAS;IACP,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,WAAW;IACT,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,gBAAgB;IACd,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,cAAc;IACZ,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,UAAU;IACR,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,iBAAiB;IACf,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,eAAe;IACb,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,WAAW;IACT,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,qBAAqB;IACnB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,cAAc;IACZ,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,uBAAuB;IACrB,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,SAAS;IACP,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,cAAc;IACZ,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,WAAW;IACT,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;EAEd,UAAU;IACR,gBAAgB;IAChB,aAAa;IACb,aAAa;IACb,WAAW;IACX,aAAa;IACb,iBAAiB;IACjB,iBAAiB;IACjB,cAAc;IACd,cAAc;IACd,cAAc;IACd,aAAa;IACb,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;;;;;ACxuBhB,IAAI,YAAY,mBAAmB,MAAM,EAAA;AACzC,IAAI,QAAQ;EAAC;EAAa;EAAS;EAAO;;AAC1C,IAAI,QAAQ;EAAC;EAAI;EAAI;EAAG;;AACxB,IAAI,IAAI;EACN;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;EAAY;EAAY;EAC5D;EAAY;EAAY;EAAY;;AAItC,IAAI,SAAS,CAAA;AAEb,SAAS,OAAO,OAAO,cAAc;AACnC,MAAI,cAAc;AAChB,WAAO,CAAA,IACL,OAAO,EAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,CAAA,IACP,OAAO,EAAA,IACP,OAAO,EAAA,IACP,OAAO,EAAA,IACP,OAAO,EAAA,IACP,OAAO,EAAA,IACP,OAAO,EAAA,IACL;AACJ,SAAK,SAAS;QAEd,MAAK,SAAS;IAAC;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;IAAG;;AAGjE,MAAI,OAAO;AACT,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;SACL;AAEL,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;AACV,SAAK,KAAK;;AAGZ,OAAK,QAAQ,KAAK,QAAQ,KAAK,QAAQ,KAAK,SAAS;AACrD,OAAK,YAAY,KAAK,SAAS;AAC/B,OAAK,QAAQ;AACb,OAAK,QAAQ;;AAGf,OAAO,UAAU,SAAS,SAAU,SAAS;AAC3C,MAAI,KAAK,UACP;AAEF,MAAI,WACF,OAAO,OAAO;AAChB,MAAI,SAAS,UAAU;AACrB,QAAI,SAAS,UACX;UAAI,YAAY,KACd,OAAM,IAAI,MAAM,KAAA;eACP,gBAAgB,QAAQ,gBAAgB,YACjD,WAAU,IAAI,WAAW,OAAA;eAChB,CAAC,MAAM,QAAQ,OAAA,GACxB;YAAI,CAAC,gBAAgB,CAAC,YAAY,OAAO,OAAA,EACvC,OAAM,IAAI,MAAM,KAAA;;UAIpB,OAAM,IAAI,MAAM,KAAA;AAElB,gBAAY;;AAEd,MAAI,MACF,QAAQ,GACR,GACA,SAAS,QAAQ,QACjBC,UAAS,KAAK;AAChB,SAAO,QAAQ,QAAQ;AACrB,QAAI,KAAK,QAAQ;AACf,WAAK,SAAS;AACd,MAAAA,QAAO,CAAA,IAAK,KAAK;AACjB,WAAK,QACHA,QAAO,EAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACL;;AAGN,QAAI,UACF,MAAK,IAAI,KAAK,OAAO,QAAQ,UAAU,IAAI,IAAI,EAAE,MAC/C,CAAAA,QAAO,MAAM,CAAA,KAAM,QAAQ,KAAA,KAAU,MAAM,MAAM,CAAA;QAGnD,MAAK,IAAI,KAAK,OAAO,QAAQ,UAAU,IAAI,IAAI,EAAE,OAAO;AACtD,aAAO,QAAQ,WAAW,KAAA;AAC1B,UAAI,OAAO,IACT,CAAAA,QAAO,MAAM,CAAA,KAAM,QAAQ,MAAM,MAAM,CAAA;eAC9B,OAAO,MAAO;AACvB,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAQ,SAAS,MAAO,MAAM,MAAM,CAAA;AACxD,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAQ,OAAO,OAAU,MAAM,MAAM,CAAA;iBAChD,OAAO,SAAU,QAAQ,OAAQ;AAC1C,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAQ,SAAS,OAAQ,MAAM,MAAM,CAAA;AACzD,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAS,SAAS,IAAK,OAAU,MAAM,MAAM,CAAA;AACjE,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAQ,OAAO,OAAU,MAAM,MAAM,CAAA;aACpD;AACL,eACE,UACG,OAAO,SAAU,KAAO,QAAQ,WAAW,EAAE,KAAA,IAAS;AAC3D,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAQ,SAAS,OAAQ,MAAM,MAAM,CAAA;AACzD,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAS,SAAS,KAAM,OAAU,MAAM,MAAM,CAAA;AAClE,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAS,SAAS,IAAK,OAAU,MAAM,MAAM,CAAA;AACjE,QAAAA,QAAO,MAAM,CAAA,MAAO,MAAQ,OAAO,OAAU,MAAM,MAAM,CAAA;;;AAK/D,SAAK,gBAAgB;AACrB,SAAK,SAAS,IAAI,KAAK;AACvB,QAAI,KAAK,IAAI;AACX,WAAK,QAAQA,QAAO,EAAA;AACpB,WAAK,QAAQ,IAAI;AACjB,WAAK,KAAA;AACL,WAAK,SAAS;UAEd,MAAK,QAAQ;;AAGjB,MAAI,KAAK,QAAQ,YAAY;AAC3B,SAAK,UAAW,KAAK,QAAQ,cAAe;AAC5C,SAAK,QAAQ,KAAK,QAAQ;;AAE5B,SAAO;;AAGT,OAAO,UAAU,WAAW,WAAY;AACtC,MAAI,KAAK,UACP;AAEF,OAAK,YAAY;AACjB,MAAIA,UAAS,KAAK,QAChB,IAAI,KAAK;AACX,EAAAA,QAAO,EAAA,IAAM,KAAK;AAClB,EAAAA,QAAO,MAAM,CAAA,KAAM,MAAM,IAAI,CAAA;AAC7B,OAAK,QAAQA,QAAO,EAAA;AACpB,MAAI,KAAK,IAAI;AACX,QAAI,CAAC,KAAK,OACR,MAAK,KAAA;AAEP,IAAAA,QAAO,CAAA,IAAK,KAAK;AACjB,IAAAA,QAAO,EAAA,IACLA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,CAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACPA,QAAO,EAAA,IACL;;AAEN,EAAAA,QAAO,EAAA,IAAO,KAAK,UAAU,IAAM,KAAK,UAAU;AAClD,EAAAA,QAAO,EAAA,IAAM,KAAK,SAAS;AAC3B,OAAK,KAAA;;AAGP,OAAO,UAAU,OAAO,WAAY;AAClC,MAAI,IAAI,KAAK,IACX,IAAI,KAAK,IACT,IAAI,KAAK,IACT,IAAI,KAAK,IACT,IAAI,KAAK,IACT,IAAI,KAAK,IACT,IAAI,KAAK,IACT,IAAI,KAAK,IACTA,UAAS,KAAK,QACd,GACA,IACA,IACA,KACA,IACA,IACA,IACA,IACA,IACA,IACA;AAEF,OAAK,IAAI,IAAI,IAAI,IAAI,EAAE,GAAG;AAExB,SAAKA,QAAO,IAAI,EAAA;AAChB,UAAO,OAAO,IAAM,MAAM,OAAS,OAAO,KAAO,MAAM,MAAQ,OAAO;AACtE,SAAKA,QAAO,IAAI,CAAA;AAChB,UAAO,OAAO,KAAO,MAAM,OAAS,OAAO,KAAO,MAAM,MAAQ,OAAO;AACvE,IAAAA,QAAO,CAAA,IAAMA,QAAO,IAAI,EAAA,IAAM,KAAKA,QAAO,IAAI,CAAA,IAAK,MAAO;;AAG5D,OAAK,IAAI;AACT,OAAK,IAAI,GAAG,IAAI,IAAI,KAAK,GAAG;AAC1B,QAAI,KAAK,OAAO;AACd,UAAI,KAAK,OAAO;AACd,aAAK;AACL,aAAKA,QAAO,CAAA,IAAK;AACjB,YAAK,KAAK,aAAc;AACxB,YAAK,KAAK,YAAa;aAClB;AACL,aAAK;AACL,aAAKA,QAAO,CAAA,IAAK;AACjB,YAAK,KAAK,cAAe;AACzB,YAAK,KAAK,aAAc;;AAE1B,WAAK,QAAQ;WACR;AACL,YACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,YACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,WAAK,IAAI;AACT,YAAM,KAAM,IAAI,IAAK;AACrB,WAAM,IAAI,IAAM,CAAC,IAAI;AACrB,WAAK,IAAI,KAAK,KAAK,EAAE,CAAA,IAAKA,QAAO,CAAA;AACjC,WAAK,KAAK;AACV,UAAK,IAAI,MAAO;AAChB,UAAK,KAAK,MAAO;;AAEnB,UACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,UACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,SAAK,IAAI;AACT,UAAM,KAAM,IAAI,IAAK;AACrB,SAAM,IAAI,IAAM,CAAC,IAAI;AACrB,SAAK,IAAI,KAAK,KAAK,EAAE,IAAI,CAAA,IAAKA,QAAO,IAAI,CAAA;AACzC,SAAK,KAAK;AACV,QAAK,IAAI,MAAO;AAChB,QAAK,KAAK,MAAO;AACjB,UACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,UACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,SAAK,IAAI;AACT,UAAM,KAAM,IAAI,IAAK;AACrB,SAAM,IAAI,IAAM,CAAC,IAAI;AACrB,SAAK,IAAI,KAAK,KAAK,EAAE,IAAI,CAAA,IAAKA,QAAO,IAAI,CAAA;AACzC,SAAK,KAAK;AACV,QAAK,IAAI,MAAO;AAChB,QAAK,KAAK,MAAO;AACjB,UACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,UACI,MAAM,IAAM,KAAK,OACjB,MAAM,KAAO,KAAK,OAClB,MAAM,KAAO,KAAK;AACtB,SAAK,IAAI;AACT,UAAM,KAAM,IAAI,IAAK;AACrB,SAAM,IAAI,IAAM,CAAC,IAAI;AACrB,SAAK,IAAI,KAAK,KAAK,EAAE,IAAI,CAAA,IAAKA,QAAO,IAAI,CAAA;AACzC,SAAK,KAAK;AACV,QAAK,IAAI,MAAO;AAChB,QAAK,KAAK,MAAO;AACjB,SAAK,sBAAsB;;AAG7B,OAAK,KAAM,KAAK,KAAK,KAAM;AAC3B,OAAK,KAAM,KAAK,KAAK,KAAM;AAC3B,OAAK,KAAM,KAAK,KAAK,KAAM;AAC3B,OAAK,KAAM,KAAK,KAAK,KAAM;AAC3B,OAAK,KAAM,KAAK,KAAK,KAAM;AAC3B,OAAK,KAAM,KAAK,KAAK,KAAM;AAC3B,OAAK,KAAM,KAAK,KAAK,KAAM;AAC3B,OAAK,KAAM,KAAK,KAAK,KAAM;;AAG7B,OAAO,UAAU,MAAM,WAAY;AACjC,OAAK,SAAA;AAEL,MAAI,KAAK,KAAK,IACZ,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK;AAEZ,MAAIC,OACF,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA,IACf,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA,IACf,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA,IACf,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA,IACf,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA,IACf,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA,IACf,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA;AACjB,MAAI,CAAC,KAAK,MACR,CAAAA,QACE,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,KAAM,EAAA,IACxB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAW,OAAO,IAAK,EAAA,IACvB,UAAU,KAAK,EAAA;AAEnB,SAAOA;;AAGT,OAAO,UAAU,WAAW,OAAO,UAAU;AAE7C,OAAO,UAAU,SAAS,WAAY;AACpC,OAAK,SAAA;AAEL,MAAI,KAAK,KAAK,IACZ,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK,IACV,KAAK,KAAK;AAEZ,MAAI,MAAM;IACP,OAAO,KAAM;IACb,OAAO,KAAM;IACb,OAAO,IAAK;IACb,KAAK;IACJ,OAAO,KAAM;IACb,OAAO,KAAM;IACb,OAAO,IAAK;IACb,KAAK;IACJ,OAAO,KAAM;IACb,OAAO,KAAM;IACb,OAAO,IAAK;IACb,KAAK;IACJ,OAAO,KAAM;IACb,OAAO,KAAM;IACb,OAAO,IAAK;IACb,KAAK;IACJ,OAAO,KAAM;IACb,OAAO,KAAM;IACb,OAAO,IAAK;IACb,KAAK;IACJ,OAAO,KAAM;IACb,OAAO,KAAM;IACb,OAAO,IAAK;IACb,KAAK;IACJ,OAAO,KAAM;IACb,OAAO,KAAM;IACb,OAAO,IAAK;IACb,KAAK;;AAEP,MAAI,CAAC,KAAK,MACR,KAAI,KACD,OAAO,KAAM,KACb,OAAO,KAAM,KACb,OAAO,IAAK,KACb,KAAK,GAAA;AAGT,SAAO;;AAGT,OAAO,UAAU,QAAQ,OAAO,UAAU;AAE1C,OAAO,UAAU,cAAc,WAAY;AACzC,OAAK,SAAA;AAEL,MAAI,SAAS,IAAI,YAAY,KAAK,QAAQ,KAAK,EAAA;AAC/C,MAAI,WAAW,IAAI,SAAS,MAAA;AAC5B,WAAS,UAAU,GAAG,KAAK,EAAA;AAC3B,WAAS,UAAU,GAAG,KAAK,EAAA;AAC3B,WAAS,UAAU,GAAG,KAAK,EAAA;AAC3B,WAAS,UAAU,IAAI,KAAK,EAAA;AAC5B,WAAS,UAAU,IAAI,KAAK,EAAA;AAC5B,WAAS,UAAU,IAAI,KAAK,EAAA;AAC5B,WAAS,UAAU,IAAI,KAAK,EAAA;AAC5B,MAAI,CAAC,KAAK,MACR,UAAS,UAAU,IAAI,KAAK,EAAA;AAE9B,SAAO;;AAGT,IAAa,SAAA,IAAa,YAAsB;AAC9C,SAAO,IAAI,OAAO,OAAO,IAAA,EAAM,OAAO,QAAQ,KAAK,EAAA,CAAG,EAAE,IAAA;;;;AClf1D,IAAI,eAA+B,YAAY,EAAE,QAAQ,MAAM,OAAO,CAAC;;;;;;;;;;ACCvE,IAAa,wBAAA,IAA4C,YACvD,OAAO,QAAQ,KAAK,GAAA,CAAI;AAE1B,SAAgB,4BACd,kBACA;AACA,MAAI,iBAAiB,YAAY,OAC/B,QAAO;IACL,MAAM,iBAAiB;IACvB,SAAS,8BAA8B,iBAAiB,OAAA;;MAG1D,QAAO,EAAE,MAAM,iBAAiB,KAAA;;AAIpC,SAAgB,oBAAoB,YAAwB;AAC1D,QAAM,kBAAoC,EACxC,MAAM,WAAW,KAAA;AAEnB,MAAK,WAA8B,YAAY,OAC7C,iBAAgB,UAAW,WAA8B,QAAQ,OAAA;AAEnE,SAAO;;AAMT,IAAsB,YAAtB,MAAkD;EACtC,aAA6B;;;;;;;EAQvC,sBAAsB,cAAoC;AACxD,SAAK,aAAa;;;AAQtB,IAAM,aAAa,oBAAI,IAAA;AAKvB,IAAa,gBAAb,MAAaC,uBAAwC,UAAa;EACxD;EAER,YAAYC,MAAsB;AAChC,UAAA;AACA,SAAK,QAAQA,QAAO,oBAAI,IAAA;;;;;;;;;EAU1B,OAAO,QAAgB,QAAmC;AACxD,WAAO,QAAQ,QACb,KAAK,MAAM,IAAI,KAAK,WAAW,QAAQ,MAAA,CAAO,KAAK,IAAA;;;;;;;;EAUvD,MAAM,OAAO,QAAgB,QAAgB,OAAyB;AACpE,SAAK,MAAM,IAAI,KAAK,WAAW,QAAQ,MAAA,GAAS,KAAA;;;;;;;EAQlD,OAAO,SAAwB;AAC7B,WAAO,IAAID,eAAc,UAAA;;;;;;;;;;;ACxE7B,IAAsB,kBAAtB,cACU,aAEV;AAAA;AAUA,IAAa,oBAAb,cACU,gBAEV;EACE,OAAO,UAAkB;AACvB,WAAO;;EAGT,eAAe,CAAC,kBAAkB,eAAA;EAElC,kBAAkB;EAElB;EAEA,YAAY,OAAe;AACzB,UAAM,EAAE,MAAA,CAAO;AACf,SAAK,QAAQ;;EAGf,WAAW;AACT,WAAO,KAAK;;EAGd,iBAAiB;AACf,WAAO,CAAC,IAAI,aAAa,KAAK,KAAA,CAAM;;;AAexC,IAAa,kBAAb,cACU,gBAEV;EACE,eAAe,CAAC,kBAAkB,eAAA;EAElC,kBAAkB;EAElB,OAAO,UAAU;AACf,WAAO;;EAGT;EAMA,YAAY,QAA+C;AACzD,QAAI,MAAM,QAAQ,MAAA,EAEhB,UAAS,EAAE,UAAU,OAAA;AAGvB,UAAM,MAAA;AACN,SAAK,WAAW,OAAO;;EAGzB,WAAW;AACT,WAAO,gBAAgB,KAAK,QAAA;;EAG9B,iBAAiB;AACf,WAAO,KAAK;;;AAoBhB,IAAa,mBAAb,cAAsC,gBAAgB;EACpD,eAAe,CAAC,kBAAkB,eAAA;EAElC,kBAAkB;EAElB,OAAO,UAAU;AACf,WAAO;;EAGT;;EAGA;EAMA,YAAY,QAA+C;AACzD,QAAI,EAAE,cAAc,QAElB,UAAS,EAAE,UAAU,OAAA;AAGvB,UAAM,MAAA;AACN,SAAK,WAAW,OAAO;;EAGzB,WAAW;AACT,WAAO,KAAK,SAAS;;EAGvB,iBAAiB;AACf,WAAO,CACL,IAAI,aAAa,EACf,SAAS,CACP;MACE,MAAM;MACN,WAAW;QACT,QAAQ,KAAK,SAAS;QACtB,KAAK,KAAK,SAAS;;KAEtB,EACF,CACF,CAAC;;;;;AC3KR,uBAAmB;AAEnB,IAAI,YAAY,OAAO;AACvB,IAAI,kBAAkB,CAAC,KAAK,KAAK,UAAU,OAAO,MAAM,UAAU,KAAK,KAAK,EAAE,YAAY,MAAM,cAAc,MAAM,UAAU,MAAM,MAAM,CAAC,IAAI,IAAI,GAAG,IAAI;AAC1J,IAAI,gBAAgB,CAAC,KAAK,KAAK,UAAU;AACvC,kBAAgB,KAAK,OAAO,QAAQ,WAAW,MAAM,KAAK,KAAK,KAAK;AACpE,SAAO;AACT;AAKA,SAAS,cAAc,OAAO,OAAO;AACnC,MAAI,QAAQ,MAAM;AAAA,IAChB,EAAE,QAAQ,MAAM,OAAO;AAAA,IACvB,CAAC,GAAG,OAAO,EAAE,OAAO,GAAG,KAAK,IAAI,EAAE;AAAA,EACpC;AACA,SAAO,MAAM,SAAS,GAAG;AACvB,QAAI,UAAU;AACd,aAAS,IAAI,GAAG,IAAI,MAAM,SAAS,GAAG,KAAK;AACzC,YAAM,QAAQ,MAAM,MAAM,MAAM,CAAC,EAAE,OAAO,MAAM,IAAI,CAAC,EAAE,GAAG;AAC1D,YAAM,OAAO,MAAM,IAAI,MAAM,KAAK,GAAG,CAAC;AACtC,UAAI,QAAQ;AACV;AACF,UAAI,WAAW,QAAQ,OAAO,QAAQ,CAAC,GAAG;AACxC,kBAAU,CAAC,MAAM,CAAC;AAAA,MACpB;AAAA,IACF;AACA,QAAI,WAAW,MAAM;AACnB,YAAM,IAAI,QAAQ,CAAC;AACnB,YAAM,CAAC,IAAI,EAAE,OAAO,MAAM,CAAC,EAAE,OAAO,KAAK,MAAM,IAAI,CAAC,EAAE,IAAI;AAC1D,YAAM,OAAO,IAAI,GAAG,CAAC;AAAA,IACvB,OAAO;AACL;AAAA,IACF;AAAA,EACF;AACA,SAAO;AACT;AACA,SAAS,eAAe,OAAO,OAAO;AACpC,MAAI,MAAM,WAAW;AACnB,WAAO,CAAC,MAAM,IAAI,MAAM,KAAK,GAAG,CAAC,CAAC;AACpC,SAAO,cAAc,OAAO,KAAK,EAAE,IAAI,CAAC,MAAM,MAAM,IAAI,MAAM,MAAM,EAAE,OAAO,EAAE,GAAG,EAAE,KAAK,GAAG,CAAC,CAAC,EAAE,OAAO,CAAC,MAAM,KAAK,IAAI;AACzH;AACA,SAAS,YAAYE,MAAK;AACxB,SAAOA,KAAI,QAAQ,uBAAuB,MAAM;AAClD;AACA,IAAI,YAAY,MAAM;AAAA;AAAA,EAEpB;AAAA;AAAA,EAEA;AAAA;AAAA,EAEA;AAAA;AAAA,EAEA,cAAc,IAAI,YAAY;AAAA;AAAA,EAE9B,cAAc,IAAI,YAAY,OAAO;AAAA;AAAA,EAErC,UAA0B,oBAAI,IAAI;AAAA;AAAA,EAElC,UAA0B,oBAAI,IAAI;AAAA,EAClC,YAAY,OAAO,uBAAuB;AACxC,SAAK,SAAS,MAAM;AACpB,UAAM,eAAe,MAAM,UAAU,MAAM,IAAI,EAAE,OAAO,OAAO,EAAE,OAAO,CAAC,MAAM,MAAM;AACnF,YAAM,CAAC,GAAG,WAAW,GAAG,MAAM,IAAI,EAAE,MAAM,GAAG;AAC7C,YAAM,SAAS,OAAO,SAAS,WAAW,EAAE;AAC5C,aAAO,QAAQ,CAAC,OAAO,MAAM,KAAK,KAAK,IAAI,SAAS,CAAC;AACrD,aAAO;AAAA,IACT,GAAG,CAAC,CAAC;AACL,eAAW,CAAC,OAAO,IAAI,KAAK,OAAO,QAAQ,YAAY,GAAG;AACxD,YAAM,QAAQ,iBAAAC,QAAO,YAAY,KAAK;AACtC,WAAK,QAAQ,IAAI,MAAM,KAAK,GAAG,GAAG,IAAI;AACtC,WAAK,QAAQ,IAAI,MAAM,KAAK;AAAA,IAC9B;AACA,SAAK,gBAAgB,EAAE,GAAG,MAAM,gBAAgB,GAAG,sBAAsB;AACzE,SAAK,uBAAuB,OAAO,QAAQ,KAAK,aAAa,EAAE,OAAO,CAAC,MAAM,CAAC,MAAM,IAAI,MAAM;AAC5F,WAAK,IAAI,IAAI,KAAK,YAAY,OAAO,IAAI;AACzC,aAAO;AAAA,IACT,GAAG,CAAC,CAAC;AAAA,EACP;AAAA,EACA,OAAO,MAAM,iBAAiB,CAAC,GAAG,oBAAoB,OAAO;AAC3D,UAAM,UAAU,IAAI,OAAO,KAAK,QAAQ,IAAI;AAC5C,UAAM,eAAe,UAAU;AAAA,MAC7B,OAAO,KAAK,KAAK,aAAa;AAAA,IAChC;AACA,UAAM,MAAM,CAAC;AACb,UAAM,oBAAoB,IAAI;AAAA,MAC5B,mBAAmB,QAAQ,OAAO,KAAK,KAAK,aAAa,IAAI;AAAA,IAC/D;AACA,UAAM,uBAAuB,IAAI;AAAA,MAC/B,sBAAsB,QAAQ,OAAO,KAAK,KAAK,aAAa,EAAE;AAAA,QAC5D,CAAC,MAAM,CAAC,kBAAkB,IAAI,CAAC;AAAA,MACjC,IAAI;AAAA,IACN;AACA,QAAI,qBAAqB,OAAO,GAAG;AACjC,YAAM,yBAAyB,UAAU,kBAAkB;AAAA,QACzD,GAAG;AAAA,MACL,CAAC;AACD,YAAM,eAAe,KAAK,MAAM,sBAAsB;AACtD,UAAI,gBAAgB,MAAM;AACxB,cAAM,IAAI;AAAA,UACR,0DAA0D,aAAa,CAAC,CAAC;AAAA,QAC3E;AAAA,MACF;AAAA,IACF;AACA,QAAI,QAAQ;AACZ,WAAO,MAAM;AACX,UAAI,cAAc;AAClB,UAAI,YAAY;AAChB,aAAO,MAAM;AACX,qBAAa,YAAY;AACzB,sBAAc,aAAa,KAAK,IAAI;AACpC,YAAI,eAAe,QAAQ,kBAAkB,IAAI,YAAY,CAAC,CAAC;AAC7D;AACF,oBAAY,YAAY,QAAQ;AAAA,MAClC;AACA,YAAM,MAAM,aAAa,SAAS,KAAK;AACvC,iBAAW,SAAS,KAAK,UAAU,OAAO,GAAG,EAAE,SAAS,OAAO,GAAG;AAChE,cAAM,QAAQ,KAAK,YAAY,OAAO,MAAM,CAAC,CAAC;AAC9C,cAAM,SAAS,KAAK,QAAQ,IAAI,MAAM,KAAK,GAAG,CAAC;AAC/C,YAAI,UAAU,MAAM;AAClB,cAAI,KAAK,MAAM;AACf;AAAA,QACF;AACA,YAAI,KAAK,GAAG,eAAe,OAAO,KAAK,OAAO,CAAC;AAAA,MACjD;AACA,UAAI,eAAe;AACjB;AACF,UAAI,QAAQ,KAAK,cAAc,YAAY,CAAC,CAAC;AAC7C,UAAI,KAAK,KAAK;AACd,cAAQ,YAAY,QAAQ,YAAY,CAAC,EAAE;AAAA,IAC7C;AACA,WAAO;AAAA,EACT;AAAA,EACA,OAAO,QAAQ;AACb,UAAM,MAAM,CAAC;AACb,QAAI,SAAS;AACb,aAAS,KAAK,GAAG,KAAK,OAAO,QAAQ,EAAE,IAAI;AACzC,YAAM,QAAQ,OAAO,EAAE;AACvB,YAAM,QAAQ,KAAK,QAAQ,IAAI,KAAK,KAAK,KAAK,qBAAqB,KAAK;AACxE,UAAI,SAAS,MAAM;AACjB,YAAI,KAAK,KAAK;AACd,kBAAU,MAAM;AAAA,MAClB;AAAA,IACF;AACA,UAAM,cAAc,IAAI,WAAW,MAAM;AACzC,QAAI,IAAI;AACR,eAAW,SAAS,KAAK;AACvB,kBAAY,IAAI,OAAO,CAAC;AACxB,WAAK,MAAM;AAAA,IACb;AACA,WAAO,KAAK,YAAY,OAAO,WAAW;AAAA,EAC5C;AACF;AACA,IAAI,WAAW;AACf,cAAc,UAAU,qBAAqB,CAAC,WAAW;AACvD,SAAO,IAAI,OAAO,OAAO,IAAI,CAAC,MAAM,YAAY,CAAC,CAAC,EAAE,KAAK,GAAG,GAAG,GAAG;AACpE,CAAC;AACD,SAAS,wBAAwB,OAAO;AACtC,UAAQ,OAAO;AAAA,IACb,KAAK,QAAQ;AACX,aAAO;AAAA,IACT;AAAA,IACA,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK,oBAAoB;AACvB,aAAO;AAAA,IACT;AAAA,IACA,KAAK;AAAA,IACL,KAAK,yBAAyB;AAC5B,aAAO;AAAA,IACT;AAAA,IACA,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK,+BAA+B;AAClC,aAAO;AAAA,IACT;AAAA,IACA,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK,0BAA0B;AAC7B,aAAO;AAAA,IACT;AAAA,IACA,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK,qBAAqB;AACxB,aAAO;AAAA,IACT;AAAA,IACA;AACE,YAAM,IAAI,MAAM,eAAe;AAAA,EACnC;AACF;;;;;;;ACjRA,IAAM,QAA2C,CAAA;AAEjD,IAAM,SAAyB,IAAI,YAAY,CAAA,CAAE;AAEjD,eAAsB,YAAY,UAA4B;AAC5D,MAAI,EAAE,YAAY,OAChB,OAAM,QAAA,IAAY,OACf,MAAM,iCAAiC,QAAA,OAAS,EAChD,KAAA,CAAM,QAAQ,IAAI,KAAA,CAAM,EACxB,KAAA,CAAM,SAAS,IAAI,SAAS,IAAA,CAAK,EACjC,MAAA,CAAO,MAAM;AACZ,WAAO,MAAM,QAAA;AACb,UAAM;;AAIZ,SAAO,MAAM,MAAM,QAAA;;AAGrB,eAAsB,iBAAiB,OAAsB;AAC3D,SAAO,YAAY,wBAAwB,KAAA,CAAM;;;;;;;;;;;;;ACInD,IAAa,0BAAA,CAA2B,cAAqC;AAC3E,MAAI,UAAU,WAAW,OAAA,EACvB,QAAO;AAGT,MAAI,UAAU,WAAW,mBAAA,EACvB,QAAO;AAGT,MAAI,UAAU,WAAW,gBAAA,EACvB,QAAO;AAGT,MAAI,UAAU,WAAW,WAAA,EACvB,QAAO;AAGT,MAAI,UAAU,WAAW,QAAA,EACvB,QAAO;AAGT,MAAI,UAAU,WAAW,QAAA,EACvB,QAAO;AAGT,SAAO;;AAGT,IAAa,0BAAA,CAA2B,cAA+B;AACrE,UAAQ,WAAR;IACE,KAAK;AACH,aAAO;IACT;AACE,aAAO;;;AAeb,IAAa,sBAAA,CAAuB,cAA8B;AAGhE,UAFuB,wBAAwB,SAAA,GAE/C;IAEE,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IACT,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;AACH,aAAO;IACT,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IACT,KAAK;IACL,KAAK;AACH,aAAO;IAGT,KAAK;IACL,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO;IACT,KAAK;IACL,KAAK;AACH,aAAO;IAET;AACE,aAAO;;;AASb,SAAgB,aAAaC,OAAuC;AAClE,MAAI,OAAOA,UAAS,YAAY,CAACA,MAAM,QAAO;AAC9C,MACE,UAAUA,SACVA,MAAK,SAAS,cACd,cAAcA,SACd,OAAOA,MAAK,aAAa,YACzBA,MAAK,YACL,UAAUA,MAAK,YACf,gBAAgBA,MAAK,SAErB,QAAO;AAET,SAAO;;AAQT,IAAa,qBAAqB,OAAO,EACvC,QACA,UAAA,MAC4B;AAC5B,MAAI;AAEJ,MAAI;AACF,iBACE,MAAM,iBAAiB,wBAAwB,SAAA,CAAU,GACzD,OAAO,MAAA,EAAQ;UACX;AACN,YAAQ,KACN,yEAAA;AAKF,gBAAY,KAAK,KAAK,OAAO,SAAS,CAAA;;AAIxC,SADkB,oBAAoB,SAAA,IACnB;;AAGrB,IAAM,eAAA,MAAqB;AAkB3B,IAAsB,gBAAtB,cAKU,SAEV;;;;EAIE;EAEA;EAEA;EAEA;EAEA,IAAI,gBAA0D;AAC5D,WAAO;MACL,WAAW;MACX,SAAS;;;EAIb,YAAY,QAA6B;AACvC,UAAM,MAAA;AACN,SAAK,UAAU,OAAO,WAAW,aAAA;AACjC,SAAK,YAAY,OAAO;AACxB,SAAK,OAAO,OAAO,QAAQ,CAAA;AAC3B,SAAK,WAAW,OAAO,YAAY,CAAA;;;AAyJvC,IAAsB,oBAAtB,cAMU,cAIV;;;;EAIE,IAAI,WAAqB;AACvB,WAAO;MAAC;MAAQ;MAAW;MAAU;MAAQ;MAAY;;;;;;;EAO3D;EAEA;EAEA,YAAY,EACV,WACA,iBACA,GAAG,OAAA,GACuB;AAC1B,UAAM,EAAE,OAAAC,QAAO,GAAG,KAAA,IAAS;AAC3B,UAAM;MACJ,WAAW,aAAa;MACxB,GAAG;KACJ;AACD,QAAI,OAAOA,WAAU,SACnB,MAAK,QAAQA;aACJA,OACT,MAAK,QAAQ,cAAc,OAAA;QAE3B,MAAK,QAAQ;AAEf,SAAK,SAAS,IAAI,YAAY,UAAU,CAAA,CAAE;;EAapC;;;;;;EAOR,MAAM,aAAa,SAAyB;AAE1C,QAAI;AACJ,QAAI,OAAO,YAAY,SACrB,eAAc;;AASd,oBAAc,QACX,IAAA,CAAK,SAAS;AACb,YAAI,OAAO,SAAS,SAAU,QAAO;AACrC,YAAI,KAAK,SAAS,UAAU,UAAU,KAAM,QAAO,KAAK;AACxD,eAAO;SAER,KAAK,EAAA;AAIV,QAAI,YAAY,KAAK,KAAK,YAAY,SAAS,CAAA;AAE/C,QAAI,CAAC,KAAK,UACR,KAAI;AACF,WAAK,YAAY,MAAM,iBACrB,eAAe,OACX,wBAAwB,KAAK,SAAA,IAC7B,MAAA;aAEC,OAAO;AACd,cAAQ,KACN,2EACA,KAAA;;AAKN,QAAI,KAAK,UACP,KAAI;AACF,kBAAY,KAAK,UAAU,OAAO,WAAA,EAAa;aACxC,OAAO;AACd,cAAQ,KACN,2EACA,KAAA;;AAKN,WAAO;;EAGT,OAAiB,2BACf,OAC0B;AAC1B,QAAI,OAAO,UAAU,SACnB,QAAO,IAAI,kBAAkB,KAAA;aACpB,MAAM,QAAQ,KAAA,EACvB,QAAO,IAAI,gBAAgB,MAAM,IAAI,0BAAA,CAA2B;QAEhE,QAAO;;;;;EAQX,qBAA0C;AACxC,WAAO,CAAA;;;;;;;EAQT,wCAEE,EAAE,QAAAC,SAAQ,GAAG,YAAA,GACL;AAER,UAAM,SAA8B;MAClC,GAAG,KAAK,mBAAA;MACR,GAAG;MACH,OAAO,KAAK,SAAA;MACZ,QAAQ,KAAK,WAAA;;AASf,WAPwB,OAAO,QAAQ,MAAA,EAAQ,OAAA,CAC5C,CAAC,GAAG,KAAA,MAAW,UAAU,MAAA,EAGzB,IAAA,CAAK,CAAC,KAAK,KAAA,MAAW,GAAG,GAAA,IAAO,KAAK,UAAU,KAAA,CAAM,EAAA,EACrD,KAAA,EACA,KAAK,GAAA;;;;;;EAQV,YAA2B;AACzB,WAAO;MACL,GAAG,KAAK,mBAAA;MACR,OAAO,KAAK,SAAA;MACZ,QAAQ,KAAK,WAAA;;;;;;;EAQjB,aAAa,YAAY,OAAkD;AACzE,UAAM,IAAI,MAAM,uBAAA;;;;;;;EAQlB,IAAI,UAAwB;AAC1B,WAAO,CAAA;;;;;ACzjBX,IAAa,sBAAb,cAAyD,SAGvD;EACA,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe,CAAC,kBAAkB,WAAA;EAElC,kBAAkB;EAElB;EAEA,YAAY,QAAuD;AACjE,UAAM,MAAA;AACN,QAAI,OACF,MAAK,OAAO,OAAO;;EAIvB,MAAM,OACJ,OACA,SACmB;AACnB,UAAMC,UAAS,aAAa,OAAA;AAC5B,QAAI,KAAK,KACP,OAAM,KAAK,KAAK,OAAOA,OAAA;AAGzB,WAAO,KAAK,gBAAA,CACTC,WAAoB,QAAQ,QAAQA,MAAA,GACrC,OACAD,OAAA;;EAIJ,OAAO,UACL,WACA,SAC0B;AAC1B,UAAMA,UAAS,aAAa,OAAA;AAC5B,QAAI;AACJ,QAAI,uBAAuB;AAE3B,qBAAiB,SAAS,KAAK,2BAC7B,WAAA,CACC,UAAoC,OACrCA,OAAA,GACC;AACD,YAAM;AACN,UAAI,qBACF,KAAI,gBAAgB,OAClB,eAAc;UAEd,KAAI;AAEF,sBAAc,OAAO,aAAa,KAAA;cAC5B;AACN,sBAAc;AACd,+BAAuB;;;AAM/B,QAAI,KAAK,QAAQ,gBAAgB,OAC/B,OAAM,KAAK,KAAK,aAAaA,OAAA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAgCjC,OAAO,OAIL,SACgD;AAChD,WAAO,IAAI,eAAe,IAAI,YAAY,EAAE,OAAO,QAAA,CAAS,CAAC;;;;;AChJjE,IAAaE,QAAA,CAAW,OAAmB,GAAA;AAE3C,SAAS,2BAAkD,SAAY;AACrE,QAAM,MAAM,QAAQ;AACpB,SAAO,IAAI,IAAI;IACb,GAAG;IACH,SAAS,QAAQ;IACjB,mBAAmB;MACjB,GAAG,QAAQ;MACX,gBAAgB;;GAEnB;;;;;;;;AC8IH,SAAS,kBAAkB,UAAwC;AACjE,QAAM,kBAAiC,CAAA;AACvC,aAAW,WAAW,UAAU;AAC9B,QAAI,iBAAiB;AACrB,QAAI,MAAM,QAAQ,QAAQ,OAAA,EACxB,UAAS,MAAM,GAAG,MAAM,QAAQ,QAAQ,QAAQ,OAAO;AACrD,YAAM,QAAQ,QAAQ,QAAQ,GAAA;AAC9B,UAAI,kBAAkB,KAAA,KAAU,qBAAqB,KAAA,GACnD;YAAI,mBAAmB,QAGrB,kBAAiB,IAAK,QAAQ,YAAoB;UAChD,GAAG;UACH,SAAS;YACP,GAAG,QAAQ,QAAQ,MAAM,GAAG,GAAA;YAC5B,0BAA0B,KAAA;YAC1B,GAAG,QAAQ,QAAQ,MAAM,MAAM,CAAA;;SAElC;;;AAKT,oBAAgB,KAAK,cAAA;;AAEvB,SAAO;;AAwBT,IAAsB,gBAAtB,MAAsBC,uBAIZ,kBAAkD;EAQ1D,eAAe;IAAC;IAAa;IAAe,KAAK,SAAA;;EAEjD,mBAAmB;EAEnB;EAEA,IAAI,WAAqB;AACvB,WAAO,CAAC,GAAG,MAAM,UAAU,eAAA;;EAG7B,YAAY,QAA6B;AACvC,UAAM,MAAA;AACN,SAAK,gBAAgBC,MAAA,MAAW;AAC9B,YAAM,gBACJ,OAAO,iBAAiB,uBAAuB,mBAAA;AACjD,UAAI,iBAAiB,CAAC,MAAM,IAAA,EAAM,SAAS,aAAA,EACzC,QAAO;AAET,aAAO;;;EAQD,6CACR,SAC6C;AAE7C,UAAM,CAAC,gBAAgB,WAAA,IACrB,MAAM,uCAAuC,OAAA;AAC9C,gBAA0C,SAAS,eAAe;AACnE,WAAO,CAAC,gBAAgB,WAAA;;;;;;;;EAsB1B,MAAM,OACJ,OACA,SAC4B;AAC5B,UAAM,cAAcD,eAAc,2BAA2B,KAAA;AAQ7D,YAPe,MAAM,KAAK,eACxB,CAAC,WAAA,GACD,SACA,SAAS,SAAA,GAEmB,YAAY,CAAA,EAAG,CAAA,EAEvB;;EAIxB,OAAO,sBACL,WACA,UACA,aACqC;AACrC,UAAM,IAAI,MAAM,kBAAA;;EAGlB,OAAO,gBACL,OACA,SACmC;AAEnC,QACE,KAAK,0BACHA,eAAc,UAAU,yBAC1B,KAAK,iBAEL,OAAM,KAAK,OAAO,OAAO,OAAA;SACpB;AAEL,YAAM,WADSA,eAAc,2BAA2B,KAAA,EAChC,eAAA;AACxB,YAAM,CAAC,gBAAgB,WAAA,IACrB,KAAK,6CAA6C,OAAA;AAEpD,YAAM,sBAAsB;QAC1B,GAAG,eAAe;QAClB,GAAG,KAAK,YAAY,WAAA;;AAEtB,YAAM,mBAAmB,MAAM,gBAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,qBACA,KAAK,UACL,EAAE,SAAS,KAAK,QAAA,CAAS;AAE3B,YAAM,QAAQ;QACZ,SAAS;QACT,mBAAmB,MAAM,iBAAiB,WAAA;QAC1C,YAAY;;AAEd,YAAM,gBAAgB,YAAY,iBAAiB,KAAK;AACxD,YAAM,cAAc,MAAM,kBAAkB,qBAC1C,KAAK,OAAA,GACL,CAAC,kBAAkB,QAAA,CAAS,GAC5B,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,OAAA;AAEjB,UAAI;AAEJ,UAAI;AACJ,UAAI;AACF,yBAAiB,SAAS,KAAK,sBAC7B,UACA,aACA,cAAc,CAAA,CAAA,GACb;AACD,sBAAY,QAAQ,eAAA;AACpB,cAAI,MAAM,QAAQ,MAAM,MAAM;AAC5B,kBAAM,QAAQ,aAAa,GAAG,CAAA,GAAI;AAClC,gBAAI,SAAS,KAAM,OAAM,QAAQ,UAAU,OAAO,KAAA,EAAA;;AAEpD,gBAAM,QAAQ,oBAAoB;YAChC,GAAG,MAAM;YACT,GAAG,MAAM,QAAQ;;AAEnB,cAAI,kBAAkB,KACpB,OAAM,2BACJ,MAAM,OAAA;cAGR,OAAM,MAAM;AAEd,cAAI,CAAC,gBACH,mBAAkB;cAElB,mBAAkB,gBAAgB,OAAO,KAAA;AAE3C,cACE,iBAAiB,MAAM,OAAA,KACvB,MAAM,QAAQ,mBAAmB,OAEjC,aAAY,EACV,YAAY;YACV,cAAc,MAAM,QAAQ,eAAe;YAC3C,kBAAkB,MAAM,QAAQ,eAAe;YAC/C,aAAa,MAAM,QAAQ,eAAe;YAC3C;;AAKP,oBAAY,QAAQ,eAAA;eACb,KAAK;AACZ,cAAM,QAAQ,KACX,eAAe,CAAA,GAAI,IAAA,CAAK,eACvB,YAAY,eAAe,GAAA,CAAI,CAChC;AAEH,cAAM;;AAER,YAAM,QAAQ,KACX,eAAe,CAAA,GAAI,IAAA,CAAK,eACvB,YAAY,aAAa;QAEvB,aAAa,CAAC,CAAC,eAAA,CAAkC;QACjD;OACD,CAAC,CACH;;;EAKP,YAAY,SAAqD;AAC/D,UAAM,eAAe,KAAK,QAAA,EAAU,WAAW,MAAA,IAC3C,KAAK,QAAA,EAAU,QAAQ,QAAQ,EAAA,IAC/B,KAAK,QAAA;AAET,WAAO;MACL,eAAe;MACf,SAAS,QAAQ;MACjB,aAAa;;;;EAKjB,MAAM,kBACJ,UACA,eACA,gBACA,oBACoB;AACpB,UAAM,eAAe,SAAS,IAAA,CAAK,gBACjC,YAAY,IAAI,0BAAA,CAA2B;AAG7C,QAAI;AACJ,QACE,uBAAuB,UACvB,mBAAmB,WAAW,aAAa,OAE3C,eAAc;SACT;AACL,YAAM,sBAAsB;QAC1B,GAAG,eAAe;QAClB,GAAG,KAAK,YAAY,aAAA;;AAGtB,YAAM,mBAAmB,MAAM,gBAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,qBACA,KAAK,UACL,EAAE,SAAS,KAAK,QAAA,CAAS;AAE3B,YAAM,QAAQ;QACZ,SAAS;QACT,mBAAmB,MAAM,iBAAiB,aAAA;QAC1C,YAAY;;AAEd,oBAAc,MAAM,kBAAkB,qBACpC,KAAK,OAAA,GACL,aAAa,IAAI,iBAAA,GACjB,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,OAAA;;AAGnB,UAAM,gBAAgB,cAAc,iBAAiB,KAAK;AAC1D,UAAM,cAAkC,CAAA;AACxC,UAAM,aAAuC,CAAA;AAO7C,QAH4B,CAAC,CAAC,cAAc,CAAA,EAAG,SAAS,KACtD,+BAAA,KAIA,CAAC,KAAK,oBACN,aAAa,WAAW,KACxB,KAAK,0BACHA,eAAc,UAAU,sBAE1B,KAAI;AACF,YAAM,SAAS,MAAM,KAAK,sBACxB,aAAa,CAAA,GACb,eACA,cAAc,CAAA,CAAA;AAEhB,UAAI;AAEJ,UAAI;AACJ,uBAAiB,SAAS,QAAQ;AAEhC,YAAI,cAAc,QAAQ,SAAS;AACjC,gBAAM,iBAAiB,YAAY;AAGnC,gBAAM,IAAI,gBACR,iCACA,cAAA;;AAGJ,YAAI,MAAM,QAAQ,MAAM,MAAM;AAC5B,gBAAM,QAAQ,aAAa,GAAG,CAAA,GAAI;AAClC,cAAI,SAAS,KAAM,OAAM,QAAQ,UAAU,OAAO,KAAA,EAAA;;AAEpD,YAAI,eAAe,OACjB,cAAa;YAEb,cAAa,OAAO,YAAY,KAAA;AAElC,YACE,iBAAiB,MAAM,OAAA,KACvB,MAAM,QAAQ,mBAAmB,OAEjC,aAAY,EACV,YAAY;UACV,cAAc,MAAM,QAAQ,eAAe;UAC3C,kBAAkB,MAAM,QAAQ,eAAe;UAC/C,aAAa,MAAM,QAAQ,eAAe;UAC3C;;AAKP,UAAI,cAAc,QAAQ,SAAS;AACjC,cAAM,iBAAiB,YAAY;AAGnC,cAAM,IAAI,gBACR,iCACA,cAAA;;AAGJ,UAAI,eAAe,OACjB,OAAM,IAAI,MAAM,+CAAA;AAElB,kBAAY,KAAK,CAAC,UAAA,CAAW;AAC7B,YAAM,cAAc,CAAA,EAAG,aAAa;QAClC;QACA;OACD;aACM,GAAG;AACV,YAAM,cAAc,CAAA,EAAG,eAAe,CAAA;AACtC,YAAM;;SAEH;AAEL,YAAM,UAAU,MAAM,QAAQ,WAC5B,aAAa,IAAI,OAAO,aAAa,MAAM;AACzC,cAAM,kBAAkB,MAAM,KAAK,UACjC,aACA;UAAE,GAAG;UAAe,aAAa;WACjC,cAAc,CAAA,CAAA;AAEhB,YAAI,kBAAkB,KACpB,YAAW,cAAc,gBAAgB,YACvC,YAAW,UAAU,2BACnB,WAAW,OAAA;AAIjB,eAAO;QACP;AAGJ,YAAM,QAAQ,IACZ,QAAQ,IAAI,OAAO,SAAS,MAAM;AAChC,YAAI,QAAQ,WAAW,aAAa;AAClC,gBAAM,SAAS,QAAQ;AACvB,qBAAW,cAAc,OAAO,aAAa;AAC3C,gBAAI,WAAW,QAAQ,MAAM,MAAM;AACjC,oBAAM,QAAQ,aAAa,GAAG,CAAA,GAAI;AAClC,kBAAI,SAAS,KAAM,YAAW,QAAQ,UAAU,OAAO,KAAA,EAAA;;AAEzD,uBAAW,QAAQ,oBAAoB;cACrC,GAAG,WAAW;cACd,GAAG,WAAW,QAAQ;;;AAG1B,cAAI,OAAO,YAAY,WAAW,EAChC,QAAO,YAAY,CAAA,EAAG,QAAQ,oBAAoB;YAChD,GAAG,OAAO;YACV,GAAG,OAAO,YAAY,CAAA,EAAG,QAAQ;;AAGrC,sBAAY,CAAA,IAAK,OAAO;AACxB,qBAAW,CAAA,IAAK,OAAO;AACvB,iBAAO,cAAc,CAAA,GAAI,aAAa;YACpC,aAAa,CAAC,OAAO,WAAA;YACrB,WAAW,OAAO;WACnB;eACI;AAEL,gBAAM,cAAc,CAAA,GAAI,eAAe,QAAQ,MAAA;AAC/C,iBAAO,QAAQ,OAAO,QAAQ,MAAA;;QAEhC;;AAIN,UAAM,SAAoB;MACxB;MACA,WAAW,WAAW,SAClB,KAAK,oBAAoB,GAAG,UAAA,IAC5B;;AAEN,WAAO,eAAe,QAAQ,SAAS;MACrC,OAAO,cACH,EAAE,QAAQ,aAAa,IAAA,CAAK,YAAY,QAAQ,KAAA,EAAM,IACtD;MACJ,cAAc;KACf;AACD,WAAO;;EAGT,MAAM,gBAAgB,EACpB,UACA,OAAAE,QACA,cACA,eACA,eAAA,GAaA;AACA,UAAM,eAAe,SAAS,IAAA,CAAK,gBACjC,YAAY,IAAI,0BAAA,CAA2B;AAG7C,UAAM,sBAAsB;MAC1B,GAAG,eAAe;MAClB,GAAG,KAAK,YAAY,aAAA;;AAGtB,UAAM,mBAAmB,MAAM,gBAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,qBACA,KAAK,UACL,EAAE,SAAS,KAAK,QAAA,CAAS;AAE3B,UAAM,QAAQ;MACZ,SAAS;MACT,mBAAmB,MAAM,iBAAiB,aAAA;MAC1C,YAAY;;AAEd,UAAM,cAAc,MAAM,kBAAkB,qBAC1C,KAAK,OAAA,GACL,aAAa,IAAI,iBAAA,GACjB,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,OAAA;AAIjB,UAAM,uBAAiC,CAAA;AAkBvC,UAAM,iBAjBU,MAAM,QAAQ,WAC5B,aAAa,IAAI,OAAO,aAAa,UAAU;AAE7C,YAAM,SACJF,eAAc,2BAA2B,WAAA,EAAa,SAAA;AACxD,YAAM,SAAS,MAAME,OAAM,OAAO,QAAQ,YAAA;AAE1C,UAAI,UAAU,KACZ,sBAAqB,KAAK,KAAA;AAG5B,aAAO;MACP,GAMD,IAAA,CAAK,QAAQ,WAAW;MAAE;MAAQ,YAAY,cAAc,KAAA;MAAQ,EACpE,OAAA,CACE,EAAE,OAAA,MACA,OAAO,WAAW,eAAe,OAAO,SAAS,QAClD,OAAO,WAAW,UAAA;AAIxB,UAAM,gBAAgB,cAAc,iBAAiB,KAAK;AAC1D,UAAM,cAA8B,CAAA;AACpC,UAAM,QAAQ,IACZ,cAAc,IAAI,OAAO,EAAE,QAAQ,eAAe,WAAA,GAAc,MAAM;AACpE,UAAI,cAAc,WAAW,aAAa;AACxC,cAAM,SAAS,cAAc;AAC7B,oBAAY,CAAA,IAAK,OAAO,IAAA,CAAKC,YAAW;AACtC,cACE,aAAaA,WACb,cAAcA,QAAO,OAAA,KACrB,YAAYA,QAAO,OAAA,GACnB;AACA,YAAAA,QAAO,QAAQ,iBAAiB;cAC9B,cAAc;cACd,eAAe;cACf,cAAc;;AAEhB,gBAAI,kBAAkB,KACpB,CAAAA,QAAO,UAAU,2BAA2BA,QAAO,OAAA;;AAGvD,UAAAA,QAAO,iBAAiB;YACtB,GAAGA,QAAO;YACV,YAAY,CAAA;;AAEd,iBAAOA;;AAET,YAAI,OAAO,OACT,OAAM,YAAY,kBAAkB,OAAO,CAAA,EAAG,IAAA;AAEhD,eAAO,YAAY,aACjB,EACE,aAAa,CAAC,MAAA,EAAO,GAEvB,QACA,QACA,QACA,EACE,QAAQ,KAAA,CACT;aAEE;AAEL,cAAM,YAAY,eAChB,cAAc,QACd,QACA,QACA,QACA,EACE,QAAQ,KAAA,CACT;AAEH,eAAO,QAAQ,OAAO,cAAc,MAAA;;MAEtC;AAGJ,UAAM,SAAS;MACb;MACA;MACA,oBAAoB;;AAMtB,WAAO,eAAe,QAAQ,SAAS;MACrC,OAAO,cACH,EAAE,QAAQ,aAAa,IAAA,CAAK,YAAY,QAAQ,KAAA,EAAM,IACtD;MACJ,cAAc;KACf;AAED,WAAO;;;;;;;;;EAUT,MAAM,SACJ,UACA,SACA,WACoB;AAEpB,QAAI;AACJ,QAAI,MAAM,QAAQ,OAAA,EAChB,iBAAgB,EAAE,MAAM,QAAA;QAExB,iBAAgB;AAGlB,UAAM,eAAe,SAAS,IAAA,CAAK,gBACjC,YAAY,IAAI,0BAAA,CAA2B;AAG7C,UAAM,CAAC,gBAAgB,WAAA,IACrB,KAAK,6CAA6C,aAAA;AACpD,mBAAe,YAAY,eAAe,aAAa;AAEvD,QAAI,CAAC,KAAK,MACR,QAAO,KAAK,kBAAkB,cAAc,aAAa,cAAA;AAG3D,UAAM,EAAE,OAAAD,OAAA,IAAU;AAClB,UAAM,eAAe,KAAK,wCACxB,WAAA;AAGF,UAAM,EAAE,aAAa,sBAAsB,mBAAA,IACzC,MAAM,KAAK,gBAAgB;MACzB,UAAU;MACV,OAAAA;MACA;MACA,eAAe;MACf,gBAAgB;KACjB;AAEH,QAAI,YAAY,CAAA;AAChB,QAAI,qBAAqB,SAAS,GAAG;AACnC,YAAM,UAAU,MAAM,KAAK,kBACzB,qBAAqB,IAAA,CAAK,MAAM,aAAa,CAAA,CAAA,GAC7C,aACA,gBACA,uBAAuB,SACnB,qBAAqB,IAAA,CAAK,MAAM,qBAAqB,CAAA,CAAA,IACrD,MAAA;AAEN,YAAM,QAAQ,IACZ,QAAQ,YAAY,IAAI,OAAO,YAAY,UAAU;AACnD,cAAM,cAAc,qBAAqB,KAAA;AACzC,oBAAY,WAAA,IAAe;AAE3B,cAAM,SAASF,eAAc,2BAC3B,aAAa,WAAA,CAAA,EACb,SAAA;AACF,eAAOE,OAAM,OAAO,QAAQ,cAAc,UAAA;QAC1C;AAEJ,kBAAY,QAAQ,aAAa,CAAA;;AAGnC,WAAO;MAAE;MAAa;;;;;;EAOxB,iBAAiB,UAA2C;AAC1D,WAAO,CAAA;;EAGT,aAAqB;AACnB,WAAO;;;;;;;;;EAYT,MAAM,eACJ,cACA,SACA,WACoB;AACpB,UAAM,iBAAkC,aAAa,IAAA,CAAK,gBACxD,YAAY,eAAA,CAAgB;AAE9B,WAAO,KAAK,SAAS,gBAAgB,SAAS,SAAA;;EAqDhD,qBAIE,cAIAE,SASI;AACJ,QAAI,OAAO,KAAK,cAAc,WAC5B,OAAM,IAAI,MACR,uEAAA;AAGJ,QAAIA,SAAQ,OACV,OAAM,IAAI,MACR,2DAAA;AAIJ,UAAM,SACJ;AACF,UAAM,OAAOA,SAAQ;AACrB,UAAM,cACJ,qBAAqB,MAAA,KAAW;AAClC,UAAM,SAASA,SAAQ;AACvB,UAAM,aAAaA,SAAQ;AAC3B,QAAI,WAAW,WACb,OAAM,IAAI,MACR,uFAAA;AAIJ,QAAI,eAAe,QAAQ;AAC3B,QAAIC;AACJ,QAAI,mBAAmB,MAAA,EACrB,CAAAA,SAAQ,CACN;MACE,MAAM;MACN,UAAU;QACR,MAAM;QACN;QACA,YAAY,aAAa,MAAA;;KAE5B;SAEE;AACL,UAAI,UAAU,OACZ,gBAAe,OAAO;AAExB,MAAAA,SAAQ,CACN;QACE,MAAM;QACN,UAAU;UACR,MAAM;UACN;UACA,YAAY;;OAEf;;AAIL,UAAM,MAAM,KAAK,UAAUA,MAAA;AAC3B,UAAM,eAAe,eAAe,KAAA,CACjC,UAAuC;AACtC,UAAI,CAAC,eAAe,WAAW,KAAA,EAC7B,OAAM,IAAI,MAAM,iCAAA;AAElB,UAAI,CAAC,MAAM,cAAc,MAAM,WAAW,WAAW,EACnD,OAAM,IAAI,MAAM,sCAAA;AAElB,YAAM,WAAW,MAAM,WAAW,KAAA,CAC/B,OAAO,GAAG,SAAS,YAAA;AAEtB,UAAI,CAAC,SACH,OAAM,IAAI,MAAM,gCAAgC,YAAA,GAAa;AAE/D,aAAO,SAAS;;AAIpB,QAAI,CAAC,WACH,QAAO,IAAI,KAAK,YAAA,EAAc,WAAW,EACvC,SAAS,mBAAA,CACV;AAGH,UAAM,eAAe,oBAAoB,OAAO,EAE9C,QAAA,CAAS,OAAYD,YAAW,aAAa,OAAO,MAAM,KAAKA,OAAA,EAAO,CACvE;AACD,UAAM,aAAa,oBAAoB,OAAO,EAC5C,QAAA,MAAc,KAAA,CACf;AACD,UAAM,qBAAqB,aAAa,cAAc,EACpD,WAAW,CAAC,UAAA,EAAW,CACxB;AACD,WAAO,iBAAiB,KAGtB,CACA,EACE,KAAK,IAAA,GAEP,kBAAA,CACD,EAAE,WAAW,EACZ,SAAS,2BAAA,CACV;;;AAQL,IAAsB,kBAAtB,cAEU,cAA2B;EAOnC,MAAM,UACJ,UACA,SACA,YACqB;AAErB,UAAM,UAAU,IAAI,UADP,MAAM,KAAK,MAAM,UAAU,SAAS,UAAA,CAAW;AAE5D,QAAI,OAAO,QAAQ,YAAY,SAC7B,OAAM,IAAI,MACR,uEAAA;AAGJ,WAAO,EACL,aAAa,CACX;MACE,MAAM,QAAQ;MACd;KACD,EACF;;;;;ACphCP,IAAa,iBAAb,cAIU,SAA8B;EACtC,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe,CAAC,kBAAkB,WAAA;EAElC,kBAAkB;EAElB;EAEA,YAAY,QAET;AACD,UAAM,MAAA;AACN,SAAK,YAAY,OAAO;;EAG1B,MAAM,OACJ,OACA,SACoB;AACpB,UAAM,EAAE,KAAK,OAAO,YAAA,IAAgB;AACpC,UAAM,WAAW,KAAK,UAAU,GAAA;AAChC,QAAI,aAAa,OACf,OAAM,IAAI,MAAM,oCAAoC,GAAA,IAAI;AAE1D,WAAO,SAAS,OAAO,aAAa,aAAa,OAAA,CAAQ;;EAqB3D,MAAM,MACJ,QACA,SACA,cACgC;AAChC,UAAM,OAAO,OAAO,IAAA,CAAK,UAAU,MAAM,GAAA;AACzC,UAAM,eAAe,OAAO,IAAA,CAAK,UAAU,MAAM,KAAA;AAEjD,QADmB,KAAK,KAAA,CAAM,QAAQ,KAAK,UAAU,GAAA,MAAS,MAAA,MAC3C,OACjB,OAAM,IAAI,MAAM,wDAAA;AAElB,UAAM,YAAY,KAAK,IAAA,CAAK,QAAQ,KAAK,UAAU,GAAA,CAAA;AACnD,UAAM,cAAc,KAAK,gBAAgB,WAAW,CAAA,GAAI,OAAO,MAAA;AAC/D,UAAM,iBACJ,YAAY,CAAA,GAAI,kBAAkB,cAAc;AAClD,UAAM,YACJ,kBAAkB,iBAAiB,IAAI,iBAAiB,OAAO;AACjE,UAAM,eAAe,CAAA;AACrB,aAAS,IAAI,GAAG,IAAI,aAAa,QAAQ,KAAK,WAAW;AACvD,YAAM,gBAAgB,aACnB,MAAM,GAAG,IAAI,SAAA,EACb,IAAA,CAAK,aAAaE,OACjB,UAAUA,EAAA,EAAG,OAAO,aAAa,YAAYA,EAAA,CAAA,CAAG;AAEpD,YAAM,cAAc,MAAM,QAAQ,IAAI,aAAA;AACtC,mBAAa,KAAK,WAAA;;AAEpB,WAAO,aAAa,KAAA;;EAGtB,MAAM,OACJ,OACA,SAC4C;AAC5C,UAAM,EAAE,KAAK,OAAO,YAAA,IAAgB;AACpC,UAAM,WAAW,KAAK,UAAU,GAAA;AAChC,QAAI,aAAa,OACf,OAAM,IAAI,MAAM,oCAAoC,GAAA,IAAI;AAE1D,WAAO,SAAS,OAAO,aAAa,OAAA;;;;;AC9DxC,IAAa,iBAAb,cAAqE,SAGnE;EACA,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe,CAAC,kBAAkB,WAAA;EAElC,kBAAkB;EAElB;EAEA;EAEA,YAAY,QAGT;AACD,UAAM,MAAA;AACN,SAAK,WAAW,OAAO;AACvB,SAAK,UAAU,OAAO;;;;;;;;;;;;;;;;;;;;;;;;;;;EA6BxB,OAAO,KACL,UAIA;AACA,QAAI,SAAS,SAAS,EACpB,OAAM,IAAI,MAAM,6CAAA;AAMlB,UAAM,kBAJc,SAAS,MAAM,GAAG,EAAA,EAI6B,IAAA,CAChE,CAAC,WAAW,QAAA,MAAc,CACzB,kBAAkB,SAAA,GAClB,kBAAkB,QAAA,CAAS,CAC5B;AAEH,UAAM,gBAAgB,kBACpB,SAAS,SAAS,SAAS,CAAA,CAAA;AAE7B,WAAO,IAAI,KAAK;MACd,UAAU;MACV,SAAS;KACV;;EAGH,MAAM,QACJ,OACAC,SACA,YACoB;AACpB,QAAI;AACJ,aAAS,IAAI,GAAG,IAAI,KAAK,SAAS,QAAQ,KAAK,GAAG;AAChD,YAAM,CAAC,WAAW,cAAA,IAAkB,KAAK,SAAS,CAAA;AAOlD,UANuB,MAAM,UAAU,OACrC,OACA,YAAYA,SAAQ,EAClB,WAAW,YAAY,SAAS,aAAa,IAAI,CAAA,EAAA,EAAI,CACtD,CAAC,GAEgB;AAClB,iBAAS,MAAM,eAAe,OAC5B,OACA,YAAYA,SAAQ,EAClB,WAAW,YAAY,SAAS,UAAU,IAAI,CAAA,EAAA,EAAI,CACnD,CAAC;AAEJ;;;AAGJ,QAAI,CAAC,OACH,UAAS,MAAM,KAAK,QAAQ,OAC1B,OACA,YAAYA,SAAQ,EAClB,WAAW,YAAY,SAAS,gBAAA,EAAiB,CAClD,CAAC;AAGN,WAAO;;EAGT,MAAM,OACJ,OACAA,UAAyB,CAAA,GACL;AACpB,WAAO,KAAK,gBAAgB,KAAK,SAAS,OAAOA,OAAA;;EAGnD,OAAO,gBAAgB,OAAiBA,SAAkC;AAExE,UAAM,aAAa,OADM,MAAM,4BAA4BA,OAAA,IAChB,iBACzC,KAAK,OAAA,GACL,cAAc,OAAO,OAAA,GACrBA,SAAQ,OACR,QACA,QACA,QACAA,SAAQ,OAAA;AAEV,QAAI;AACJ,QAAI,uBAAuB;AAC3B,QAAI;AACJ,QAAI;AACF,eAAS,IAAI,GAAG,IAAI,KAAK,SAAS,QAAQ,KAAK,GAAG;AAChD,cAAM,CAAC,WAAW,cAAA,IAAkB,KAAK,SAAS,CAAA;AAOlD,YANuB,MAAM,UAAU,OACrC,OACA,YAAYA,SAAQ,EAClB,WAAW,YAAY,SAAS,aAAa,IAAI,CAAA,EAAA,EAAI,CACtD,CAAC,GAEgB;AAClB,mBAAS,MAAM,eAAe,OAC5B,OACA,YAAYA,SAAQ,EAClB,WAAW,YAAY,SAAS,UAAU,IAAI,CAAA,EAAA,EAAI,CACnD,CAAC;AAEJ,2BAAiB,SAAS,QAAQ;AAChC,kBAAM;AACN,gBAAI,qBACF,KAAI,gBAAgB,OAClB,eAAc;gBAEd,KAAI;AACF,4BAAc,OAAO,aAAa,KAAA;oBAC5B;AACN,4BAAc;AACd,qCAAuB;;;AAK/B;;;AAGJ,UAAI,WAAW,QAAW;AACxB,iBAAS,MAAM,KAAK,QAAQ,OAC1B,OACA,YAAYA,SAAQ,EAClB,WAAW,YAAY,SAAS,gBAAA,EAAiB,CAClD,CAAC;AAEJ,yBAAiB,SAAS,QAAQ;AAChC,gBAAM;AACN,cAAI,qBACF,KAAI,gBAAgB,OAClB,eAAc;cAEd,KAAI;AACF,0BAAc,OAAO,aAAa,KAAA;kBAC5B;AACN,0BAAc;AACd,mCAAuB;;;;aAM1B,GAAG;AACV,YAAM,YAAY,iBAAiB,CAAA;AACnC,YAAM;;AAER,UAAM,YAAY,eAAe,eAAe,CAAA,CAAE;;;;;AC9JtD,IAAa,6BAAb,cAGU,gBAAqC;EAC7C;EAEA;EAEA;EAEA;EAEA;EAEA,YAAY,QAA+D;AACzE,QAAI,eAAyB,eAAe,KAAA,CAAM,OAAO,YACvD,KAAK,cAAc,OAAO,WAAW,CAAA,CAAE,CAAC,EACxC,WAAW,EAAE,SAAS,cAAA,CAAe;AAEvC,UAAM,cAAc,OAAO,sBAAsB,OAAO;AACxD,QAAI,YACF,gBAAe,oBAAoB,OAAO,EAAA,CACvC,WAAA,GAAc,aAAA,CAChB,EAAE,WAAW,EAAE,SAAS,gBAAA,CAAiB;AAG5C,UAAM,QAAQ,aACX,KACC,OAAO,SAAS,cAAc,EAC5B,OAAA,CAAQ,KAAKC,YAAW,KAAK,aAAa,KAAKA,WAAU,CAAA,CAAE,EAAC,CAC7D,CAAC,EAEH,WAAW,EAAE,SAAS,6BAAA,CAA8B;AAEvD,UAAMA,UAAS,OAAO,UAAU,CAAA;AAEhC,UAAM;MACJ,GAAG;MACH,QAAAA;MACA;KACD;AACD,SAAK,WAAW,OAAO;AACvB,SAAK,oBAAoB,OAAO;AAChC,SAAK,mBAAmB,OAAO;AAC/B,SAAK,oBAAoB,OAAO;AAChC,SAAK,qBAAqB,OAAO;;EAGnC,kBAEE,YACoB;AACpB,QAAI;AACJ,QACE,OAAO,eAAe,YACtB,CAAC,MAAM,QAAQ,UAAA,KACf,CAAC,cAAc,UAAA,GACf;AACA,UAAI;AACJ,UAAI,KAAK,iBACP,OAAM,KAAK;eACF,OAAO,KAAK,UAAA,EAAY,WAAW,EAC5C,OAAM,OAAO,KAAK,UAAA,EAAY,CAAA;UAE9B,OAAM;AAER,UAAI,MAAM,QAAQ,WAAW,GAAA,CAAA,KAAS,MAAM,QAAQ,WAAW,GAAA,EAAK,CAAA,CAAA,EAClE,oBAAmB,WAAW,GAAA,EAAK,CAAA;UAEnC,oBAAmB,WAAW,GAAA;UAGhC,oBAAmB;AAErB,QAAI,OAAO,qBAAqB,SAC9B,QAAO,CAAC,IAAI,aAAa,gBAAA,CAAiB;aACjC,MAAM,QAAQ,gBAAA,EACvB,QAAO;aACE,cAAc,gBAAA,EACvB,QAAO,CAAC,gBAAA;QAER,OAAM,IAAI,MACR;MAAkE,KAAK,UACrE,kBACA,MACA,CAAA,CACD,EAAA;;EAKP,mBAEE,aACoB;AACpB,QAAI;AACJ,QACE,CAAC,MAAM,QAAQ,WAAA,KACf,CAAC,cAAc,WAAA,KACf,OAAO,gBAAgB,UACvB;AACA,UAAI;AACJ,UAAI,KAAK,sBAAsB,OAC7B,OAAM,KAAK;eACF,OAAO,KAAK,WAAA,EAAa,WAAW,EAC7C,OAAM,OAAO,KAAK,WAAA,EAAa,CAAA;UAE/B,OAAM;AAIR,UAAI,YAAY,gBAAgB,OAC9B,qBAAoB,YAAY,YAAY,CAAA,EAAG,CAAA,EAAG;UAElD,qBAAoB,YAAY,GAAA;UAGlC,qBAAoB;AAGtB,QAAI,OAAO,sBAAsB,SAC/B,QAAO,CAAC,IAAI,UAAU,iBAAA,CAAkB;aAC/B,MAAM,QAAQ,iBAAA,EACvB,QAAO;aACE,cAAc,iBAAA,EACvB,QAAO,CAAC,iBAAA;QAER,OAAM,IAAI,MACR,uEAAuE,KAAK,UAC1E,mBACA,MACA,CAAA,CACD,EAAA;;EAKP,MAAM,cAEJ,OACA,QACwB;AAExB,UAAM,WAAW,OADD,QAAQ,cAAc,gBACP,YAAA;AAC/B,QAAI,KAAK,uBAAuB,OAC9B,QAAO,SAAS,OAAO,KAAK,kBAAkB,KAAA,CAAM;AAEtD,WAAO;;EAGT,MAAM,aAAa,KAAUA,SAAuC;AAClE,UAAM,UAAUA,QAAO,cAAc;AAGrC,QAAI;AAEJ,QAAI,MAAM,QAAQ,IAAI,MAAA,KAAW,MAAM,QAAQ,IAAI,OAAO,CAAA,CAAA,EACxD,UAAS,IAAI,OAAO,CAAA;QAEpB,UAAS,IAAI;AAEf,QAAI,gBAAgB,KAAK,kBAAkB,MAAA;AAG3C,QAAI,KAAK,uBAAuB,QAAW;AACzC,YAAM,mBAAmB,MAAM,QAAQ,YAAA;AACvC,sBAAgB,cAAc,MAAM,iBAAiB,MAAA;;AAGvD,UAAM,cAAc,IAAI;AACxB,QAAI,CAAC,YACH,OAAM,IAAI,MACR,4CAA4C,KAAK,UAC/C,KACA,MACA,CAAA,CACD,EAAA;AAGL,UAAM,iBAAiB,KAAK,mBAAmB,WAAA;AAC/C,UAAM,QAAQ,YAAY,CAAC,GAAG,eAAe,GAAG,cAAA,CAAe;;EAGjE,MAAM,gBAAgB,SAA4C;AAChE,UAAMA,UAAS,MAAM,MAAM,aAAa,GAAG,OAAA;AAE3C,QAAI,CAACA,QAAO,gBAAgB,CAACA,QAAO,aAAa,WAAW;AAC1D,YAAM,eAAe,EAAA,CAClB,KAAK,oBAAoB,OAAA,GAAU,MAAA;AAGtC,YAAM,IAAI,MACR;mBACsB,KAAK,UAAU,YAAA,CAAa,KAAK,KAAK,UAHxC,EAAE,cAAc,EAAE,WAAW,MAAA,EAAO,CAAE,CAKvD,GAAC;;AAIR,UAAM,EAAE,UAAA,IAAcA,QAAO;AAC7B,IAAAA,QAAO,aAAa,iBAClB,MAAM,KAAK,kBAAkB,SAAA;AAC/B,WAAOA;;;;;ACxSX,IAAI,oBAAoC,YAAY;AAAA,EACnD,gBAAgB,MAAM;AAAA,EACtB,UAAU,MAAM;AAAA,EAChB,gBAAgB,MAAM;AAAA,EACtB,iBAAiB,MAAM;AAAA,EACvB,gBAAgB,MAAM;AAAA,EACtB,cAAc,MAAM;AAAA,EACpB,gBAAgB,MAAM;AAAA,EACtB,aAAa,MAAM;AAAA,EACnB,kBAAkB,MAAM;AAAA,EACxB,qBAAqB,MAAM;AAAA,EAC3B,cAAc,MAAM;AAAA,EACpB,eAAe,MAAM;AAAA,EACrB,kBAAkB,MAAM;AAAA,EACxB,kBAAkB,MAAM;AAAA,EACxB,uBAAuB,MAAM;AAAA,EAC7B,4BAA4B,MAAM;AAAA,EAClC,mBAAmB,MAAM;AAAA,EACzB,cAAc,MAAM;AAAA,EACpB,6BAA6B,MAAM;AAAA,EACnC,cAAc,MAAM;AAAA,EACpB,aAAa,MAAM;AAAA,EACnB,wBAAwB,MAAM;AAAA,EAC9B,gBAAgB,MAAM;AACvB,CAAC;;;AChBD,IAAsB,sBAAtB,cAA+D,SAG7D;;;;;;;;;EAsBA,sBACE,aACA,SACA,WACY;AACZ,WAAO,KAAK,YAAY,aAAa,SAAA;;EAG7B,qBAAqB,SAA8B;AAC3D,WAAO,OAAO,QAAQ,YAAY,WAC9B,QAAQ,UACR,KAAK,4BAA4B,QAAQ,OAAA;;EAGrC,4BAA4B,SAAiC;AACrE,WAAO,KAAK,UAAU,OAAA;;;;;;;;;;;;EAaxB,MAAM,OACJ,OACA,SACY;AACZ,QAAI,OAAO,UAAU,SACnB,QAAO,KAAK,gBACV,OAAOC,QAAeC,aACpB,KAAK,YAAY,CAAC,EAAE,MAAMD,OAAA,CAAO,GAAGC,UAAS,SAAA,GAC/C,OACA;MAAE,GAAG;MAAS,SAAS;KAAU;QAGnC,QAAO,KAAK,gBACV,OAAOD,QAAoBC,aACzB,KAAK,YACH,CACE;MACE,SAASD;MACT,MAAM,KAAK,qBAAqBA,MAAA;KACjC,GAEHC,UAAS,SAAA,GAEb,OACA;MAAE,GAAG;MAAS,SAAS;KAAU;;;AASzC,IAAsB,mBAAtB,cAEU,oBAAuB;EAC/B,YACE,aACA,WACY;AACZ,WAAO,KAAK,MAAM,YAAY,CAAA,EAAG,MAAM,SAAA;;EAWzC,MAAM,gBACJ,MACA,SACA,WACY;AACZ,WAAO,KAAK,MAAM,MAAM,SAAA;;;;;EAmB1B,QAAgB;AACd,UAAM,IAAI,MAAM,uBAAA;;;AAsBpB,IAAa,wBAAb,cAA2C,MAAM;EAC/C;EAEA;EAEA;EAEA,YACE,SACA,WACA,aACA,YAAY,OACZ;AACA,UAAM,OAAA;AACN,SAAK,YAAY;AACjB,SAAK,cAAc;AACnB,SAAK,YAAY;AAEjB,QAAI,WACF;UAAI,gBAAgB,UAAa,cAAc,OAC7C,OAAM,IAAI,MACR,2EAAA;;AAKN,4BAAwB,MAAM,wBAAA;;;;;AChLlC,IAAsB,4BAAtB,cAEU,iBAAoB;EAC5B,OAAO,WACL,gBACmB;AACnB,qBAAiB,SAAS,eACxB,KAAI,OAAO,UAAU,SACnB,OAAM,KAAK,YAAY,CAAC,EAAE,MAAM,MAAA,CAAO,CAAC;QAExC,OAAM,KAAK,YAAY,CACrB;MACE,SAAS;MACT,MAAM,KAAK,qBAAqB,KAAA;KACjC,CACF;;;;;;;;;EAYP,OAAO,UACL,gBACA,SACmB;AACnB,WAAO,KAAK,2BACV,gBACA,KAAK,WAAW,KAAK,IAAA,GACrB;MACE,GAAG;MACH,SAAS;KACV;;;AAYP,IAAsB,sCAAtB,cAEU,0BAA6B;EAC3B,OAAO;EAEjB,YAAY,QAAmD;AAC7D,UAAM,MAAA;AACN,SAAK,OAAO,QAAQ,QAAQ,KAAK;;EAUnC,OAAO,WACL,gBACmB;AACnB,QAAI;AACJ,QAAI;AACJ,qBAAiB,SAAS,gBAAgB;AACxC,UAAI,OAAO,UAAU,YAAY,OAAO,MAAM,YAAY,SACxD,OAAM,IAAI,MAAM,kCAAA;AAElB,UAAI;AACJ,UAAI,mBAAmB,KAAA,GAAQ;AAC7B,YAAI,OAAO,MAAM,YAAY,SAC3B,OAAM,IAAI,MAAM,0CAAA;AAElB,mBAAW,IAAI,oBAAoB;UACjC,SAAS;UACT,MAAM,MAAM;SACb;iBACQ,cAAc,KAAA,GAAQ;AAC/B,YAAI,OAAO,MAAM,YAAY,SAC3B,OAAM,IAAI,MAAM,0CAAA;AAElB,mBAAW,IAAI,oBAAoB;UACjC,SAAS,eAAe,KAAA;UACxB,MAAM,MAAM;SACb;YAED,YAAW,IAAI,gBAAgB,EAAE,MAAM,MAAA,CAAO;AAGhD,UAAI,WAAW,OACb,UAAS;UAET,UAAS,OAAO,OAAO,QAAA;AAGzB,YAAM,SAAS,MAAM,KAAK,mBAAmB,CAAC,MAAA,CAAO;AACrD,UACE,WAAW,UACX,WAAW,QACX,CAAC,kBAAkB,QAAQ,UAAA,GAC3B;AACA,YAAI,KAAK,KACP,OAAM,KAAK,MAAM,YAAY,MAAA;YAE7B,OAAM;AAER,qBAAa;;;;EAKnB,wBAAgC;AAC9B,WAAO;;;;;ACpIX,IAAa,oBAAb,cAAuC,0BAAsC;EAC3E,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe;IAAC;IAAkB;IAAkB;;EAEpD,kBAAkB;EAER,cAAgD,IAAI,YAAA;EAE9D,MAAM,MAAmC;AACvC,WAAO,QAAQ,QAAQ,KAAK,YAAY,OAAO,IAAA,CAAK;;EAGtD,wBAAgC;AAC9B,WAAO;;;;;ACdX,IAAsB,mBAAtB,cAA+C,0BAE7C;EACA;EAEA,OAAO,WACL,gBAC0B;AAC1B,QAAI,SAAS;AACb,qBAAiB,SAAS,gBAAgB;AACxC,UAAI,OAAO,UAAU,SAEnB,WAAU;UAGV,WAAU,MAAM;AAGlB,UAAI,CAAC,KAAK,IAAI;AACZ,cAAM,QAAQ,MAAM,KAAK,MAAM,MAAA;AAC/B,YAAI,MAAM,SAAS,GAAG;AAEpB,qBAAW,QAAQ,MAAM,MAAM,GAAG,EAAA,EAChC,OAAM,CAAC,IAAA;AAGT,mBAAS,MAAM,MAAM,SAAS,CAAA;;aAE3B;AAEL,cAAM,UAAU,CAAC,GAAG,OAAO,SAAS,KAAK,EAAA,CAAG;AAC5C,YAAI,QAAQ,SAAS,GAAG;AACtB,cAAI,UAAU;AAEd,qBAAW,SAAS,QAAQ,MAAM,GAAG,EAAA,GAAK;AACxC,kBAAM,CAAC,MAAM,CAAA,CAAA;AACb,wBAAY,MAAM,SAAS,KAAK,MAAM,CAAA,EAAG;;AAG3C,mBAAS,OAAO,MAAM,OAAA;;;;AAM5B,eAAW,QAAQ,MAAM,KAAK,MAAM,MAAA,EAClC,OAAM,CAAC,IAAA;;;AASb,IAAa,iCAAb,cAAoD,iBAAiB;EACnE,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe;IAAC;IAAkB;IAAkB;;EAEpD,kBAAkB;;;;;;;EAQlB,MAAM,MAAM,MAAiC;AAC3C,QAAI;AACF,aAAO,KACJ,KAAA,EACA,MAAM,GAAA,EACN,IAAA,CAAK,MAAM,EAAE,KAAA,CAAM;YAChB;AACN,YAAM,IAAI,sBAAsB,2BAA2B,IAAA,IAAQ,IAAA;;;;;;;;EASvE,wBAAgC;AAC9B,WAAO;;;AAQX,IAAa,yBAAb,cAA4C,iBAAiB;EAC3D,eAAe;IAAC;IAAkB;IAAkB;;EAE5C;EAEA;EAER,YAAY,EAAE,QAAQ,UAAA,GAAsD;AAC1E,UAAM,GAAG,SAAA;AACT,SAAK,SAAS;AACd,SAAK,YAAY,aAAa;;;;;;;;;EAUhC,MAAM,MAAM,MAAiC;AAC3C,QAAI;AACF,YAAM,QAAQ,KACX,KAAA,EACA,MAAM,KAAK,SAAA,EACX,IAAA,CAAK,MAAM,EAAE,KAAA,CAAM;AACtB,UAAI,KAAK,WAAW,UAAa,MAAM,WAAW,KAAK,OACrD,OAAM,IAAI,sBACR,uCAAuC,KAAK,MAAA,SAAe,MAAM,MAAA,GAAO;AAG5E,aAAO;aACA,GAAG;AACV,UAAI,OAAO,eAAe,CAAA,MAAO,sBAAsB,UACrD,OAAM;AAER,YAAM,IAAI,sBAAsB,2BAA2B,IAAA,EAAA;;;;;;;;;EAU/D,wBAAgC;AAC9B,WAAO,qCACL,KAAK,WAAW,SAAY,KAAK,GAAG,KAAK,MAAA,GAAO,uBAC3B,KAAK,SAAA,eAAwB,KAAK,SAAA,OACvD,KAAK,SAAA;;;AAKX,IAAa,2BAAb,cAA8C,iBAAiB;EAC7D,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe;IAAC;IAAkB;IAAkB;;EAEpD,kBAAkB;EAElB,wBAAgC;AAC9B,WAAO;;;;;;;;EAGT,KAAK;EAEL,MAAM,MAAM,MAAiC;AAC3C,WAAO,CAAC,GAAI,KAAK,SAAS,KAAK,EAAA,KAAO,CAAA,CAAE,EAAG,IAAA,CAAK,MAAM,EAAE,CAAA,CAAA;;;AAI5D,IAAa,2BAAb,cAA8C,iBAAiB;EAC7D,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe;IAAC;IAAkB;IAAkB;;EAEpD,kBAAkB;EAElB,wBAAgC;AAC9B,WAAO;;;;;;;;EAGT,KAAK;EAEL,MAAM,MAAM,MAAiC;AAC3C,WAAO,CAAC,GAAI,KAAK,SAAS,KAAK,EAAA,KAAO,CAAA,CAAE,EAAG,IAAA,CAAK,MAAM,EAAE,CAAA,CAAA;;;;;AC7K5D,IAAa,qBAAb,cAAwC,0BAAkC;EACxE,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe;IAAC;IAAkB;IAAkB;;EAEpD,kBAAkB;;;;;;;;;EAUlB,MAAM,MAA+B;AACnC,WAAO,QAAQ,QAAQ,IAAA;;EAGzB,wBAAgC;AAC9B,WAAO;;EAGC,qBAAqB,SAAoC;AACjE,WAAO,QAAQ;;EAGP,yBACR,UACQ;AACR,UAAM,IAAI,MACR,oEAAA;;EAIM,wBAAwB,SAA+B;AAC/D,YAAQ,QAAQ,MAAhB;MACE,KAAK;MACL,KAAK;AACH,YAAI,UAAU,QAEZ,QAAO,KAAK,qBAAqB,OAAA;AAEnC;MACF,KAAK;AACH,YAAI,eAAe,QAEjB,QAAO,KAAK,yBACV,OAAA;AAGJ;MACF,KAAK;MACL,KAAK;MACL,KAAK;AACH,eAAO;MACT;AACE,cAAM,IAAI,MACR,kBAAkB,QAAQ,IAAA,+BAAK;;AAGrC,UAAM,IAAI,MAAM,yBAAyB,QAAQ,IAAA,EAAA;;EAGzC,4BAA4B,SAAiC;AACrE,WAAO,QAAQ,OAAA,CACZ,KAAa,SACZ,MAAM,KAAK,wBAAwB,IAAA,GACrC,EAAA;;;;;AC9DN,IAAa,yBAAb,cAEU,iBAA2C;EACnD,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe;IAAC;IAAa;IAAkB;;EAE/C,SAAS;AACP,WAAO,KAAK,qBAAA;;EAGd,YAAmB,QAAW;AAC5B,UAAM,MAAA;AADW,SAAA,SAAA;;;;;;;EASnB,OAAO,cAAwC,QAAW;AACxD,WAAO,IAAI,KAAK,MAAA;;;;;;;;EASlB,OAAO,yBACL,SACA;AACA,UAAM,YAAY,iBAAE,OAClB,OAAO,YACL,OAAO,QAAQ,OAAA,EAAS,IAAA,CACrB,CAAC,MAAM,WAAA,MACN,CAAC,MAAM,iBAAE,OAAA,EAAS,SAAS,WAAA,CAAY,CAAC,CAC3C,CACF;AAGH,WAAO,IAAI,KAAK,SAAA;;;;;;;;EASlB,wBAAgC;AAC9B,WAAO;;;;;;;;;;;;EAYT,KAAK,UAAU,aAAa,KAAK,MAAA,CAAO,CAAC;;;;;;;;;EAUzC,MAAM,MAAM,MAAiD;AAC3D,QAAI;AACF,YAAM,cAAc,KAAK,KAAA;AAUzB,YAAM,eANJ,YAAY,MAAM,+BAAA,IAAmC,CAAA,KAErD,YAAY,MAAM,yBAAA,IAA6B,CAAA,KAE/C,aAGC,QAAQ,6BAAA,CAA8B,QAAQ,kBAAkB;AAE/D,eAAO,IADqB,cAAc,QAAQ,OAAO,KAAA,CAAM;SAGhE,QAAQ,OAAO,EAAA;AAElB,aAAO,MAAM,kBAAkB,KAAK,QAAQ,KAAK,MAAM,WAAA,CAAY;aAC5D,GAAG;AACV,YAAM,IAAI,sBACR,2BAA2B,IAAA,aAAiB,CAAA,IAC5C,IAAA;;;;AAUR,IAAa,qCAAb,cAEU,uBAA0B;EAClC,OAAO,UAAU;AACf,WAAO;;EAGT,sBACE,SACQ;AACR,UAAM,qBAAqB,SAAS,sBAAsB;AAC1D,QAAI,qBAAqB,EACvB,OAAM,IAAI,MAAM,iDAAA;AAGlB,WAAO;;EAA0F,KAAK,qBACpG,aAAa,KAAK,MAAA,CAAO,EAExB,WAAW,KAAK,IAAI,OAAO,kBAAA,CAAmB,EAC9C,WAAW,KAAK,IAAI,OAAO,kBAAA,CAAmB,CAAC;;;EAG5C,qBACN,aACA,SAAS,GACD;AACR,UAAM,SAAS;AASf,QAAI,UAAU,QAAQ;AACpB,UAAIC,YAAW;AACf,UAAI;AACJ,UAAI,MAAM,QAAQ,OAAO,IAAA,GAAO;AAC9B,cAAM,UAAU,OAAO,KAAK,UAAA,CAAWC,UAASA,UAAS,MAAA;AACzD,YAAI,YAAY,IAAI;AAClB,UAAAD,YAAW;AACX,iBAAO,KAAK,OAAO,SAAS,CAAA;;AAE9B,eAAO,OAAO,KAAK,KAAK,KAAA;YAExB,QAAO,OAAO;AAGhB,UAAI,OAAO,SAAS,YAAY,OAAO,YAAY;AACjD,cAAME,eAAc,OAAO,cACvB,OAAO,OAAO,WAAA,KACd;AAYJ,eAAO;EAXY,OAAO,QAAQ,OAAO,UAAA,EACtC,IAAA,CAAK,CAAC,KAAK,KAAA,MAAW;AACrB,gBAAM,aAAa,OAAO,UAAU,SAAS,GAAA,IACzC,KACA;AACJ,iBAAO,GAAG,IAAI,OAAO,MAAA,CAAO,IAAI,GAAA,MAAS,KAAK,qBAC5C,OACA,SAAS,CAAA,CACV,GAAG,UAAA;WAEL,KAAK,IAAA,CAAK;EACe,IAAI,OAAO,SAAS,CAAA,CAAE,IAAIA,YAAA;;AAExD,UAAI,OAAO,SAAS,WAAW,OAAO,OAAO;AAC3C,cAAMA,eAAc,OAAO,cACvB,OAAO,OAAO,WAAA,KACd;AACJ,eAAO;EAAW,IAAI,OAAO,MAAA,CAAO,GAAG,KAAK,qBAC1C,OAAO,OACP,SAAS,CAAA,CACV;EAAK,IAAI,OAAO,SAAS,CAAA,CAAE,KAAKA,YAAA;;AAEnC,YAAMC,cAAaH,YAAW,gBAAgB;AAC9C,YAAM,cAAc,OAAO,cAAc,OAAO,OAAO,WAAA,KAAgB;AACvE,aAAO,GAAG,IAAA,GAAO,WAAA,GAAcG,WAAA;;AAGjC,QAAI,WAAW,OACb,QAAO,OAAO,MACX,IAAA,CAAK,MAAM,KAAK,qBAAqB,GAAG,MAAA,CAAO,EAC/C,KAAK;EAAK,IAAI,OAAO,SAAS,CAAA,CAAE,EAAA;AAGrC,UAAM,IAAI,MAAM,yBAAA;;EAGlB,OAAO,cAAwC,QAAW;AACxD,WAAO,IAAI,KAAQ,MAAA;;EAGrB,OAAO,yBACL,SACA;AACA,UAAM,YAAY,iBAAE,OAClB,OAAO,YACL,OAAO,QAAQ,OAAA,EAAS,IAAA,CACrB,CAAC,MAAM,WAAA,MACN,CAAC,MAAM,iBAAE,OAAA,EAAS,SAAS,WAAA,CAAY,CAAC,CAC3C,CACF;AAGH,WAAO,IAAI,KAAuB,SAAA;;;AActC,IAAsB,mCAAtB,cAGU,iBAAoB;EACpB;EAER,YAAY,EAAE,YAAA,GAA0D;AACtE,UAAM,GAAG,SAAA;AACT,SAAK,wBAAwB,IAAI,mCAC/B,WAAA;;EAYJ,MAAM,MAAM,MAA0B;AACpC,QAAI;AACJ,QAAI;AACF,oBAAc,MAAM,KAAK,sBAAsB,MAAM,IAAA;aAC9C,GAAG;AACV,YAAM,IAAI,sBACR,2BAA2B,IAAA,aAAiB,CAAA,IAC5C,IAAA;;AAIJ,WAAO,KAAK,gBAAgB,WAAA;;EAG9B,wBAAgC;AAC9B,WAAO,KAAK,sBAAsB,sBAAA;;;;;AC/RtC,IAAI,qBAAqC,YAAY;AAAA,EACpD,YAAY,MAAM;AAAA,EAClB,SAAS,MAAM;AAChB,CAAC;;;ACAD,IAAa,mBAAb,cAGU,oCAAuC;EAC/C,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe,CAAC,kBAAkB,gBAAA;EAElC,kBAAkB;;EAGT,oBAAuB,OAAU,QAAc;AACtD,QAAI,KAAK,KACP,QAAO,MAAM,oBAAoB,OAAO,MAAA;AAE1C,WAAO;;EAGC,MACR,MACA,MACyB;AACzB,QAAI,CAAC,KACH;AAEF,QAAI,CAAC,KACH,QAAO,CAAC;MAAE,IAAI;MAAW,MAAM;MAAI,OAAO;KAAM;AAElD,WAAO,QAAQ,MAAM,IAAA;;EAKvB,MAAM,mBACJ,aACwB;AACxB,WAAO,kBAAkB,YAAY,CAAA,EAAG,IAAA;;EAG1C,MAAM,MAAM,MAA0B;AACpC,WAAO,kBAAkB,MAAM,KAAK,KAAA;;EAGtC,wBAAgC;AAC9B,WAAO;;;;;;;;;EAUC,qBAAqB,SAA8B;AAC3D,WAAO,QAAQ;;;;;AC7DnB,IAAM,gBAAgB,WAAY;AAChC,QAAMC,OAAW,CAAA;AACjB,EAAAA,KAAI,SAAS,SAAU,QAAQ,KAAK;AAClC,WAAO,IAAI,UAAU,QAAQ,GAAA;;AAE/B,EAAAA,KAAI,YAAY;AAChB,EAAAA,KAAI,YAAY;AAChB,EAAAA,KAAI,eAAe;AAWnB,EAAAA,KAAI,oBAAoB,KAAK;AAE7B,QAAM,UAAU;IACd;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;AAGF,EAAAA,KAAI,SAAS;IACX;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;AAGF,WAAS,UAAU,QAAQ,KAAK;AAC9B,QAAI,EAAE,gBAAgB,WACpB,QAAO,IAAI,UAAU,QAAQ,GAAA;AAG/B,QAAI,SAAS;AACb,iBAAa,MAAA;AACb,WAAO,IAAI,OAAO,IAAI;AACtB,WAAO,sBAAsBA,KAAI;AACjC,WAAO,MAAM,OAAO,CAAA;AACpB,WAAO,IAAI,YAAY,OAAO,IAAI,aAAa,OAAO,IAAI;AAC1D,WAAO,YAAY,OAAO,IAAI,YAAY,gBAAgB;AAC1D,WAAO,OAAO,CAAA;AACd,WAAO,SAAS,OAAO,aAAa,OAAO,UAAU;AACrD,WAAO,MAAM,OAAO,QAAQ;AAC5B,WAAO,SAAS,CAAC,CAAC;AAClB,WAAO,WAAW,CAAC,EAAE,UAAU,OAAO,IAAI;AAC1C,WAAO,QAAQ,EAAE;AACjB,WAAO,iBAAiB,OAAO,IAAI;AACnC,WAAO,WAAW,OAAO,iBACrB,OAAO,OAAOA,KAAI,YAAA,IAClB,OAAO,OAAOA,KAAI,QAAA;AACtB,WAAO,aAAa,CAAA;AAKpB,QAAI,OAAO,IAAI,MACb,QAAO,KAAK,OAAO,OAAO,MAAA;AAI5B,WAAO,gBAAgB,OAAO,IAAI,aAAa;AAC/C,QAAI,OAAO,cACT,QAAO,WAAW,OAAO,OAAO,OAAO,SAAS;AAElD,SAAK,QAAQ,SAAA;;AAGf,MAAI,CAAC,OAAO,OACV,QAAO,SAAS,SAAU,GAAG;AAC3B,aAAS,IAAI;IAAA;AACb,MAAE,YAAY;AAEd,WADW,IAAI,EAAA;;AAKnB,MAAI,CAAC,OAAO,KACV,QAAO,OAAO,SAAU,GAAG;AACzB,QAAI,IAAI,CAAA;AACR,aAAS,KAAK,EAAG,KAAI,EAAE,eAAe,CAAA,EAAI,GAAE,KAAK,CAAA;AACjD,WAAO;;AAIX,WAAS,kBAAkB,QAAQ;AACjC,QAAI,aAAa,KAAK,IAAIA,KAAI,mBAAmB,EAAA;AACjD,QAAI,YAAY;AAChB,aAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,IAAI,GAAG,KAAK;AAC9C,UAAI,MAAM,OAAO,QAAQ,CAAA,CAAA,EAAI;AAC7B,UAAI,MAAM,WAKR,SAAQ,QAAQ,CAAA,GAAhB;QACE,KAAK;AACH,oBAAU,MAAA;AACV;QAEF,KAAK;AACH,mBAAS,QAAQ,WAAW,OAAO,KAAA;AACnC,iBAAO,QAAQ;AACf;QAEF,KAAK;AACH,mBAAS,QAAQ,YAAY,OAAO,MAAA;AACpC,iBAAO,SAAS;AAChB;QAEF;AACE,gBAAM,QAAQ,iCAAiC,QAAQ,CAAA,CAAA;;AAG7D,kBAAY,KAAK,IAAI,WAAW,GAAA;;AAIlC,WAAO,sBADCA,KAAI,oBAAoB,YACC,OAAO;;AAG1C,WAAS,aAAa,QAAQ;AAC5B,aAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,IAAI,GAAG,IACzC,QAAO,QAAQ,CAAA,CAAA,IAAM;;AAIzB,WAAS,aAAa,QAAQ;AAC5B,cAAU,MAAA;AACV,QAAI,OAAO,UAAU,IAAI;AACvB,eAAS,QAAQ,WAAW,OAAO,KAAA;AACnC,aAAO,QAAQ;;AAEjB,QAAI,OAAO,WAAW,IAAI;AACxB,eAAS,QAAQ,YAAY,OAAO,MAAA;AACpC,aAAO,SAAS;;;AAIpB,YAAU,YAAY;IACpB,KAAK,WAAY;AACf,UAAI,IAAA;;IAEC;IACP,QAAQ,WAAY;AAClB,WAAK,QAAQ;AACb,aAAO;;IAET,OAAO,WAAY;AACjB,aAAO,KAAK,MAAM,IAAA;;IAEpB,OAAO,WAAY;AACjB,mBAAa,IAAA;;;AAIjB,MAAIC,UAAS;AACb,MAAI,CAACA,QAAQ,CAAAA,UAAS,WAAY;EAAA;AAElC,MAAI,cAAcD,KAAI,OAAO,OAAO,SAAU,IAAI;AAChD,WAAO,OAAO,WAAW,OAAO;;AAGlC,WAAS,aAAa,QAAQ,KAAK;AACjC,WAAO,IAAI,UAAU,QAAQ,GAAA;;AAG/B,WAAS,UAAU,QAAQ,KAAK;AAC9B,QAAI,EAAE,gBAAgB,WACpB,QAAO,IAAI,UAAU,QAAQ,GAAA;AAG/B,IAAAC,QAAO,MAAM,IAAA;AAEb,SAAK,UAAU,IAAI,UAAU,QAAQ,GAAA;AACrC,SAAK,WAAW;AAChB,SAAK,WAAW;AAEhB,QAAI,KAAK;AAET,SAAK,QAAQ,QAAQ,WAAY;AAC/B,SAAG,KAAK,KAAA;;AAGV,SAAK,QAAQ,UAAU,SAAU,IAAI;AACnC,SAAG,KAAK,SAAS,EAAA;AAIjB,SAAG,QAAQ,QAAQ;;AAGrB,SAAK,WAAW;AAEhB,gBAAY,QAAQ,SAAU,IAAI;AAChC,aAAO,eAAe,IAAI,OAAO,IAAI;QACnC,KAAK,WAAY;AACf,iBAAO,GAAG,QAAQ,OAAO,EAAA;;QAE3B,KAAK,SAAU,GAAG;AAChB,cAAI,CAAC,GAAG;AACN,eAAG,mBAAmB,EAAA;AACtB,eAAG,QAAQ,OAAO,EAAA,IAAM;AACxB,mBAAO;;AAET,aAAG,GAAG,IAAI,CAAA;;QAEZ,YAAY;QACZ,cAAc;OACf;;;AAIL,YAAU,YAAY,OAAO,OAAOA,QAAO,WAAW,EACpD,aAAa,EACX,OAAO,UAAA,EACR,CACF;AAED,YAAU,UAAU,QAAQ,SAAU,MAAM;AAC1C,SAAK,QAAQ,MAAM,KAAK,SAAA,CAAU;AAClC,SAAK,KAAK,QAAQ,IAAA;AAClB,WAAO;;AAGT,YAAU,UAAU,MAAM,SAAU,OAAO;AACzC,QAAI,SAAS,MAAM,OACjB,MAAK,MAAM,KAAA;AAEb,SAAK,QAAQ,IAAA;AACb,WAAO;;AAGT,YAAU,UAAU,KAAK,SAAU,IAAI,SAAS;AAC9C,QAAI,KAAK;AACT,QAAI,CAAC,GAAG,QAAQ,OAAO,EAAA,KAAO,YAAY,QAAQ,EAAA,MAAQ,GACxD,IAAG,QAAQ,OAAO,EAAA,IAAM,WAAY;AAClC,UAAI,OACF,UAAU,WAAW,IACjB,CAAC,UAAU,CAAA,CAAA,IACX,MAAM,MAAM,MAAM,SAAA;AACxB,WAAK,OAAO,GAAG,GAAG,EAAA;AAClB,SAAG,KAAK,MAAM,IAAI,IAAA;;AAItB,WAAOA,QAAO,UAAU,GAAG,KAAK,IAAI,IAAI,OAAA;;AAK1C,MAAI,QAAQ;AACZ,MAAI,UAAU;AACd,MAAI,gBAAgB;AACpB,MAAI,kBAAkB;AACtB,MAAI,SAAS;IAAE,KAAK;IAAe,OAAO;;AAQ1C,MAAI,YACF;AAEF,MAAI,WACF;AAEF,MAAI,cACF;AACF,MAAI,aACF;AAEF,WAAS,aAAa,GAAG;AACvB,WAAO,MAAM,OAAO,MAAM,QAAQ,MAAM,QAAQ,MAAM;;AAGxD,WAAS,QAAQ,GAAG;AAClB,WAAO,MAAM,OAAO,MAAM;;AAG5B,WAAS,YAAY,GAAG;AACtB,WAAO,MAAM,OAAO,aAAa,CAAA;;AAGnC,WAAS,QAAQ,OAAO,GAAG;AACzB,WAAO,MAAM,KAAK,CAAA;;AAGpB,WAAS,SAAS,OAAO,GAAG;AAC1B,WAAO,CAAC,QAAQ,OAAO,CAAA;;AAGzB,MAAI,IAAI;AACR,EAAAD,KAAI,QAAQ;IACV,OAAO;IACP,kBAAkB;IAClB,MAAM;IACN,aAAa;IACb,WAAW;IACX,WAAW;IACX,kBAAkB;IAClB,SAAS;IACT,gBAAgB;IAChB,aAAa;IACb,oBAAoB;IACpB,kBAAkB;IAClB,SAAS;IACT,gBAAgB;IAChB,eAAe;IACf,OAAO;IACP,cAAc;IACd,gBAAgB;IAChB,WAAW;IACX,gBAAgB;IAChB,kBAAkB;IAClB,UAAU;IACV,gBAAgB;IAChB,QAAQ;IACR,aAAa;IACb,uBAAuB;IACvB,cAAc;IACd,qBAAqB;IACrB,qBAAqB;IACrB,uBAAuB;IACvB,uBAAuB;IACvB,uBAAuB;IACvB,WAAW;IACX,qBAAqB;IACrB,QAAQ;IACR,eAAe;;AAGjB,EAAAA,KAAI,eAAe;IACjB,KAAK;IACL,IAAI;IACJ,IAAI;IACJ,MAAM;IACN,MAAM;;AAGR,EAAAA,KAAI,WAAW;IACb,KAAK;IACL,IAAI;IACJ,IAAI;IACJ,MAAM;IACN,MAAM;IACN,OAAO;IACP,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,KAAK;IACL,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,QAAQ;IACR,QAAQ;IACR,MAAM;IACN,OAAO;IACP,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,QAAQ;IACR,OAAO;IACP,OAAO;IACP,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,KAAK;IACL,MAAM;IACN,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,QAAQ;IACR,QAAQ;IACR,MAAM;IACN,OAAO;IACP,OAAO;IACP,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,MAAM;IACN,MAAM;IACN,KAAK;IACL,MAAM;IACN,OAAO;IACP,MAAM;IACN,OAAO;IACP,QAAQ;IACR,KAAK;IACL,QAAQ;IACR,MAAM;IACN,KAAK;IACL,MAAM;IACN,OAAO;IACP,KAAK;IACL,KAAK;IACL,MAAM;IACN,KAAK;IACL,QAAQ;IACR,MAAM;IACN,MAAM;IACN,MAAM;IACN,OAAO;IACP,OAAO;IACP,MAAM;IACN,QAAQ;IACR,OAAO;IACP,MAAM;IACN,OAAO;IACP,QAAQ;IACR,QAAQ;IACR,QAAQ;IACR,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,OAAO;IACP,OAAO;IACP,QAAQ;IACR,QAAQ;IACR,MAAM;IACN,MAAM;IACN,MAAM;IACN,OAAO;IACP,OAAO;IACP,MAAM;IACN,OAAO;IACP,OAAO;IACP,SAAS;IACT,MAAM;IACN,KAAK;IACL,OAAO;IACP,MAAM;IACN,OAAO;IACP,QAAQ;IACR,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,SAAS;IACT,IAAI;IACJ,KAAK;IACL,OAAO;IACP,KAAK;IACL,SAAS;IACT,KAAK;IACL,KAAK;IACL,KAAK;IACL,OAAO;IACP,OAAO;IACP,MAAM;IACN,OAAO;IACP,OAAO;IACP,SAAS;IACT,MAAM;IACN,KAAK;IACL,OAAO;IACP,MAAM;IACN,OAAO;IACP,QAAQ;IACR,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,SAAS;IACT,IAAI;IACJ,KAAK;IACL,QAAQ;IACR,OAAO;IACP,KAAK;IACL,SAAS;IACT,KAAK;IACL,KAAK;IACL,KAAK;IACL,OAAO;IACP,UAAU;IACV,OAAO;IACP,KAAK;IACL,MAAM;IACN,MAAM;IACN,QAAQ;IACR,MAAM;IACN,KAAK;IACL,KAAK;IACL,KAAK;IACL,OAAO;IACP,OAAO;IACP,OAAO;IACP,OAAO;IACP,OAAO;IACP,OAAO;IACP,OAAO;IACP,OAAO;IACP,QAAQ;IACR,QAAQ;IACR,MAAM;IACN,QAAQ;IACR,QAAQ;IACR,OAAO;IACP,OAAO;IACP,QAAQ;IACR,QAAQ;IACR,OAAO;IACP,OAAO;IACP,MAAM;IACN,OAAO;IACP,QAAQ;IACR,MAAM;IACN,OAAO;IACP,SAAS;IACT,MAAM;IACN,MAAM;IACN,MAAM;IACN,MAAM;IACN,MAAM;IACN,OAAO;IACP,MAAM;IACN,MAAM;IACN,MAAM;IACN,MAAM;IACN,MAAM;IACN,QAAQ;IACR,MAAM;IACN,OAAO;IACP,OAAO;IACP,OAAO;IACP,MAAM;IACN,OAAO;IACP,IAAI;IACJ,MAAM;IACN,KAAK;IACL,OAAO;IACP,QAAQ;IACR,OAAO;IACP,MAAM;IACN,OAAO;IACP,KAAK;IACL,KAAK;IACL,IAAI;IACJ,KAAK;IACL,KAAK;IACL,KAAK;IACL,QAAQ;IACR,KAAK;IACL,MAAM;IACN,OAAO;IACP,IAAI;IACJ,OAAO;IACP,IAAI;IACJ,IAAI;IACJ,KAAK;IACL,KAAK;IACL,MAAM;IACN,MAAM;IACN,MAAM;IACN,OAAO;IACP,QAAQ;IACR,MAAM;IACN,MAAM;IACN,OAAO;IACP,OAAO;IACP,QAAQ;IACR,QAAQ;IACR,MAAM;IACN,MAAM;IACN,KAAK;IACL,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,OAAO;;AAGT,SAAO,KAAKA,KAAI,QAAA,EAAU,QAAQ,SAAU,KAAK;AAC/C,QAAI,IAAIA,KAAI,SAAS,GAAA;AACrB,QAAIE,KAAI,OAAO,MAAM,WAAW,OAAO,aAAa,CAAA,IAAK;AACzD,IAAAF,KAAI,SAAS,GAAA,IAAOE;;AAGtB,WAAS,KAAKF,KAAI,MAChB,CAAAA,KAAI,MAAMA,KAAI,MAAM,CAAA,CAAA,IAAM;AAI5B,MAAIA,KAAI;AAER,WAAS,KAAK,QAAQ,OAAO,MAAM;AACjC,WAAO,KAAA,KAAU,OAAO,KAAA,EAAO,IAAA;;AAGjC,WAAS,SAAS,QAAQ,UAAU,MAAM;AACxC,QAAI,OAAO,SAAU,WAAU,MAAA;AAC/B,SAAK,QAAQ,UAAU,IAAA;;AAGzB,WAAS,UAAU,QAAQ;AACzB,WAAO,WAAW,SAAS,OAAO,KAAK,OAAO,QAAA;AAC9C,QAAI,OAAO,SAAU,MAAK,QAAQ,UAAU,OAAO,QAAA;AACnD,WAAO,WAAW;;AAGpB,WAAS,SAAS,KAAK,MAAM;AAC3B,QAAI,IAAI,KAAM,QAAO,KAAK,KAAA;AAC1B,QAAI,IAAI,UAAW,QAAO,KAAK,QAAQ,QAAQ,GAAA;AAC/C,WAAO;;AAGT,WAAS,MAAM,QAAQ,IAAI;AACzB,cAAU,MAAA;AACV,QAAI,OAAO,cACT,OACE,aACA,OAAO,OACP,eACA,OAAO,SACP,aACA,OAAO;AAEX,SAAK,IAAI,MAAM,EAAA;AACf,WAAO,QAAQ;AACf,SAAK,QAAQ,WAAW,EAAA;AACxB,WAAO;;AAGT,WAAS,IAAI,QAAQ;AACnB,QAAI,OAAO,WAAW,CAAC,OAAO,WAC5B,YAAW,QAAQ,mBAAA;AACrB,QACE,OAAO,UAAU,EAAE,SACnB,OAAO,UAAU,EAAE,oBACnB,OAAO,UAAU,EAAE,KAEnB,OAAM,QAAQ,gBAAA;AAEhB,cAAU,MAAA;AACV,WAAO,IAAI;AACX,WAAO,SAAS;AAChB,SAAK,QAAQ,OAAA;AACb,cAAU,KAAK,QAAQ,OAAO,QAAQ,OAAO,GAAA;AAC7C,WAAO;;AAGT,WAAS,WAAW,QAAQ,SAAS;AACnC,QAAI,OAAO,WAAW,YAAY,EAAE,kBAAkB,WACpD,OAAM,IAAI,MAAM,wBAAA;AAElB,QAAI,OAAO,OACT,OAAM,QAAQ,OAAA;;AAIlB,WAAS,OAAO,QAAQ;AACtB,QAAI,CAAC,OAAO,OAAQ,QAAO,UAAU,OAAO,QAAQ,OAAO,SAAA,EAAA;AAC3D,QAAI,SAAS,OAAO,KAAK,OAAO,KAAK,SAAS,CAAA,KAAM;AACpD,QAAI,MAAO,OAAO,MAAM;MAAE,MAAM,OAAO;MAAS,YAAY,CAAA;;AAG5D,QAAI,OAAO,IAAI,MACb,KAAI,KAAK,OAAO;AAElB,WAAO,WAAW,SAAS;AAC3B,aAAS,QAAQ,kBAAkB,GAAA;;AAGrC,WAAS,MAAM,MAAM,WAAW;AAE9B,QAAI,WADI,KAAK,QAAQ,GAAA,IACF,IAAI,CAAC,IAAI,IAAA,IAAQ,KAAK,MAAM,GAAA;AAC/C,QAAI,SAAS,SAAS,CAAA;AACtB,QAAI,QAAQ,SAAS,CAAA;AAGrB,QAAI,aAAa,SAAS,SAAS;AACjC,eAAS;AACT,cAAQ;;AAGV,WAAO;MAAU;MAAe;;;AAGlC,WAAS,OAAO,QAAQ;AACtB,QAAI,CAAC,OAAO,OACV,QAAO,aAAa,OAAO,WAAW,OAAO,SAAA,EAAA;AAG/C,QACE,OAAO,WAAW,QAAQ,OAAO,UAAA,MAAgB,MACjD,OAAO,IAAI,WAAW,eAAe,OAAO,UAAA,GAC5C;AACA,aAAO,aAAa,OAAO,cAAc;AACzC;;AAGF,QAAI,OAAO,IAAI,OAAO;AACpB,UAAI,KAAK,MAAM,OAAO,YAAY,IAAA;AAClC,UAAI,SAAS,GAAG;AAChB,UAAI,QAAQ,GAAG;AAEf,UAAI,WAAW,QAEb,KAAI,UAAU,SAAS,OAAO,gBAAgB,cAC5C,YACE,QACA,kCACE,gBACA,eAEA,OAAO,WAAA;eAGX,UAAU,WACV,OAAO,gBAAgB,gBAEvB,YACE,QACA,oCACE,kBACA,eAEA,OAAO,WAAA;WAEN;AACL,YAAI,MAAM,OAAO;AACjB,YAAI,SAAS,OAAO,KAAK,OAAO,KAAK,SAAS,CAAA,KAAM;AACpD,YAAI,IAAI,OAAO,OAAO,GACpB,KAAI,KAAK,OAAO,OAAO,OAAO,EAAA;AAEhC,YAAI,GAAG,KAAA,IAAS,OAAO;;AAO3B,aAAO,WAAW,KAAK,CAAC,OAAO,YAAY,OAAO,WAAA,CAAY;WACzD;AAEL,aAAO,IAAI,WAAW,OAAO,UAAA,IAAc,OAAO;AAClD,eAAS,QAAQ,eAAe;QAC9B,MAAM,OAAO;QACb,OAAO,OAAO;OACf;;AAGH,WAAO,aAAa,OAAO,cAAc;;AAG3C,WAAS,QAAQ,QAAQ,aAAa;AACpC,QAAI,OAAO,IAAI,OAAO;AAEpB,UAAI,MAAM,OAAO;AAGjB,UAAI,KAAK,MAAM,OAAO,OAAA;AACtB,UAAI,SAAS,GAAG;AAChB,UAAI,QAAQ,GAAG;AACf,UAAI,MAAM,IAAI,GAAG,GAAG,MAAA,KAAW;AAE/B,UAAI,IAAI,UAAU,CAAC,IAAI,KAAK;AAC1B,mBACE,QACA,+BAA+B,KAAK,UAAU,OAAO,OAAA,CAAQ;AAE/D,YAAI,MAAM,GAAG;;AAGf,UAAI,SAAS,OAAO,KAAK,OAAO,KAAK,SAAS,CAAA,KAAM;AACpD,UAAI,IAAI,MAAM,OAAO,OAAO,IAAI,GAC9B,QAAO,KAAK,IAAI,EAAA,EAAI,QAAQ,SAAU,GAAG;AACvC,iBAAS,QAAQ,mBAAmB;UAClC,QAAQ;UACR,KAAK,IAAI,GAAG,CAAA;SACb;;AAOL,eAAS,IAAI,GAAG,IAAI,OAAO,WAAW,QAAQ,IAAI,GAAG,KAAK;AACxD,YAAI,KAAK,OAAO,WAAW,CAAA;AAC3B,YAAI,OAAO,GAAG,CAAA;AACd,YAAI,QAAQ,GAAG,CAAA;AACf,YAAI,WAAW,MAAM,MAAM,IAAA;AAC3B,YAAI,SAAS,SAAS;AACtB,YAAI,QAAQ,SAAS;AACrB,YAAI,MAAM,WAAW,KAAK,KAAK,IAAI,GAAG,MAAA,KAAW;AACjD,YAAI,IAAI;UACA;UACC;UACC;UACD;UACF;;AAKP,YAAI,UAAU,WAAW,WAAW,CAAC,KAAK;AACxC,qBACE,QACA,+BAA+B,KAAK,UAAU,MAAA,CAAO;AAEvD,YAAE,MAAM;;AAEV,eAAO,IAAI,WAAW,IAAA,IAAQ;AAC9B,iBAAS,QAAQ,eAAe,CAAA;;AAElC,aAAO,WAAW,SAAS;;AAG7B,WAAO,IAAI,gBAAgB,CAAC,CAAC;AAG7B,WAAO,UAAU;AACjB,WAAO,KAAK,KAAK,OAAO,GAAA;AACxB,aAAS,QAAQ,aAAa,OAAO,GAAA;AACrC,QAAI,CAAC,aAAa;AAEhB,UAAI,CAAC,OAAO,YAAY,OAAO,QAAQ,YAAA,MAAkB,SACvD,QAAO,QAAQ,EAAE;UAEjB,QAAO,QAAQ,EAAE;AAEnB,aAAO,MAAM;AACb,aAAO,UAAU;;AAEnB,WAAO,aAAa,OAAO,cAAc;AACzC,WAAO,WAAW,SAAS;;AAG7B,WAAS,SAAS,QAAQ;AACxB,QAAI,CAAC,OAAO,SAAS;AACnB,iBAAW,QAAQ,wBAAA;AACnB,aAAO,YAAY;AACnB,aAAO,QAAQ,EAAE;AACjB;;AAGF,QAAI,OAAO,QAAQ;AACjB,UAAI,OAAO,YAAY,UAAU;AAC/B,eAAO,UAAU,OAAO,OAAO,UAAU;AACzC,eAAO,UAAU;AACjB,eAAO,QAAQ,EAAE;AACjB;;AAEF,eAAS,QAAQ,YAAY,OAAO,MAAA;AACpC,aAAO,SAAS;;AAKlB,QAAI,IAAI,OAAO,KAAK;AACpB,QAAI,UAAU,OAAO;AACrB,QAAI,CAAC,OAAO,OACV,WAAU,QAAQ,OAAO,SAAA,EAAA;AAE3B,QAAI,UAAU;AACd,WAAO,IAEL,KADY,OAAO,KAAK,CAAA,EACd,SAAS,QAEjB,YAAW,QAAQ,sBAAA;QAEnB;AAKJ,QAAI,IAAI,GAAG;AACT,iBAAW,QAAQ,4BAA4B,OAAO,OAAA;AACtD,aAAO,YAAY,OAAO,OAAO,UAAU;AAC3C,aAAO,QAAQ,EAAE;AACjB;;AAEF,WAAO,UAAU;AACjB,QAAIE,KAAI,OAAO,KAAK;AACpB,WAAOA,OAAM,GAAG;AACd,UAAI,MAAO,OAAO,MAAM,OAAO,KAAK,IAAA;AACpC,aAAO,UAAU,OAAO,IAAI;AAC5B,eAAS,QAAQ,cAAc,OAAO,OAAA;AAEtC,UAAI,IAAI,CAAA;AACR,eAAS,KAAK,IAAI,GAChB,GAAE,CAAA,IAAK,IAAI,GAAG,CAAA;AAGhB,UAAI,SAAS,OAAO,KAAK,OAAO,KAAK,SAAS,CAAA,KAAM;AACpD,UAAI,OAAO,IAAI,SAAS,IAAI,OAAO,OAAO,GAExC,QAAO,KAAK,IAAI,EAAA,EAAI,QAAQ,SAAU,GAAG;AACvC,YAAI,IAAI,IAAI,GAAG,CAAA;AACf,iBAAS,QAAQ,oBAAoB;UAAE,QAAQ;UAAG,KAAK;SAAG;;;AAIhE,QAAI,MAAM,EAAG,QAAO,aAAa;AACjC,WAAO,UAAU,OAAO,cAAc,OAAO,aAAa;AAC1D,WAAO,WAAW,SAAS;AAC3B,WAAO,QAAQ,EAAE;;AAGnB,WAAS,YAAY,QAAQ;AAC3B,QAAI,SAAS,OAAO;AACpB,QAAI,WAAW,OAAO,YAAA;AACtB,QAAI;AACJ,QAAI,SAAS;AAEb,QAAI,OAAO,SAAS,MAAA,EAClB,QAAO,OAAO,SAAS,MAAA;AAEzB,QAAI,OAAO,SAAS,QAAA,EAClB,QAAO,OAAO,SAAS,QAAA;AAEzB,aAAS;AACT,QAAI,OAAO,OAAO,CAAA,MAAO,IACvB,KAAI,OAAO,OAAO,CAAA,MAAO,KAAK;AAC5B,eAAS,OAAO,MAAM,CAAA;AACtB,YAAM,SAAS,QAAQ,EAAA;AACvB,eAAS,IAAI,SAAS,EAAA;WACjB;AACL,eAAS,OAAO,MAAM,CAAA;AACtB,YAAM,SAAS,QAAQ,EAAA;AACvB,eAAS,IAAI,SAAS,EAAA;;AAG1B,aAAS,OAAO,QAAQ,OAAO,EAAA;AAC/B,QAAI,MAAM,GAAA,KAAQ,OAAO,YAAA,MAAkB,QAAQ;AACjD,iBAAW,QAAQ,0BAAA;AACnB,aAAO,MAAM,OAAO,SAAS;;AAG/B,WAAO,OAAO,cAAc,GAAA;;AAG9B,WAAS,gBAAgB,QAAQ,GAAG;AAClC,QAAI,MAAM,KAAK;AACb,aAAO,QAAQ,EAAE;AACjB,aAAO,mBAAmB,OAAO;eACxB,CAAC,aAAa,CAAA,GAAI;AAG3B,iBAAW,QAAQ,kCAAA;AACnB,aAAO,WAAW;AAClB,aAAO,QAAQ,EAAE;;;AAIrB,WAAS,OAAO,OAAO,GAAG;AACxB,QAAI,SAAS;AACb,QAAI,IAAI,MAAM,OACZ,UAAS,MAAM,OAAO,CAAA;AAExB,WAAO;;AAGT,WAAS,MAAM,OAAO;AACpB,QAAI,SAAS;AACb,QAAI,KAAK,MACP,OAAM,KAAK;AAEb,QAAI,OAAO,OACT,QAAO,MACL,QACA,sDAAA;AAGJ,QAAI,UAAU,KACZ,QAAO,IAAI,MAAA;AAEb,QAAI,OAAO,UAAU,SACnB,SAAQ,MAAM,SAAA;AAEhB,QAAI,IAAI;AACR,QAAI,IAAI;AACR,WAAO,MAAM;AACX,UAAI,OAAO,OAAO,GAAA;AAClB,aAAO,IAAI;AAEX,UAAI,CAAC,EACH;AAGF,UAAI,OAAO,eAAe;AACxB,eAAO;AACP,YAAI,MAAM,MAAM;AACd,iBAAO;AACP,iBAAO,SAAS;cAEhB,QAAO;;AAIX,cAAQ,OAAO,OAAf;QACE,KAAK,EAAE;AACL,iBAAO,QAAQ,EAAE;AACjB,cAAI,MAAM,SACR;AAEF,0BAAgB,QAAQ,CAAA;AACxB;QAEF,KAAK,EAAE;AACL,0BAAgB,QAAQ,CAAA;AACxB;QAEF,KAAK,EAAE;AACL,cAAI,OAAO,WAAW,CAAC,OAAO,YAAY;AACxC,gBAAI,SAAS,IAAI;AACjB,mBAAO,KAAK,MAAM,OAAO,MAAM,KAAK;AAClC,kBAAI,OAAO,OAAO,GAAA;AAClB,kBAAI,KAAK,OAAO,eAAe;AAC7B,uBAAO;AACP,oBAAI,MAAM,MAAM;AACd,yBAAO;AACP,yBAAO,SAAS;sBAEhB,QAAO;;;AAIb,mBAAO,YAAY,MAAM,UAAU,QAAQ,IAAI,CAAA;;AAEjD,cACE,MAAM,OACN,EAAE,OAAO,WAAW,OAAO,cAAc,CAAC,OAAO,SACjD;AACA,mBAAO,QAAQ,EAAE;AACjB,mBAAO,mBAAmB,OAAO;iBAC5B;AACL,gBAAI,CAAC,aAAa,CAAA,MAAO,CAAC,OAAO,WAAW,OAAO,YACjD,YAAW,QAAQ,iCAAA;AAErB,gBAAI,MAAM,IACR,QAAO,QAAQ,EAAE;gBAEjB,QAAO,YAAY;;AAGvB;QAEF,KAAK,EAAE;AAEL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;cAEjB,QAAO,UAAU;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;eACZ;AACL,mBAAO,UAAU,MAAM;AACvB,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AAEL,cAAI,MAAM,KAAK;AACb,mBAAO,QAAQ,EAAE;AACjB,mBAAO,WAAW;qBACT,aAAa,CAAA,GAAI;UAAA,WAEjB,QAAQ,WAAW,CAAA,GAAI;AAChC,mBAAO,QAAQ,EAAE;AACjB,mBAAO,UAAU;qBACR,MAAM,KAAK;AACpB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,UAAU;qBACR,MAAM,KAAK;AACpB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,eAAe,OAAO,eAAe;iBACvC;AACL,uBAAW,QAAQ,aAAA;AAEnB,gBAAI,OAAO,mBAAmB,IAAI,OAAO,UAAU;AACjD,kBAAI,MAAM,OAAO,WAAW,OAAO;AACnC,kBAAI,IAAI,MAAM,GAAA,EAAK,KAAK,GAAA,IAAO;;AAEjC,mBAAO,YAAY,MAAM;AACzB,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AACL,eAAK,OAAO,WAAW,GAAG,YAAA,MAAkB,OAAO;AACjD,qBAAS,QAAQ,aAAA;AACjB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,WAAW;AAClB,mBAAO,QAAQ;qBACN,OAAO,WAAW,MAAM,MAAM;AACvC,mBAAO,QAAQ,EAAE;AACjB,mBAAO,UAAU;AACjB,mBAAO,WAAW;sBACR,OAAO,WAAW,GAAG,YAAA,MAAkB,SAAS;AAC1D,mBAAO,QAAQ,EAAE;AACjB,gBAAI,OAAO,WAAW,OAAO,QAC3B,YAAW,QAAQ,6CAAA;AAErB,mBAAO,UAAU;AACjB,mBAAO,WAAW;qBACT,MAAM,KAAK;AACpB,qBAAS,QAAQ,qBAAqB,OAAO,QAAA;AAC7C,mBAAO,WAAW;AAClB,mBAAO,QAAQ,EAAE;qBACR,QAAQ,CAAA,GAAI;AACrB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,YAAY;gBAEnB,QAAO,YAAY;AAErB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,OAAO,GAAG;AAClB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,IAAI;;AAEb,iBAAO,YAAY;AACnB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,KAAK;AACb,mBAAO,QAAQ,EAAE;AACjB,qBAAS,QAAQ,aAAa,OAAO,OAAA;AACrC,mBAAO,UAAU;iBACZ;AACL,mBAAO,WAAW;AAClB,gBAAI,MAAM,IACR,QAAO,QAAQ,EAAE;qBACR,QAAQ,CAAA,GAAI;AACrB,qBAAO,QAAQ,EAAE;AACjB,qBAAO,IAAI;;;AAGf;QAEF,KAAK,EAAE;AACL,iBAAO,WAAW;AAClB,cAAI,MAAM,OAAO,GAAG;AAClB,mBAAO,IAAI;AACX,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AACL,iBAAO,WAAW;AAClB,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;mBACR,QAAQ,CAAA,GAAI;AACrB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,IAAI;;AAEb;QAEF,KAAK,EAAE;AACL,iBAAO,WAAW;AAClB,cAAI,MAAM,OAAO,GAAG;AAClB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,IAAI;;AAEb;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;cAEjB,QAAO,WAAW;AAEpB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,KAAK;AACb,mBAAO,QAAQ,EAAE;AACjB,mBAAO,UAAU,SAAS,OAAO,KAAK,OAAO,OAAA;AAC7C,gBAAI,OAAO,QACT,UAAS,QAAQ,aAAa,OAAO,OAAA;AAEvC,mBAAO,UAAU;iBACZ;AACL,mBAAO,WAAW,MAAM;AACxB,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,KAAK;AACb,uBAAW,QAAQ,mBAAA;AAGnB,mBAAO,WAAW,OAAO;AACzB,mBAAO,QAAQ,EAAE;gBAEjB,QAAO,QAAQ,EAAE;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;cAEjB,QAAO,SAAS;AAElB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;eACZ;AACL,mBAAO,SAAS,MAAM;AACtB,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,KAAK;AACb,gBAAI,OAAO,MACT,UAAS,QAAQ,WAAW,OAAO,KAAA;AAErC,qBAAS,QAAQ,cAAA;AACjB,mBAAO,QAAQ;AACf,mBAAO,QAAQ,EAAE;qBACR,MAAM,IACf,QAAO,SAAS;eACX;AACL,mBAAO,SAAS,OAAO;AACvB,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;mBACR,aAAa,CAAA,EACtB,QAAO,QAAQ,EAAE;cAEjB,QAAO,gBAAgB;AAEzB;QAEF,KAAK,EAAE;AACL,cAAI,CAAC,OAAO,gBAAgB,aAAa,CAAA,EACvC;mBACS,MAAM,IACf,QAAO,QAAQ,EAAE;cAEjB,QAAO,gBAAgB;AAEzB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,KAAK;AACb,qBAAS,QAAQ,2BAA2B;cAC1C,MAAM,OAAO;cACb,MAAM,OAAO;aACd;AACD,mBAAO,eAAe,OAAO,eAAe;AAC5C,mBAAO,QAAQ,EAAE;iBACZ;AACL,mBAAO,gBAAgB,MAAM;AAC7B,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,QAAQ,UAAU,CAAA,EACpB,QAAO,WAAW;eACb;AACL,mBAAO,MAAA;AACP,gBAAI,MAAM,IACR,SAAQ,MAAA;qBACC,MAAM,IACf,QAAO,QAAQ,EAAE;iBACZ;AACL,kBAAI,CAAC,aAAa,CAAA,EAChB,YAAW,QAAQ,+BAAA;AAErB,qBAAO,QAAQ,EAAE;;;AAGrB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,KAAK;AACb,oBAAQ,QAAQ,IAAA;AAChB,qBAAS,MAAA;iBACJ;AACL,uBACE,QACA,gDAAA;AAEF,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AAEL,cAAI,aAAa,CAAA,EACf;mBACS,MAAM,IACf,SAAQ,MAAA;mBACC,MAAM,IACf,QAAO,QAAQ,EAAE;mBACR,QAAQ,WAAW,CAAA,GAAI;AAChC,mBAAO,aAAa;AACpB,mBAAO,cAAc;AACrB,mBAAO,QAAQ,EAAE;gBAEjB,YAAW,QAAQ,wBAAA;AAErB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;mBACR,MAAM,KAAK;AACpB,uBAAW,QAAQ,yBAAA;AACnB,mBAAO,cAAc,OAAO;AAC5B,mBAAO,MAAA;AACP,oBAAQ,MAAA;qBACC,aAAa,CAAA,EACtB,QAAO,QAAQ,EAAE;mBACR,QAAQ,UAAU,CAAA,EAC3B,QAAO,cAAc;cAErB,YAAW,QAAQ,wBAAA;AAErB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,IACR,QAAO,QAAQ,EAAE;mBACR,aAAa,CAAA,EACtB;eACK;AACL,uBAAW,QAAQ,yBAAA;AACnB,mBAAO,IAAI,WAAW,OAAO,UAAA,IAAc;AAC3C,mBAAO,cAAc;AACrB,qBAAS,QAAQ,eAAe;cAC9B,MAAM,OAAO;cACb,OAAO;aACR;AACD,mBAAO,aAAa;AACpB,gBAAI,MAAM,IACR,SAAQ,MAAA;qBACC,QAAQ,WAAW,CAAA,GAAI;AAChC,qBAAO,aAAa;AACpB,qBAAO,QAAQ,EAAE;mBACZ;AACL,yBAAW,QAAQ,wBAAA;AACnB,qBAAO,QAAQ,EAAE;;;AAGrB;QAEF,KAAK,EAAE;AACL,cAAI,aAAa,CAAA,EACf;mBACS,QAAQ,CAAA,GAAI;AACrB,mBAAO,IAAI;AACX,mBAAO,QAAQ,EAAE;iBACZ;AACL,uBAAW,QAAQ,0BAAA;AACnB,mBAAO,QAAQ,EAAE;AACjB,mBAAO,cAAc;;AAEvB;QAEF,KAAK,EAAE;AACL,cAAI,MAAM,OAAO,GAAG;AAClB,gBAAI,MAAM,IACR,QAAO,QAAQ,EAAE;gBAEjB,QAAO,eAAe;AAExB;;AAEF,iBAAO,MAAA;AACP,iBAAO,IAAI;AACX,iBAAO,QAAQ,EAAE;AACjB;QAEF,KAAK,EAAE;AACL,cAAI,aAAa,CAAA,EACf,QAAO,QAAQ,EAAE;mBACR,MAAM,IACf,SAAQ,MAAA;mBACC,MAAM,IACf,QAAO,QAAQ,EAAE;mBACR,QAAQ,WAAW,CAAA,GAAI;AAChC,uBAAW,QAAQ,kCAAA;AACnB,mBAAO,aAAa;AACpB,mBAAO,cAAc;AACrB,mBAAO,QAAQ,EAAE;gBAEjB,YAAW,QAAQ,wBAAA;AAErB;QAEF,KAAK,EAAE;AACL,cAAI,CAAC,YAAY,CAAA,GAAI;AACnB,gBAAI,MAAM,IACR,QAAO,QAAQ,EAAE;gBAEjB,QAAO,eAAe;AAExB;;AAEF,iBAAO,MAAA;AACP,cAAI,MAAM,IACR,SAAQ,MAAA;cAER,QAAO,QAAQ,EAAE;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,CAAC,OAAO,QACV,KAAI,aAAa,CAAA,EACf;mBACS,SAAS,WAAW,CAAA,EAC7B,KAAI,OAAO,QAAQ;AACjB,mBAAO,UAAU,OAAO;AACxB,mBAAO,QAAQ,EAAE;gBAEjB,YAAW,QAAQ,iCAAA;cAGrB,QAAO,UAAU;mBAEV,MAAM,IACf,UAAS,MAAA;mBACA,QAAQ,UAAU,CAAA,EAC3B,QAAO,WAAW;mBACT,OAAO,QAAQ;AACxB,mBAAO,UAAU,OAAO,OAAO;AAC/B,mBAAO,UAAU;AACjB,mBAAO,QAAQ,EAAE;iBACZ;AACL,gBAAI,CAAC,aAAa,CAAA,EAChB,YAAW,QAAQ,gCAAA;AAErB,mBAAO,QAAQ,EAAE;;AAEnB;QAEF,KAAK,EAAE;AACL,cAAI,aAAa,CAAA,EACf;AAEF,cAAI,MAAM,IACR,UAAS,MAAA;cAET,YAAW,QAAQ,mCAAA;AAErB;QAEF,KAAK,EAAE;QACP,KAAK,EAAE;QACP,KAAK,EAAE;AACL,cAAI;AACJ,cAAI;AACJ,kBAAQ,OAAO,OAAf;YACE,KAAK,EAAE;AACL,4BAAc,EAAE;AAChB,uBAAS;AACT;YAEF,KAAK,EAAE;AACL,4BAAc,EAAE;AAChB,uBAAS;AACT;YAEF,KAAK,EAAE;AACL,4BAAc,EAAE;AAChB,uBAAS;AACT;;AAGJ,cAAI,MAAM,IACR,KAAI,OAAO,IAAI,kBAAkB;AAC/B,gBAAI,eAAe,YAAY,MAAA;AAC/B,mBAAO,SAAS;AAChB,mBAAO,QAAQ;AACf,mBAAO,MAAM,YAAA;iBACR;AACL,mBAAO,MAAA,KAAW,YAAY,MAAA;AAC9B,mBAAO,SAAS;AAChB,mBAAO,QAAQ;;mBAGjB,QAAQ,OAAO,OAAO,SAAS,aAAa,aAAa,CAAA,EAEzD,QAAO,UAAU;eACZ;AACL,uBAAW,QAAQ,kCAAA;AACnB,mBAAO,MAAA,KAAW,MAAM,OAAO,SAAS;AACxC,mBAAO,SAAS;AAChB,mBAAO,QAAQ;;AAGjB;QAEF;AACE,gBAAM,IAAI,MAAM,QAAQ,oBAAoB,OAAO,KAAA;;;AAKzD,QAAI,OAAO,YAAY,OAAO,oBAC5B,mBAAkB,MAAA;AAEpB,WAAO;;AAKT,MAAI,CAAC,OAAO,cACV,EAAC,WAAY;AACX,QAAI,qBAAqB,OAAO;AAChC,QAAI,QAAQ,KAAK;AACjB,QAAI,gBAAgB,WAAY;AAC9B,UAAI,WAAW;AACf,UAAI,YAAY,CAAA;AAChB,UAAI;AACJ,UAAI;AACJ,UAAI,QAAQ;AACZ,UAAI,SAAS,UAAU;AACvB,UAAI,CAAC,OACH,QAAO;AAET,UAAI,SAAS;AACb,aAAO,EAAE,QAAQ,QAAQ;AACvB,YAAI,YAAY,OAAO,UAAU,KAAA,CAAA;AACjC,YACE,CAAC,SAAS,SAAA,KACV,YAAY,KACZ,YAAY,WACZ,MAAM,SAAA,MAAe,UAErB,OAAM,WAAW,yBAAyB,SAAA;AAE5C,YAAI,aAAa,MAEf,WAAU,KAAK,SAAA;aACV;AAGL,uBAAa;AACb,2BAAiB,aAAa,MAAM;AACpC,yBAAgB,YAAY,OAAS;AACrC,oBAAU,KAAK,eAAe,YAAA;;AAEhC,YAAI,QAAQ,MAAM,UAAU,UAAU,SAAS,UAAU;AACvD,oBAAU,mBAAmB,MAAM,MAAM,SAAA;AACzC,oBAAU,SAAS;;;AAGvB,aAAO;;AAGT,QAAI,OAAO,eACT,QAAO,eAAe,QAAQ,iBAAiB;MAC7C,OAAO;MACP,cAAc;MACd,UAAU;KACX;QAED,QAAO,gBAAgB;;AAI7B,SAAOF;;AAGT,IAAM,MAAuB,cAAA;;;ACzlD7B,IAAa,0BAA0B;;;;;;;;;;;;;;;;;;;;;;;AA6BvC,IAAa,kBAAb,cAAqC,oCAA+C;EAClF;EAEA,YAAY,QAAgC;AAC1C,UAAM,MAAA;AAEN,SAAK,OAAO,QAAQ;;EAGtB,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe,CAAC,kBAAkB,gBAAA;EAElC,kBAAkB;EAER,MACR,MACA,MACyB;AACzB,QAAI,CAAC,KACH;AAEF,QAAI,CAAC,KACH,QAAO,CAAC;MAAE,IAAI;MAAW,MAAM;MAAI,OAAO;KAAM;AAElD,WAAO,QAAQ,MAAM,IAAA;;EAGvB,MAAM,mBACJ,aACgC;AAChC,WAAO,iBAAiB,YAAY,CAAA,EAAG,IAAA;;EAGzC,MAAM,MAAM,MAAkC;AAC5C,WAAO,iBAAiB,IAAA;;EAG1B,wBAAgC;AAE9B,WADiB,CAAC,EAAE,KAAK,QAAQ,KAAK,KAAK,SAAS,KAEhD,wBAAwB,QAAQ,UAAU,KAAK,MAAM,KAAK,IAAA,KAAS,EAAA,IACnE;;;AAIR,IAAM,QAAA,CAAS,SACb,KACG,MAAM,IAAA,EACN,IAAA,CAAK,SAAS,KAAK,QAAQ,QAAQ,EAAA,CAAG,EACtC,KAAK,IAAA,EACL,KAAA;AAWL,IAAM,oBAAA,CAAqB,UAAmC;AAC5D,MAAI,OAAO,KAAK,KAAA,EAAO,WAAW,EAChC,QAAO,CAAA;AAET,QAAM,SAAoB,CAAA;AAC1B,MAAI,MAAM,SAAS,SAAS,GAAG;AAC7B,WAAO,MAAM,IAAA,IAAQ,MAAM,SAAS,IAAI,iBAAA;AACxC,WAAO;SACF;AACL,WAAO,MAAM,IAAA,IAAQ,MAAM,QAAQ;AACnC,WAAO;;;AAIX,SAAgB,iBAAiB,GAAsB;AACrD,QAAM,gBAAgB,MAAM,CAAA;AAC5B,QAAM,SAAS,IAAI,OAAO,IAAA;AAC1B,MAAI,eAA6B,CAAA;AACjC,QAAM,eAA+B,CAAA;AAGrC,SAAO,YAAA,CAAa,SAAc;AAChC,UAAM,UAAU;MACd,MAAM,KAAK;MACX,YAAY,KAAK;MACjB,UAAU,CAAA;MACV,MAAM;MACN,eAAe,KAAK;;AAGtB,QAAI,aAAa,SAAS,EACF,cAAa,aAAa,SAAS,CAAA,EAC3C,SAAS,KAAK,OAAA;QAE5B,gBAAe;AAGjB,QAAI,CAAC,KAAK,cACR,cAAa,KAAK,OAAA;;AAItB,SAAO,aAAA,MAAmB;AACxB,QAAI,aAAa,SAAS,GAAG;AAC3B,YAAM,cAAc,aAAa,IAAA;AACjC,UAAI,aAAa,WAAW,KAAK,YAC/B,gBAAe;;;AAMrB,SAAO,SAAA,CAAU,SAAc;AAC7B,QAAI,aAAa,SAAS,GAAG;AAC3B,YAAM,iBAAiB,aAAa,aAAa,SAAS,CAAA;AAC1D,qBAAe,QAAQ;;;AAK3B,SAAO,cAAA,CAAe,SAAc;AAClC,QAAI,aAAa,SAAS,GAAG;AAC3B,YAAM,iBAAiB,aAAa,aAAa,SAAS,CAAA;AAC1D,qBAAe,WAAW,KAAK,IAAA,IAAQ,KAAK;;;AAKhD,QAAM,QAAQ,oBAAoB,KAAK,aAAA;AACvC,QAAM,YAAY,QAAQ,MAAM,CAAA,IAAK;AACrC,SAAO,MAAM,SAAA,EAAW,MAAA;AAGxB,MAAI,gBAAgB,aAAa,SAAS,OACxC,gBAAe,aAAa,SAAS,CAAA;AAGvC,SAAO,kBAAkB,YAAA;;;;ACtK3B,IAAI,yBAAyC,YAAY;AAAA,EACxD,kCAAkC,MAAM;AAAA,EACxC,qCAAqC,MAAM;AAAA,EAC3C,qBAAqB,MAAM;AAAA,EAC3B,kBAAkB,MAAM;AAAA,EACxB,2BAA2B,MAAM;AAAA,EACjC,mBAAmB,MAAM;AAAA,EACzB,gCAAgC,MAAM;AAAA,EACtC,wBAAwB,MAAM;AAAA,EAC9B,oCAAoC,MAAM;AAAA,EAC1C,kBAAkB,MAAM;AAAA,EACxB,kBAAkB,MAAM;AAAA,EACxB,0BAA0B,MAAM;AAAA,EAChC,0BAA0B,MAAM;AAAA,EAChC,uBAAuB,MAAM;AAAA,EAC7B,oBAAoB,MAAM;AAAA,EAC1B,wBAAwB,MAAM;AAAA,EAC9B,iBAAiB,MAAM;AAAA,EACvB,yBAAyB,MAAM;AAAA,EAC/B,mBAAmB,MAAM;AAAA,EACzB,kBAAkB,MAAM;AAAA,EACxB,kBAAkB,MAAM;AACzB,CAAC;;;ACWD,SAAgBG,eAEd,aACA,SACsB;AACtB,MAAI,YAAY,aAAa,OAC3B;AAEF,MAAI;AACJ,MAAI,SAAS,QACX,KAAI;AACF,mBAAe,iBAAiB,YAAY,SAAS,aAAa,IAAA;UAC5D;AACN;;MAGF,KAAI;AACF,mBAAe,KAAK,MAAM,YAAY,SAAS,SAAA;WAExC,GAAQ;AACf,UAAM,IAAI,sBACR;MACE,aAAa,YAAY,SAAS,IAAA;MAClC;MACA,YAAY,SAAS;MACrB;MACA;MACA,UAAU,EAAE,OAAA;MACZ,KAAK,IAAA,CAAK;;AAKlB,QAAM,iBAA2B;IAC/B,MAAM,YAAY,SAAS;IAC3B,MAAM;IACN,MAAM;;AAGR,MAAI,SAAS,SACX,gBAAe,KAAK,YAAY;AAGlC,SAAO;;AAGT,SAAgB,iCAAiC,UAAoB;AACnE,MAAI,SAAS,OAAO,OAClB,OAAM,IAAI,MAAM,gDAAA;AAElB,SAAO;IACL,IAAI,SAAS;IACb,MAAM;IACN,UAAU;MACR,MAAM,SAAS;MACf,WAAW,KAAK,UAAU,SAAS,IAAA;;;;AAKzC,SAAgB,oBAEd,aACA,UACiB;AACjB,SAAO;IACL,MAAM,YAAY,UAAU;IAC5B,MAAM,YAAY,UAAU;IAC5B,IAAI,YAAY;IAChB,OAAO;IACP,MAAM;;;AAOV,IAAa,wBAAb,cAEU,oCAAuC;EAC/C,OAAO,UAAU;AACf,WAAO;;EAGT,WAAW;EAEX,eAAe;IAAC;IAAa;IAAkB;;EAE/C,kBAAkB;EAElB,YAAY,QAAsC;AAChD,UAAM,MAAA;AACN,SAAK,WAAW,QAAQ,YAAY,KAAK;;EAGjC,QAAQ;AAChB,UAAM,IAAI,MAAM,gBAAA;;EAGlB,MAAM,QAAoB;AACxB,UAAM,IAAI,MAAM,kBAAA;;EAGlB,MAAM,YAAY,aAA2C;AAE3D,WADe,MAAM,KAAK,mBAAmB,aAAa,KAAA;;;;;;;;EAU5D,MAAM,mBACJ,aACA,UAAU,MAEI;AACd,UAAM,UAAU,YAAY,CAAA,EAAG;AAC/B,QAAI;AACJ,QAAI,YAAY,OAAA,KAAY,QAAQ,YAAY,OAC9C,aAAY,QAAQ,WAAW,IAAA,CAAK,aAAa;AAC/C,YAAM,EAAE,IAAI,GAAG,KAAA,IAAS;AACxB,UAAI,CAAC,KAAK,SACR,QAAO;AAET,aAAO;QACL;QACA,GAAG;;;aAGE,QAAQ,kBAAkB,eAAe,OAIlD,aAHqB,KAAK,MACxB,KAAK,UAAU,QAAQ,kBAAkB,UAAA,CAAW,EAE7B,IAAA,CAAK,gBAAyC;AACrE,aAAOA,eAAc,aAAa;QAAE,UAAU,KAAK;QAAU;OAAS;;AAG1E,QAAI,CAAC,UACH,QAAO,CAAA;AAET,UAAM,kBAAkB,CAAA;AACxB,eAAW,YAAY,UACrB,KAAI,aAAa,QAAW;AAC1B,YAAM,8BAA8C;QAClD,MAAM,SAAS;QACf,MAAM,SAAS;QACf,IAAI,SAAS;;AAEf,sBAAgB,KAAK,2BAAA;;AAGzB,WAAO;;;AAkCX,IAAa,2BAAb,cAGU,sBAAyB;EACjC,OAAO,UAAU;AACf,WAAO;;EAGT,eAAe;IAAC;IAAa;IAAkB;;EAE/C,kBAAkB;EAElB,WAAW;;EAGX;;EAGA,eAAe;EAEf;EAQA,YACE,QAIA;AACA,UAAM,MAAA;AACN,SAAK,UAAU,OAAO;AACtB,SAAK,eAAe,OAAO,gBAAgB,KAAK;AAChD,SAAK,YAAY,OAAO;;EAG1B,MAAgB,gBAAgB,QAA6B;AAC3D,QAAI,KAAK,cAAc,OACrB,QAAO;AAET,UAAM,kBAAkB,MAAM,sBAAsB,KAAK,WAAW,MAAA;AACpE,QAAI,gBAAgB,QAClB,QAAO,gBAAgB;QAEvB,OAAM,IAAI,sBACR,2BAA2B,KAAK,UAC9B,QACA,MACA,CAAA,CACD,aAAa,KAAK,UAAU,gBAAgB,OAAO,MAAA,CAAO,IAC3D,KAAK,UAAU,QAAQ,MAAM,CAAA,CAAE;;EAMrC,MAAM,mBAAmB,aAA6C;AAEpE,UAAM,mBADU,MAAM,MAAM,mBAAmB,WAAA,GACf,OAAA,CAC7B,WAA2B,OAAO,SAAS,KAAK,OAAA;AAGnD,QAAI,iBACF;AACF,QAAI,CAAC,gBAAgB,OACnB;AAEF,QAAI,CAAC,KAAK,SACR,kBAAiB,gBAAgB,IAAA,CAC9B,WAA2B,OAAO,IAAA;AAGvC,QAAI,KAAK,aACP,QAAO,eAAe,CAAA;AAExB,WAAO;;EAIT,MAAM,YAAY,aAA6C;AAE7D,UAAM,mBADU,MAAM,MAAM,mBAAmB,aAAa,KAAA,GAC5B,OAAA,CAC7B,WAA2B,OAAO,SAAS,KAAK,OAAA;AAGnD,QAAI,iBACF;AACF,QAAI,CAAC,gBAAgB,OACnB;AAEF,QAAI,CAAC,KAAK,SACR,kBAAiB,gBAAgB,IAAA,CAC9B,WAA2B,OAAO,IAAA;AAGvC,QAAI,KAAK,aACP,QAAO,KAAK,gBAAgB,eAAe,CAAA,CAAA;AAK7C,WAHwB,MAAM,QAAQ,IACpC,eAAe,IAAA,CAAK,UAAU,KAAK,gBAAgB,KAAA,CAAM,CAAC;;;;;AC5UhE,IAAI,uBAAuC,YAAY;AAAA,EACtD,0BAA0B,MAAM;AAAA,EAChC,uBAAuB,MAAM;AAAA,EAC7B,kCAAkC,MAAM;AAAA,EACxC,qBAAqB,MAAM;AAAA,EAC3B,eAAe,MAAMC;AACtB,CAAC;;;AC0ND,SAAgB,yBACd,eACA,WACqB;AACrB,MAAI,OAAO,kBAAkB,SAC3B,QAAO;IAAE,OAAO;IAAe,GAAI,aAAa,CAAA;;AAElD,MAAI,iBAAiB,KACnB,QAAO;AAET,SAAO;;AAIT,IAAsB,iBAAtB,cAGU,cAEV;EACE;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA,QAAQ;EAER;EAEA;EAEA;EAEA;EAEA;EAEA,YAAY;EAEZ,cAAc;EAEd;EAEA;EAEA;EAEA;EAEA;EAEA;;EAGA;;EAGA;;;;;EAMA;EAEA;EAEA;EAEA;;;;;;;;;;;;EAaA;;;;;EAMA;;;;;;EAOA;;;;EAKA;;;;EAKA;EAEU;EAEV,WAAW;AACT,WAAO;;EAGT,OAAO,UAAU;AACf,WAAO;;EAGT,IAAI,WAAW;AACb,WAAO;MACL,GAAG,MAAM;MACT;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;;;EAIJ,kBAAkB;EAElB,IAAI,aAAoD;AACtD,WAAO;MACL,QAAQ;MACR,cAAc;;;EAIlB,IAAI,aAAqC;AACvC,WAAO;MACL,QAAQ;MACR,WAAW;;;EAIf,IAAI,uBAAiC;AACnC,WAAO;MACL;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;;;EAIJ,YAAY,SAAqD;AAC/D,UAAM,SAAS,KAAK,iBAAiB,OAAA;AACrC,WAAO;MACL,aAAa;MACb,eAAe,KAAK;MACpB,eAAe;MACf,gBAAgB,OAAO,eAAe;MACtC,eAAe,OAAO,cAAc;MACpC,SAAS,QAAQ;;;;EAKrB,qBAKkB;AAChB,WAAO;MACL,YAAY,KAAK;MACjB,GAAG,KAAK,iBAAA;MACR,GAAG,KAAK;;;;;;EAOZ,oBAAoB;AAClB,WAAO,KAAK,mBAAA;;EAGd,YAAY,QAA+B;AACzC,UAAM,UAAU,CAAA,CAAE;AAElB,UAAM,eACJ,OAAO,QAAQ,eAAe,WAAW,YACzC,OAAO,QAAQ,eAAe,WAAW,aACrC,QAAQ,eAAe,SACvB;AACN,SAAK,SACH,QAAQ,UACR,gBACA,uBAAuB,gBAAA;AACzB,SAAK,eACH,QAAQ,eAAe,gBACvB,uBAAuB,qBAAA;AAEzB,SAAK,QAAQ,QAAQ,SAAS,QAAQ,aAAa,KAAK;AACxD,SAAK,cAAc,QAAQ,eAAe,CAAA;AAC1C,SAAK,UAAU,QAAQ;AAEvB,SAAK,cAAc,QAAQ,eAAe,KAAK;AAC/C,SAAK,OAAO,QAAQ,QAAQ,KAAK;AACjC,SAAK,mBAAmB,QAAQ,oBAAoB,KAAK;AACzD,SAAK,kBAAkB,QAAQ,mBAAmB,KAAK;AACvD,SAAK,WAAW,QAAQ;AACxB,SAAK,cAAc,QAAQ;AAC3B,SAAK,IAAI,QAAQ,KAAK,KAAK;AAC3B,SAAK,YAAY,QAAQ;AACzB,SAAK,OAAO,QAAQ,iBAAiB,QAAQ;AAC7C,SAAK,gBAAgB,KAAK;AAC1B,SAAK,OAAO,QAAQ;AACpB,SAAK,uBAAuB,QAAQ;AACpC,SAAK,QAAQ,QAAQ;AACrB,SAAK,aAAa,QAAQ;AAC1B,SAAK,YAAY,QAAQ;AACzB,SAAK,YAAY,QAAQ,uBAAuB,QAAQ;AACxD,SAAK,iBAAiB,QAAQ,kBAAkB,KAAK;AACrD,SAAK,uBACH,QAAQ,wBAAwB,KAAK;AACvC,SAAK,YAAY,QAAQ,aAAa,KAAK;AAE3C,SAAK,mBAAmB,QAAQ,qBAAqB;AACrD,SAAK,YAAY,QAAQ,cAAc;AACvC,QAAI,KAAK,iBAAkB,MAAK,YAAY;AAE5C,QAAI,QAAQ,cAAc,MAAO,MAAK,mBAAmB;AAEzD,SAAK,cAAc,QAAQ,eAAe,KAAK;AAC/C,QAAI,KAAK,iBAAkB,MAAK,cAAc;AAE9C,SAAK,eAAe;MAClB,QAAQ,KAAK;MACb,cAAc,KAAK;MACnB,yBAAyB;MACzB,GAAG,QAAQ;;AAKb,QAAI,QAAQ,8BAA8B,OACxC,MAAK,4BAA4B,OAAO;AAG1C,QAAI,QAAQ,iBAAiB,OAC3B,MAAK,eAAe,OAAO;AAG7B,SAAK,aAAa,QAAQ,cAAc;;;;;;EAOhC,oBACR,SACoC;AACpC,QAAI,CAAC,iBAAiB,KAAK,KAAA,EACzB;AAIF,QAAI;AACJ,QAAI,KAAK,cAAc,OACrB,aAAY;MACV,GAAG;MACH,GAAG,KAAK;;AAGZ,QAAI,SAAS,cAAc,OACzB,aAAY;MACV,GAAG;MACH,GAAG,QAAQ;;AAKf,QACE,SAAS,oBAAoB,UAC7B,WAAW,WAAW,OAEtB,aAAY;MACV,GAAG;MACH,QAAQ,QAAQ;;AAIpB,WAAO;;;;;;EAOC,mBACR,WACyC;AACzC,QACE,aACA,UAAU,SAAS,iBACnB,UAAU,YAAY,UACtB,mBAAmB,UAAU,YAAY,MAAA,EAEzC,QAAO,yBACL,UAAU,YAAY,QACtB,UAAU,YAAY,MACtB,EACE,aAAa,UAAU,YAAY,YAAA,CACpC;AAGL,WAAO;;EAGC,oBACR,mBAC2B;AAC3B,WAAO;MACL,GAAG,KAAK;MACR,GAAI,qBAAqB,CAAA;;;;EAK7B,kBACE,SAC0B;AAC1B,QAAI,CAAC,KAAK,QAAQ;AAKhB,YAAM,WAAW,YAJkC,EACjD,SAAS,KAAK,aAAa,QAAA,CAC5B;AAGD,YAAM,SAAS;QACb,GAAG,KAAK;QACR,SAAS;QACT,SAAS,KAAK;QACd,YAAY;;AAEd,UAAI,CAAC,OAAO,QACV,QAAO,OAAO;AAGhB,aAAO,iBAAiB,wBAAwB,OAAO,cAAA;AAEvD,WAAK,SAAS,IAAIC,OAAa,MAAA;;AAMjC,WAJuB;MACrB,GAAG,KAAK;MACR,GAAG;;;EAMG,wCACRC,OACA,QACiC;AACjC,QAAI,aAAaA,KAAA,EACf,QAAO,2BAA2BA,MAAK,SAAS,UAAA;AAElD,QAAIC,aAAqBD,KAAA,GAAO;AAC9B,UAAI,QAAQ,WAAW,OACrB,QAAO;QACL,GAAGA;QACH,UAAU;UACR,GAAGA,MAAK;UACR,QAAQ,OAAO;;;AAKrB,aAAOA;;AAET,WAAO,qBAAqBA,OAAM,MAAA;;EAG3B,UACPE,QACA,QAC+D;AAC/D,QAAI;AACJ,QAAI,QAAQ,WAAW,OACrB,UAAS,OAAO;aACP,KAAK,8BAA8B,OAC5C,UAAS,KAAK;AAEhB,WAAO,KAAK,WAAW;MACrB,OAAOA,OAAM,IAAA,CAAKF,UAAS;AAEzB,YAAI,cAAcA,KAAA,KAAS,aAAaA,KAAA,EACtC,QAAOA;AAIT,YAAI,0BAA0BA,KAAA,EAC5B,QAAOA,MAAK,OAAO;AAGrB,eAAO,KAAK,wCAAwCA,OAAM,EAAE,OAAA,CAAQ;;MAEtE,GAAG;KACJ;;EAGH,MAAe,OAAO,OAA+B,SAAuB;AAC1E,WAAO,MAAM,OACX,OACA,KAAK,oBAAoB,OAAA,CAAQ;;EAIrC,MAAe,OAAO,OAA+B,SAAuB;AAC1E,WAAO,MAAM,OACX,OACA,KAAK,oBAAoB,OAAA,CAAQ;;;EAKrC,qBAAqB,YAAgD;AACnE,WAAO,WAAW,OAAA,CAGf,KAAK,cAAc;AAClB,UAAI,aAAa,UAAU,YAAY;AACrC,YAAI,WAAW,oBACb,UAAU,WAAW,oBAAoB;AAC3C,YAAI,WAAW,gBAAgB,UAAU,WAAW,gBAAgB;AACpE,YAAI,WAAW,eAAe,UAAU,WAAW,eAAe;;AAEpE,aAAO;OAET,EACE,YAAY;MACV,kBAAkB;MAClB,cAAc;MACd,aAAa;MACd,CACF;;EAIL,MAAM,yBAAyB,UAAyB;AACtD,QAAI,aAAa;AACjB,QAAI,mBAAmB;AACvB,QAAI,gBAAgB;AAGpB,QAAI,KAAK,UAAU,sBAAsB;AACvC,yBAAmB;AACnB,sBAAgB;WACX;AACL,yBAAmB;AACnB,sBAAgB;;AAGlB,UAAM,kBAAkB,MAAM,QAAQ,IACpC,SAAS,IAAI,OAAO,YAAY;AAC9B,YAAM,CAAC,WAAW,SAAA,IAAa,MAAM,QAAQ,IAAI,CAC/C,KAAK,aAAa,QAAQ,OAAA,GAC1B,KAAK,aAAa,oBAAoB,OAAA,CAAQ,CAAC,CAChD;AACD,YAAM,YACJ,QAAQ,SAAS,SACb,gBAAiB,MAAM,KAAK,aAAa,QAAQ,IAAA,IACjD;AACN,UAAI,QAAQ,YAAY,mBAAmB,YAAY;AAGvD,YAAM,gBAAgB;AACtB,UAAI,cAAc,SAAA,MAAe,WAC/B,UAAS;AAEX,UAAI,cAAc,mBAAmB,cACnC,UAAS;AAEX,UAAI,eAAe,kBAAkB,eAAe,KAClD,UAAS,MAAM,KAAK,aAClB,cAAc,kBAAkB,eAAe,IAAA;AAGnD,UAAI,cAAc,kBAAkB,eAAe,UACjD,KAAI;AACF,iBAAS,MAAM,KAAK,aAElB,KAAK,UACH,KAAK,MACH,cAAc,kBAAkB,eAAe,SAAA,CAChD,CACF;eAEI,OAAO;AACd,gBAAQ,MACN,oCACA,OACA,KAAK,UAAU,cAAc,kBAAkB,aAAA,CAAc;AAE/D,iBAAS,MAAM,KAAK,aAClB,cAAc,kBAAkB,eAAe,SAAA;;AAKrD,oBAAc;AACd,aAAO;MACP;AAGJ,kBAAc;AAEd,WAAO;MAAE;MAAY;;;;EAIvB,MAAgB,6BAA6B,aAA+B;AAY1E,YAXyB,MAAM,QAAQ,IACrC,YAAY,IAAI,OAAO,eAAe;AACpC,UAAI,WAAW,QAAQ,mBAAmB,cACxC,SAAQ,MAAM,KAAK,yBAAyB,CAAC,WAAW,OAAA,CAAQ,GAC7D,gBAAgB,CAAA;UAEnB,QAAO,MAAM,KAAK,aAAa,WAAW,QAAQ,OAAA;MAEpD,GAGoB,OAAA,CAAQ,GAAG,MAAM,IAAI,GAAG,CAAA;;;EAIlD,MAAgB,kCACd,UACA,WACA,eAIiB;AAIjB,QAAI,UAAU,MAAM,KAAK,yBAAyB,QAAA,GAAW;AAG7D,QAAI,aAAa,kBAAkB,QAAQ;AACzC,YAAM,oBAAoB,0BACxB,SAAA;AAEF,gBAAU,MAAM,KAAK,aAAa,iBAAA;AAClC,gBAAU;;AAMZ,QAAI,aAAa,SAAS,KAAA,CAAM,MAAM,EAAE,SAAA,MAAe,QAAA,EACrD,WAAU;AAMZ,QAAI,kBAAkB,OACpB,WAAU;aACD,OAAO,kBAAkB,SAClC,WAAW,MAAM,KAAK,aAAa,cAAc,IAAA,IAAS;AAG5D,WAAO;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAyCT,MAAM,gBACJ,OACA,QAIgD;AAChD,UAAM,gBAAgB,KAAK,kBAAkB,QAAQ,OAAA;AAErD,UAAM,oBAAyD;MAC7D;MACA,OAHsB,QAAQ,SAAS;;AAMzC,WAAO,KAAK,OAAO,KAAK,YAAY;AAClC,UAAI;AAKF,eAJiB,MAAM,KAAK,OAAO,YAAY,OAC7C,mBACA,aAAA;eAGK,GAAG;AAEV,cADc,sBAAsB,CAAA;;;;;;;;;;;;;;;;;;;;;EAuB1C,IAAI,UAAwB;AAC1B,WAAO,SAAS,KAAK,KAAA,KAAU,CAAA;;;EAIvB,2BACRG,SACA;AACA,UAAM,gBAAgB,EAAE,GAAGA,QAAA;AAC3B,QACE,CAAC,KAAK,MAAM,WAAW,OAAA,KACvB,CAAC,KAAK,MAAM,WAAW,QAAA,KACvB,KAAK,UAAU,SAEf;UAAI,eAAe,WAAW,OAC5B,QAAO;eAEA,cAAc,WAAW,aAClC,SAAQ,KACN,sDAAsD,KAAK,KAAA,kCAAM;AAGrE,WAAO,cAAc;;;;;;;;;;;;;;;;;;;;;EAyDvB,qBAGE,cACAA,SACA;AACA,QAAI;AACJ,QAAI;AAEJ,UAAM,EAAE,QAAQ,MAAM,WAAA,IAAe;MACnC,GAAGA;MACH,QAAQ;;AAGV,QAAIA,SAAQ,WAAW,UAAaA,QAAO,WAAW,WACpD,OAAM,IAAI,MACR,uEAAA;AAIJ,UAAM,SAAS,0BAA0B,KAAK,OAAOA,SAAQ,MAAA;AAE7D,QAAI,WAAW,YAAY;AACzB,UAAI,mBAAmB,MAAA,EACrB,gBAAe,uBAAuB,cAAc,MAAA;UAEpD,gBAAe,IAAI,iBAAA;AAErB,YAAM,eAAe,aAAa,MAAA;AAClC,YAAM,KAAK,WAAW;QACpB,eAAe;QACf,iBAAiB,EAAE,MAAM,cAAA;QACzB,6BAA6B;UAC3B,QAAQ,EAAE,QAAQ,YAAA;UAClB,QAAQ;YAAE,OAAO,QAAQ;YAAW,GAAG;;;OAE1C;eACQ,WAAW,cAAc;AAClC,YAAM,yBAAyB;QAC7B,MAAM,QAAQ;QACd,aAAa,qBAAqB,MAAA;QAClC;QACA,QAAQA,SAAQ;;AAElB,YAAM,eAAe,aAAa,uBAAuB,MAAA;AACzD,YAAM,KAAK,WAAW;QACpB,eAAe;QACf,iBAAiB;UACf,MAAM;UACN,aAAa;;QAEf,6BAA6B;UAC3B,QAAQ,EAAE,QAAQ,cAAA;UAClB,QAAQ;YACN,OAAO,uBAAuB;YAC9B,aAAa,uBAAuB;YACpC,GAAG;;;OAGR;AACD,UAAI,mBAAmB,MAAA,GAAS;AAC9B,cAAM,YAAY,uBAAuB,cAAc,MAAA;AACvD,uBAAe,eAAe,KAAA,CAC3B,cAA8B;AAC7B,cAAI,YAAY,UAAU,kBACxB,QAAO,UAAU,kBAAkB;AAErC,iBAAO;;YAIX,gBAAe,IAAI,iBAAA;WAEhB;AACL,UAAI,eAAe,QAAQ;AAE3B,UAAI,mBAAmB,MAAA,GAAS;AAC9B,cAAM,eAAe,aAAa,MAAA;AAClC,cAAM,KAAK,WAAW;UACpB,eAAe;UACf,OAAO,CACL;YACE,MAAM;YACN,UAAU;cACR,MAAM;cACN,aAAa,aAAa;cAC1B,YAAY;;WAEf;UAEH,aAAa;YACX,MAAM;YACN,UAAU,EACR,MAAM,aAAA;;UAGV,6BAA6B;YAC3B,QAAQ,EAAE,QAAQ,mBAAA;YAClB,QAAQ;cAAE,OAAO;cAAc,GAAG;;;UAGpC,GAAIA,SAAQ,WAAW,SAAY,EAAE,QAAQA,QAAO,OAAA,IAAW,CAAA;SAChE;AACD,uBAAe,IAAI,yBAAyB;UAC1C,cAAc;UACd,SAAS;UACT,WAAW;SACZ;aACI;AACL,YAAI;AACJ,YACE,OAAO,OAAO,SAAS,YACvB,OAAO,OAAO,eAAe,YAC7B,OAAO,cAAc,MACrB;AACA,qCAA2B;AAC3B,yBAAe,OAAO;eACjB;AACL,yBAAgB,OAAO,SAAoB;AAC3C,qCAA2B;YACzB,MAAM;YACN,aAAc,OAAO,eAA0B;YAC/C,YAAY;;;AAGhB,cAAM,eAAe,aAAa,MAAA;AAClC,cAAM,KAAK,WAAW;UACpB,eAAe;UACf,OAAO,CACL;YACE,MAAM;YACN,UAAU;WACX;UAEH,aAAa;YACX,MAAM;YACN,UAAU,EACR,MAAM,aAAA;;UAGV,6BAA6B;YAC3B,QAAQ,EAAE,QAAQ,mBAAA;YAClB,QAAQ;cAAE,OAAO;cAAc,GAAG;;;UAGpC,GAAIA,SAAQ,WAAW,SAAY,EAAE,QAAQA,QAAO,OAAA,IAAW,CAAA;SAChE;AACD,uBAAe,IAAI,yBAAoC;UACrD,cAAc;UACd,SAAS;SACV;;;AAIL,QAAI,CAAC,WACH,QAAO,IAAI,KAAK,YAAA;AAMlB,UAAM,eAAe,oBAAoB,OAAO,EAE9C,QAAA,CAAS,OAAYA,YAAW,aAAa,OAAO,MAAM,KAAKA,OAAA,EAAO,CACvE;AACD,UAAM,aAAa,oBAAoB,OAAO,EAC5C,QAAA,MAAc,KAAA,CACf;AACD,UAAM,qBAAqB,aAAa,cAAc,EACpD,WAAW,CAAC,UAAA,EAAW,CACxB;AACD,WAAO,iBAAiB,KAGtB,CAAC,EAAE,KAAK,IAAA,GAAO,kBAAA,CAAmB;;;;;ACxnCxC,IAAa,sCAKR;EACH,cAAc;EAEd,sBAAsB,OAAsC;AAC1D,WAAO;MAAE,MAAM;MAAQ,MAAM,MAAM;;;EAGrC,uBAAuB,OAAuC;AAC5D,QAAI,MAAM,gBAAgB,MACxB,QAAO;MACL,MAAM;MACN,WAAW;QACT,KAAK,MAAM;QACX,GAAI,MAAM,UAAU,SAChB,EAAE,QAAQ,MAAM,SAAS,OAAA,IACzB,CAAA;;;AAKV,QAAI,MAAM,gBAAgB,SAExB,QAAO;MACL,MAAM;MACN,WAAW;QACT,KAJQ,QAAQ,MAAM,aAAa,EAAA,WAAa,MAAM,IAAA;QAKtD,GAAI,MAAM,UAAU,SAChB,EAAE,QAAQ,MAAM,SAAS,OAAA,IACzB,CAAA;;;AAKV,UAAM,IAAI,MACR,yCAAyC,MAAM,WAAA,mCAAY;;EAI/D,uBAAuB,OAA4C;AACjE,QAAI,MAAM,gBAAgB,OAAO;AAC/B,YAAM,OAAO,mBAAmB,EAAE,SAAS,MAAM,IAAA,CAAK;AACtD,UAAI,CAAC,KACH,OAAM,IAAI,MACR,qCAAqC,MAAM,WAAA,iDAAY;AAI3D,YAAM,cAAc,KAAK,aAAa,MAAM,aAAa;AACzD,UAAI;AAEJ,UAAI;AACF,mBAAW,cAAc,WAAA;cACnB;AACN,cAAM,IAAI,MACR,iCAAiC,MAAM,WAAA,gDAAY;;AAIvD,UACE,SAAS,SAAS,WACjB,SAAS,YAAY,SAAS,SAAS,YAAY,MAEpD,OAAM,IAAI,MACR,iCAAiC,MAAM,WAAA,gDAAY;AAIvD,aAAO;QACL,MAAM;QACN,aAAa;UACX,QAAQ,SAAS;UACjB,MAAM,KAAK;;;;AAKjB,QAAI,MAAM,gBAAgB,UAAU;AAClC,UAAI;AAEJ,UAAI;AACF,mBAAW,cAAc,MAAM,aAAa,EAAA;cACtC;AACN,cAAM,IAAI,MACR,iCAAiC,MAAM,WAAA,gDAAY;;AAIvD,UACE,SAAS,SAAS,WACjB,SAAS,YAAY,SAAS,SAAS,YAAY,MAEpD,OAAM,IAAI,MACR,iCAAiC,MAAM,WAAA,gDAAY;AAIvD,aAAO;QACL,MAAM;QACN,aAAa;UACX,QAAQ,SAAS;UACjB,MAAM,MAAM;;;;AAKlB,UAAM,IAAI,MACR,yCAAyC,MAAM,WAAA,mCAAY;;EAI/D,sBAAsB,OAAuC;AAC3D,QAAI,MAAM,gBAAgB,OAAO;AAC/B,YAAM,OAAO,mBAAmB,EAAE,SAAS,MAAM,IAAA,CAAK;AAEtD,YAAM,WAAW,gCAAgC,KAAA;AAEjD,UAAI,CAAC,KACH,OAAM,IAAI,MACR,oCAAoC,MAAM,WAAA,iDAAY;AAI1D,aAAO;QACL,MAAM;QACN,MAAM;UACJ,WAAW,MAAM;UACjB,GAAI,MAAM,UAAU,YAAY,MAAM,UAAU,OAC5C,EACE,SAAA,IAEF,CAAA;;;;AAKV,QAAI,MAAM,gBAAgB,UAAU;AAClC,YAAM,WAAW,gCAAgC,KAAA;AAEjD,aAAO;QACL,MAAM;QACN,MAAM;UACJ,WAAW,QAAQ,MAAM,aAAa,EAAA,WAAa,MAAM,IAAA;UACzD,GAAI,MAAM,UAAU,YACpB,MAAM,UAAU,QAChB,MAAM,UAAU,QACZ,EACE,SAAA,IAEF,CAAA;;;;AAKV,QAAI,MAAM,gBAAgB,KACxB,QAAO;MACL,MAAM;MACN,MAAM,EACJ,SAAS,MAAM,GAAA;;AAKrB,UAAM,IAAI,MACR,wCAAwC,MAAM,WAAA,mCAAY;;;AA6DhE,IAAa,yCAAA,CAOR,EAAE,SAAS,aAAa,mBAAA,MAAyB;AACpD,QAAM,eAA6C,QAAQ;AAG3D,UAAQ,QAAQ,MAAhB;IACE,KAAK,aAAa;AAChB,YAAM,YAAY,CAAA;AAClB,YAAM,mBAAmB,CAAA;AACzB,iBAAW,eAAe,gBAAgB,CAAA,EACxC,KAAI;AACF,kBAAU,KAAKC,eAAc,aAAa,EAAE,UAAU,KAAA,CAAM,CAAC;eAEtD,GAAQ;AACf,yBAAiB,KAAK,oBAAoB,aAAa,EAAE,OAAA,CAAQ;;AAGrE,YAAM,oBAA6C;QACjD,eAAe,QAAQ;QACvB,YAAY;;AAEd,UAAI,uBAAuB,OACzB,mBAAkB,iBAAiB;AAErC,YAAM,oBAAyD;QAC7D,gBAAgB;QAChB,YAAY,YAAY;QACxB,GAAI,YAAY,qBACZ;UACE,OAAO,EAAE,GAAG,YAAY,MAAA;UACxB,oBAAoB,YAAY;YAElC,CAAA;;AAGN,UAAI,QAAQ,MACV,mBAAkB,QAAQ,QAAQ;AAOpC,aAAO,IAAI,UAAU;QACnB,SALc,uBACd,QAAQ,WAAW,IACnB,YAAY,UAAU,CAAA,GAAI,OAAA;QAI1B,YAAY;QACZ,oBAAoB;QACpB;QACA;QACA,IAAI,YAAY;OACjB;;IAEH;AACE,aAAO,IAAI,YAAY,QAAQ,WAAW,IAAI,QAAQ,QAAQ,SAAA;;;AA+DpE,IAAa,4CAAA,CASR,EAAE,OAAO,aAAa,oBAAoB,YAAA,MAAkB;AAC/D,QAAM,OAAO,MAAM,QAAQ;AAC3B,QAAM,UAAU,MAAM,WAAW;AACjC,MAAI;AACJ,MAAI,MAAM,cACR,qBAAoB,EAClB,eAAe,MAAM,cAAA;WAEd,MAAM,WACf,qBAAoB,EAClB,YAAY,MAAM,WAAA;MAGpB,qBAAoB,CAAA;AAEtB,MAAI,mBACF,mBAAkB,iBAAiB;AAGrC,MAAI,MAAM,MACR,mBAAkB,QAAQ;IACxB,GAAG,MAAM;IACT,OAAO,YAAY,QAAQ,CAAA,EAAG;;AAIlC,QAAM,oBAAoB;IACxB,gBAAgB;IAChB,OAAO,EAAE,GAAG,YAAY,MAAA;;AAE1B,MAAI,SAAS,OACX,QAAO,IAAI,kBAAkB;IAAE;IAAS;GAAmB;WAClD,SAAS,aAAa;AAC/B,UAAM,iBAAkC,CAAA;AACxC,QAAI,MAAM,QAAQ,MAAM,UAAA,EACtB,YAAW,eAAe,MAAM,WAC9B,gBAAe,KAAK;MAClB,MAAM,YAAY,UAAU;MAC5B,MAAM,YAAY,UAAU;MAC5B,IAAI,YAAY;MAChB,OAAO,YAAY;MACnB,MAAM;KACP;AAGL,WAAO,IAAI,eAAe;MACxB;MACA,kBAAkB;MAClB;MACA,IAAI,YAAY;MAChB;KACD;aACQ,SAAS,SAClB,QAAO,IAAI,mBAAmB;IAAE;IAAS;GAAmB;WACnD,SAAS,YAClB,QAAO,IAAI,mBAAmB;IAC5B;IACA;IACA,mBAAmB,EACjB,iBAAiB,YAAA;GAEpB;WACQ,SAAS,WAClB,QAAO,IAAI,qBAAqB;IAC9B;IACA;IACA,MAAM,MAAM;IACZ;GACD;WACQ,SAAS,OAClB,QAAO,IAAI,iBAAiB;IAC1B;IACA;IACA,cAAc,MAAM;IACpB;GACD;MAED,QAAO,IAAI,iBAAiB;IAAE;IAAS;IAAM;GAAmB;;AAgCpE,IAAa,sDAAA,CAMR,UAAU;AACb,MAAI,MAAM,SAAS,SACjB;QAAI,MAAM,IACR,QAAO;MACL,MAAM;MACN,WAAW,EACT,KAAK,MAAM,IAAA;;aAGN,MAAM,KACf,QAAO;MACL,MAAM;MACN,WAAW,EACT,KAAK,QAAQ,MAAM,QAAA,WAAmB,MAAM,IAAA,GAAA;;;AAKpD,MAAI,MAAM,SAAS,SACjB;QAAI,MAAM,MAAM;AACd,YAAM,SAAS,KAAA,MAAW;AACxB,cAAM,CAAA,EAAGC,OAAA,IAAU,MAAM,SAAS,MAAM,GAAA;AACxC,YAAIA,YAAW,SAASA,YAAW,MACjC,QAAOA;AAET,eAAO;;AAET,aAAO;QACL,MAAM;QACN,aAAa;UACX,MAAM,MAAM,KAAK,SAAA;UACjB;;;;;AAKR,MAAI,MAAM,SAAS,QAAQ;AACzB,QAAI,MAAM,MAAM;AACd,YAAM,WAAW,gCAAgC,KAAA;AAEjD,aAAO;QACL,MAAM;QACN,MAAM;UACJ,WAAW,QAAQ,MAAM,QAAA,WAAmB,MAAM,IAAA;UACxC;;;;AAIhB,QAAI,MAAM,OACR,QAAO;MACL,MAAM;MACN,MAAM,EACJ,SAAS,MAAM,OAAA;;;;AAyEzB,IAAa,oDAAA,CAGR,EAAE,SAAS,MAAA,MAAY;AAC1B,MAAI,OAAO,oBAAoB,OAAA;AAC/B,MAAI,SAAS,YAAY,iBAAiB,KAAA,EACxC,QAAO;AAET,MAAI,SAAS,YACX,QAAO;IACL,MAAM;IACN,SAAS,QAAQ,cAAc,OAAA,CAAQ,UAAU,MAAM,SAAS,MAAA;;WAEzD,SAAS,SAClB,QAAO;IACL,MAAM;IACN,SAAS,QAAQ,cAAc,OAAA,CAAQ,UAAU,MAAM,SAAS,MAAA;;WAEzD,SAAS,YAClB,QAAO;IACL,MAAM;IACN,SAAS,QAAQ,cAAc,OAAA,CAAQ,UAAU,MAAM,SAAS,MAAA;;WAEzD,SAAS,UAAU,YAAY,WAAW,OAAA,EACnD,QAAO;IACL,MAAM;IACN,cAAc,QAAQ;IACtB,SAAS,QAAQ,cAAc,OAAA,CAAQ,UAAU,MAAM,SAAS,MAAA;;WAEzD,SAAS,WAClB,QAAO;IACL,MAAM;IACN,MAAM,QAAQ,QAAQ;IACtB,SAAS,QAAQ,cACd,OAAA,CAAQ,UAAU,MAAM,SAAS,MAAA,EACjC,KAAK,EAAA;;AAIZ,YAAU,mBAAmBC,SAAiC;AAC5D,eAAW,SAASA,SAAQ;AAC1B,UAAI,MAAM,SAAS,OACjB,OAAM;QACJ,MAAM;QACN,MAAM,MAAM;;AAGhB,YAAM,OAAO,oDAAoD,KAAA;AACjE,UAAI,KACF,OAAM;;;AAIZ,SAAO;IACL,MAAM;IACN,SAAS,MAAM,KAAK,mBAAmB,QAAQ,aAAA,CAAc;;;AA2EjE,IAAa,4CAAA,CAGR,EAAE,UAAU,MAAA,MAAY;AAC3B,SAAO,SAAS,QAAA,CAAS,YAAY;AACnC,QACE,oBAAoB,QAAQ,qBAC5B,QAAQ,mBAAmB,mBAAmB,KAE9C,QAAO,kDAAkD,EAAE,QAAA,CAAS;AAEtE,QAAI,OAAO,oBAAoB,OAAA;AAC/B,QAAI,SAAS,YAAY,iBAAiB,KAAA,EACxC,QAAO;AAGT,UAAM,UACJ,OAAO,QAAQ,YAAY,WACvB,QAAQ,UACR,QAAQ,QAAQ,QAAA,CAAS,MAAM;AAC7B,UAAI,mBAAmB,CAAA,EACrB,QAAO,8BACL,GACA,mCAAA;AAMJ,UACE,OAAO,MAAM,YACb,MAAM,QACN,UAAU,KACV,EAAE,SAAS,WAEX,QAAO,CAAA;AAET,aAAO;;AAGf,UAAM,kBAAuC;MAC3C;MACA;;AAEF,QAAI,QAAQ,QAAQ,KAClB,iBAAgB,OAAO,QAAQ;AAEjC,QAAI,QAAQ,kBAAkB,iBAAiB,KAC7C,iBAAgB,gBAAgB,QAAQ,kBAAkB;AAE5D,QAAI,UAAU,WAAW,OAAA,KAAY,CAAC,CAAC,QAAQ,YAAY,OACzD,iBAAgB,aAAa,QAAQ,WAAW,IAC9C,gCAAA;SAEG;AACL,UAAI,QAAQ,kBAAkB,cAAc,KAC1C,iBAAgB,aAAa,QAAQ,kBAAkB;AAEzD,UAAI,YAAY,WAAW,OAAA,KAAY,QAAQ,gBAAgB,KAC7D,iBAAgB,eAAe,QAAQ;;AAI3C,QACE,QAAQ,kBAAkB,SAC1B,OAAO,QAAQ,kBAAkB,UAAU,YAC3C,QAAQ,QAAQ,kBAAkB,MAQlC,QAAO,CACL,iBAPmB;MACnB,MAAM;MACN,OAAO,EACL,IAAI,QAAQ,kBAAkB,MAAM,GAAA;KAEvC;AAOH,WAAO;;;;;ACxyBX,IAAa,wBAAb,cAGU,eAA4B;EAGpC,YACE,eACA,WACA;AACA,UAAM,yBAAyB,eAAe,SAAA,CAAU;;;EAIjD,iBACP,SACA,OACiC;AACjC,QAAI;AACJ,QAAI,SAAS,WAAW,OACtB,UAAS,QAAQ;aACR,KAAK,8BAA8B,OAC5C,UAAS,KAAK;AAGhB,QAAI,sBAAsB,CAAA;AAC1B,QAAI,SAAS,mBAAmB,OAC9B,uBAAsB,EAAE,gBAAgB,QAAQ,eAAA;aACvC,KAAK,gBAAgB,KAAK,aAAa,OAAO,WACvD,uBAAsB,EAAE,gBAAgB,EAAE,eAAe,KAAA,EAAM;AAGjE,UAAM,SAAmD;MACvD,OAAO,KAAK;MACZ,aAAa,KAAK;MAClB,OAAO,KAAK;MACZ,mBAAmB,KAAK;MACxB,kBAAkB,KAAK;MACvB,UAAU,KAAK;MACf,cAAc,KAAK;MACnB,GAAG,KAAK;MACR,YAAY,KAAK;MACjB,MAAM,SAAS,QAAQ,KAAK;MAC5B,MAAM,KAAK;MAEX,QAAQ,KAAK;MACb,WAAW,SAAS;MACpB,eAAe,SAAS;MACxB,OAAO,SAAS,OAAO,SACnB,QAAQ,MAAM,IAAA,CAAKC,UACjB,KAAK,wCAAwCA,OAAM,EAAE,OAAA,CAAQ,CAAC,IAEhE;MACJ,aAAa,yBACX,SAAS,WAAA;MAEX,iBAAiB,KAAK,mBAAmB,SAAS,eAAA;MAClD,MAAM,SAAS;MACf,GAAG;MACH,qBAAqB,SAAS;MAC9B,GAAI,KAAK,SAAS,SAAS,QACvB,EAAE,OAAO,KAAK,SAAS,SAAS,MAAA,IAChC,CAAA;MACJ,GAAI,KAAK,cAAc,SAAS,aAC5B,EAAE,YAAY,KAAK,cAAc,SAAS,WAAA,IAC1C,CAAA;MACJ,GAAG,KAAK;MACR,kBAAkB,SAAS,kBAAkB,KAAK;MAClD,wBACE,SAAS,wBAAwB,KAAK;MACxC,WAAW,SAAS,aAAa,KAAK;;AAExC,QAAI,SAAS,eAAe,OAC1B,QAAO,aAAa,QAAQ;AAE9B,QAAI,KAAK,iBAAiB,OACxB,QAAO,eAAe,KAAK;AAE7B,QAAI,SAAS,iBAAiB,OAC5B,QAAO,eAAe,QAAQ;AAEhC,UAAM,YAAY,KAAK,oBAAoB,OAAA;AAC3C,QAAI,cAAc,UAAa,UAAU,WAAW,OAClD,QAAO,mBAAmB,UAAU;AAEtC,QAAI,iBAAiB,OAAO,KAAA,EAC1B,QAAO,wBACL,KAAK,cAAc,KAAK,SAAY,KAAK;QAE3C,QAAO,aAAa,KAAK,cAAc,KAAK,SAAY,KAAK;AAG/D,WAAO;;EAGT,MAAM,UACJ,UACA,SACA,YACqB;AACrB,YAAQ,QAAQ,eAAA;AAChB,UAAM,gBAAgB,CAAA;AACtB,UAAM,SAAS,KAAK,iBAAiB,OAAA;AACrC,UAAM,iBACJ,0CAA0C;MACxC;MACA,OAAO,KAAK;KACb;AAEH,QAAI,OAAO,QAAQ;AACjB,YAAM,SAAS,KAAK,sBAAsB,UAAU,SAAS,UAAA;AAC7D,YAAM,cAAmD,CAAA;AACzD,uBAAiB,SAAS,QAAQ;AAChC,cAAM,QAAQ,oBAAoB;UAChC,GAAG,MAAM;UACT,GAAG,MAAM,QAAQ;;AAEnB,cAAM,QACH,MAAM,gBAAoC,cAAc;AAC3D,YAAI,YAAY,KAAA,MAAW,OACzB,aAAY,KAAA,IAAS;YAErB,aAAY,KAAA,IAAS,YAAY,KAAA,EAAO,OAAO,KAAA;;AAGnD,YAAM,cAAc,OAAO,QAAQ,WAAA,EAChC,KAAA,CAAM,CAAC,IAAA,GAAO,CAAC,IAAA,MAAU,SAAS,MAAM,EAAA,IAAM,SAAS,MAAM,EAAA,CAAG,EAChE,IAAA,CAAK,CAAC,GAAG,KAAA,MAAW,KAAA;AAEvB,YAAM,EAAE,WAAW,cAAA,IAAkB,KAAK,iBAAiB,OAAA;AAK3D,YAAM,mBAAmB,MAAM,KAAK,kCAClC,UACA,WACA,aAAA;AAEF,YAAM,uBACJ,MAAM,KAAK,6BAA6B,WAAA;AAE1C,oBAAc,eAAe;AAC7B,oBAAc,gBAAgB;AAC9B,oBAAc,eAAe,mBAAmB;AAChD,aAAO;QACL;QACA,WAAW,EACT,qBAAqB;UACnB,cAAc,cAAc;UAC5B,kBAAkB,cAAc;UAChC,aAAa,cAAc;UAC5B;;WAGA;AACL,YAAM,OAAO,MAAM,KAAK,oBACtB;QACE,GAAG;QACH,QAAQ;QACR,UAAU;SAEZ;QACE,QAAQ,SAAS;QACjB,GAAG,SAAS;OACb;AAGH,YAAM,EACJ,mBAAmB,kBACnB,eAAe,cACf,cAAc,aACd,uBAAuB,qBACvB,2BAA2B,wBAAA,IACzB,MAAM,SAAS,CAAA;AAEnB,UAAI,iBACF,eAAc,iBACX,cAAc,iBAAiB,KAAK;AAGzC,UAAI,aACF,eAAc,gBACX,cAAc,gBAAgB,KAAK;AAGxC,UAAI,YACF,eAAc,gBACX,cAAc,gBAAgB,KAAK;AAGxC,UACE,qBAAqB,iBAAiB,QACtC,qBAAqB,kBAAkB,KAEvC,eAAc,sBAAsB;QAClC,GAAI,qBAAqB,iBAAiB,QAAQ,EAChD,OAAO,qBAAqB,aAAA;QAE9B,GAAI,qBAAqB,kBAAkB,QAAQ,EACjD,YAAY,qBAAqB,cAAA;;AAKvC,UACE,yBAAyB,iBAAiB,QAC1C,yBAAyB,qBAAqB,KAE9C,eAAc,uBAAuB;QACnC,GAAI,yBAAyB,iBAAiB,QAAQ,EACpD,OAAO,yBAAyB,aAAA;QAElC,GAAI,yBAAyB,qBAAqB,QAAQ,EACxD,WAAW,yBAAyB,iBAAA;;AAK1C,YAAM,cAAgC,CAAA;AACtC,iBAAW,QAAQ,MAAM,WAAW,CAAA,GAAI;AAEtC,cAAM,aAA6B;UACjC,MAFW,KAAK,SAAS,WAAW;UAGpC,SAAS,KAAK,wCACZ,KAAK,WAAW,EAAE,MAAM,YAAA,GACxB,IAAA;;AAGJ,mBAAW,iBAAiB;UAC1B,GAAI,KAAK,gBAAgB,EAAE,eAAe,KAAK,cAAA,IAAkB,CAAA;UACjE,GAAI,KAAK,WAAW,EAAE,UAAU,KAAK,SAAA,IAAa,CAAA;;AAEpD,YAAI,YAAY,WAAW,OAAA,EACzB,YAAW,QAAQ,iBAAiB;AAItC,mBAAW,UAAU,IAAI,UACvB,OAAO,YACL,OAAO,QAAQ,WAAW,OAAA,EAAS,OAAA,CAChC,CAAC,GAAA,MAAS,CAAC,IAAI,WAAW,KAAA,CAAM,CAClC,CACF;AAEH,oBAAY,KAAK,UAAA;;AAEnB,aAAO;QACL;QACA,WAAW,EACT,YAAY;UACV,cAAc,cAAc;UAC5B,kBAAkB,cAAc;UAChC,aAAa,cAAc;UAC5B;;;;EAMT,OAAO,sBACL,UACA,SACA,YACqC;AACrC,UAAM,iBACJ,0CAA0C;MACxC;MACA,OAAO,KAAK;KACb;AAEH,UAAM,SAAS;MACb,GAAG,KAAK,iBAAiB,SAAS,EAChC,WAAW,KAAA,CACZ;MACD,UAAU;MACV,QAAQ;;AAEV,QAAI;AAEJ,UAAM,iBAAiB,MAAM,KAAK,oBAAoB,QAAQ,OAAA;AAC9D,QAAI;AACJ,qBAAiB,QAAQ,gBAAgB;AACvC,UAAI,QAAQ,QAAQ,QAClB;AAEF,YAAM,SAAS,MAAM,UAAU,CAAA;AAC/B,UAAI,KAAK,MACP,SAAQ,KAAK;AAEf,UAAI,CAAC,OACH;AAGF,YAAM,EAAE,MAAA,IAAU;AAClB,UAAI,CAAC,MACH;AAEF,YAAM,QAAQ,KAAK,2CACjB,OACA,MACA,WAAA;AAEF,oBAAc,MAAM,QAAQ;AAC5B,YAAM,kBAAkB;QACtB,QAAQ,QAAQ,eAAe;QAC/B,YAAY,OAAO,SAAS;;AAE9B,UAAI,OAAO,MAAM,YAAY,UAAU;AACrC,gBAAQ,IACN,sFAAA;AAEF;;AAGF,YAAM,iBAAsC,EAAE,GAAG,gBAAA;AACjD,UAAI,OAAO,iBAAiB,MAAM;AAChC,uBAAe,gBAAgB,OAAO;AAGtC,uBAAe,qBAAqB,KAAK;AACzC,uBAAe,aAAa,KAAK;AACjC,uBAAe,eAAe,KAAK;;AAErC,UAAI,KAAK,SACP,gBAAe,WAAW,OAAO;AAEnC,YAAM,kBAAkB,IAAI,oBAAoB;QAC9C,SAAS;QACT,MAAM,MAAM;QACZ;OACD;AACD,YAAM;AACN,YAAM,YAAY,kBAChB,gBAAgB,QAAQ,IACxB,iBACA,QACA,QACA,QACA,EAAE,OAAO,gBAAA,CAAiB;;AAG9B,QAAI,OAAO;AACT,YAAM,oBAAoB;QACxB,GAAI,MAAM,uBAAuB,iBAAiB,QAAQ,EACxD,OAAO,MAAM,uBAAuB,aAAA;QAEtC,GAAI,MAAM,uBAAuB,kBAAkB,QAAQ,EACzD,YAAY,MAAM,uBAAuB,cAAA;;AAG7C,YAAM,qBAAqB;QACzB,GAAI,MAAM,2BAA2B,iBAAiB,QAAQ,EAC5D,OAAO,MAAM,2BAA2B,aAAA;QAE1C,GAAI,MAAM,2BAA2B,qBAAqB,QAAQ,EAChE,WAAW,MAAM,2BAA2B,iBAAA;;AAGhD,YAAM,kBAAkB,IAAI,oBAAoB;QAC9C,SAAS,IAAI,eAAe;UAC1B,SAAS;UACT,mBAAmB,EACjB,OAAO,EAAE,GAAG,MAAA,EAAO;UAErB,gBAAgB;YACd,cAAc,MAAM;YACpB,eAAe,MAAM;YACrB,cAAc,MAAM;YACpB,GAAI,OAAO,KAAK,iBAAA,EAAmB,SAAS,KAAK,EAC/C,qBAAqB,kBAAA;YAEvB,GAAI,OAAO,KAAK,kBAAA,EAAoB,SAAS,KAAK,EAChD,sBAAsB,mBAAA;;SAG3B;QACD,MAAM;OACP;AACD,YAAM;AACN,YAAM,YAAY,kBAChB,gBAAgB,QAAQ,IACxB;QAAE,QAAQ;QAAG,YAAY;SACzB,QACA,QACA,QACA,EAAE,OAAO,gBAAA,CAAiB;;AAG9B,QAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM,YAAA;;EAcpB,MAAM,oBACJ,SACA,gBAIA;AACA,UAAM,gBAAgB,KAAK,kBAAkB,cAAA;AAC7C,UAAM,oBACJ,QAAQ,mBAAmB,QAAQ,gBAAgB,SAAS;AAC9D,WAAO,KAAK,OAAO,KAAK,YAAY;AAClC,UAAI;AACF,YAAI,qBAAqB,CAAC,QAAQ,OAChC,QAAO,MAAM,KAAK,OAAO,KAAK,YAAY,MACxC,SACA,aAAA;YAGF,QAAO,MAAM,KAAK,OAAO,KAAK,YAAY,OACxC,SACA,aAAA;eAGG,GAAG;AAEV,cADc,sBAAsB,CAAA;;;;;;;;;;;EAahC,2CAER,OACA,aACA,aACkB;AAClB,WAAO,0CAA0C;MAC/C;MACA;MACA,oBAAoB,KAAK;MACzB;KACD;;;;;;;;;EAUO,wCACR,SACA,aACa;AACb,WAAO,uCAAuC;MAC5C;MACA;MACA,oBAAoB,KAAK;KAC1B;;;;;ACtfL,IAAa,gBAAgB;EAC3B,cAAc;EACd,kBAAkB;EAClB,gBAAgB;EAChB,gBAAgB;EAChB,qBAAqB;EACrB,uBAAuB;EACvB,qBAAqB;EACrB,8BAA8B;;AAGhC,IAAa,gBAAgB,EAC3B,mBAAmB,uBAAA;AAGrB,IAAa,0BAA0B;EACrC;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;AAYF,SAAgB,yBACd,eACA,WAImC;AACnC,MAAI,OAAO,kBAAkB,SAC3B,QAAO;IACL,OAAO;IACP,gBAAgB;IAChB,8BAA8B;IAC9B,GAAI,aAAa,CAAA;;AAGrB,MAAI,iBAAiB,KACnB,QAAO;AAET,SAAO;;AAGT,SAAgB,sBAEd,QACA;AACA,OAAK,oBACH,QAAQ,sBACP,OAAO,QAAQ,iBAAiB,WAC7B,QAAQ,eACR,YACH,OAAO,QAAQ,WAAW,WAAW,QAAQ,SAAS,WACvD,uBAAuB,sBAAA;AAEzB,OAAK,6BACH,QAAQ,8BACR,uBAAuB,gCAAA;AAEzB,OAAK,+BACH,QAAQ,gCACR,QAAQ,kBACR,uBAAuB,kCAAA;AAEzB,OAAK,wBACH,QAAQ,yBACR,QAAQ,oBACR,uBAAuB,0BAAA;AAEzB,OAAK,sBACH,QAAQ,uBACR,uBAAuB,wBAAA;AAEzB,OAAK,sBACH,QAAQ,uBACR,uBAAuB,uBAAA;AAEzB,OAAK,uBAAuB,QAAQ;AAEpC,MAAI,CAAC,KAAK,qBAAqB,CAAC,KAAK,UAAU,CAAC,KAAK,qBACnD,OAAM,IAAI,MAAM,kDAAA;;AAIpB,SAAgB,uBAEd,SAC0B;AAC1B,MAAI,CAAC,KAAK,QAAQ;AAChB,UAAM,uBAA6C;MACjD,8BAA8B,KAAK;MACnC,4BAA4B,KAAK;MACjC,mBAAmB,KAAK;MACxB,qBAAqB,KAAK;MAC1B,sBAAsB,KAAK;MAC3B,SAAS,KAAK,aAAa;MAC3B,qBAAqB,KAAK;;AAG5B,UAAM,WAAW,YAAY,oBAAA;AAE7B,UAAM,EAAE,QAAQ,gBAAgB,GAAG,iBAAA,IAAqB,KAAK;AAC7D,UAAM,SAA8D;MAClE,GAAG;MACH,SAAS;MACT,SAAS,KAAK;MACd,YAAY;;AAGd,QAAI,CAAC,KAAK,qBACR,QAAO,SAAS,qBAAqB;AAGvC,QAAI,CAAC,OAAO,QACV,QAAO,OAAO;AAGhB,WAAO,iBAAiB,wBACtB,OAAO,gBACP,MACA,OAAA;AAGF,SAAK,SAAS,IAAIC,YAAkB;MAClC,YAAY,KAAK;MACjB,sBAAsB,KAAK;MAC3B,YAAY,KAAK;MACjB,GAAG;KACJ;;AAEH,QAAM,iBAAiB;IACrB,GAAG,KAAK;IACR,GAAG;;AAEL,MAAI,KAAK,mBAAmB;AAC1B,mBAAe,UAAU;MACvB,WAAW,KAAK;MAChB,GAAG,eAAe;;AAEpB,mBAAe,QAAQ;MACrB,eAAe,KAAK;MACpB,GAAG,eAAe;;;AAGtB,SAAO;;AAGT,SAAgB,oBAEd,OACA;AACA,QAAMC,QAAO;AAEb,WAAS,SAAS,KAA8C;AAC9D,WAAO,OAAO,QAAQ,YAAY,OAAO;;AAG3C,MAAI,SAASA,KAAA,KAAS,SAASA,MAAK,MAAA,GAAS;AAC3C,WAAOA,MAAK,OAAO;AACnB,WAAOA,MAAK,OAAO;AACnB,WAAOA,MAAK,OAAO;AACnB,WAAOA,MAAK,OAAO;AACnB,WAAOA,MAAK,OAAO;AAEnB,QAAI,CAACA,MAAK,OAAO,kBAAkB,KAAK,oBACtC,CAAAA,MAAK,OAAO,iBAAiB,KAAK;AAEpC,QAAI,CAACA,MAAK,OAAO,kBAAkB,KAAK,qBAAqB;AAC3D,YAAM,QAAQ,KAAK,oBAAoB,MAAM,sBAAA;AAC7C,UAAI,MAAM,WAAW,KAAK,MAAM,CAAA,EAAG,WAAW,MAAA,GAAS;AACrD,cAAM,CAAC,QAAA,IAAY;AACnB,QAAAA,MAAK,OAAO,iBAAiB;;;AAGjC,QAAI,CAACA,MAAK,OAAO,kBAAkB,KAAK,2BACtC,CAAAA,MAAK,OAAO,iBAAiB,WAAW,KAAK,0BAAA;AAE/C,QAAI,CAACA,MAAK,OAAO,mBAAmB,KAAK,6BACvC,CAAAA,MAAK,OAAO,kBAAkB,KAAK;AAErC,QAAI,CAACA,MAAK,OAAO,mBAAmB,KAAK,qBAAqB;AAC5D,YAAM,QAAQ,KAAK,oBAAoB,MAAM,sBAAA;AAC7C,UAAI,MAAM,WAAW,GAAG;AACtB,cAAM,CAAA,EAAG,UAAA,IAAc;AACvB,QAAAA,MAAK,OAAO,kBAAkB;;;AAIlC,QACEA,MAAK,OAAO,kBACZA,MAAK,OAAO,mBACZA,MAAK,OAAO,gBAEZ,QAAOA,MAAK,OAAO;AAErB,QACEA,MAAK,OAAO,kCACZA,MAAK,OAAO,eAEZ,QAAOA,MAAK,OAAO;;AAIvB,SAAOA;;;;ACtNT,IAAa,6BAAb,cAIU,sBAEV;EACE;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA,WAAmB;AACjB,WAAO;;EAGT,IAAI,aAAqC;AACvC,WAAO;MACL,GAAG,MAAM;MACT,GAAG;;;EAIP,IAAI,aAAoD;AACtD,WAAO;MACL,GAAG,MAAM;MACT,GAAG;;;EAIP,IAAI,uBAAiC;AACnC,WAAO,CAAC,GAAG,MAAM,sBAAsB,GAAG,uBAAA;;EAG5C,YAAY,SAAqD;AAC/D,UAAM,SAAS,MAAM,YAAY,OAAA;AACjC,WAAO,cAAc;AACrB,WAAO;;EAWT,YACE,oBACA,WAIA;AACA,UAAM,SAAS,yBAAyB,oBAAoB,SAAA;AAC5D,UAAM,MAAA;AACN,0BAAsB,KAAK,MAAM,MAAA;;EAG1B,kBACP,SAC0B;AAC1B,WAAO,uBAAuB,KAAK,MAAM,OAAA;;EAGlC,SAAqB;AAC5B,WAAO,oBAAoB,KAAK,MAAM,MAAM,OAAA,CAAQ;;;;;ACpDxD,IAAM,6BAA6B;AAgBnC,SAAS,mCACP,YACkD;AAClD,MAAI,WAAW,SAAS,eACtB,QAAO;IACL,MAAM;IACN,QAAQ;IACR,KAAK,WAAW;IAChB,OAAO,WAAW;IAClB,YAAY,WAAW;IACvB,UAAU,WAAW;;AAIzB,MAAI,WAAW,SAAS,gBACtB,QAAO;IACL,MAAM;IACN,QAAQ;IACR,OAAO,WAAW;IAClB,YAAY,WAAW;IAEvB,SAAS,WAAW;;AAIxB,MAAI,WAAW,SAAS,0BACtB,QAAO;IACL,MAAM;IACN,QAAQ;IACR,OAAO,WAAW;IAClB,YAAY,WAAW;IACvB,UAAU,WAAW;IAErB,SAAS,WAAW;IACpB,cAAc,WAAW;;AAI7B,MAAI,WAAW,SAAS,YACtB,QAAO;IACL,MAAM;IACN,QAAQ;IACR,YAAY,WAAW;IACvB,SAAS,WAAW;;AAKxB,SAAO;IACL,MAAM;IACN,OAAO;;;AAWX,SAAS,mCACP,YAIkB;AAElB,MACE,WAAW,SAAS,kBACpB,WAAW,SAAS,mBACpB,WAAW,SAAS,6BACpB,WAAW,SAAS,YAEpB,QAAO;AAIT,MAAI,WAAW,SAAS,YAAY;AAClC,UAAM,WAAW;AAKjB,QAAI,SAAS,WAAW,eACtB,QAAO;MACL,MAAM;MACN,KAAK,SAAS,OAAO;MACrB,OAAO,SAAS,SAAS;MACzB,aAAa,SAAS,cAAc;MACpC,WAAW,SAAS,YAAY;;AAIpC,QAAI,SAAS,WAAW,gBACtB,QAAO;MACL,MAAM;MACN,SAAS,SAAS,WAAW;MAC7B,UAAU,SAAS,SAAS;MAC5B,OAAO,SAAS,cAAc;;AAIlC,QAAI,SAAS,WAAW,0BACtB,QAAO;MACL,MAAM;MACN,SAAS,SAAS,WAAW;MAC7B,UAAU,SAAS,SAAS;MAC5B,cAAc,SAAS,gBAAgB;MACvC,aAAa,SAAS,cAAc;MACpC,WAAW,SAAS,YAAY;;AAIpC,QAAI,SAAS,WAAW,YACtB,QAAO;MACL,MAAM;MACN,SAAS,SAAS,WAAW;MAC7B,OAAO,SAAS,cAAc;;;AAMpC,MAAI,WAAW,SAAS,eACtB,QAAQ,WACL;AAIL,SAAO;;AAmET,IAAa,uCAAA,CAGR,UAAU;AACb,QAAM,oBAAoB,EACxB,GAAI,OAAO,sBAAsB,iBAAiB,QAAQ,EACxD,YAAY,OAAO,sBAAsB,cAAA,EAC1C;AAEH,QAAM,qBAAqB,EACzB,GAAI,OAAO,uBAAuB,oBAAoB,QAAQ,EAC5D,WAAW,OAAO,uBAAuB,iBAAA,EAC1C;AAEH,SAAO;IACL,cAAc,OAAO,gBAAgB;IACrC,eAAe,OAAO,iBAAiB;IACvC,cAAc,OAAO,gBAAgB;IACrC,qBAAqB;IACrB,sBAAsB;;;AAqD1B,IAAa,qCAAA,CAGR,aAAa;AAChB,MAAI,SAAS,OAAO;AAElB,UAAM,QAAQ,IAAI,MAAM,SAAS,MAAM,OAAA;AACvC,UAAM,OAAO,SAAS,MAAM;AAC5B,UAAM;;AAGR,MAAI;AACJ,QAAM,UAA0B,CAAA;AAChC,QAAM,aAAyB,CAAA;AAC/B,QAAM,qBAAwC,CAAA;AAK9C,QAAM,gBAAgB,SAAS,OAAO,IAAA,CAAK,SAAS;AAClD,QAAI,KAAK,SAAS,mBAAmB,sBAAsB,MAAM;AAC/D,YAAM,UAAU,EAAE,GAAG,KAAA;AACrB,aAAQ,QAAoC;AAC5C,aAAO;;AAET,WAAO;;AAGT,QAAM,oBAA6C;IACjD,gBAAgB;IAChB,OAAO,SAAS;IAChB,YAAY,SAAS;IACrB,IAAI,SAAS;IACb,oBAAoB,SAAS;IAC7B,UAAU,SAAS;IACnB,QAAQ,SAAS;IACjB,QAAQ;IACR,QAAQ,SAAS;IACjB,MAAM,SAAS;IACf,cAAc,SAAS;IAEvB,YAAY,SAAS;;AAGvB,QAAM,oBAOF,CAAA;AAEJ,aAAW,QAAQ,SAAS,OAC1B,KAAI,KAAK,SAAS,WAAW;AAC3B,gBAAY,KAAK;AACjB,YAAQ,KACN,GAAG,KAAK,QAAQ,QAAA,CAAS,SAAS;AAChC,UAAI,KAAK,SAAS,eAAe;AAC/B,YAAI,YAAY,QAAQ,KAAK,UAAU,KACrC,mBAAkB,SAAS,KAAK;AAElC,eAAO;UACL,MAAM;UACN,MAAM,KAAK;UACX,aAAa,KAAK,YAAY,IAC5B,kCAAA;;;AAKN,UAAI,KAAK,SAAS,WAAW;AAC3B,0BAAkB,UAAU,KAAK;AACjC,eAAO,CAAA;;AAGT,aAAO;MACP;aAEK,KAAK,SAAS,iBAAiB;AACxC,UAAM,YAAY;MAChB,UAAU;QAAE,MAAM,KAAK;QAAM,WAAW,KAAK;;MAC7C,IAAI,KAAK;;AAGX,QAAI;AACF,iBAAW,KAAKC,eAAc,WAAW,EAAE,UAAU,KAAA,CAAM,CAAC;aACrD,GAAY;AACnB,UAAI;AACJ,UACE,OAAO,MAAM,YACb,KAAK,QACL,aAAa,KACb,OAAO,EAAE,YAAY,SAErB,cAAa,EAAE;AAEjB,yBAAmB,KAAK,oBAAoB,WAAW,UAAA,CAAW;;AAGpE,sBAAkB,0BAAA,MAAgC,CAAA;AAClD,QAAI,KAAK,GACP,mBAAkB,0BAAA,EAA4B,KAAK,OAAA,IAAW,KAAK;aAE5D,KAAK,SAAS,aAAa;AACpC,sBAAkB,YAAY;AAE9B,UAAM,gBAAgB,KAAK,SACvB,IAAA,CAAK,MAAM,EAAE,IAAA,EACd,OAAO,OAAA,EACP,KAAK,EAAA;AACR,QAAI,cACF,SAAQ,KAAK;MACX,MAAM;MACN,WAAW;KACZ;aAEM,KAAK,SAAS,oBAAoB;AAC3C,UAAM,SAAS,oBAAoB,IAAA;AACnC,QAAI,OACF,YAAW,KAAK,MAAA;QAEhB,oBAAmB,KACjB,oBAAoB,MAAM,4BAAA,CAA6B;aAGlD,KAAK,SAAS,iBAAiB;AACxC,UAAM,SAAS,kBAAkB,IAAA;AACjC,QAAI,OACF,YAAW,KAAK,MAAA;QAEhB,oBAAmB,KACjB,oBAAoB,MAAM,yBAAA,CAA0B;aAG/C,KAAK,SAAS,yBAAyB;AAEhD,QAAI,KAAK,OACP,SAAQ,KAAK;MACX,MAAM;MACN,UAAU;MACV,MAAM,KAAK;MACX,IAAI,KAAK;MACT,UAAU,EACR,QAAQ,KAAK,OAAA;KAEhB;AAGH,sBAAkB,iBAAiB,CAAA;AACnC,sBAAkB,aAAa,KAAK,IAAA;SAC/B;AACL,sBAAkB,iBAAiB,CAAA;AACnC,sBAAkB,aAAa,KAAK,IAAA;;AAIxC,SAAO,IAAI,UAAU;IACnB,IAAI;IACJ;IACA;IACA;IACA,gBAAgB,qCAAqC,SAAS,KAAA;IAC9D;IACA;GACD;;AA8DH,IAAa,kDAAA,CAGR,cAAc;AAEjB,QAAM,WACJ,UAAU,QAAQ,SAAS,IACvB,UAAU,QAAQ,OAAA,CACf,KAAK,SAAS;AACb,UAAM,OAAO,IAAI,IAAI,SAAS,CAAA;AAE9B,QAAI,KAAM,UAAU,KAAK,MACvB,MAAM,QAAQ,KAAK;QAEnB,KAAI,KAAK,IAAA;AAEX,WAAO;KAET,CAAC,EAAE,GAAG,UAAU,QAAQ,CAAA,EAAA,CAAI,CAAC,IAE/B,UAAU,SACd,IAAA,CAAK,MACL,OAAO,YAAY,OAAO,QAAQ,CAAA,EAAG,OAAA,CAAQ,CAAC,CAAA,MAAO,MAAM,OAAA,CAAQ,CAAC;AAGtE,SAAO;IACL,GAAG;IACH;;;AA2DJ,IAAa,6CAAA,CAGR,UAAU;AACb,QAAM,UAA0B,CAAA;AAChC,MAAI,iBAA0C,CAAA;AAC9C,MAAI;AACJ,QAAM,mBAAoC,CAAA;AAC1C,QAAM,oBAA6C,EACjD,gBAAgB,SAAA;AAElB,QAAM,oBAIF,CAAA;AACJ,MAAI;AACJ,MAAI,MAAM,SAAS,6BACjB,SAAQ,KAAK;IACX,MAAM;IACN,MAAM,MAAM;IACZ,OAAO,MAAM;GACd;WACQ,MAAM,SAAS,wCACxB,SAAQ,KAAK;IACX,MAAM;IACN,MAAM;IACN,aAAa,CACX,mCACE,MAAM,UAAA,CACP;IAEH,OAAO,MAAM;GACd;WAED,MAAM,SAAS,gCACf,MAAM,KAAK,SAAS,UAEpB,MAAK,MAAM,KAAK;WAEhB,MAAM,SAAS,gCACf,MAAM,KAAK,SAAS,iBACpB;AACA,qBAAiB,KAAK;MACpB,MAAM;MACN,MAAM,MAAM,KAAK;MACjB,MAAM,MAAM,KAAK;MACjB,IAAI,MAAM,KAAK;MACf,OAAO,MAAM;KACd;AAED,sBAAkB,0BAAA,IAA8B,EAAA,CAC7C,MAAM,KAAK,OAAA,GAAU,MAAM,KAAK,GAAA;aAGnC,MAAM,SAAS,+BACf,MAAM,KAAK,SAAS,iBACpB;AAEA,qBAAiB,KAAK;MACpB,MAAM;MACN,MAAM;MACN,MAAM,KAAK,UAAU,EAAE,QAAQ,MAAM,KAAK,OAAA,CAAQ;MAClD,IAAI,MAAM,KAAK;MACf,OAAO,MAAM;KACd;AAED,sBAAkB,eAAe,CAAC,MAAM,IAAA;aAExC,MAAM,SAAS,+BACf,MAAM,KAAK,SAAS,yBACpB;AAEA,QAAI,MAAM,KAAK,OACb,SAAQ,KAAK;MACX,MAAM;MACN,UAAU;MACV,MAAM,MAAM,KAAK;MACjB,IAAI,MAAM,KAAK;MACf,UAAU,EACR,QAAQ,MAAM,KAAK,OAAA;KAEtB;AAGH,sBAAkB,eAAe,CAAC,MAAM,IAAA;aAExC,MAAM,SAAS,+BACf;IACE;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,SAAS,MAAM,KAAK,IAAA,EAEtB,mBAAkB,eAAe,CAAC,MAAM,IAAA;WAC/B,MAAM,SAAS,oBAAoB;AAC5C,sBAAkB,KAAK,MAAM,SAAS;AACtC,sBAAkB,aAAa,MAAM,SAAS;AAC9C,sBAAkB,QAAQ,MAAM,SAAS;aAChC,MAAM,SAAS,sBAAsB;AAC9C,UAAM,MAAM,mCAAmC,MAAM,QAAA;AAErD,qBAAiB,qCAAqC,MAAM,SAAS,KAAA;AAErE,QAAI,MAAM,SAAS,MAAM,QAAQ,SAAS,cACxC,mBAAkB,WAAW,KAAK,MAAM,IAAI,IAAA;AAE9C,eAAW,CAAC,KAAK,KAAA,KAAU,OAAO,QAAQ,MAAM,QAAA,GAAW;AACzD,UAAI,QAAQ,KAAM;AAGlB,UAAI,QAAQ,SACV,mBAAkB,GAAA,IAAO,IAAI,kBAAkB;UAE/C,mBAAkB,GAAA,IAAO;;aAI7B,MAAM,SAAS,4CACf,MAAM,SAAS,wCAEf,kBAAiB,KAAK;IACpB,MAAM;IACN,MAAM,MAAM;IACZ,OAAO,MAAM;GACd;WAED,MAAM,SAAS,wCACf,MAAM,SAAS,sCAEf,kBAAiB,EACf,cAAc;IACZ,IAAI,MAAM;IACV,MAAM,MAAM,KAAK,QAAQ,aAAa,EAAA,EAAI,QAAQ,cAAc,EAAA;IAChE,QAAQ;IACT;WAEM,MAAM,SAAS,wBACxB,mBAAkB,UAAU,MAAM;WAElC,MAAM,SAAS,gCACf,UAAU,SACV,MAAM,KAAK,SAAS,aACpB;AACA,UAAM,UAA6D,MAChE,KAAK,UACJ,MAAM,KAAK,QAAQ,IAAA,CAAK,GAAG,WAAW;MACpC,GAAG;MACH;MACD,IACD;AAEJ,sBAAkB,YAAY;MAI5B,IAAI,MAAM,KAAK;MACf,MAAM,MAAM,KAAK;MACjB,GAAI,UAAU,EAAE,QAAA,IAAY,CAAA;;AAI9B,UAAM,gBAAgB,MAAM,KAAK,SAC7B,IAAA,CAAK,MAAM,EAAE,IAAA,EACd,OAAO,OAAA,EACP,KAAK,EAAA;AACR,QAAI,cACF,SAAQ,KAAK;MACX,MAAM;MACN,WAAW;KACZ;aAEM,MAAM,SAAS,yCAAyC;AACjE,sBAAkB,YAAY;MAC5B,MAAM;MACN,SAAS,CAAC;QAAE,GAAG,MAAM;QAAM,OAAO,MAAM;OAAe;;AAIzD,QAAI,MAAM,KAAK,KACb,SAAQ,KAAK;MACX,MAAM;MACN,WAAW,MAAM,KAAK;KACvB;aAEM,MAAM,SAAS,yCAAyC;AACjE,sBAAkB,YAAY;MAC5B,MAAM;MACN,SAAS,CACP;QACE,MAAM,MAAM;QACZ,MAAM;QACN,OAAO,MAAM;OACd;;AAKL,QAAI,MAAM,MACR,SAAQ,KAAK;MACX,MAAM;MACN,WAAW,MAAM;KAClB;aAEM,MAAM,SAAS,+CAGxB,QAAO;MAEP,QAAO;AAGT,SAAO,IAAI,oBAAoB;IAE7B,MAAM,QAAQ,IAAA,CAAK,SAAS,KAAK,IAAA,EAAM,KAAK,EAAA;IAC5C,SAAS,IAAI,eAAe;MAC1B;MACA;MACA;MACA;MACA;MACA;KACD;IACD;GACD;;AA0CH,IAAa,gDAAA,CAGR,YAAY;AACf,QAAM,qBACJ,UAAU,WAAW,OAAA,KACrB,QAAQ,mBAAmB,mBAAmB;AAEhD,YAAU,eAAoE;AAC5E,UAAM,cAAcC,OAAAA,MAAW;AAC7B,UAAI;AACF,cAAM,OAAO,oBAAoB,OAAA;AACjC,YACE,SAAS,YACT,SAAS,eACT,SAAS,eACT,SAAS,OAET,QAAO;AAET,eAAO;cACD;AACN,eAAO;;;AAIX,QAAI,iBACF;AAEF,UAAM,4BAA4B,oBAAI,IAAA;AACtC,UAAM,kCAAkC,oBAAI,IAAA;AAE5C,UAAM,wBAAwB,oBAAI,IAAA;AAIlC,UAAM,8BAA8B,oBAAI,IAAA;AAKxC,cAAU,eAAe;AACvB,UAAI,CAAC,eAAgB;AACrB,YAAM,UAAU,eAAe;AAC/B,UACG,OAAO,YAAY,YAAY,QAAQ,SAAS,KAChD,MAAM,QAAQ,OAAA,KAAY,QAAQ,SAAS,EAE5C,OAAM;AAER,uBAAiB;;AAGnB,UAAM,qBAAA,CAEO,YAAY;AACvB,UAAI,CAAC,eACH,kBAAiB;QACf,MAAM;QACN,MAAM;QACN,SAAS,CAAA;;AAGb,UAAI,OAAO,eAAe,YAAY,SACpC,gBAAe,UACb,eAAe,QAAQ,SAAS,IAC5B,CAAC;QAAE,MAAM;QAAc,MAAM,eAAe;SAAW,GAAG,OAAA,IAC1D,CAAC,GAAG,OAAA;UAEV,gBAAe,QAAQ,KAAK,GAAG,OAAA;;AAInC,UAAM,eAAA,CAAgB,UAAmB;AACvC,UAAI,OAAO,UAAU,SACnB,QAAO;AAET,UAAI;AACF,eAAO,KAAK,UAAU,SAAS,CAAA,CAAE;cAC3B;AACN,eAAO;;;AAIX,UAAM,mBAAA,CACJ,UAC0D;AAC1D,YAAM,SAASA,OAAAA,MAAW;AACxB,cAAM,MAAM,MAAM,UAAU;AAC5B,YAAI,QAAQ,SAAS,QAAQ,UAAU,QAAQ,OAC7C,QAAO;AAET,eAAO;;AAET,UAAI,MAAM,OACR,QAAO;QACL,MAAM;QACN;QACA,SAAS,MAAM;;AAGnB,UAAI,MAAM,IACR,QAAO;QACL,MAAM;QACN;QACA,WAAW,MAAM;;AAGrB,UAAI,MAAM,MAAM;AACd,cAAM,aACJ,OAAO,MAAM,SAAS,WAClB,MAAM,OACN,OAAO,KAAK,MAAM,IAAA,EAAM,SAAS,QAAA;AAEvC,eAAO;UACL,MAAM;UACN;UACA,WAAW,QAJI,MAAM,YAAY,WAAA,WAIK,UAAA;;;;AAM5C,UAAM,kBAAA,CACJ,UACyD;AACzD,UAAI,MAAM,QAAQ;AAChB,cAAM,WAAW,wBAAwB,KAAA;AACzC,eAAO;UACL,MAAM;UACN,SAAS,MAAM;UACf,GAAI,WAAW,EAAE,SAAA,IAAa,CAAA;;;AAGlC,UAAI,MAAM,KAAK;AACb,cAAM,WAAW,wBAAwB,KAAA;AACzC,eAAO;UACL,GAAI,WAAW,EAAE,SAAA,IAAa,CAAA;UAC9B,MAAM;UACN,UAAU,MAAM;;;AAGpB,UAAI,MAAM,MAAM;AACd,cAAM,WAAW,gCAAgC,KAAA;AACjD,cAAM,UACJ,OAAO,MAAM,SAAS,WAClB,MAAM,OACN,OAAO,KAAK,MAAM,IAAA,EAAM,SAAS,QAAA;AAEvC,eAAO;UACL,MAAM;UACN,WAAW,QAHI,MAAM,YAAY,0BAAA,WAGK,OAAA;UACtC,GAAI,WAAW,EAAE,SAAA,IAAa,CAAA;;;;AAMpC,UAAM,wBAAA,CACJ,UACiD;AACjD,YAAM,iBAAiBA,OAAAA,MAAW;AAChC,YAAI,MAAM,QAAQ,MAAM,OAAA,GAAU;AAEhC,gBAAM,SADY,MAAM,SAGlB,IAAA,CAAK,SAAS,MAAM,IAAA,EACrB,OAAA,CAAQ,SAAyB,OAAO,SAAS,QAAA,KAAa,CAAA;AACnE,cAAI,OAAO,SAAS,EAClB,QAAO;;AAGX,eAAO,MAAM,YAAY,CAAC,MAAM,SAAA,IAAa,CAAA;;AAG/C,YAAM,UACJ,eAAe,SAAS,IACpB,eAAe,IAAA,CAAK,UAAU;QAC5B,MAAM;QACN;QACD,IACD,CAAC;QAAE,MAAM;QAAyB,MAAM;OAAI;AAElD,YAAM,gBAA8D;QAClE,MAAM;QACN,IAAI,MAAM,MAAM;QAChB;;AAGF,UAAI,MAAM,UACR,eAAc,UAAU,CACtB;QACE,MAAM;QACN,MAAM,MAAM;OACb;AAGL,aAAO;;AAGT,UAAM,sBAAA,CACJ,WACqD;MACrD,MAAM;MACN,MAAM,MAAM,QAAQ;MACpB,SAAS,MAAM,MAAM;MACrB,WAAW,aAAa,MAAM,IAAA;;AAGhC,UAAM,4BAAA,CACJ,UACgE;AAChE,YAAM,SAAS,aAAa,MAAM,MAAA;AAClC,YAAM,SACJ,MAAM,WAAW,YACb,cACA,MAAM,WAAW,UACf,eACA;AACR,aAAO;QACL,MAAM;QACN,SAAS,MAAM,cAAc;QAC7B;QACA,GAAI,SAAS,EAAE,OAAA,IAAW,CAAA;;;AAI9B,eAAW,SAAS,QAAQ,cAC1B,KAAI,MAAM,SAAS,OACjB,oBAAmB,CAAC;MAAE,MAAM;MAAc,MAAM,MAAM;KAAM,CAAC;aACpD,MAAM,SAAS,qBAAqB;IAAA,WAEpC,MAAM,SAAS,aAAa;AACrC,aAAO,aAAA;AACP,YAAM,sBACJ,KAAA;eAEO,MAAM,SAAS,aAAa;AACrC,aAAO,aAAA;AACP,YAAM,KAAK,MAAM,MAAM;AACvB,UAAI,IAAI;AACN,kCAA0B,IAAI,EAAA;AAC9B,8BAAsB,OAAO,EAAA;;AAE/B,YAAM,oBACJ,KAAA;eAEO,MAAM,SAAS,mBACxB;UAAI,MAAM,IAAI;AACZ,cAAM,WAAW,sBAAsB,IAAI,MAAM,EAAA,KAAO;UACtD,MAAM,MAAM;UACZ,MAAM,CAAA;;AAER,YAAI,MAAM,KAAM,UAAS,OAAO,MAAM;AACtC,YAAI,MAAM,KAAM,UAAS,KAAK,KAAK,MAAM,IAAA;AACzC,8BAAsB,IAAI,MAAM,IAAI,QAAA;;eAE7B,MAAM,SAAS,oBAAoB;AAC5C,aAAO,aAAA;AACP,YAAM,KAAK,MAAM,MAAM;AACvB,UAAI,IAAI;AACN,wCAAgC,IAAI,EAAA;AACpC,oCAA4B,OAAO,EAAA;;AAErC,YAAM,oBAAoB,KAAA;eACjB,MAAM,SAAS,0BACxB;UAAI,MAAM,IAAI;AACZ,cAAM,WAAW,4BAA4B,IAAI,MAAM,EAAA,KAAO;UAC5D,MAAM,MAAM;UACZ,MAAM,CAAA;;AAER,YAAI,MAAM,KAAM,UAAS,OAAO,MAAM;AACtC,YAAI,MAAM,KAAM,UAAS,KAAK,KAAK,MAAM,IAAA;AACzC,oCAA4B,IAAI,MAAM,IAAI,QAAA;;eAEnC,MAAM,SAAS,2BAA2B;AACnD,aAAO,aAAA;AACP,YAAM,0BAA0B,KAAA;eACvB,MAAM,SAAS,SAAS;IAAA,WAExB,MAAM,SAAS,QAAQ;AAChC,YAAM,WAAW,gBAAgB,KAAA;AACjC,UAAI,SACF,oBAAmB,CAAC,QAAA,CAAS;eAEtB,MAAM,SAAS,SAAS;AACjC,YAAM,YAAY,iBAAiB,KAAA;AACnC,UAAI,UACF,oBAAmB,CAAC,SAAA,CAAU;eAEvB,MAAM,SAAS,SAAS;AACjC,YAAM,YAAY,gBAAgB,KAAA;AAClC,UAAI,UACF,oBAAmB,CAAC,SAAA,CAAU;eAEvB,MAAM,SAAS,cACxB;UAAI,MAAM,KACR,oBAAmB,CACjB;QACE,MAAM;QACN,MAAM,MAAM;OACb,CACF;eAEM,MAAM,SAAS,kBAAkB,oBAAoB;AAC9D,aAAO,aAAA;AACP,YAAM,MAAM;;AAGhB,WAAO,aAAA;AAEP,eAAW,CAAC,IAAI,KAAA,KAAU,uBAAuB;AAC/C,UAAI,CAAC,MAAM,0BAA0B,IAAI,EAAA,EAAK;AAC9C,YAAM,OAAO,MAAM,KAAK,KAAK,EAAA;AAC7B,UAAI,CAAC,MAAM,QAAQ,CAAC,KAAM;AAC1B,YAAM;QACJ,MAAM;QACN,SAAS;QACT,MAAM,MAAM,QAAQ;QACpB,WAAW;;;AAIf,eAAW,CAAC,IAAI,KAAA,KAAU,6BAA6B;AACrD,UAAI,CAAC,MAAM,gCAAgC,IAAI,EAAA,EAAK;AACpD,YAAM,OAAO,MAAM,KAAK,KAAK,EAAA;AAC7B,UAAI,CAAC,MAAM,QAAQ,CAAC,KAAM;AAC1B,YAAM;QACJ,MAAM;QACN,SAAS;QACT,MAAM,MAAM,QAAQ;QACpB,WAAW;;;;AAIjB,SAAO,MAAM,KAAK,aAAA,CAAc;;AAiClC,IAAa,kCAAA,CAGR,EAAE,UAAU,YAAY,MAAA,MAAY;AACvC,SAAO,SAAS,QAAA,CACb,UAAqD;AACpD,UAAM,mBAAmB,MAAM;AAG/B,QAAI,kBAAkB,mBAAmB,KACvC,QAAO,8CAA8C,KAAA;AAGvD,UAAM,oBACJ,MAAM;AAOR,QAAI,OAAO,oBAAoB,KAAA;AAC/B,QAAI,SAAS,YAAY,iBAAiB,KAAA,EAAQ,QAAO;AAEzD,QAAI,SAAS,WACX,OAAM,IAAI,MAAM,sDAAA;AAGlB,QAAI,SAAS,QAAQ;AACnB,YAAM,cAAc;AAGpB,UAAI,mBAAmB,SAAS;AAuD9B,eAAO;UACL,MAAM;UACN,SAAA,MAxDoB;AACpB,gBAAI,OAAO,YAAY,YAAY,SACjC,QAAO;cACL,MAAM;cACN,WAAW,YAAY;;AAI3B,gBAAI,MAAM,QAAQ,YAAY,OAAA,GAAU;AAItC,oBAAM,aAAa,YAAY,QAAQ,KAAA,CACpC,MAAM,EAAE,SAAS,aAAA;AAGpB,kBAAI,WAAY,QAAO;AAKvB,oBAAM,gBAAgB,YAAY,QAAQ,KAAA,CACvC,MAAM,EAAE,SAAS,qBAAA;AAKpB,kBAAI,cAAe,QAAO;AAK1B,oBAAM,UAAU,YAAY,QAAQ,KAAA,CACjC,MAAM,EAAE,SAAS,WAAA;AAGpB,kBAAI,QACF,QAAO;gBACL,MAAM;gBACN,WACE,OAAO,QAAQ,cAAc,WACzB,QAAQ,YACR,QAAQ,UAAU;;;AAK9B,kBAAM,IAAI,MAAM,8BAAA;;UAUhB,SAAS,YAAY;;AAKzB,UAAI,YAAY,mBAAmB,WACjC,QAAO;QACL,MAAM;QACN,SAAS,YAAY;QACrB,QAAQ,YAAY;;AAMxB,YAAM,0BACJ,MAAM,QAAQ,YAAY,OAAA,KAC1B,YAAY,QAAQ,MAAA,CACjB,SACC,OAAO,SAAS,YAChB,SAAS,QACT,UAAU,SACT,KAAK,SAAS,gBACb,KAAK,SAAS,iBACd,KAAK,SAAS,aAAA;AAGtB,aAAO;QACL,MAAM;QACN,SAAS,YAAY;QACrB,IAAI,YAAY,IAAI,WAAW,KAAA,IAAS,YAAY,KAAK;QACzD,QAAQ,0BACH,YAAY,UACb,OAAO,YAAY,YAAY,WAC7B,KAAK,UAAU,YAAY,OAAA,IAC3B,YAAY;;;AAItB,QAAI,SAAS,aAAa;AAExB,UACE,CAAC,cACD,kBAAkB,UAAU,QAC5B,MAAM,QAAQ,kBAAkB,MAAA,KAChC,kBAAkB,OAAO,SAAS,KAClC,kBAAkB,OAAO,MAAA,CAAO,SAAS,UAAU,IAAA,EAEnD,QAAO,kBAAkB;AAK3B,YAAM,QAA8B,CAAA;AAGpC,YAAM,YAAY,mBAAmB;AACrC,YAAM,sBAAsB,CAAC,CAAC,WAAW;AAMzC,UAAI,cAAc,CAAC,cAAc,sBAAsB;AACrD,cAAM,gBACJ,gDAAgD,SAAA;AAClD,cAAM,KAAK,aAAA;;AAIb,UAAI,EAAE,QAAA,IAAY;AAClB,UAAI,mBAAmB,SAAS;AAC9B,YAAI,OAAO,YAAY,SACrB,WAAU,CAAC;UAAE,MAAM;UAAe,MAAM;UAAS,aAAa,CAAA;SAAI;AAEpE,kBAAU,CACR,GAAI,SACJ;UAAE,MAAM;UAAW,SAAS,kBAAkB;SAAS;;AAI3D,UAAI,OAAO,YAAY,YAAY,QAAQ,SAAS,EAClD,OAAM,KAAK;QACT,MAAM;QACN,MAAM;QACN,GAAI,MAAM,MAAM,CAAC,cAAc,MAAM,GAAG,WAAW,MAAA,IAC/C,EAAE,IAAI,MAAM,GAAA,IACZ,CAAA;QACJ,SAASA,OAAAA,MAAW;AAClB,cAAI,OAAO,YAAY,SACrB,QAAO;AAET,iBAAO,QAAQ,QAAA,CAAS,SAAS;AAC/B,gBAAI,KAAK,SAAS,OAChB,QAAO;cACL,MAAM;cACN,MAAM,KAAK;cACX,cAAc,KAAK,eAAe,CAAA,GAAI,IACpC,kCAAA;;AAKN,gBAAI,KAAK,SAAS,iBAAiB,KAAK,SAAS,UAC/C,QAAO;AAGT,mBAAO,CAAA;;;OAGZ;AAGH,YAAM,kBAAkB,oBAAoB,0BAAA;AAE5C,UAAI,UAAU,WAAW,KAAA,KAAU,CAAC,CAAC,MAAM,YAAY,OACrD,OAAM,KACJ,GAAG,MAAM,WAAW,IAAA,CAAK,aAAiC;AACxD,YAAI,iBAAiB,QAAA,EACnB,QAAO;UACL,MAAM;UACN,IAAI,SAAS;UACb,SAAS,SAAS,MAAM;UACxB,OAAO,SAAS,KAAK;UACrB,MAAM,SAAS;;AAGnB,YAAI,mBAAmB,QAAA,EACrB,QAAO;UACL,MAAM;UACN,IAAI,SAAS;UACb,SAAS,SAAS,MAAM;UACxB,QAAQ,SAAS,KAAK;;AAG1B,eAAO;UACL,MAAM;UACN,MAAM,SAAS;UACf,WAAW,KAAK,UAAU,SAAS,IAAA;UACnC,SAAS,SAAS;UAClB,GAAI,CAAC,aAAa,EAAE,IAAI,kBAAkB,SAAS,EAAA,EAAA,IAAS,CAAA;;QAE9D;eAEK,mBAAmB,WAC5B,OAAM,KACJ,GAAG,kBAAkB,WAAW,IAAA,CAC7B,cAAkC;QACjC,MAAM;QACN,MAAM,SAAS,SAAS;QACxB,SAAS,SAAS;QAClB,WAAW,SAAS,SAAS;QAC7B,GAAI,CAAC,aAAa,EAAE,IAAI,kBAAkB,SAAS,EAAA,EAAA,IAAQ,CAAA;QAC5D,CACF;AAIL,YAAM,cACJ,kBAAkB,QACjB,SACC,kBAAkB,SAClB,kBAAkB;AAEtB,YAAM,uBAAqD;QACzD;QACA;QACA;QACA;QACA;QACA;;AAGF,UAAI,eAAe,MAAM;AAEvB,cAAM,mBADkB,aACkB,OAAA,CAAQ,SAChD,qBAAqB,SAAS,KAAK,IAAA,CAAK;AAE1C,YAAI,iBAAiB,SAAS,EAAG,OAAM,KAAK,GAAG,gBAAA;;AAGjD,aAAO;;AAGT,QAAI,SAAS,UAAU,SAAS,YAAY,SAAS,aAAa;AAChE,UAAI,OAAO,MAAM,YAAY,SAC3B,QAAO;QAAE,MAAM;QAAW;QAAM,SAAS,MAAM;;AAGjD,YAAMC,YAAiC,CAAA;AACvC,YAAM,UAAW,MAAM,QAA2B,QAAA,CAAS,SAAS;AAClE,YAAI,KAAK,SAAS,wBAChB,CAAAA,UAAS,KAAK;UACZ,MAAM;UACN,qBAAqB,KAAK;UAC1B,SAAS,KAAK;SACf;AAEH,YAAI,mBAAmB,IAAA,EACrB,QAAO,8BACL,MACA,mCAAA;AAGJ,YAAI,KAAK,SAAS,OAChB,QAAO;UACL,MAAM;UACN,MAAM,KAAK;;AAGf,YAAI,KAAK,SAAS,YAyBhB,QAAO;UACL,MAAM;UACN,WA1BeD,OAAAA,MAAW;AAC1B,gBAAI,OAAO,KAAK,cAAc,SAC5B,QAAO,KAAK;qBAEZ,OAAO,KAAK,cAAc,YAC1B,KAAK,cAAc,QACnB,SAAS,KAAK,UAEd,QAAO,KAAK,UAAU;;UAmBxB,QAfaA,OAAAA,MAAW;AACxB,gBAAI,OAAO,KAAK,cAAc,SAC5B,QAAO;qBAEP,OAAO,KAAK,cAAc,YAC1B,KAAK,cAAc,QACnB,YAAY,KAAK,UAEjB,QAAO,KAAK,UAAU;;;AAU5B,YACE,KAAK,SAAS,gBACd,KAAK,SAAS,iBACd,KAAK,SAAS,aAEd,QAAO;AAET,eAAO,CAAA;;AAGT,UAAI,QAAQ,SAAS,EACnB,CAAAC,UAAS,KAAK;QACZ,MAAM;QACN;QACS;OACV;AAEH,aAAOA;;AAGT,YAAQ,KACN,mEAAmE,IAAA,EAAA;AAErE,WAAO,CAAA;;;;;AC5hDb,IAAa,sBAAb,cAGU,eAA4B;EAGpC,YACE,eACA,WACA;AACA,UAAM,yBAAyB,eAAe,SAAA,CAAU;;EAGjD,iBACP,SAC+B;AAC/B,QAAI;AACJ,QAAI,SAAS,WAAW,OACtB,UAAS,QAAQ;AAEnB,QAAI,WAAW,UAAa,KAAK,8BAA8B,OAC7D,UAAS,KAAK;AAGhB,UAAM,SAAwC;MAC5C,OAAO,KAAK;MACZ,aAAa,KAAK;MAClB,OAAO,KAAK;MACZ,MAAM,KAAK;MACX,cAAc,KAAK;MAGnB,QAAQ,KAAK;MACb,sBAAsB,SAAS;MAC/B,YAAY,SAAS;MACrB,SAAS,SAAS;MAClB,OAAO,SAAS,OAAO,SACnB,KAAK,uBAAuB,QAAQ,OAAO;QACzC,QAAQ,KAAK;QACb;OACD,IACD;MACJ,aAAa,oBAAoB,SAAS,WAAA,IACtC,SAAS,eAAA,MACF;AACL,cAAM,YAAY,yBAAyB,SAAS,WAAA;AACpD,YAAI,OAAO,cAAc,YAAY,UAAU,WAC7C;cAAI,UAAU,SAAS,WACrB,QAAO;YAAE,MAAM;YAAY,MAAM,UAAU,SAAS;;mBAC3C,UAAU,SAAS,gBAC5B,QAAO;YACL,MAAM;YACN,MAAM,UAAU,cAAc;YAC9B,OAAO,UAAU,cAAc;;mBAExB,UAAU,SAAS,SAC5B,QAAO;YACL,MAAM;YACN,MAAM,UAAU,OAAO;;;;MAMnC,OAAA,MAAa;AACX,YAAI,SAAS,KAAM,QAAO,QAAQ;AAClC,cAAM,SAAS,KAAK,mBAAmB,SAAS,eAAA;AAChD,YAAI,QAAQ,SAAS,eAAe;AAClC,cAAI,OAAO,YAAY,UAAU,KAC/B,QAAO;YACL,QAAQ;cACN,MAAM;cACN,QAAQ,OAAO,YAAY;cAC3B,aAAa,OAAO,YAAY;cAChC,MAAM,OAAO,YAAY;cACzB,QAAQ,OAAO,YAAY;;YAE7B,WAAW,SAAS;;AAGxB;;AAEF,eAAO;UAAE;UAAQ,WAAW,SAAS;;;MAEvC,qBAAqB,SAAS;MAC9B,mBAAmB,KAAK,cAAc,KAAK,SAAY,KAAK;MAC5D,kBAAkB,SAAS,kBAAkB,KAAK;MAClD,wBACE,SAAS,wBAAwB,KAAK;MACxC,GAAI,KAAK,aAAa,EAAE,OAAO,MAAA,IAAU,CAAA;MACzC,GAAG,KAAK;;AAGV,UAAM,YAAY,KAAK,oBAAoB,OAAA;AAE3C,QAAI,cAAc,OAChB,QAAO,YAAY;AAGrB,WAAO;;EAGT,MAAM,UACJ,UACA,SACA,YACqB;AACrB,YAAQ,QAAQ,eAAA;AAChB,UAAM,mBAAmB,KAAK,iBAAiB,OAAA;AAC/C,QAAI,iBAAiB,QAAQ;AAC3B,YAAM,SAAS,KAAK,sBAAsB,UAAU,SAAS,UAAA;AAC7D,UAAI;AACJ,uBAAiB,SAAS,QAAQ;AAChC,cAAM,QAAQ,oBAAoB;UAChC,GAAG,MAAM;UACT,GAAG,MAAM,QAAQ;;AAEnB,qBAAa,YAAY,OAAO,KAAA,KAAU;;AAG5C,aAAO;QACL,aAAa,aAAa,CAAC,UAAA,IAAc,CAAA;QACzC,WAAW,EACT,qBAAsB,YAAY,SAC9B,eAAA;;WAGH;AACL,YAAM,OAAO,MAAM,KAAK,oBACtB;QACE,OAAO,gCAAgC;UACrC;UACA,YAAY,KAAK,cAAc;UAC/B,OAAO,KAAK;SACb;QACD,GAAG;QACH,QAAQ;SAEV;QAAE,QAAQ,SAAS;QAAQ,GAAG,SAAS;OAAS;AAGlD,aAAO;QACL,aAAa,CACX;UACE,MAAM,KAAK;UACX,SAAS,mCAAmC,IAAA;SAC7C;QAEH,WAAW;UACT,IAAI,KAAK;UACT,qBAAqB,KAAK,QACtB;YACE,cAAc,KAAK,MAAM;YACzB,kBAAkB,KAAK,MAAM;YAC7B,aAAa,KAAK,MAAM;cAE1B;;;;;EAMZ,OAAO,sBACL,UACA,SACA,YACqC;AACrC,UAAM,iBAAiB,MAAM,KAAK,oBAChC;MACE,GAAG,KAAK,iBAAiB,OAAA;MACzB,OAAO,gCAAgC;QACrC;QACA,YAAY,KAAK,cAAc;QAC/B,OAAO,KAAK;OACb;MACD,QAAQ;OAEV,OAAA;AAGF,qBAAiB,QAAQ,gBAAgB;AACvC,UAAI,QAAQ,QAAQ,QAClB;AAEF,YAAM,QAAQ,2CAA2C,IAAA;AACzD,UAAI,SAAS,KAAM;AACnB,YAAM;AACN,YAAM,YAAY,kBAChB,MAAM,QAAQ,IACd;QACE,QAAQ,QAAQ,eAAe;QAC/B,YAAY;SAEd,QACA,QACA,QACA,EAAE,MAAA,CAAO;;;EAqBf,MAAM,oBACJ,SACA,gBAIA;AACA,WAAO,KAAK,OAAO,KAAK,YAAY;AAClC,YAAM,gBAAgB,KAAK,kBAAkB,cAAA;AAC7C,UAAI;AAEF,YAAI,QAAQ,MAAM,QAAQ,SAAS,iBAAiB,CAAC,QAAQ,OAC3D,QAAO,MAAM,KAAK,OAAO,UAAU,MAAM,SAAS,aAAA;AAEpD,eAAO,MAAM,KAAK,OAAO,UAAU,OAAO,SAAS,aAAA;eAC5C,GAAG;AAEV,cADc,sBAAsB,CAAA;;;;;EAOhC,uBACRC,QACA,QACiB;AACjB,UAAM,eAAgC,CAAA;AACtC,eAAWC,SAAQD,OACjB,KAAI,cAAcC,KAAA,GAAO;AACvB,UAAIA,MAAK,SAAS,sBAAsB,QAAQ,OAG9C,CAAAA,MAAK,iBAAiB;AAExB,mBAAa,KAAKA,KAAA;eACT,aAAaA,KAAA,GAAO;AAC7B,YAAM,iBAAiBA,MAAK,SAAS;AACrC,mBAAa,KAAK;QAChB,MAAM;QACN,MAAM,eAAe;QACrB,aAAa,eAAe;QAC5B,QAAQ,eAAe;OACxB;eACQC,aAAqBD,KAAA,EAC9B,cAAa,KAAK;MAChB,MAAM;MACN,MAAMA,MAAK,SAAS;MACpB,YAAYA,MAAK,SAAS;MAC1B,aAAaA,MAAK,SAAS;MAC3B,QAAQ,QAAQ,UAAU;KAC3B;aACQ,mBAAmBA,KAAA,EAC5B,cAAa,KAAK,6BAA6BA,KAAA,CAAK;AAGxD,WAAO;;;;;ACrUX,IAAa,2BAAb,cAIU,oBAEV;EACE;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA,WAAmB;AACjB,WAAO;;EAGT,IAAI,aAAqC;AACvC,WAAO;MACL,GAAG,MAAM;MACT,GAAG;;;EAIP,IAAI,aAAoD;AACtD,WAAO;MACL,GAAG,MAAM;MACT,GAAG;;;EAIP,IAAI,uBAAiC;AACnC,WAAO,CAAC,GAAG,MAAM,sBAAsB,GAAG,uBAAA;;EAG5C,YAAY,SAAqD;AAC/D,UAAM,SAAS,MAAM,YAAY,OAAA;AACjC,WAAO,cAAc;AACrB,WAAO;;EAWT,YACE,oBACA,WAIA;AACA,UAAM,SAAS,yBAAyB,oBAAoB,SAAA;AAC5D,UAAM,MAAA;AACN,0BAAsB,KAAK,MAAM,MAAA;;EAG1B,kBACP,SAC0B;AAC1B,WAAO,uBAAuB,KAAK,MAAM,OAAA;;EAGlC,SAAqB;AAC5B,WAAO,oBAAoB,KAAK,MAAM,MAAM,OAAA,CAAQ;;;;;ACifxD,IAAa,aAAb,MAAaE,oBAEH,eAA4B;;;;;EAKpC,kBAAkB;EAER;EAEA;EAEV,IAAI,uBAAiC;AACnC,WAAO,CAAC,GAAG,MAAM,sBAAsB,iBAAA;;EAGzC,IAAI,WAAqB;AACvB,WAAO,CAAC,GAAG,MAAM,UAAU,iBAAA;;EAGnB;EAIV,YACE,eACA,WACA;AACA,UAAM,SAAS,yBAAyB,eAAe,SAAA;AACvD,UAAM,MAAA;AACN,SAAK,SAAS;AACd,SAAK,kBAAkB,QAAQ,mBAAmB;AAClD,SAAK,YAAY,QAAQ,aAAa,IAAI,oBAAoB,MAAA;AAC9D,SAAK,cAAc,QAAQ,eAAe,IAAI,sBAAsB,MAAA;;EAG5D,iBAAiB,SAAgD;AACzE,UAAM,mBAAmB,SAAS,OAAO,KAAK,aAAA;AAC9C,UAAM,yBACJ,SAAS,wBAAwB,QACjC,SAAS,QAAQ,QACjB,SAAS,cAAc,QACvB,SAAS,WAAW,QACpB,SAAS,WAAW,WAAW,QAC/B,KAAK,WAAW,WAAW;AAC7B,UAAM,iBACJ,SAAS,OAAO,KAAK,kBAAA,KACrB,SAAS,OAAO,KAAK,YAAA;AAEvB,WACE,KAAK,mBACL,oBACA,0BACA,kBACA,0BAA0B,KAAK,KAAA;;EAI1B,YAAY,SAAoC;AACvD,UAAM,sBAAsB,KAAK,oBAAoB,OAAA;AACrD,QAAI,KAAK,iBAAiB,OAAA,EACxB,QAAO,KAAK,UAAU,YAAY,mBAAA;AAEpC,WAAO,KAAK,YAAY,YAAY,mBAAA;;EAG7B,iBAAiB,SAAqC;AAC7D,UAAM,sBAAsB,KAAK,oBAAoB,OAAA;AACrD,QAAI,KAAK,iBAAiB,OAAA,EACxB,QAAO,KAAK,UAAU,iBAAiB,mBAAA;AAEzC,WAAO,KAAK,YAAY,iBAAiB,mBAAA;;;EAI3C,MAAe,UACb,UACA,SACA,YACqB;AACrB,QAAI,KAAK,iBAAiB,OAAA,EACxB,QAAO,KAAK,UAAU,UAAU,UAAU,SAAS,UAAA;AAErD,WAAO,KAAK,YAAY,UAAU,UAAU,SAAS,UAAA;;EAGvD,OAAgB,sBACd,UACA,SACA,YACqC;AACrC,QAAI,KAAK,iBAAiB,OAAA,GAAU;AAClC,aAAO,KAAK,UAAU,sBACpB,UACA,KAAK,oBAAoB,OAAA,GACzB,UAAA;AAEF;;AAEF,WAAO,KAAK,YAAY,sBACtB,UACA,KAAK,oBAAoB,OAAA,GACzB,UAAA;;EAIK,WACPC,SAC+D;AAC/D,UAAM,WAAW,IAAID,YAAwB,KAAK,MAAA;AAClD,aAAS,iBAAiB;MAAE,GAAG,KAAK;MAAgB,GAAGC;;AACvD,WAAO;;;;;AClRX,IAAa,kBAAb,cAGU,WAEV;EACE;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA,WAAmB;AACjB,WAAO;;EAGT,IAAI,aAAqC;AACvC,WAAO;MACL,GAAG,MAAM;MACT,GAAG;;;EAIP,IAAI,aAAoD;AACtD,WAAO;MACL,GAAG,MAAM;MACT,GAAG;;;EAIP,IAAI,uBAAiC;AACnC,WAAO,CAAC,GAAG,MAAM,sBAAsB,GAAG,uBAAA;;EAG5C,YAAY,SAAqD;AAC/D,UAAM,SAAS,MAAM,YAAY,OAAA;AACjC,WAAO,cAAc;AACrB,WAAO;;EAWT,YACE,oBACA,WAIA;AACA,UAAM,SAAS,yBAAyB,oBAAoB,SAAA;AAC5D,UAAM;MACJ,GAAG;MACH,aAAa,IAAI,2BAA2B,MAAA;MAC5C,WAAW,IAAI,yBAAyB,MAAA;KACzC;AACD,0BAAsB,KAAK,MAAM,MAAA;;;EAI1B,2BACPC,SACA;AACA,UAAM,gBAAgB,EAAE,GAAGA,QAAA;AAE3B,QAAI,KAAK,MAAM,WAAW,QAAA,GACxB;UAAI,eAAe,WAAW,OAC5B,QAAO;;AAGX,WAAO,MAAM,2BAA2B,aAAA;;EAGjC,SAAqB;AAC5B,WAAO,oBAAoB,KAAK,MAAM,MAAM,OAAA,CAAQ;;;;;;;;;AC/dxD,IAAsB,UAAtB,MAAsBC,iBAEZ,kBAAuC;EAQ/C,eAAe;IAAC;IAAa;IAAQ,KAAK,SAAA;;;;;;;;;;EAU1C,MAAM,OACJ,OACA,SACiB;AACjB,UAAM,cAAcA,SAAQ,2BAA2B,KAAA;AAMvD,YALe,MAAM,KAAK,eACxB,CAAC,WAAA,GACD,SACA,SAAS,SAAA,GAEG,YAAY,CAAA,EAAG,CAAA,EAAG;;EAIlC,OAAO,sBACL,QACA,UACA,aACiC;AACjC,UAAM,IAAI,MAAM,kBAAA;;EAGR,6CACR,SAC6C;AAE7C,UAAM,CAAC,gBAAgB,WAAA,IACrB,MAAM,uCAAuC,OAAA;AAC9C,gBAA0C,SAAS,eAAe;AACnE,WAAO,CAAC,gBAAgB,WAAA;;EAG1B,OAAO,gBACL,OACA,SACwB;AAExB,QACE,KAAK,0BAA0BA,SAAQ,UAAU,sBAEjD,OAAM,KAAK,OAAO,OAAO,OAAA;SACpB;AACL,YAAM,SAASA,SAAQ,2BAA2B,KAAA;AAClD,YAAM,CAAC,gBAAgB,WAAA,IACrB,KAAK,6CAA6C,OAAA;AACpD,YAAM,mBAAmB,MAAM,gBAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,eAAe,UACf,KAAK,UACL,EAAE,SAAS,KAAK,QAAA,CAAS;AAE3B,YAAM,QAAQ;QACZ,SAAS;QACT,mBAAmB,MAAM,iBAAiB,WAAA;QAC1C,YAAY;;AAEd,YAAM,cAAc,MAAM,kBAAkB,eAC1C,KAAK,OAAA,GACL,CAAC,OAAO,SAAA,CAAU,GAClB,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,OAAA;AAEjB,UAAI,aAAa,IAAI,gBAAgB,EACnC,MAAM,GAAA,CACP;AACD,UAAI;AACF,yBAAiB,SAAS,KAAK,sBAC7B,OAAO,SAAA,GACP,aACA,cAAc,CAAA,CAAA,GACb;AACD,cAAI,CAAC,WACH,cAAa;cAEb,cAAa,WAAW,OAAO,KAAA;AAEjC,cAAI,OAAO,MAAM,SAAS,SACxB,OAAM,MAAM;;eAGT,KAAK;AACZ,cAAM,QAAQ,KACX,eAAe,CAAA,GAAI,IAAA,CAAK,eACvB,YAAY,eAAe,GAAA,CAAI,CAChC;AAEH,cAAM;;AAER,YAAM,QAAQ,KACX,eAAe,CAAA,GAAI,IAAA,CAAK,eACvB,YAAY,aAAa,EACvB,aAAa,CAAC,CAAC,UAAA,CAAW,EAAC,CAC5B,CAAC,CACH;;;;;;;;;;;EAaP,MAAM,eACJ,cACA,SACA,WACoB;AACpB,UAAM,UAAoB,aAAa,IAAA,CAAK,gBAC1C,YAAY,SAAA,CAAU;AAExB,WAAO,KAAK,SAAS,SAAS,SAAS,SAAA;;;;;EAgBzC,iBAAiB,UAA2C;AAC1D,WAAO,CAAA;;EAGT,kBAAkB,WAAmC;AACnD,UAAM,aAA0B,CAAA;AAEhC,aAAS,IAAI,GAAG,IAAI,UAAU,YAAY,QAAQ,KAAK,GAAG;AACxD,YAAM,UAAU,UAAU,YAAY,CAAA;AAEtC,UAAI,MAAM,EACR,YAAW,KAAK;QACd,aAAa,CAAC,OAAA;QACd,WAAW,UAAU;OACtB;WACI;AACL,cAAM,YAAY,UAAU,YACxB;UAAE,GAAG,UAAU;UAAW,YAAY,CAAA;YACtC;AAEJ,mBAAW,KAAK;UACd,aAAa,CAAC,OAAA;UACd;SACD;;;AAIL,WAAO;;;EAIT,MAAM,kBACJ,SACA,eACA,gBACA,oBACoB;AACpB,QAAI;AACJ,QACE,uBAAuB,UACvB,mBAAmB,WAAW,QAAQ,OAEtC,eAAc;SACT;AACL,YAAM,mBAAmB,MAAM,gBAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,eAAe,UACf,KAAK,UACL,EAAE,SAAS,KAAK,QAAA,CAAS;AAE3B,YAAM,QAAQ;QACZ,SAAS;QACT,mBAAmB,MAAM,iBAAiB,aAAA;QAC1C,YAAY,QAAQ;;AAEtB,oBAAc,MAAM,kBAAkB,eACpC,KAAK,OAAA,GACL,SACA,eAAe,OACf,QACA,OACA,QACA,QACA,gBAAgB,OAAA;;AAMpB,UAAM,sBAAsB,CAAC,CAAC,cAAc,CAAA,EAAG,SAAS,KACtD,+BAAA;AAEF,QAAI;AACJ,QACE,uBACA,QAAQ,WAAW,KACnB,KAAK,0BAA0BA,SAAQ,UAAU,sBAEjD,KAAI;AACF,YAAM,SAAS,MAAM,KAAK,sBACxB,QAAQ,CAAA,GACR,eACA,cAAc,CAAA,CAAA;AAEhB,UAAI;AACJ,uBAAiB,SAAS,OACxB,KAAI,eAAe,OACjB,cAAa;UAEb,cAAa,OAAO,YAAY,KAAA;AAGpC,UAAI,eAAe,OACjB,OAAM,IAAI,MAAM,+CAAA;AAElB,eAAS;QAAE,aAAa,CAAC,CAAC,UAAA,CAAW;QAAG,WAAW,CAAA;;AACnD,YAAM,cAAc,CAAA,EAAG,aAAa,MAAA;aAC7B,GAAG;AACV,YAAM,cAAc,CAAA,EAAG,eAAe,CAAA;AACtC,YAAM;;SAEH;AACL,UAAI;AACF,iBAAS,MAAM,KAAK,UAAU,SAAS,eAAe,cAAc,CAAA,CAAA;eAC7D,KAAK;AACZ,cAAM,QAAQ,KACX,eAAe,CAAA,GAAI,IAAA,CAAK,eACvB,YAAY,eAAe,GAAA,CAAI,CAChC;AAEH,cAAM;;AAGR,YAAM,mBAAgC,KAAK,kBAAkB,MAAA;AAC7D,YAAM,QAAQ,KACX,eAAe,CAAA,GAAI,IAAA,CAAK,YAAY,MACnC,YAAY,aAAa,iBAAiB,CAAA,CAAA,CAAG,CAC9C;;AAGL,UAAM,SAAS,aAAa,IAAA,CAAK,YAAY,QAAQ,KAAA,KAAU;AAI/D,WAAO,eAAe,QAAQ,SAAS;MACrC,OAAO,SAAS,EAAE,OAAA,IAAW;MAC7B,cAAc;KACf;AACD,WAAO;;EAGT,MAAM,gBAAgB,EACpB,SACA,OAAAC,QACA,cACA,eACA,gBACA,MAAA,GAcA;AACA,UAAM,mBAAmB,MAAM,gBAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,eAAe,UACf,KAAK,UACL,EAAE,SAAS,KAAK,QAAA,CAAS;AAE3B,UAAM,QAAQ;MACZ,SAAS;MACT,mBAAmB,MAAM,iBAAiB,aAAA;MAC1C,YAAY,QAAQ;;AAEtB,UAAM,cAAc,MAAM,kBAAkB,eAC1C,KAAK,OAAA,GACL,SACA,OACA,QACA,OACA,QACA,QACA,gBAAgB,OAAA;AAIlB,UAAM,uBAAiC,CAAA;AAavC,UAAM,iBAZU,MAAM,QAAQ,WAC5B,QAAQ,IAAI,OAAO,QAAQ,UAAU;AACnC,YAAM,SAAS,MAAMA,OAAM,OAAO,QAAQ,YAAA;AAC1C,UAAI,UAAU,KACZ,sBAAqB,KAAK,KAAA;AAE5B,aAAO;MACP,GAMD,IAAA,CAAK,QAAQ,WAAW;MAAE;MAAQ,YAAY,cAAc,KAAA;MAAQ,EACpE,OAAA,CACE,EAAE,OAAA,MACA,OAAO,WAAW,eAAe,OAAO,SAAS,QAClD,OAAO,WAAW,UAAA;AAIxB,UAAM,cAA8B,CAAA;AACpC,UAAM,QAAQ,IACZ,cAAc,IAAI,OAAO,EAAE,QAAQ,eAAe,WAAA,GAAc,MAAM;AACpE,UAAI,cAAc,WAAW,aAAa;AACxC,cAAM,SAAS,cAAc;AAC7B,oBAAY,CAAA,IAAK,OAAO,IAAA,CAAKC,YAAW;AACtC,UAAAA,QAAO,iBAAiB;YACtB,GAAGA,QAAO;YACV,YAAY,CAAA;;AAEd,iBAAOA;;AAET,YAAI,OAAO,OACT,OAAM,YAAY,kBAAkB,OAAO,CAAA,EAAG,IAAA;AAEhD,eAAO,YAAY,aACjB,EACE,aAAa,CAAC,MAAA,EAAO,GAEvB,QACA,QACA,QACA,EACE,QAAQ,KAAA,CACT;aAEE;AAEL,cAAM,YAAY,eAChB,cAAc,QACd,QACA,QACA,QACA,EACE,QAAQ,KAAA,CACT;AAEH,eAAO,QAAQ,OAAO,cAAc,MAAA;;MAEtC;AAGJ,UAAM,SAAS;MACb;MACA;MACA,oBAAoB;;AAMtB,WAAO,eAAe,QAAQ,SAAS;MACrC,OAAO,cACH,EAAE,QAAQ,aAAa,IAAA,CAAK,YAAY,QAAQ,KAAA,EAAM,IACtD;MACJ,cAAc;KACf;AAED,WAAO;;;;;EAMT,MAAM,SACJ,SACA,SACA,WACoB;AACpB,QAAI,CAAC,MAAM,QAAQ,OAAA,EACjB,OAAM,IAAI,MAAM,iDAAA;AAGlB,QAAI;AACJ,QAAI,MAAM,QAAQ,OAAA,EAChB,iBAAgB,EAAE,MAAM,QAAA;QAExB,iBAAgB;AAGlB,UAAM,CAAC,gBAAgB,WAAA,IACrB,KAAK,6CAA6C,aAAA;AACpD,mBAAe,YAAY,eAAe,aAAa;AAEvD,QAAI,CAAC,KAAK,MACR,QAAO,KAAK,kBAAkB,SAAS,aAAa,cAAA;AAGtD,UAAM,EAAE,OAAAD,OAAA,IAAU;AAClB,UAAM,eAAe,KAAK,wCACxB,WAAA;AAEF,UAAM,EAAE,aAAa,sBAAsB,mBAAA,IACzC,MAAM,KAAK,gBAAgB;MACzB;MACA,OAAAA;MACA;MACA,eAAe;MACf,gBAAgB;MAChB,OAAO,eAAe;KACvB;AAEH,QAAI,YAAY,CAAA;AAChB,QAAI,qBAAqB,SAAS,GAAG;AACnC,YAAM,UAAU,MAAM,KAAK,kBACzB,qBAAqB,IAAA,CAAK,MAAM,QAAQ,CAAA,CAAA,GACxC,aACA,gBACA,uBAAuB,SACnB,qBAAqB,IAAA,CAAK,MAAM,qBAAqB,CAAA,CAAA,IACrD,MAAA;AAEN,YAAM,QAAQ,IACZ,QAAQ,YAAY,IAAI,OAAO,YAAY,UAAU;AACnD,cAAM,cAAc,qBAAqB,KAAA;AACzC,oBAAY,WAAA,IAAe;AAC3B,eAAOA,OAAM,OAAO,QAAQ,WAAA,GAAc,cAAc,UAAA;QACxD;AAEJ,kBAAY,QAAQ,aAAa,CAAA;;AAGnC,WAAO;MAAE;MAAa;;;;;;EAOxB,qBAA0C;AACxC,WAAO,CAAA;;EAQT,aAAqB;AACnB,WAAO;;;AAWX,IAAsB,MAAtB,cAEU,QAAqB;EAU7B,MAAM,UACJ,SACA,SACA,YACoB;AAQpB,WAAO,EAAE,aAP2B,MAAM,QAAQ,IAChD,QAAQ,IAAA,CAAK,QAAQ,gBACnB,KAAK,MAAM,QAAQ;MAAE,GAAG;MAAS;OAAe,UAAA,EAAY,KAAA,CACzD,SAAS,CAAC,EAAE,KAAA,CAAM,CAAC,CACrB,CACF,EACF;;;;;;AC1jBL,IAAa,aAAA,CAAiB,KAAU,cACtC,IAAI,OAAA,CAAQ,QAAQ,MAAM,UAAU;AAClC,QAAM,aAAa,KAAK,MAAM,QAAQ,SAAA;AAEtC,SAAO,UAAA,KADO,OAAO,UAAA,KAAe,CAAA,GACT,OAAO,CAAC,IAAA,CAAK;AACxC,SAAO;GACN,CAAA,CAAE;;;ACyDP,IAAaE,WAAb,cACU,QAEV;EACE,OAAO,UAAU;AACf,WAAO;;EAGT,IAAI,WAAW;AACb,WAAO,CAAC,GAAG,MAAM,UAAU,SAAA;;EAG7B,kBAAkB;EAElB,IAAI,aAAoD;AACtD,WAAO;MACL,cAAc;MACd,QAAQ;MACR,cAAc;;;EAIlB,IAAI,aAAqC;AACvC,WAAO;MACL,WAAW;MACX,cAAc;MACd,QAAQ;;;EAIZ;EAEA;EAEA;EAEA;EAEA;EAEA,IAAI;EAEJ;EAEA;EAEA,QAAQ;;EAGR;EAEA;EAEA,YAAY;EAEZ;EAEA;EAEA;EAEA;EAEA,YAAY;EAEZ;EAEA;EAEA;EAEU;EAEA;EAEV,YACE,QAIA;AACA,UAAM,UAAU,CAAA,CAAE;AAElB,SAAK,eACH,QAAQ,UACR,QAAQ,gBACR,uBAAuB,gBAAA;AACzB,SAAK,SAAS,KAAK;AAEnB,SAAK,eACH,QAAQ,eAAe,gBACvB,uBAAuB,qBAAA;AAEzB,SAAK,QAAQ,QAAQ,SAAS,QAAQ,aAAa,KAAK;AACxD,SACG,KAAK,OAAO,WAAW,eAAA,KACtB,KAAK,OAAO,WAAW,OAAA,KACvB,KAAK,OAAO,WAAW,IAAA,MACzB,CAAC,KAAK,OAAO,SAAS,WAAA,EAEtB,OAAM,IAAI,MACR;MACE,8BAA8B,KAAK,KAAA;MACnC;MACA;MACA;MACA;MACA;MACA;MACA,KAAK,IAAA,CAAK;AAGhB,SAAK,YAAY,KAAK;AACtB,SAAK,cAAc,QAAQ,eAAe,CAAA;AAC1C,SAAK,YAAY,QAAQ,aAAa,KAAK;AAC3C,SAAK,UAAU,QAAQ;AAEvB,SAAK,cAAc,QAAQ,eAAe,KAAK;AAC/C,SAAK,YAAY,QAAQ,aAAa,KAAK;AAC3C,SAAK,OAAO,QAAQ,QAAQ,KAAK;AACjC,SAAK,mBAAmB,QAAQ,oBAAoB,KAAK;AACzD,SAAK,kBAAkB,QAAQ,mBAAmB,KAAK;AACvD,SAAK,IAAI,QAAQ,KAAK,KAAK;AAC3B,SAAK,SAAS,QAAQ,UAAU,KAAK;AACrC,SAAK,YAAY,QAAQ;AACzB,SAAK,OAAO,QAAQ,iBAAiB,QAAQ;AAC7C,SAAK,gBAAgB,KAAK;AAC1B,SAAK,OAAO,QAAQ;AAEpB,SAAK,YAAY,QAAQ,aAAa;AAEtC,QAAI,KAAK,aAAa,KAAK,UAAU,KAAK,SAAS,EACjD,OAAM,IAAI,MAAM,uCAAA;AAGlB,SAAK,eAAe;MAClB,QAAQ,KAAK;MACb,cAAc,KAAK;MACnB,yBAAyB;MACzB,GAAG,QAAQ;;;;;;EAOf,iBACE,SACqD;AACrD,WAAO;MACL,OAAO,KAAK;MACZ,aAAa,KAAK;MAClB,YAAY,KAAK;MACjB,OAAO,KAAK;MACZ,mBAAmB,KAAK;MACxB,kBAAkB,KAAK;MACvB,GAAG,KAAK;MACR,SAAS,KAAK;MACd,YAAY,KAAK;MACjB,MAAM,SAAS,QAAQ,KAAK;MAC5B,MAAM,KAAK;MACX,QAAQ,KAAK;MACb,GAAG,KAAK;;;;EAKZ,qBAEkB;AAChB,WAAO;MACL,YAAY,KAAK;MACjB,GAAG,KAAK,iBAAA;MACR,GAAG,KAAK;;;;;;EAOZ,oBAEkB;AAChB,WAAO,KAAK,mBAAA;;;;;;;;;;;;;;;;;;EAmBd,MAAM,UACJ,SACA,SACA,YACoB;AACpB,UAAM,aAAa,WAAW,SAAS,KAAK,SAAA;AAC5C,UAAM,UAA2C,CAAA;AACjD,UAAM,aAAyB,CAAA;AAE/B,UAAM,SAAS,KAAK,iBAAiB,OAAA;AAErC,QAAI,OAAO,eAAe,IAAI;AAC5B,UAAI,QAAQ,WAAW,EACrB,OAAM,IAAI,MACR,wDAAA;AAGJ,aAAO,aAAa,MAAM,mBAAmB;QAC3C,QAAQ,QAAQ,CAAA;QAEhB,WAAW,KAAK;OACjB;;AAGH,aAAS,IAAI,GAAG,IAAI,WAAW,QAAQ,KAAK,GAAG;AAC7C,YAAM,OAAO,OAAO,SAChB,OAAO,YAAY;AACjB,cAAMC,WAA2C,CAAA;AACjD,YAAI;AACJ,cAAM,SAAS,MAAM,KAAK,oBACxB;UACE,GAAG;UACH,QAAQ;UACR,QAAQ,WAAW,CAAA;WAErB,OAAA;AAEF,yBAAiB,WAAW,QAAQ;AAElC,cAAI,CAAC,SACH,YAAW;YACT,IAAI,QAAQ;YACZ,QAAQ,QAAQ;YAChB,SAAS,QAAQ;YACjB,OAAO,QAAQ;;AAKnB,qBAAW,QAAQ,QAAQ,SAAS;AAClC,gBAAI,CAACA,SAAQ,KAAK,KAAA,EAChB,CAAAA,SAAQ,KAAK,KAAA,IAAS;iBACjB;AACL,oBAAM,SAASA,SAAQ,KAAK,KAAA;AAC5B,qBAAO,QAAQ,KAAK;AACpB,qBAAO,gBAAgB,KAAK;AAC5B,qBAAO,WAAW,KAAK;;AAGpB,wBAAY,kBAAkB,KAAK,MAAM;cAC5C,QAAQ,KAAK,MAAM,KAAK,QAAQ,KAAK,CAAA;cACrC,YAAY,KAAK,QAAQ,KAAK;aAC/B;;;AAGL,YAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM,YAAA;AAElB,eAAO;UAAE,GAAG;UAAU,SAAAA;;aAExB,MAAM,KAAK,oBACT;QACE,GAAG;QACH,QAAQ;QACR,QAAQ,WAAW,CAAA;SAErB;QACE,QAAQ,QAAQ;QAChB,GAAG,QAAQ;OACZ;AAGP,cAAQ,KAAK,GAAG,KAAK,OAAA;AACrB,YAAM,EACJ,mBAAmB,kBACnB,eAAe,cACf,cAAc,YAAA,IACZ,KAAK,QACL,KAAK,QACL;QACE,mBAAmB;QACnB,eAAe;QACf,cAAc;;AAGpB,UAAI,iBACF,YAAW,oBACR,WAAW,oBAAoB,KAAK;AAGzC,UAAI,aACF,YAAW,gBAAgB,WAAW,gBAAgB,KAAK;AAG7D,UAAI,YACF,YAAW,eAAe,WAAW,eAAe,KAAK;;AAa7D,WAAO;MACL,aAVkB,WAAW,SAAS,KAAK,CAAA,EAAG,IAAA,CAAK,kBACnD,cAAc,IAAA,CAAK,YAAY;QAC7B,MAAM,OAAO,QAAQ;QACrB,gBAAgB;UACd,cAAc,OAAO;UACrB,UAAU,OAAO;;QAEpB,CAAE;MAIH,WAAW,EAAE,WAAA;;;EAKjB,OAAO,sBACL,OACA,SACA,YACiC;AACjC,UAAM,SAAS;MACb,GAAG,KAAK,iBAAiB,OAAA;MACzB,QAAQ;MACR,QAAQ;;AAEV,UAAM,SAAS,MAAM,KAAK,oBAAoB,QAAQ,OAAA;AACtD,qBAAiB,QAAQ,QAAQ;AAC/B,YAAM,SAAS,MAAM,QAAQ,CAAA;AAC7B,UAAI,CAAC,OACH;AAEF,YAAM,QAAQ,IAAI,gBAAgB;QAChC,MAAM,OAAO;QACb,gBAAgB,EACd,cAAc,OAAO,cAAA;OAExB;AACD,YAAM;AAED,kBAAY,kBAAkB,MAAM,QAAQ,EAAA;;AAEnD,QAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM,YAAA;;EAoBpB,MAAM,oBACJ,SAGA,SAGA;AACA,UAAM,iBAAiB,KAAK,kBAAkB,OAAA;AAC9C,WAAO,KAAK,OAAO,KAAK,YAAY;AAClC,UAAI;AAKF,eAJY,MAAM,KAAK,OAAO,YAAY,OACxC,SACA,cAAA;eAGK,GAAG;AAEV,cADc,sBAAsB,CAAA;;;;;;;;;;EAYhC,kBACR,SAC0B;AAC1B,QAAI,CAAC,KAAK,QAAQ;AAKhB,YAAM,WAAW,YAJkC,EACjD,SAAS,KAAK,aAAa,QAAA,CAC5B;AAID,YAAM,SAAS;QACb,GAAG,KAAK;QACR,SAAS;QACT,SAAS,KAAK;QACd,YAAY;;AAGd,UAAI,CAAC,OAAO,QACV,QAAO,OAAO;AAGhB,aAAO,iBAAiB,wBAAwB,OAAO,cAAA;AAEvD,WAAK,SAAS,IAAIC,OAAa,MAAA;;AAMjC,WAJuB;MACrB,GAAG,KAAK;MACR,GAAG;;;EAKP,WAAW;AACT,WAAO;;;;;ACpeX,IAAaC,gBAAb,cAAiCC,SAAO;EACtC;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA,IAAI,aAAqC;AACvC,WAAO;MACL,GAAG,MAAM;MACT,cAAc;MACd,kBAAkB;MAClB,gBAAgB;MAChB,gBAAgB;MAChB,qBAAqB;MACrB,uBAAuB;MACvB,qBAAqB;MACrB,8BAA8B;;;EAIlC,IAAI,aAAoD;AACtD,WAAO;MACL,GAAG,MAAM;MACT,mBAAmB;;;EAIvB,YACE,QASA;AACA,UAAM,MAAA;AAEN,SAAK,gCACF,QAAQ,2CACP,QAAQ,kCACT,uBAAuB,8CAAA,KACtB,uBAAuB,kCAAA;AAE3B,SAAK,oBACH,QAAQ,sBACP,OAAO,QAAQ,iBAAiB,WAC7B,QAAQ,eACR,YACH,OAAO,QAAQ,WAAW,WAAW,QAAQ,SAAS,WACvD,uBAAuB,sBAAA;AAEzB,SAAK,6BACH,QAAQ,8BACR,uBAAuB,gCAAA;AAEzB,SAAK,wBACH,QAAQ,yBACR,QAAQ,oBACR,uBAAuB,0BAAA;AAEzB,SAAK,sBACH,QAAQ,uBACR,uBAAuB,wBAAA;AAEzB,SAAK,sBACH,QAAQ,uBACR,uBAAuB,uBAAA;AAEzB,SAAK,uBAAuB,QAAQ;AAEpC,QAAI,CAAC,KAAK,qBAAqB,CAAC,KAAK,UAAU,CAAC,KAAK,qBACnD,OAAM,IAAI,MAAM,kDAAA;;EAIV,kBACR,SAC0B;AAC1B,QAAI,CAAC,KAAK,QAAQ;AAChB,YAAM,uBAA6C;QACjD,8BAA8B,KAAK;QACnC,4BAA4B,KAAK;QACjC,mBAAmB,KAAK;QACxB,qBAAqB,KAAK;QAC1B,sBAAsB,KAAK;QAC3B,SAAS,KAAK,aAAa;;AAG7B,YAAM,WAAW,YAAY,oBAAA;AAE7B,YAAM,EAAE,QAAQ,gBAAgB,GAAG,iBAAA,IAAqB,KAAK;AAC7D,YAAM,SAA8D;QAClE,GAAG;QACH,SAAS;QACT,SAAS,KAAK;QACd,YAAY;;AAGd,UAAI,CAAC,KAAK,qBACR,QAAO,SAAS,qBAAqB;AAGvC,UAAI,CAAC,OAAO,QACV,QAAO,OAAO;AAGhB,aAAO,iBAAiB,wBACtB,OAAO,gBACP,MACA,OAAA;AAGF,WAAK,SAAS,IAAIC,YAAkB;QAClC,YAAY,KAAK;QACjB,sBAAsB,KAAK;QAC3B,GAAG;OACJ;;AAGH,UAAM,iBAAiB;MACrB,GAAG,KAAK;MACR,GAAG;;AAEL,QAAI,KAAK,mBAAmB;AAC1B,qBAAe,UAAU;QACvB,WAAW,KAAK;QAChB,GAAG,eAAe;;AAEpB,qBAAe,QAAQ;QACrB,eAAe,KAAK;QACpB,GAAG,eAAe;;;AAGtB,WAAO;;EAIT,SAAc;AACZ,UAAMC,QAAO,MAAM,OAAA;AAEnB,aAAS,SAAS,KAA8C;AAC9D,aAAO,OAAO,QAAQ,YAAY,OAAO;;AAG3C,QAAI,SAASA,KAAA,KAAS,SAASA,MAAK,MAAA,GAAS;AAC3C,aAAOA,MAAK,OAAO;AACnB,aAAOA,MAAK,OAAO;AACnB,aAAOA,MAAK,OAAO;AACnB,aAAOA,MAAK,OAAO;AACnB,aAAOA,MAAK,OAAO;;AAGrB,WAAOA;;;;;;ACpJX,IAAsBC,cAAtB,MAE0C;;;;;EAKxC;EAEA,YAAY,QAA0B;AACpC,SAAK,SAAS,IAAI,YAAY,UAAU,CAAA,CAAE;;;;;ACsC9C,IAAa,mBAAb,cACUC,YAEV;EACE,QAAQ;;EAGR;EAEA,YAAY;EAGZ,gBAAgB;;;;;EAMhB;EAEA;EAEA;EAEA;EAEU;EAEA;EAEA;EAEV,YACE,QAWA;AACA,UAAM,qBAAqB;MAAE,gBAAgB;MAAG,GAAG;;AAEnD,UAAM,kBAAA;AAEN,UAAM,SACJ,oBAAoB,UACpB,oBAAoB,gBACpB,uBAAuB,gBAAA;AAEzB,SAAK,eACH,oBAAoB,eAAe,gBACnC,uBAAuB,qBAAA;AAEzB,SAAK,QACH,oBAAoB,SAAS,oBAAoB,aAAa,KAAK;AACrE,SAAK,YAAY,KAAK;AACtB,SAAK,YAAY,oBAAoB,aAAa,KAAK;AACvD,SAAK,gBACH,oBAAoB,iBAAiB,KAAK;AAC5C,SAAK,UAAU,oBAAoB;AACnC,SAAK,aAAa,oBAAoB;AACtC,SAAK,iBAAiB,oBAAoB;AAE1C,SAAK,eAAe;MAClB;MACA,cAAc,KAAK;MACnB,yBAAyB;MACzB,GAAG,QAAQ;;;;;;;;;;EAWf,MAAM,eAAe,OAAqC;AACxD,UAAM,UAAU,WACd,KAAK,gBAAgB,MAAM,IAAA,CAAK,MAAM,EAAE,QAAQ,OAAO,GAAA,CAAI,IAAI,OAC/D,KAAK,SAAA;AAGP,UAAM,gBAAgB,QAAQ,IAAA,CAAK,UAAU;AAC3C,YAAM,SAA6C;QACjD,OAAO,KAAK;QACZ,OAAO;;AAET,UAAI,KAAK,WACP,QAAO,aAAa,KAAK;AAE3B,UAAI,KAAK,eACP,QAAO,kBAAkB,KAAK;AAEhC,aAAO,KAAK,mBAAmB,MAAA;;AAEjC,UAAM,iBAAiB,MAAM,QAAQ,IAAI,aAAA;AAEzC,UAAM,aAAwB,CAAA;AAC9B,aAAS,IAAI,GAAG,IAAI,eAAe,QAAQ,KAAK,GAAG;AACjD,YAAM,QAAQ,QAAQ,CAAA;AACtB,YAAM,EAAE,MAAM,cAAA,IAAkB,eAAe,CAAA;AAC/C,eAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK,EACrC,YAAW,KAAK,cAAc,CAAA,EAAG,SAAA;;AAGrC,WAAO;;;;;;;;EAST,MAAM,WAAW,MAAgC;AAC/C,UAAM,SAA6C;MACjD,OAAO,KAAK;MACZ,OAAO,KAAK,gBAAgB,KAAK,QAAQ,OAAO,GAAA,IAAO;;AAEzD,QAAI,KAAK,WACP,QAAO,aAAa,KAAK;AAE3B,QAAI,KAAK,eACP,QAAO,kBAAkB,KAAK;AAEhC,UAAM,EAAE,KAAA,IAAS,MAAM,KAAK,mBAAmB,MAAA;AAC/C,WAAO,KAAK,CAAA,EAAG;;;;;;;;;EAUjB,MAAgB,mBACd,SACA;AACA,QAAI,CAAC,KAAK,QAAQ;AAKhB,YAAM,WAAW,YAJkC,EACjD,SAAS,KAAK,aAAa,QAAA,CAC5B;AAID,YAAM,SAAS;QACb,GAAG,KAAK;QACR,SAAS;QACT,SAAS,KAAK;QACd,YAAY;;AAGd,UAAI,CAAC,OAAO,QACV,QAAO,OAAO;AAGhB,aAAO,iBAAiB,wBAAwB,OAAO,cAAA;AAEvD,WAAK,SAAS,IAAIC,OAAa,MAAA;;AAEjC,UAAM,iBAAiB,CAAA;AAEvB,WAAO,KAAK,OAAO,KAAK,YAAY;AAClC,UAAI;AAKF,eAJY,MAAM,KAAK,OAAO,WAAW,OACvC,SACA,cAAA;eAGK,GAAG;AAEV,cADc,sBAAsB,CAAA;;;;;;;AClP5C,IAAa,wBAAb,cAA2C,iBAAiB;EAC1D;EAEA;EAEA;EAEA;EAEA;EAEA;EAEA,YACE,QASA;AACA,UAAM,MAAA;AACN,SAAK,YAAY,QAAQ,aAAa;AACtC,SAAK,oBACH,QAAQ,sBACP,OAAO,QAAQ,WAAW,WAAW,QAAQ,SAAS,WACvD,uBAAuB,sBAAA;AAEzB,SAAK,wBACH,QAAQ,yBACR,QAAQ,oBACR,uBAAuB,0BAAA;AAEzB,SAAK,sBACH,QAAQ,uBACR,uBAAuB,wBAAA;AAEzB,SAAK,6BACH,QAAQ,8BACR,uBAAuB,gCAAA;AAEzB,SAAK,gCACF,QAAQ,0CACP,QAAQ,kCACT,uBAAuB,6CAAA,KACtB,uBAAuB,kCAAA;AAE3B,SAAK,uBAAuB,QAAQ;;EAGtC,MAAgB,mBACd,SACA;AACA,QAAI,CAAC,KAAK,QAAQ;AAChB,YAAM,uBAA6C;QACjD,8BAA8B,KAAK;QACnC,4BAA4B,KAAK;QACjC,mBAAmB,KAAK;QACxB,qBAAqB,KAAK;QAC1B,sBAAsB,KAAK;QAC3B,SAAS,KAAK,aAAa;;AAG7B,YAAM,WAAW,YAAY,oBAAA;AAE7B,YAAM,EAAE,QAAQ,gBAAgB,GAAG,iBAAA,IAAqB,KAAK;AAC7D,YAAM,SAA8D;QAClE,GAAG;QACH,SAAS;QACT,SAAS,KAAK;QACd,YAAY;;AAGd,UAAI,CAAC,KAAK,qBACR,QAAO,SAAS,qBAAqB;AAGvC,UAAI,CAAC,OAAO,QACV,QAAO,OAAO;AAGhB,aAAO,iBAAiB,wBACtB,OAAO,gBACP,MACA,OAAA;AAGF,WAAK,SAAS,IAAIC,YAAkB;QAClC,YAAY,KAAK;QACjB,sBAAsB,KAAK;QAC3B,YAAY,KAAK;QACjB,GAAG;OACJ;;AAEH,UAAM,iBAA2C,CAAA;AACjD,QAAI,KAAK,mBAAmB;AAC1B,qBAAe,UAAU;QACvB,WAAW,KAAK;QAChB,GAAG,eAAe;;AAEpB,qBAAe,QAAQ;QACrB,eAAe,KAAK;QACpB,GAAG,eAAe;;;AAGtB,WAAO,KAAK,OAAO,KAAK,YAAY;AAClC,UAAI;AAKF,eAJY,MAAM,KAAK,OAAO,WAAW,OACvC,SACA,cAAA;eAGK,GAAG;AAEV,cADc,sBAAsB,CAAA;;;;;;;;;;;;;;;;;;;;ACxC5C,IAAsB,iBAAtB,cAMU,cAKV;;;;;;;EAaE;;;;;;;EAQA,eAAe;EAEf,uBAAuB;EAEvB,IAAI,eAAe;AACjB,WAAO,CAAC,aAAa,OAAA;;;;;;;;;;;EAYvB,iBAAkC;;;;EAKlC;EAEA,YAAY,QAAqB;AAC/B,UAAM,UAAU,CAAA,CAAE;AAElB,SAAK,uBACH,QAAQ,wBAAwB,KAAK;AACvC,SAAK,iBAAiB,QAAQ,kBAAkB,KAAK;AACrD,SAAK,gBAAgB,QAAQ,iBAAiB,KAAK;AACnD,SAAK,WAAW,QAAQ,YAAY,KAAK;AACzC,SAAK,SAAS,QAAQ,UAAU,KAAK;;;;;;;;EAevC,MAAM,OAIJ,OACAC,SACuD;AACvD,QAAI;AAKJ,QAAI,iBAAqC,aACvC,aAAa,KAAK,eAAeA,OAAA,CAAO;AAE1C,QAAI,YAAY,KAAA,GAAQ;AACtB,kBAAY,MAAM;AAIlB,uBAAiB;QACf,GAAG;QACH,UAAU;;UAGZ,aAAY;AAMd,WAAO,KAAK,KAAK,WAAW,cAAA;;;;;;;;;;;;;EAgB9B,MAAM,KAIJ,KACA,WAEA,MACqD;AAGrD,UAAM,qBAAqB,YAAY,GAAA,IAAO,IAAI,OAAO;AAEzD,QAAI;AACJ,QAAI,mBAAmB,KAAK,MAAA,EAC1B,KAAI;AAEF,eAAS,MAAM,kBACb,KAAK,QACL,kBAAA;aAEK,GAAG;AACV,UAAI,UAAU;AACd,UAAI,KAAK,qBACP,WAAU,GAAG,OAAA;WAAsB,EAAY,OAAA;AAEjD,UAAI,kBAAkB,CAAA,EACpB,WAAU,GAAG,OAAA;;EAAcC,kBAAG,cAAc,CAAA,CAAc;AAG5D,YAAM,IAAI,0BAA0B,SAAS,KAAK,UAAU,GAAA,CAAI;;SAE7D;AACL,YAAMC,UAAS,SACb,oBACA,KAAK,MAAA;AAEP,UAAI,CAACA,QAAO,OAAO;AACjB,YAAI,UAAU;AACd,YAAI,KAAK,qBACP,WAAU,GAAG,OAAA;WAAqBA,QAAO,OACtC,IAAA,CAAK,MAAM,GAAG,EAAE,eAAA,KAAoB,EAAE,KAAA,EAAA,EACtC,KAAK,IAAA,CAAK;AAGf,cAAM,IAAI,0BAA0B,SAAS,KAAK,UAAU,GAAA,CAAI;;AAIlE,eAAS;;AAGX,UAAMF,UAAS,uBAAuB,SAAA;AACtC,UAAM,mBAAmB,gBAAgB,UACvCA,QAAO,WACP,KAAK,WACLA,QAAO,QAAQ,MACf,KAAK,MACLA,QAAO,UACP,KAAK,UACL,EAAE,SAAS,KAAK,QAAA,CAAS;AAG3B,QAAI;AAEJ,QAAI,YAAY,GAAA,EACd,cAAa,IAAI;AAGnB,QAAI,CAAC,cAAc,qBAAqBA,OAAA,EACtC,cAAaA,QAAO,SAAS;AAG/B,UAAM,aAAa,MAAM,kBAAkB,gBACzC,KAAK,OAAA,GAEL,OAAO,QAAQ,WAAW,MAAM,KAAK,UAAU,GAAA,GAC/CA,QAAO,OACP,QACA,QACA,QACAA,QAAO,SACP,UAAA;AAEF,WAAOA,QAAO;AAEd,QAAI;AACJ,QAAI;AACF,YAAM,MAAM,MAAM,KAAK,MAAM,QAAQ,YAAYA,OAAA;AACjD,eAAS,iBAAiB,GAAA,IACtB,MAAM,sBAAsB,KAAK,OAAO,UAAU;AAChD,YAAI;AACF,gBAAM,YAAY,gBAAgB,KAAA;iBAC3B,aAAa;AACpB,gBAAM,YAAY,gBAAgB,WAAA;;WAGtC;aACG,GAAG;AACV,YAAM,YAAY,gBAAgB,CAAA;AAClC,YAAM;;AAGR,QAAI;AACJ,QAAI;AACJ,QAAI,KAAK,mBAAmB,uBAC1B,KAAI,MAAM,QAAQ,MAAA,KAAW,OAAO,WAAW,EAC7C,EAAC,SAAS,QAAA,IAAY;QAEtB,OAAM,IAAI,MACR;UAA+F,KAAK,UAClG,MAAA,CACD,EAAA;QAIL,WAAU;AAGZ,UAAM,kBAAkB,kBAA+B;MACrD;MACA;MACA;MACA,MAAM,KAAK;MACX,UAAU,KAAK;KAChB;AACD,UAAM,YAAY,cAAc,eAAA;AAChC,WAAO;;;AAOX,IAAsB,OAAtB,cACU,eAYV;EACE,SAAS,iBACN,OAAO,EAAE,OAAO,iBAAE,OAAA,EAAS,SAAA,EAAU,CAAE,EACvC,UAAA,CAAW,QAAQ,IAAI,KAAA;EAE1B,YAAY,QAAqB;AAC/B,UAAM,MAAA;;;;;;;;;;;EAaR,KAIE,KACA,WACkE;AAGlE,UAAM,gBACJ,OAAO,QAAQ,YAAY,OAAO,OAAO,EAAE,OAAO,IAAA,IAAQ;AAG5D,WAAO,MAAM,KAAK,eAAe,SAAA;;;AAOrC,IAAa,cAAb,cAEU,KAAkB;EAC1B,OAAO,UAAU;AACf,WAAO;;EAGT;EAEA;EAEA;EAEA,YAAY,QAAuC;AACjD,UAAM,MAAA;AACN,SAAK,OAAO,OAAO;AACnB,SAAK,cAAc,OAAO;AAC1B,SAAK,OAAO,OAAO;AACnB,SAAK,eAAe,OAAO,gBAAgB,KAAK;;;;;EAMlD,MAAM,KAIJ,KACA,WACkE;AAClE,UAAMA,UAAS,uBAAuB,SAAA;AACtC,QAAIA,QAAO,YAAY,OACrB,CAAAA,QAAO,UAAU,KAAK;AAIxB,WAAO,MAAM,KAAoB,KAAKA,OAAA;;;EAIxC,MACE,OACA,YACA,cAC6D;AAC7D,WAAO,KAAK,KAAK,OAAO,YAAY,YAAA;;;AAmBxC,IAAa,wBAAb,cAMU,eAAkE;EAC1E,OAAO,UAAU;AACf,WAAO;;EAKT;EAEA;EAEA;EAEA,YACE,QAGA;AACA,UAAM,MAAA;AACN,SAAK,OAAO,OAAO;AACnB,SAAK,cAAc,OAAO;AAC1B,SAAK,OAAO,OAAO;AACnB,SAAK,eAAe,OAAO,gBAAgB,KAAK;AAChD,SAAK,SAAS,OAAO;;;;;EAOvB,MAAM,KAIJ,KACA,WAEA,MACkE;AAClE,UAAMA,UAAS,uBAAuB,SAAA;AACtC,QAAIA,QAAO,YAAY,OACrB,CAAAA,QAAO,UAAU,KAAK;AAKxB,WAAO,MAAM,KAAoB,KAAKA,SAAmB,IAAA;;EAGjD,MACR,KAGA,YACA,cAC6D;AAC7D,WAAO,KAAK,KAAK,KAAK,YAAY,YAAA;;;AAStC,IAAsB,cAAtB,MAAkC;EAGhC,WAAsC;AACpC,WAAO,KAAK;;;AA6PhB,SAAgB,KAUd,MAIA,QAS2B;AAC3B,QAAM,uBAAuB,wBAAwB,OAAO,MAAA;AAC5D,QAAM,qBAAqB,qBAAqB,OAAO,MAAA;AAGvD,MAAI,CAAC,OAAO,UAAU,wBAAwB,mBAC5C,QAAO,IAAI,YAAyB;IAClC,GAAG;IACH,aACE,OAAO,eACN,OAAO,QAAiD,eACzD,GAAG,OAAO,IAAA;IACZ,MAAM,OAAO,OAAO,YAAYA,YAAW;AACzC,aAAO,IAAI,QAAA,CAAsB,SAAS,WAAW;AACnD,cAAM,cAAc,YAAYA,SAAQ,EACtC,WAAW,YAAY,SAAA,EAAU,CAClC;AAEI,2CAAmC,cACtC,uBAAuB,WAAA,GACvB,YAAY;AACV,cAAI;AAEF,oBAAQ,KAAK,OAAc,WAAA,CAAmB;mBACvC,GAAG;AACV,mBAAO,CAAA;;;;;GAMlB;AAGH,QAAM,SAAS,OAAO;AAEtB,QAAM,cACJ,OAAO,eACN,OAAO,OAAoC,eAC5C,GAAG,OAAO,IAAA;AAEZ,SAAO,IAAI,sBAMT;IACA,GAAG;IACH;IACA;IACA,MAAM,OAAO,OAAO,YAAYA,YAAW;AACzC,aAAO,IAAI,QAAA,CAAsB,SAAS,WAAW;AACnD,YAAI;AACJ,cAAM,UAAA,MAAgB;AACpB,cAAIA,SAAQ,UAAU,SACpB,CAAAA,QAAO,OAAO,oBAAoB,SAAS,QAAA;;AAI/C,YAAIA,SAAQ,QAAQ;AAClB,qBAAA,MAAiB;AACf,oBAAA;AACA,mBAAO,oBAAoBA,QAAO,MAAA,CAAO;;AAE3C,UAAAA,QAAO,OAAO,iBAAiB,SAAS,UAAU,EAAE,MAAM,KAAA,CAAM;;AAGlE,cAAM,cAAc,YAAYA,SAAQ,EACtC,WAAW,YAAY,SAAA,EAAU,CAClC;AAEI,2CAAmC,cACtC,uBAAuB,WAAA,GACvB,YAAY;AACV,cAAI;AAEF,kBAAM,SAAS,MAAM,KAAK,OAAc,WAAA;AAMxC,gBAAIA,SAAQ,QAAQ,SAAS;AAC3B,sBAAA;AACA;;AAGF,oBAAA;AACA,oBAAQ,MAAA;mBACD,GAAG;AACV,oBAAA;AACA,mBAAO,CAAA;;;;;GAMlB;;AASH,SAAS,kBAAkD,QAMjC;AACxB,QAAM,EAAE,SAAS,UAAU,YAAY,SAAA,IAAa;AACpD,MAAI,cAAc,CAAC,mBAAmB,OAAA,EACpC,KACE,OAAO,YAAY,YAClB,MAAM,QAAQ,OAAA,KACb,QAAQ,MAAA,CAAO,SAAS,OAAO,SAAS,QAAA,EAE1C,QAAO,IAAI,YAAY;IACrB,QAAQ;IACR;IACA;IACA,cAAc;IACd,MAAM,OAAO;IACb;GACD;MAED,QAAO,IAAI,YAAY;IACrB,QAAQ;IACR,SAAS,WAAW,OAAA;IACpB;IACA,cAAc;IACd,MAAM,OAAO;IACb;GACD;MAGH,QAAO;;AAIX,SAAS,WAAW,SAA0B;AAC5C,MAAI;AACF,WAAO,KAAK,UAAU,OAAA,KAAY;WAC3B,OAAO;AACd,WAAO,GAAG,OAAA;;;;;ACr2Bd,IAAa,kBAAb,cAAqC,KAAK;EACxC,OAAO,UAAU;AACf,WAAO;;EAGT,OAAO;EAEP,cACE;EAEQ;EAEV,OAAgB,WAAW;EAEnB,QAAQ;EAER,QAA6B;EAE7B,UAA6B;EAE7B,IAAI;EAEJ,OAKU;EAEV,sBAA0C;EAE1C;EAER,YAAY,QAAgC;AAE1C,QACE,QAAQ,mBAAmB,UAC3B,CAAC,OAAO,UAAA,EAAY,SAAS,OAAO,cAAA,GACpC;AAEA,aAAO,sBAAsB,OAAO;AACpC,aAAO,iBAAiB;;AAE1B,UAAM,MAAA;AAeN,SAAK,SAAS,IAAIG,OANG;MACnB,QARA,QAAQ,UACR,QAAQ,gBACR,uBAAuB,gBAAA;MAOvB,cAJA,QAAQ,gBAAgB,uBAAuB,qBAAA;MAK/C,yBAAyB;MACzB,SAAS,QAAQ;KAClB;AAED,SAAK,QAAQ,QAAQ,SAAS,QAAQ,aAAa,KAAK;AACxD,SAAK,QAAQ,QAAQ,SAAS,KAAK;AACnC,SAAK,UAAU,QAAQ,WAAW,KAAK;AACvC,SAAK,IAAI,QAAQ,KAAK,KAAK;AAC3B,SAAK,OAAO,QAAQ,QAAQ,KAAK;AACjC,SAAK,sBACH,QAAQ,uBAAuB,KAAK;AACtC,SAAK,OAAO,QAAQ;;;;;;;;;;;;EAad,6BACN,UAC0B;AAC1B,QAAI,KAAK,wBAAwB,MAC/B,QAAO,SAAS,QAAA,CAAS,QAAQ;AAiB/B,aAfE,IAAI,MACA,QAAA,CAAS,SAAS;AAClB,YAAI,CAAC,KAAK,IAAK,QAAO,CAAA;AACtB,eAAO;UACL,MAAM;UACN,WAAW,KAAK;;SAGnB,OAAA,CACE,SACC,SAAS,UACT,KAAK,SAAS,eACd,OAAO,KAAK,cAAc,YAC1B,KAAK,cAAc,MAAA,KAClB,CAAA;;QAIX,QAAO,SAAS,QAAA,CAAS,QAAQ;AAqB/B,aAnBE,IAAI,MACA,QAAA,CAAS,SAAS;AAClB,YAAI,CAAC,KAAK,SAAU,QAAO,CAAA;AAC3B,eAAO;UACL,MAAM;UACN,WAAW,EACT,KAAK,KAAK,SAAA;;SAIf,OAAA,CACE,SACC,SAAS,UACT,KAAK,SAAS,eACd,OAAO,KAAK,cAAc,YAC1B,SAAS,KAAK,aACd,OAAO,KAAK,UAAU,QAAQ,YAC9B,KAAK,UAAU,QAAQ,MAAA,KACtB,CAAA;;;;EAOf,MAAM,MAAM,OAA0D;AACpE,UAAM,sBAAsB;MAC1B,OAAO,KAAK;MACZ,QAAQ;MACR,GAAG;MACH,MAAM,KAAK;MACX,iBAAiB,KAAK;MACtB,OAAO,KAAK;MACZ,SAAS,KAAK;MACd,MAAM,KAAK;;AAGb,QAAI,KAAK,IAAI,GAAG;AACd,YAAM,UAAU,MAAM,QAAQ,IAC5B,MAAM,KAAK,EAAE,QAAQ,KAAK,EAAA,CAAG,EAAE,IAAA,MAC7B,KAAK,OAAO,OAAO,SAAS,mBAAA,CAAoB,CACjD;AAGH,aAAO,KAAK,6BAA6B,OAAA;;AAG3C,UAAM,WAAW,MAAM,KAAK,OAAO,OAAO,SAAS,mBAAA;AAEnD,QAAI,OAAO;AACX,QAAI,KAAK,wBAAwB,MAC/B,EAAC,IAAA,IACC,SAAS,MACL,IAAA,CAAK,SAAS,KAAK,GAAA,EACpB,OAAA,CAAQC,SAAuBA,SAAQ,WAAA,KAAgB,CAAA;QAE5D,EAAC,IAAA,IACC,SAAS,MACL,IAAA,CAAK,SAAS,KAAK,QAAA,EACpB,OAAA,CAAQ,aAAiC,aAAa,WAAA,KACzD,CAAA;AAEJ,WAAO;;;;;ACzIX,SAAgB,UAAU,SAAwC;AAChE,SAAO;IACL,MAAM;IACN,SAAS,SAAS,SAAS,iBACvB,EAAE,iBAAiB,QAAQ,QAAQ,eAAA,IACnC;IACJ,eAAe,SAAS;IACxB,qBAAqB,SAAS;;;;;ACXlC,SAAS,kBACP,QAC+C;AAC/C,SAAO;IACL,YAAY,OAAO;IACnB,WAAW,OAAO;;;AAOtB,SAAS,oBACP,cAC2E;AAC3E,MAAI,CAAC,aAAc,QAAO;AAC1B,MAAI,MAAM,QAAQ,YAAA,EAAe,QAAO;AACxC,SAAO,kBAAkB,YAAA;;AAM3B,SAAS,uBACP,iBAKY;AACZ,MAAI,CAAC,gBAAiB,QAAO;AAC7B,MAAI,OAAO,oBAAoB,SAAU,QAAO;AAChD,SAAO;IACL,QAAQ,gBAAgB,SACpB,kBAAkB,gBAAgB,MAAA,IAClC;IACJ,OAAO,gBAAgB,QACnB,kBAAkB,gBAAgB,KAAA,IAClC;;;AAsER,SAAgB,IACd,SACY;AACZ,QAAM,aAAsB;IAC1B,MAAM;IACN,cAAc,QAAQ;IACtB,eAAe,oBAAoB,QAAQ,YAAA;IAC3C,eAAe,QAAQ;IACvB,SAAS,QAAQ;IACjB,kBAAkB,uBAAuB,QAAQ,eAAA;IACjD,oBAAoB,QAAQ;;AAG9B,MAAI,eAAe,QACjB,QAAO;IACL,GAAG;IACH,YAAY,QAAQ;;AAIxB,SAAO;IACL,GAAG;IACH,cAAc,QAAQ;;;;;ACxM1B,SAAS,iBACP,WAGsE;AAEtE,MAAI,OAAO,cAAc,SACvB,QAAO;AAIT,SAAO;IACL,MAAM;IACN,UAAU,WAAW;IACrB,cAAc,WAAW;;;AA4E7B,SAAgB,gBAAgB,SAA8C;AAC5E,SAAO;IACL,MAAM;IACN,WAAW,iBAAiB,SAAS,SAAA;;;;;AClBzC,SAAS,sBACP,SACkE;AAClE,MAAI,CAAC,QAAS,QAAO;AACrB,SAAO;IACL,QAAQ,QAAQ;IAChB,iBAAiB,QAAQ;IACzB,eAAe,QAAQ,eACnB;MACE,kBAAkB,QAAQ,aAAa;MACvC,aAAa,QAAQ,aAAa;QAEpC;;;AAgHR,SAAgB,WAAW,SAAwC;AACjE,SAAO;IACL,MAAM;IACN,kBAAkB,QAAQ;IAC1B,iBAAiB,QAAQ;IACzB,SAAS,QAAQ;IACjB,iBAAiB,sBAAsB,QAAQ,cAAA;;;;;AC3InD,SAAS,sBACP,MACyC;AACzC,MAAI,CAAC,KAAM,QAAO;AAClB,SAAO;IACL,WAAW,KAAK;IAChB,SAAS,KAAK;;;AAqGlB,SAAgB,gBAAgB,SAA8C;AAC5E,SAAO;IACL,MAAM;IACN,QAAQ,SAAS;IACjB,YAAY,SAAS;IACrB,gBAAgB,SAAS;IACzB,kBAAkB,sBAAsB,SAAS,cAAA;IACjD,OAAO,SAAS;IAChB,YAAY,SAAS;IACrB,oBAAoB,SAAS;IAC7B,eAAe,SAAS;IACxB,gBAAgB,SAAS;IACzB,SAAS,SAAS;IAClB,MAAM,SAAS;;;;;AC9LnB,IAAM,oCAAoCC,kBAAE,OAAO,EACjD,MAAMA,kBAAE,QAAQ,YAAA,EAAa,CAC9B;AAED,IAAM,+BAA+BA,kBAAE,OAAO;EAC5C,MAAMA,kBAAE,QAAQ,OAAA;EAChB,GAAGA,kBAAE,OAAA;EACL,GAAGA,kBAAE,OAAA;EACL,QAAQA,kBAAE,KAAK;IAAC;IAAQ;IAAS;IAAS;IAAQ;GAAU,EAAE,QAAQ,MAAA;CACvE;AAED,IAAM,qCAAqCA,kBAAE,OAAO;EAClD,MAAMA,kBAAE,QAAQ,cAAA;EAChB,GAAGA,kBAAE,OAAA;EACL,GAAGA,kBAAE,OAAA;EACL,QAAQA,kBAAE,KAAK;IAAC;IAAQ;IAAS;IAAS;IAAQ;GAAU,EAAE,QAAQ,MAAA;CACvE;AAED,IAAM,8BAA8BA,kBAAE,OAAO;EAC3C,MAAMA,kBAAE,QAAQ,MAAA;EAChB,MAAMA,kBAAE,MAAMA,kBAAE,OAAO;IAAE,GAAGA,kBAAE,OAAA;IAAU,GAAGA,kBAAE,OAAA;GAAU,CAAC;CACzD;AAED,IAAM,kCAAkCA,kBAAE,OAAO;EAC/C,MAAMA,kBAAE,QAAQ,UAAA;EAChB,MAAMA,kBAAE,MAAMA,kBAAE,OAAA,CAAQ;CACzB;AAED,IAAM,8BAA8BA,kBAAE,OAAO;EAC3C,MAAMA,kBAAE,QAAQ,MAAA;EAChB,GAAGA,kBAAE,OAAA;EACL,GAAGA,kBAAE,OAAA;CACN;AAED,IAAM,gCAAgCA,kBAAE,OAAO;EAC7C,MAAMA,kBAAE,QAAQ,QAAA;EAChB,GAAGA,kBAAE,OAAA;EACL,GAAGA,kBAAE,OAAA;EACL,UAAUA,kBAAE,OAAA;EACZ,UAAUA,kBAAE,OAAA;CACb;AAED,IAAM,8BAA8BA,kBAAE,OAAO;EAC3C,MAAMA,kBAAE,QAAQ,MAAA;EAChB,MAAMA,kBAAE,OAAA;CACT;AAED,IAAM,8BAA8BA,kBAAE,OAAO;EAC3C,MAAMA,kBAAE,QAAQ,MAAA;EAChB,UAAUA,kBAAE,OAAA,EAAS,SAAA;CACtB;AAGD,IAAM,+BAA+BA,kBAAE,MAAM;EAC3C;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;CACD;AAID,IAAa,0BAA0BA,kBAAE,OAAO,EAC9C,QAAQ,6BAAA,CACT;AAqFD,IAAM,YAAY;AAkHlB,SAAgB,YAAY,SAA6B;AACvD,QAAM,eAAe,KACnB,OACE,OACA,YACG;AAQH,UAAM,qBAJY,QAAQ,OAAO,SAAS,GAAG,EAAA,GACT,YAAY,KAAA,CAC7C,OAAO,GAAG,SAAS,cAAA,GAEuB;AAC7C,QAAI,CAAC,mBACH,OAAM,IAAI,MAAM,gCAAA;AAGlB,UAAM,SAAS,MAAM,QAAQ,QAAQ,MAAM,QAAQ,OAAA;AAKnD,QAAI,OAAO,WAAW,SACpB,QAAO,IAAI,YAAY;MACrB,SAAS;MACT,cAAc;MACd,mBAAmB,EACjB,MAAM,uBAAA;KAET;AAMH,WAAO,IAAI,YAAY;MACrB,GAAG;MACH,cAAc;MACd,mBAAmB;QACjB,MAAM;QACN,GAAG,OAAO;;KAEb;KAEH;IACE,MAAM;IACN,aACE;IACF,QAAQ;GACT;AAGH,eAAa,SAAS;IACpB,GAAI,aAAa,UAAU,CAAA;IAC3B,wBAAwB;MACtB,MAAM;MACN,eAAe,QAAQ;MACvB,gBAAgB,QAAQ;MACxB,aAAa,QAAQ;;;AAQzB,SAAO;;;;ACpXT,IAAa,6BAA6BC,kBAAE,OAAO;EACjD,MAAMA,kBAAE,QAAQ,MAAA;EAChB,SAASA,kBAAE,MAAMA,kBAAE,OAAA,CAAQ;EAC3B,KAAKA,kBAAE,OAAOA,kBAAE,OAAA,GAAUA,kBAAE,OAAA,CAAQ,EAAE,SAAA;EACtC,mBAAmBA,kBAAE,OAAA,EAAS,SAAA;EAC9B,YAAYA,kBAAE,OAAA,EAAS,SAAA;EACvB,MAAMA,kBAAE,OAAA,EAAS,SAAA;CAClB;AAGD,IAAa,yBAAyBA,kBAAE,MAAM,CAAC,0BAAA,CAA2B;AAkC1E,IAAMC,aAAY;AAoHlB,SAAgB,WAAW,SAA4B;AACrD,QAAM,YAAY,KAAK,QAAQ,SAAS;IACtC,MAAMA;IACN,aACE;IACF,QAAQ;GACT;AAED,YAAU,SAAS;IACjB,GAAI,UAAU,UAAU,CAAA;IACxB,wBAAwB,EACtB,MAAM,cAAA;;AAIV,SAAO;;;;AChLT,IAAa,oBAAoBC,kBAAE,OAAO;EACxC,UAAUA,kBAAE,MAAMA,kBAAE,OAAA,CAAQ,EAAE,SAAS,oCAAA;EACvC,YAAYA,kBACT,OAAA,EACA,SAAA,EACA,SAAS,mDAAA;EACZ,mBAAmBA,kBAChB,OAAA,EACA,SAAA,EACA,SACC,mEAAA;CAEL;AAiFD,IAAMC,aAAY;AAmIlB,SAAgB,MAAM,SAAuB;AAE3C,QAAM,iBAAiB,OAAO,WAAyC;AACrE,UAAM,SAAS,MAAM,QAAQ,QAAQ,MAAA;AAErC,WAAO,KAAK,UAAU;MACpB,QAAQ,OAAO;MACf,mBAAmB,OAAO;KAC3B;;AAGH,QAAM,YAAY,KAAK,gBAAgB;IACrC,MAAMA;IACN,aACE;IACF,QAAQ;GACT;AAED,YAAU,SAAS;IACjB,GAAI,UAAU,UAAU,CAAA;IACxB,wBAAwB,EACtB,MAAM,QAAA;;AAIV,SAAO;;;;AC/OT,IAAa,sCAAsCC,kBAAE,OAAO;EAC1D,MAAMA,kBAAE,QAAQ,aAAA;EAChB,MAAMA,kBAAE,OAAA;EACR,MAAMA,kBAAE,OAAA;CACT;AAED,IAAa,sCAAsCA,kBAAE,OAAO;EAC1D,MAAMA,kBAAE,QAAQ,aAAA;EAChB,MAAMA,kBAAE,OAAA;EACR,MAAMA,kBAAE,OAAA;CACT;AAED,IAAa,sCAAsCA,kBAAE,OAAO;EAC1D,MAAMA,kBAAE,QAAQ,aAAA;EAChB,MAAMA,kBAAE,OAAA;CACT;AAGD,IAAa,4BAA4BA,kBAAE,MAAM;EAC/C;EACA;EACA;CACD;AA8CD,IAAMC,aAAY;AA6ElB,SAAgBC,YAAW,SAA4B;AACrD,QAAM,YAAY,KAAK,QAAQ,SAAS;IACtC,MAAMD;IACN,aACE;IACF,QAAQ;GACT;AAED,YAAU,SAAS;IACjB,GAAI,UAAU,UAAU,CAAA;IACxB,wBAAwB,EACtB,MAAM,cAAA;;AAIV,SAAO;;;;AC3FT,IAAa,QAAQ;EACnB;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA,YAAAE;;;;ACzFF,SAAgB,WACd,MACA,QACqB;AACrB,SAAO,IAAI,YAAY;IACrB,GAAG;IACH,aAAa;IACb,UAAU,EACR,YAAY,OAAA;IAEd,MAAM,OAAO,OAAO,YAAYC,YAC9B,IAAI,QAAA,CAAiB,SAAS,WAAW;AACvC,YAAM,cAAc,YAAYA,SAAQ,EACtC,WAAW,YAAY,SAAA,EAAU,CAClC;AAEI,yCAAmC,cACtC,uBAAuB,WAAA,GACvB,YAAY;AACV,YAAI;AACF,kBAAQ,KAAK,OAAO,WAAA,CAAY;iBACzB,GAAG;AACV,iBAAO,CAAA;;;;GAKlB;;;;ACFH,SAAgB,sBAAsB,iBAEpC;AAEA,SAAO,EACL,UAAU,0CAA0C,EAClD,UAHa,gBAAgB,eAAA,EAAgB,CAI9C,EAAC;;",
  "names": ["len", "i", "len2", "addLangChainErrorFields", "crypto", "url", "ReadableStream", "array", "str", "string", "date", "object", "str", "str", "json", "parseResponse", "transform", "Page", "client", "process", "fetch", "promise", "str", "path", "tool", "tool", "inputTool", "tools", "tool", "tools", "content", "name", "_AbstractChatCompletionRunner_getFinalMessage", "_AbstractChatCompletionRunner_getFinalFunctionToolCall", "_AbstractChatCompletionRunner_getFinalFunctionToolCallResult", "_AbstractChatCompletionRunner_calculateTotalUsage", "_AbstractChatCompletionRunner_validateParams", "_AbstractChatCompletionRunner_stringifyFunctionCallResult", "escape", "e", "_ChatCompletionStream_beginRequest", "_ChatCompletionStream_getChoiceEventState", "_ChatCompletionStream_addChunk", "_ChatCompletionStream_emitToolCallDoneEvent", "tool", "_ChatCompletionStream_emitContentDoneEvents", "_ChatCompletionStream_endRequest", "_ChatCompletionStream_getAutoParseableResponseFormat", "_ChatCompletionStream_accumulateChatCompletion", "content", "refusal", "rest", "_a", "index", "chunk", "id", "Sessions", "Sessions", "Messages", "chunk", "assertNever", "_AssistantStream_endRequest", "_AssistantStream_handleMessage", "_AssistantStream_handleRunStep", "_AssistantStream_handleEvent", "_AssistantStream_accumulateRunStep", "_AssistantStream_accumulateMessage", "_AssistantStream_accumulateContent", "_AssistantStream_handleRun", "Threads", "Messages", "Threads", "Completions", "response", "Runs", "Runs", "Files", "file", "Checkpoints", "Checkpoints", "Graders", "Realtime", "hasAutoParseableInput", "parseToolCall", "content", "output", "isAutoParsableTool", "tool", "parseToolCall", "_ResponseStream_beginRequest", "_ResponseStream_addEvent", "event", "_ResponseStream_endRequest", "_ResponseStream_accumulateResponse", "Content", "Content", "Content", "Content", "Files", "file", "Files", "_Webhooks_getRequiredHeader", "Completions", "Files", "Graders", "Realtime", "_a", "path", "url", "opts", "retryMessage", "err", "Page", "addLangChainErrorFields", "iife", "config", "tool", "tool", "tool", "formatToOpenAITool", "external_exports", "ZodFirstPartyTypeKind", "date", "describe", "encode", "meta", "_null", "parse", "_undefined", "_void", "date", "describe", "meta", "_null", "_undefined", "_void", "issues", "parse", "encode", "def", "meta", "parse", "encode", "check", "json", "_undefined", "json", "_null", "json", "_void", "json", "date", "json", "output", "json", "json", "describe", "meta", "_null", "map", "ZodFirstPartyTypeKind", "path", "zodSchema", "objectSchema", "bigint", "boolean", "date", "number", "string", "string", "number", "boolean", "bigint", "date", "isEmptyObj", "check", "check", "check", "object", "types", "x", "base", "check", "isEmptyObj", "definitions", "schema", "path", "resolveRef", "makeParseableResponseFormat", "parseV4", "blocks", "hex", "InMemoryCache", "map", "str", "base64", "tool", "cache", "config", "config", "input", "iife", "BaseChatModel", "iife", "cache", "result", "config", "tools", "i", "config", "config", "input", "options", "nullable", "type", "description", "isNullable", "sax", "Stream", "s", "parseToolCall", "parseToolCall", "OpenAIClient", "tool", "isOpenAIFunctionTool", "tools", "config", "parseToolCall", "format", "blocks", "tool", "AzureOpenAIClient", "json", "parseToolCall", "iife", "messages", "tools", "tool", "isOpenAIFunctionTool", "ChatOpenAI", "config", "config", "BaseLLM", "cache", "result", "OpenAI", "choices", "OpenAIClient", "AzureOpenAI", "OpenAI", "AzureOpenAIClient", "json", "Embeddings", "Embeddings", "OpenAIClient", "AzureOpenAIClient", "config", "z4", "result", "OpenAIClient", "url", "external_exports", "external_exports", "TOOL_NAME", "external_exports", "TOOL_NAME", "external_exports", "TOOL_NAME", "applyPatch", "applyPatch", "config"]
}
